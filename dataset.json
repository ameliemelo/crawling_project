[
  {
    "id": "00411460f7c92d2124a67ea0f4cb5f85",
    "title": "Topology Constraints in Graphical Models",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/00411460f7c92d2124a67ea0f4cb5f85-Paper.pdf",
    "abstract": "Graphical models are a very useful tool to describe and understand natural phenomena, from gene expression to climate change and social interactions. The topological structure of these graphs/networks is a fundamental part of the analysis, and in many cases the main goal of the study. However, little work has been done on incorporating prior topological knowledge onto the estimation of the underlying graphical models from sample data. In this work we propose extensions to the basic joint regression model for network estimation, which explicitly incorporate graph-topological constraints into the corresponding optimization approach. The first proposed extension includes an eigenvector centrality constraint, thereby promoting this important prior topological property. The second developed extension promotes the formation of certain motifs, triangle-shaped ones in particular, which are known to exist for example in genetic regulatory networks. The presentation of the underlying formulations, which serve as examples of the introduction of topological constraints in network estimation, is complemented with examples in diverse datasets demonstrating the importance of incorporating such critical prior knowledge.",
    "authors": [
      "Fiori, Marcelo",
      "Mus\u00e9, Pablo",
      "Sapiro, Guillermo"
    ]
  },
  {
    "id": "01386bd6d8e091c2ab4c7c7de644d37b",
    "title": "Clustering Aggregation as Maximum-Weight Independent Set",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/01386bd6d8e091c2ab4c7c7de644d37b-Paper.pdf",
    "abstract": "We formulate clustering aggregation as a special instance of Maximum-Weight Independent Set (MWIS) problem. For a given dataset, an attributed graph is constructed from the union of the input clusterings generated by different underlying clustering algorithms with different parameters. The vertices, which represent the distinct clusters, are weighted by an internal index measuring both cohesion and separation. The edges connect the vertices whose corresponding clusters overlap. Intuitively, an optimal aggregated clustering can be obtained by selecting an optimal subset of non-overlapping clusters partitioning the dataset together. We formalize this intuition as the MWIS problem on the attributed graph, i.e., finding the heaviest subset of mutually non-adjacent vertices.  This MWIS problem exhibits a special structure. Since the clusters of each input clustering form a partition of the dataset, the vertices corresponding to each clustering form a maximal independent set (MIS) in the attributed graph. We propose a variant of simulated annealing method that takes advantage of this special structure. Our algorithm starts from each MIS, which is close to a distinct local optimum of the MWIS problem, and utilizes a local search heuristic to explore its neighborhood in order to find the MWIS. Extensive experiments on many challenging datasets show that: 1. our approach to clustering aggregation automatically decides the optimal number of clusters; 2. it does not require any parameter tuning for the underlying clustering algorithms; 3. it can combine the advantages of different underlying clustering algorithms to achieve superior performance; 4. it is robust against moderate or even bad input clusterings.",
    "authors": [
      "Li, Nan",
      "Latecki, Longin"
    ]
  },
  {
    "id": "018b59ce1fd616d874afad0f44ba338d",
    "title": "FastEx: Hash Clustering with Exponential Families",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/018b59ce1fd616d874afad0f44ba338d-Paper.pdf",
    "abstract": "Clustering is a key component in data analysis toolbox. Despite its   importance, scalable algorithms often eschew rich statistical models   in favor of simpler descriptions such as $k$-means clustering. In   this paper we present a sampler, capable of estimating   mixtures of exponential families. At its heart lies a novel proposal distribution using random   projections to achieve high throughput in generating proposals, which is crucial   for clustering models with large numbers of clusters.",
    "authors": [
      "Ahmed, Amr",
      "Ravi, Sujith",
      "Smola, Alex",
      "Narayanamurthy, Shravan"
    ]
  },
  {
    "id": "03afdbd66e7929b125f8597834fa83a4",
    "title": "The Bethe Partition Function of Log-supermodular Graphical Models",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/03afdbd66e7929b125f8597834fa83a4-Paper.pdf",
    "abstract": "Sudderth, Wainwright, and Willsky conjectured that the Bethe approximation corresponding to any fixed point of the belief propagation algorithm over an attractive, pairwise binary graphical model provides a lower bound on the true partition function. In this work, we resolve this conjecture in the affirmative by demonstrating that, for any graphical model with binary variables whose potential functions (not necessarily pairwise) are all log-supermodular, the Bethe partition function always lower bounds the true partition function.  The proof of this result follows from a new variant of the \u201cfour functions\u201d theorem that may be of independent interest.",
    "authors": [
      "Ruozzi, Nicholas"
    ]
  },
  {
    "id": "045117b0e0a11a242b9765e79cbf113f",
    "title": "Selective Labeling via Error Bound Minimization",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/045117b0e0a11a242b9765e79cbf113f-Paper.pdf",
    "abstract": "In many practical machine learning problems, the acquisition of labeled data is often expensive and/or time consuming. This motivates us to study a problem as follows: given a label budget, how to select data points to label such that the learning performance is optimized. We propose a selective labeling method by analyzing the generalization error of Laplacian regularized Least Squares (LapRLS). In particular, we derive a deterministic generalization error bound for LapRLS trained on subsampled data, and propose to select a subset of data points to label by minimizing this upper bound. Since the minimization is a combinational problem, we relax it into continuous domain and solve it by projected gradient descent. Experiments on benchmark datasets show that the proposed method outperforms the state-of-the-art methods.",
    "authors": [
      "Gu, Quanquan",
      "Zhang, Tong",
      "Han, Jiawei",
      "Ding, Chris"
    ]
  },
  {
    "id": "05311655a15b75fab86956663e1819cd",
    "title": "Practical Bayesian Optimization of Machine Learning Algorithms",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/05311655a15b75fab86956663e1819cd-Paper.pdf",
    "abstract": "The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a \u201cblack art\u201d requiring expert experience, rules of thumb, or sometimes brute-force search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm\u2019s generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expert-level performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including Latent Dirichlet Allocation, Structured SVMs and convolutional neural networks.",
    "authors": [
      "Snoek, Jasper",
      "Larochelle, Hugo",
      "Adams, Ryan P."
    ]
  },
  {
    "id": "062ddb6c727310e76b6200b7c71f63b5",
    "title": "Optimal Neural Tuning Curves for Arbitrary Stimulus Distributions: Discrimax, Infomax and Minimum $L_p$ Loss",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/062ddb6c727310e76b6200b7c71f63b5-Paper.pdf",
    "abstract": "In this work we study how the stimulus distribution influences the optimal coding of an individual neuron. Closed-form solutions to the optimal sigmoidal tuning curve are provided for a neuron obeying Poisson statistics under a given stimulus distribution. We consider a variety of optimality criteria, including maximizing discriminability, maximizing mutual information and minimizing estimation error under a general $L_p$ norm.  We generalize the Cramer-Rao lower bound and show how the $L_p$ loss can be written as a functional of the Fisher Information in the asymptotic limit, by proving the moment convergence of certain functions of Poisson random variables.  In this manner, we show how the optimal tuning curve depends upon the loss function, and the equivalence of maximizing mutual information with minimizing $L_p$ loss in the limit as $p$ goes to zero.",
    "authors": [
      "Wang, Zhuo",
      "Stocker, Alan A.",
      "Lee, Daniel D."
    ]
  },
  {
    "id": "0663a4ddceacb40b095eda264a85f15c",
    "title": "Probabilistic Event Cascades for Alzheimer's disease",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/0663a4ddceacb40b095eda264a85f15c-Paper.pdf",
    "abstract": "Accurate and detailed models of the progression of neurodegenerative diseases such as  Alzheimer's (AD) are crucially important for reliable early diagnosis and the determination and deployment of effective treatments. In this paper, we introduce the ALPACA (Alzheimer's disease Probabilistic Cascades) model, a generative model linking latent Alzheimer's progression dynamics to observable biomarker data. In contrast with previous works which model disease progression as a fixed ordering of events, we explicitly model the variability over such orderings among patients which is more realistic, particularly for highly detailed disease progression models. We describe efficient learning algorithms for ALPACA and discuss promising experimental results on a real cohort of Alzheimer's patients from the  Alzheimer's Disease Neuroimaging Initiative.",
    "authors": [
      "Huang, Jonathan",
      "Alexander, Daniel"
    ]
  },
  {
    "id": "072b030ba126b2f4b2374f342be9ed44",
    "title": "Super-Bit Locality-Sensitive Hashing",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/072b030ba126b2f4b2374f342be9ed44-Paper.pdf",
    "abstract": "Sign-random-projection locality-sensitive hashing (SRP-LSH) is a probabilistic dimension reduction method which provides an unbiased estimate of angular similarity, yet suffers from the large variance of its estimation. In this work, we propose the Super-Bit locality-sensitive hashing (SBLSH). It is easy to implement, which orthogonalizes the random projection vectors in batches, and it is theoretically guaranteed that SBLSH also provides an unbiased estimate of angular similarity, yet with a smaller variance when the angle to estimate is within $(0,\\pi/2]$. The extensive experiments on real data well validate that given the same length of binary code, SBLSH may achieve significant mean squared error reduction in estimating pairwise angular similarity. Moreover, SBLSH shows the superiority over SRP-LSH in approximate nearest neighbor (ANN) retrieval experiments.",
    "authors": [
      "Ji, Jianqiu",
      "Li, Jianmin",
      "Yan, Shuicheng",
      "Zhang, Bo",
      "Tian, Qi"
    ]
  },
  {
    "id": "0768281a05da9f27df178b5c39a51263",
    "title": "Bayesian nonparametric models for bipartite graphs",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/0768281a05da9f27df178b5c39a51263-Paper.pdf",
    "abstract": "We develop a novel Bayesian nonparametric model for random bipartite graphs. The model is based on the theory of completely random measures and is able to handle a potentially infinite number of nodes. We show that the model has appealing properties and in particular it may exhibit a power-law behavior. We derive a posterior characterization, an Indian Buffet-like generative process for network growth, and a simple and efficient Gibbs sampler for posterior simulation. Our model is shown to be well fitted to several real-world social networks.",
    "authors": [
      "Caron, Francois"
    ]
  },
  {
    "id": "0829424ffa0d3a2547b6c9622c77de03",
    "title": "Learning Label Trees for Probabilistic Modelling of Implicit Feedback",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/0829424ffa0d3a2547b6c9622c77de03-Paper.pdf",
    "abstract": "User preferences for items can be inferred from either explicit feedback, such as item ratings, or implicit feedback, such as rental histories. Research in collaborative filtering has concentrated on explicit feedback, resulting in the development of accurate and scalable models. However, since explicit feedback is often difficult to collect it is important to develop effective models that take advantage of the more widely available implicit feedback. We introduce a probabilistic approach to collaborative filtering with implicit feedback based on modelling the user's item selection process. In the interests of scalability, we restrict our attention to tree-structured distributions over items and develop a principled and efficient algorithm for learning item trees from data. We also identify a problem with a widely used protocol for evaluating implicit feedback models and propose a way of addressing it using a small quantity of explicit feedback data.",
    "authors": [
      "Mnih, Andriy",
      "Teh, Yee"
    ]
  },
  {
    "id": "08c5433a60135c32e34f46a71175850c",
    "title": "Factoring nonnegative matrices with linear programs",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/08c5433a60135c32e34f46a71175850c-Paper.pdf",
    "abstract": "This paper describes a new approach for computing nonnegative matrix factorizations (NMFs) with linear programming. The key idea is a data-driven model for the factorization, in which the most salient features in the data are used to express the remaining features.  More precisely, given a data matrix X, the algorithm identifies a matrix C that satisfies X = CX and some linear constraints.  The matrix C selects features, which are then used to compute a low-rank NMF of X.  A theoretical analysis demonstrates that this approach has the same type of guarantees as the recent NMF algorithm of Arora et al.~(2012).  In contrast with this earlier work, the proposed method has (1) better noise tolerance, (2) extends to more general noise models, and (3) leads to efficient, scalable algorithms.  Experiments with synthetic and real datasets provide evidence that the new approach is also superior in practice.  An optimized C++ implementation of the new algorithm can factor a multi-Gigabyte matrix in a matter of minutes.",
    "authors": [
      "Recht, Ben",
      "Re, Christopher",
      "Tropp, Joel",
      "Bittorf, Victor"
    ]
  },
  {
    "id": "08d98638c6fcd194a4b1e6992063e944",
    "title": "Privacy Aware Learning",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf",
    "abstract": "We study statistical risk minimization problems under a version of privacy in which the data is kept confidential even from the learner.  In this local privacy framework, we show sharp upper and lower bounds on the convergence rates of statistical estimation procedures. As a consequence, we exhibit a precise tradeoff between the amount of privacy the data preserves and the utility, measured by convergence rate, of any statistical estimator.",
    "authors": [
      "Wainwright, Martin J.",
      "Jordan, Michael",
      "Duchi, John C."
    ]
  },
  {
    "id": "091d584fced301b442654dd8c23b3fc9",
    "title": "Truncation-free Online Variational Inference for Bayesian Nonparametric Models",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/091d584fced301b442654dd8c23b3fc9-Paper.pdf",
    "abstract": "We present a truncation-free online variational inference algorithm for Bayesian nonparametric models. Unlike traditional (online) variational inference algorithms that require truncations for the model or the variational distribution, our method adapts model complexity on the fly. Our experiments for Dirichlet process mixture models and hierarchical Dirichlet process topic models on two large-scale data sets show better performance than previous online variational inference algorithms.",
    "authors": [
      "Wang, Chong",
      "Blei, David"
    ]
  },
  {
    "id": "09c6c3783b4a70054da74f2538ed47c6",
    "title": "Provable ICA with Unknown Gaussian Noise, with Implications for Gaussian Mixtures and Autoencoders",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/09c6c3783b4a70054da74f2538ed47c6-Paper.pdf",
    "abstract": "We present a new algorithm for Independent Component Analysis (ICA) which has provable performance guarantees. In particular, suppose we are given samples of the form $y = Ax + \\eta$ where $A$ is an unknown $n \\times n$ matrix and $x$ is chosen uniformly at random from $\\{+1, -1\\}^n$, $\\eta$ is an $n$-dimensional Gaussian random variable with unknown covariance $\\Sigma$: We give an algorithm that provable recovers $A$ and $\\Sigma$ up to an additive $\\epsilon$ whose running time and sample complexity are polynomial in $n$ and $1 / \\epsilon$. To accomplish this, we introduce a novel ``quasi-whitening'' step that may be useful in other contexts in which the covariance of Gaussian noise is not known in advance. We also give a general framework for finding all local optima of a function (given an oracle for approximately finding just one) and this is a crucial step in our algorithm, one that has been overlooked in previous attempts, and allows us to control the accumulation of error when we find the columns of $A$ one by one via local search.",
    "authors": [
      "Arora, Sanjeev",
      "Ge, Rong",
      "Moitra, Ankur",
      "Sachdeva, Sushant"
    ]
  },
  {
    "id": "0a09c8844ba8f0936c20bd791130d6b6",
    "title": "Learning Image Descriptors with the Boosting-Trick",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/0a09c8844ba8f0936c20bd791130d6b6-Paper.pdf",
    "abstract": "In this paper we  apply   boosting  to  learn   complex  non-linear  local   visual  feature representations, drawing  inspiration from its successful  application to visual object detection. The main  goal of  local feature descriptors  is to distinctively  represent a salient image  region while remaining invariant to  viewpoint and illumination changes. This representation can  be improved using machine learning, however, past approaches  have been mostly limited to learning  linear feature mappings in either the original input or a  kernelized input feature space.  While kernelized  methods have proven somewhat effective for learning non-linear local  feature descriptors,  they rely heavily  on the choice  of an appropriate kernel  function whose selection is often  difficult and non-intuitive. We propose  to use the boosting-trick  to  obtain a  non-linear  mapping  of  the input  to  a high-dimensional feature space. The non-linear feature mapping  obtained with the  boosting-trick is  highly intuitive. We employ gradient-based weak learners resulting in a learned descriptor that closely resembles the well-known SIFT. As demonstrated in our experiments, the resulting descriptor   can  be  learned   directly  from   intensity  patches  achieving state-of-the-art performance.",
    "authors": [
      "Trzcinski, Tomasz",
      "Christoudias, Mario",
      "Lepetit, Vincent",
      "Fua, Pascal"
    ]
  },
  {
    "id": "0a1bf96b7165e962e90cb14648c9462d",
    "title": "A latent factor model for highly multi-relational data",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/0a1bf96b7165e962e90cb14648c9462d-Paper.pdf",
    "abstract": "Many data such as social networks, movie preferences or knowledge bases are multi-relational, in that they describe multiple relationships between entities. While there is a large body of work focused on modeling these data, few considered modeling these multiple types of relationships jointly. Further, existing approaches tend to breakdown when the number of these types grows. In this paper, we propose a method for modeling large multi-relational datasets, with possibly thousands of relations. Our model is based on a bilinear structure, which captures the various orders of interaction of the data, but also shares sparse latent factors across different relations. We illustrate the performance of our approach on standard tensor-factorization datasets where we attain, or outperform, state-of-the-art results. Finally, a NLP application demonstrates our scalability and the ability of our model to learn efficient, and semantically meaningful verb representations.",
    "authors": [
      "Jenatton, Rodolphe",
      "Roux, Nicolas",
      "Bordes, Antoine",
      "Obozinski, Guillaume R."
    ]
  },
  {
    "id": "0b8aff0438617c055eb55f0ba5d226fa",
    "title": "Bayesian estimation of discrete entropy with mixtures of stick-breaking priors",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/0b8aff0438617c055eb55f0ba5d226fa-Paper.pdf",
    "abstract": "We consider the problem of estimating Shannon's entropy H in the   under-sampled regime, where the number of possible symbols may be   unknown or countably infinite.  Pitman-Yor processes (a   generalization of Dirichlet processes) provide tractable prior   distributions over the space of countably infinite discrete   distributions, and have found major applications in Bayesian   non-parametric statistics and machine learning. Here we show that   they also provide natural priors for Bayesian entropy estimation,   due to the remarkable fact that the moments of the induced posterior   distribution over H can be computed analytically. We derive   formulas for the posterior mean (Bayes' least squares estimate) and   variance under such priors.  Moreover, we show that a fixed   Dirichlet or Pitman-Yor process prior implies a narrow prior on H,   meaning the prior strongly determines the entropy estimate in the   under-sampled regime. We derive a family of continuous mixing   measures such that the resulting mixture of Pitman-Yor processes   produces an approximately flat (improper) prior over H.  We   explore the theoretical properties of the resulting estimator, and   show that it performs well on data sampled from both exponential and   power-law tailed distributions.",
    "authors": [
      "Archer, Evan",
      "Park, Il Memming",
      "Pillow, Jonathan"
    ]
  },
  {
    "id": "0deb1c54814305ca9ad266f53bc82511",
    "title": "Timely Object Recognition",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/0deb1c54814305ca9ad266f53bc82511-Paper.pdf",
    "abstract": "In a large visual multi-class detection framework, the timeliness of results can be crucial. Our method for timely multi-class detection aims to give the best possible performance at any single point after a start time; it is terminated at a deadline time. Toward this goal, we formulate a dynamic, closed-loop policy that infers the contents of the image in order to decide which detector to deploy next. In contrast to previous work, our method significantly diverges from the predominant greedy strategies, and is able to learn to take actions with deferred values. We evaluate our method with a novel timeliness measure, computed as the area under an Average Precision vs. Time curve. Experiments are conducted on the eminent PASCAL VOC object detection dataset. If execution is stopped when only half the detectors have been run, our method obtains $66\\%$ better AP than a random ordering, and $14\\%$ better performance than an intelligent baseline. On the timeliness measure, our method obtains at least $11\\%$ better performance. Our code, to be made available upon publication, is easily extensible as it treats detectors and classifiers as black boxes and learns from execution traces using reinforcement learning.",
    "authors": [
      "Karayev, Sergey",
      "Baumgartner, Tobias",
      "Fritz, Mario",
      "Darrell, Trevor"
    ]
  },
  {
    "id": "0e01938fc48a2cfb5f2217fbfb00722d",
    "title": "Efficient high dimensional maximum entropy modeling via symmetric partition functions",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/0e01938fc48a2cfb5f2217fbfb00722d-Paper.pdf",
    "abstract": "The application of the maximum entropy principle to sequence   modeling has been popularized by methods such as Conditional Random   Fields (CRFs).  However, these approaches are generally limited to   modeling paths in discrete spaces of low dimensionality.  We   consider the problem of modeling distributions over paths in   continuous spaces of high dimensionality---a problem for which   inference is generally intractable.  Our main contribution is to   show that maximum entropy modeling of high-dimensional, continuous   paths is tractable as long as the constrained features    possess a certain kind of low dimensional structure.   In this case, we show that the associated {\\em partition function} is   symmetric and that this symmetry can be exploited to compute the   partition function efficiently in a compressed form.  Empirical   results are given showing an application of our method to maximum   entropy modeling of high dimensional human motion capture data.",
    "authors": [
      "Vernaza, Paul",
      "Bagnell, Drew"
    ]
  },
  {
    "id": "0e9fa1f3e9e66792401a6972d477dcc3",
    "title": "Topic-Partitioned Multinetwork Embeddings",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/0e9fa1f3e9e66792401a6972d477dcc3-Paper.pdf",
    "abstract": "We introduce a joint model of network content and context designed for exploratory analysis of email networks via visualization of topic-specific communication patterns. Our model is an admixture model for text and network attributes which uses multinomial distributions over words as mixture components for explaining text and latent Euclidean positions of actors as mixture components for explaining network attributes.  We validate the appropriateness of our model by achieving state-of-the-art performance on a link prediction task and by achieving semantic coherence equivalent to that of latent Dirichlet allocation. We demonstrate the capability of our model for descriptive, explanatory, and exploratory analysis by investigating the inferred topic-specific communication patterns of a new government email dataset, the New Hanover County email corpus.",
    "authors": [
      "Krafft, Peter",
      "Moore, Juston",
      "Desmarais, Bruce",
      "Wallach, Hanna"
    ]
  },
  {
    "id": "0f2c9a93eea6f38fabb3acb1c31488c6",
    "title": "Recovery of Sparse Probability Measures via Convex Programming",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/0f2c9a93eea6f38fabb3acb1c31488c6-Paper.pdf",
    "abstract": "We consider the problem of cardinality penalized optimization of a convex function over the probability simplex with additional convex constraints. It's well-known that the classical L1 regularizer fails to promote sparsity on the probability simplex since L1 norm on the probability simplex is trivially constant. We propose a direct relaxation of the minimum cardinality problem and show that it can be efficiently solved using convex programming. As a first application we consider recovering a sparse probability measure given moment constraints, in which our formulation becomes linear programming, hence can be solved very efficiently. A sufficient condition for exact recovery of the minimum cardinality solution is derived for arbitrary affine constraints. We then develop a penalized version for the noisy setting which can be solved using second order cone programs. The proposed method outperforms known heuristics based on L1 norm. As a second application we consider convex clustering using a sparse Gaussian mixture and compare our results with the well known soft k-means algorithm.",
    "authors": [
      "Pilanci, Mert",
      "Ghaoui, Laurent",
      "Chandrasekaran, Venkat"
    ]
  },
  {
    "id": "0f304eddb4ad6007a3093fd6d963a1d2",
    "title": "Proximal Newton-type methods for convex optimization",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/0f304eddb4ad6007a3093fd6d963a1d2-Paper.pdf",
    "abstract": "We seek to solve convex optimization problems in composite form:",
    "authors": [
      "Lee, Jason D.",
      "Sun, Yuekai",
      "Saunders, Michael"
    ]
  },
  {
    "id": "0f96613235062963ccde717b18f97592",
    "title": "Learning visual motion in recurrent neural networks",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/0f96613235062963ccde717b18f97592-Paper.pdf",
    "abstract": "We present a dynamic nonlinear generative model for visual motion based on a latent representation of binary-gated Gaussian variables. Trained on sequences of images, the model learns to represent different movement directions in different variables. We use an online approximate-inference scheme that can be mapped to the dynamics of networks of neurons. Probed with drifting grating stimuli and moving bars of light, neurons in the model show patterns of responses analogous to those of direction-selective simple cells in primary visual cortex. Most model neurons also show speed tuning and respond equally well to a range of motion directions and speeds aligned to the constraint line of their respective preferred speed. We show how these computations are enabled by a specific pattern of recurrent connections learned by the model.",
    "authors": [
      "Pachitariu, Marius",
      "Sahani, Maneesh"
    ]
  },
  {
    "id": "0ff8033cf9437c213ee13937b1c4c455",
    "title": "Graphical Models via Generalized Linear Models",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/0ff8033cf9437c213ee13937b1c4c455-Paper.pdf",
    "abstract": "Undirected graphical models, or Markov networks, such as Gaussian graphical models and Ising models enjoy popularity in a variety of applications.  In many settings, however, data may not follow a Gaussian or binomial distribution assumed by these models. We introduce a new class of graphical models based on generalized linear models (GLM) by assuming that node-wise conditional distributions arise from exponential families.  Our models allow one to estimate networks for a wide class of exponential distributions, such as the Poisson, negative binomial, and exponential, by fitting penalized GLMs to select the neighborhood for each node. A major contribution of this paper is the rigorous statistical analysis showing that with high probability, the neighborhood of our graphical models can be recovered exactly. We provide examples of high-throughput genomic networks learned via our GLM graphical models for multinomial and Poisson distributed data.",
    "authors": [
      "Yang, Eunho",
      "Allen, Genevera",
      "Liu, Zhandong",
      "Ravikumar, Pradeep"
    ]
  },
  {
    "id": "1068c6e4c8051cfd4e9ea8072e3189e2",
    "title": "Searching for objects driven by context",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/1068c6e4c8051cfd4e9ea8072e3189e2-Paper.pdf",
    "abstract": "The dominant visual search paradigm for object class detection is sliding windows. Although simple and effective, it is also wasteful, unnatural and rigidly hardwired. We propose strategies to search for objects which intelligently explore the space of windows by making sequential observations at locations decided based on previous observations. Our strategies adapt to the class being searched and to the content of a particular test image. Their driving force is exploiting context as the statistical relation between the appearance of a window and its location relative to the object, as observed in the training set. In addition to being more elegant than sliding windows, we demonstrate experimentally on the PASCAL VOC 2010 dataset that our strategies evaluate two orders of magnitude fewer windows while at the same time achieving higher detection accuracy.",
    "authors": [
      "Alexe, Bogdan",
      "Heess, Nicolas",
      "Teh, Yee",
      "Ferrari, Vittorio"
    ]
  },
  {
    "id": "10a7cdd970fe135cf4f7bb55c0e3b59f",
    "title": "Learning Mixtures of Tree Graphical Models",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/10a7cdd970fe135cf4f7bb55c0e3b59f-Paper.pdf",
    "abstract": "We consider  unsupervised estimation of mixtures of discrete graphical models, where the class variable   is hidden and each mixture component  can have a potentially different Markov graph structure and parameters over the observed variables. We propose a novel method for estimating the mixture components with  provable guarantees.   Our output is   a tree-mixture model which serves as a good approximation to the underlying graphical model mixture. The   sample and computational requirements for our method scale as $\\poly(p,  r)$,   for an $r$-component mixture of $p$-variate graphical models, for a wide class of models which includes tree mixtures and mixtures over bounded degree graphs.",
    "authors": [
      "Anandkumar, Anima",
      "Hsu, Daniel J.",
      "Huang, Furong",
      "Kakade, Sham M."
    ]
  },
  {
    "id": "1141938ba2c2b13f5505d7c424ebae5f",
    "title": "Convex Multi-view Subspace Learning",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/1141938ba2c2b13f5505d7c424ebae5f-Paper.pdf",
    "abstract": "Subspace learning seeks a low dimensional representation of data that enables accurate reconstruction.  However, in many applications, data is obtained from multiple sources rather than a single source (e.g. an object might be viewed by cameras at different angles, or a document might consist of text and images).  The conditional independence of separate sources imposes constraints on their shared latent representation, which, if respected, can improve the quality of the learned low dimensional representation.  In this paper, we present a convex formulation of multi-view subspace learning that enforces conditional independence while reducing dimensionality.  For this formulation, we develop an efficient algorithm that recovers an optimal data reconstruction by exploiting an implicit convex regularizer, then recovers the corresponding latent representation and reconstruction model, jointly and optimally.  Experiments illustrate that the proposed method produces high quality results.",
    "authors": [
      "White, Martha",
      "Zhang, Xinhua",
      "Schuurmans, Dale",
      "Yu, Yao-liang"
    ]
  },
  {
    "id": "115f89503138416a242f40fb7d7f338e",
    "title": "Learning Invariant Representations of Molecules for Atomization Energy Prediction",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/115f89503138416a242f40fb7d7f338e-Paper.pdf",
    "abstract": "The accurate prediction of molecular energetics in chemical compound space is a crucial ingredient for rational compound design.  The inherently graph-like, non-vectorial nature of molecular data gives rise to a unique and difficult machine learning problem. In this paper, we adopt a learning-from-scratch approach where quantum-mechanical molecular energies are predicted directly from the raw molecular geometry. The study suggests a benefit from setting flexible priors and enforcing invariance stochastically rather than structurally. Our results improve the state-of-the-art by a factor of almost three, bringing statistical methods one step closer to the holy grail of ''chemical accuracy''.",
    "authors": [
      "Montavon, Gr\u00e9goire",
      "Hansen, Katja",
      "Fazli, Siamac",
      "Rupp, Matthias",
      "Biegler, Franziska",
      "Ziehe, Andreas",
      "Tkatchenko, Alexandre",
      "Lilienfeld, Anatole",
      "M\u00fcller, Klaus-Robert"
    ]
  },
  {
    "id": "11b921ef080f7736089c757404650e40",
    "title": "On Multilabel Classification and Ranking with Partial Feedback",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/11b921ef080f7736089c757404650e40-Paper.pdf",
    "abstract": "We present a novel multilabel/ranking algorithm working in partial information settings. The algorithm is based on 2nd-order descent methods, and relies on upper-confidence bounds to trade-off exploration and exploitation.  We analyze this algorithm in a partial adversarial setting, where covariates can be adversarial, but multilabel probabilities are ruled by (generalized) linear models. We show $O(T^{1/2}\\log T)$ regret bounds, which improve in several ways on the existing results. We test the effectiveness of our upper-confidence scheme by contrasting against full-information baselines on real-world multilabel datasets, often obtaining comparable performance.",
    "authors": [
      "Gentile, Claudio",
      "Orabona, Francesco"
    ]
  },
  {
    "id": "12780ea688a71dabc284b064add459a4",
    "title": "A dynamic excitatory-inhibitory network in a VLSI chip for spiking information reregistrations",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/12780ea688a71dabc284b064add459a4-Paper.pdf",
    "abstract": "Abstract Unavailable",
    "authors": [
      "Huo, Juan"
    ]
  },
  {
    "id": "1385974ed5904a438616ff7bdb3f7439",
    "title": "Distributed Non-Stochastic Experts",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/1385974ed5904a438616ff7bdb3f7439-Paper.pdf",
    "abstract": "We consider the online distributed non-stochastic experts problem, where the distributed system consists of one coordinator node that is connected to k sites, and the sites are required to communicate with each other via the coordinator. At each time-step t, one of the k site nodes has to pick an expert from the set {1, . . . , n}, and the same site receives information about payoffs of all experts for that round. The goal of the distributed system is to minimize regret at time horizon T, while simultaneously keeping communication to a minimum. The two extreme solutions to this problem are: (i) Full communication: This essentially simulates the non-distributed setting to obtain the optimal O(\\sqrt{log(n)T}) regret bound at the cost of T communication. (ii) No communication: Each site runs an independent copy \u2013 the regret is O(\\sqrt{log(n)kT}) and the communication is 0. This paper shows the difficulty of simultaneously achieving regret asymptotically better than \\sqrt{kT} and communication better than T. We give a novel algorithm that for an oblivious adversary achieves a non-trivial trade-off: regret O(\\sqrt{k^{5(1+\\epsilon)/6} T}) and communication O(T/k^\\epsilon), for any value of \\epsilon in (0, 1/5). We also consider a variant of the model, where the coordinator picks the expert. In this model, we show that the label-efficient forecaster of Cesa-Bianchi et al. (2005) already gives us strategy that is near optimal in regret vs communication trade-off.",
    "authors": [
      "Kanade, Varun",
      "Liu, Zhenming",
      "Radunovic, Bozidar"
    ]
  },
  {
    "id": "13d7dc096493e1f77fb4ccf3eaf79df1",
    "title": "Deep Learning of Invariant Features via Simulated Fixations in Video",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/13d7dc096493e1f77fb4ccf3eaf79df1-Paper.pdf",
    "abstract": "We apply salient feature detection and tracking in videos to simulate \ufb01xations and smooth pursuit in human vision. With tracked sequences as input, a hierarchical network of modules learns invariant features using a temporal slowness constraint. The network encodes invariance which are increasingly complex with hierarchy. Although learned from videos, our features are spatial instead of spatial-temporal, and well suited for extracting features from still images. We applied our features to four datasets (COIL-100, Caltech 101, STL-10, PubFig), and observe a consistent improvement of 4% to 5% in classi\ufb01cation accuracy. With this approach, we achieve state-of-the-art recognition accuracy 61% on STL-10 dataset.",
    "authors": [
      "Zou, Will",
      "Zhu, Shenghuo",
      "Yu, Kai",
      "Ng, Andrew"
    ]
  },
  {
    "id": "13f9896df61279c928f19721878fac41",
    "title": "Homeostatic plasticity in Bayesian spiking networks as Expectation Maximization with posterior constraints",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/13f9896df61279c928f19721878fac41-Paper.pdf",
    "abstract": "Recent spiking network models of Bayesian inference and unsupervised learning frequently assume either inputs to arrive in a special format or employ complex computations in neuronal activation functions and synaptic plasticity rules. Here we show in a rigorous mathematical treatment how homeostatic processes, which have previously received little attention in this context, can overcome common theoretical limitations and facilitate the neural implementation and performance of existing models. In particular, we show that homeostatic plasticity can be understood as the enforcement of a 'balancing' posterior constraint during probabilistic inference and learning with Expectation Maximization. We link homeostatic dynamics to the theory of variational inference, and show that nontrivial terms, which typically appear during probabilistic inference in a large class of models, drop out. We demonstrate the feasibility of our approach in a spiking Winner-Take-All architecture of Bayesian inference and learning. Finally, we sketch how the mathematical framework can be extended to richer recurrent network architectures. Altogether, our theory provides a novel perspective on the interplay of homeostatic processes and synaptic plasticity in cortical microcircuits, and points to an essential role of homeostasis during inference and learning in spiking networks.",
    "authors": [
      "Habenschuss, Stefan",
      "Bill, Johannes",
      "Nessler, Bernhard"
    ]
  },
  {
    "id": "140f6969d5213fd0ece03148e62e461e",
    "title": "Nonparametric Bayesian Inverse Reinforcement Learning for Multiple Reward Functions",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/140f6969d5213fd0ece03148e62e461e-Paper.pdf",
    "abstract": "We present a nonparametric Bayesian approach to inverse reinforcement learning (IRL) for multiple reward functions. Most previous IRL algorithms assume that the behaviour data is obtained from an agent who is optimizing a single reward function, but this assumption is hard to be met in practice. Our approach is based on integrating the Dirichlet process mixture model into Bayesian IRL. We provide an efficient Metropolis-Hastings sampling algorithm utilizing the gradient of the posterior to estimate the underlying reward functions, and demonstrate that our approach outperforms the previous ones via experiments on a number of problem domains.",
    "authors": [
      "Choi, Jaedeug",
      "Kim, Kee-eung"
    ]
  },
  {
    "id": "14d9e8007c9b41f57891c48e07c23f57",
    "title": "Human memory search as a random walk in a semantic network",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/14d9e8007c9b41f57891c48e07c23f57-Paper.pdf",
    "abstract": "The human mind has a remarkable ability to store a vast amount of information in memory, and an even more remarkable ability to retrieve these experiences when needed. Understanding the representations and algorithms that underlie human memory search could potentially be useful in other information retrieval settings, including internet search. Psychological studies have revealed clear regularities in how people search their memory, with clusters of semantically related items tending to be retrieved together. These findings have recently been taken as evidence that human memory search is similar to animals foraging for food in patchy environments, with people making a rational decision to switch away from a cluster of related information as it becomes depleted. We demonstrate that the results that were taken as evidence for this account also emerge from a random walk on a semantic network, much like the random web surfer model used in internet search engines. This offers a simpler and more unified account of how people search their memory, postulating a single process rather than one process for exploring a cluster and one process for switching between clusters.",
    "authors": [
      "Austerweil, Joseph",
      "Abbott, Joshua T.",
      "Griffiths, Thomas"
    ]
  },
  {
    "id": "1543843a4723ed2ab08e18053ae6dc5b",
    "title": "Multi-criteria Anomaly Detection using Pareto Depth Analysis",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/1543843a4723ed2ab08e18053ae6dc5b-Paper.pdf",
    "abstract": "We consider the problem of identifying patterns in a data set that exhibit anomalous behavior, often referred to as anomaly detection. In most anomaly detection algorithms, the dissimilarity between data samples is calculated by a single criterion, such as Euclidean distance. However, in many cases there may not exist a single dissimilarity measure that captures all possible anomalous patterns. In such a case, multiple criteria can be defined, and one can test for anomalies by scalarizing the multiple criteria by taking some linear combination of them. If the importance of the different criteria are not known in advance, the algorithm may need to be executed multiple times with different choices of weights in the linear combination. In this paper, we introduce a novel non-parametric multi-criteria anomaly detection method using Pareto depth analysis (PDA). PDA uses the concept of Pareto optimality to detect anomalies under multiple criteria without having to run an algorithm multiple times with different choices of weights. The proposed PDA approach scales linearly in the number of criteria and is provably better than linear combinations of the criteria.",
    "authors": [
      "Hsiao, Ko-jen",
      "Xu, Kevin",
      "Calder, Jeff",
      "Hero, Alfred"
    ]
  },
  {
    "id": "15d4e891d784977cacbfcbb00c48f133",
    "title": "A Spectral Algorithm for Latent Dirichlet Allocation",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/15d4e891d784977cacbfcbb00c48f133-Paper.pdf",
    "abstract": "Topic modeling is a generalization of clustering that posits that observations (words in a document) are generated by \\emph{multiple} latent factors (topics), as opposed to just one. This increased representational power comes at the cost of a more challenging unsupervised learning problem of estimating the topic-word distributions when only words are observed, and the topics are hidden.  This work provides a simple and efficient learning procedure that is guaranteed to recover the parameters for a wide class of topic models, including Latent Dirichlet Allocation (LDA). For LDA, the procedure correctly recovers both the topic-word distributions and the parameters of the Dirichlet prior over the topic mixtures, using only trigram statistics (\\emph{i.e.}, third order moments, which may be estimated with documents containing just three words). The method, called Excess Correlation Analysis, is based on a spectral decomposition of low-order moments via two singular value decompositions (SVDs). Moreover, the algorithm is scalable, since the SVDs are carried out only on $k \\times k$ matrices, where $k$ is the number of latent factors (topics) and is typically much smaller than the dimension of the observation (word) space.",
    "authors": [
      "Anandkumar, Anima",
      "Foster, Dean P.",
      "Hsu, Daniel J.",
      "Kakade, Sham M.",
      "Liu, Yi-kai"
    ]
  },
  {
    "id": "168908dd3227b8358eababa07fcaf091",
    "title": "The Perturbed Variation",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/168908dd3227b8358eababa07fcaf091-Paper.pdf",
    "abstract": "We introduce a new discrepancy score between two distributions that gives an indication on their \\emph{similarity}. While much research has been done to determine if two samples come from exactly the same distribution, much less research considered the problem of determining if two finite samples come from similar distributions. The new score gives an intuitive interpretation of similarity;  it optimally perturbs the distributions so that they best fit each other. The score is defined between distributions, and can be efficiently estimated from samples. We provide convergence bounds of the estimated score, and develop hypothesis testing procedures that test if two data sets come from similar distributions. The statistical power of this procedures is presented in simulations. We also compare the score's capacity to detect similarity with that of other known measures on real data.",
    "authors": [
      "Harel, Maayan",
      "Mannor, Shie"
    ]
  },
  {
    "id": "16a5cdae362b8d27a1d8f8c7b78b4330",
    "title": "Discriminatively Trained Sparse Code Gradients for Contour Detection",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/16a5cdae362b8d27a1d8f8c7b78b4330-Paper.pdf",
    "abstract": "Finding contours in natural images is a fundamental problem that serves as the basis of many tasks such as image segmentation and object recognition. At the core of contour detection technologies are a set of hand-designed gradient features, used by most existing approaches including the state-of-the-art Global Pb (gPb) operator.  In this work, we show that contour detection accuracy can be significantly improved by computing Sparse Code Gradients (SCG), which measure contrast using patch representations automatically learned through sparse coding.  We use K-SVD and Orthogonal Matching Pursuit for efficient dictionary learning and encoding, and use multi-scale pooling and power transforms to code oriented local neighborhoods before computing gradients and applying linear SVM. By extracting rich representations from pixels and avoiding collapsing them prematurely, Sparse Code Gradients effectively learn how to measure local contrasts and find contours. We improve the F-measure metric on the BSDS500 benchmark to 0.74 (up from 0.71 of gPb contours).  Moreover, our learning approach can easily adapt to novel sensor data such as Kinect-style RGB-D cameras: Sparse Code Gradients on depth images and surface normals lead to promising contour detection using depth and depth+color, as verified on the NYU Depth Dataset.  Our work combines the concept of oriented gradients with sparse representation and opens up future possibilities for learning contour detection and segmentation.",
    "authors": [
      "Xiaofeng, Ren",
      "Bo, Liefeng"
    ]
  },
  {
    "id": "16c222aa19898e5058938167c8ab6c57",
    "title": "A Bayesian Approach for Policy Learning from Trajectory Preference Queries",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/16c222aa19898e5058938167c8ab6c57-Paper.pdf",
    "abstract": "We consider the problem of learning control policies via trajectory preference queries to an expert. In particular, the learning agent can present an expert with short runs of a pair of policies originating from the same state and the expert then indicates the preferred trajectory. The agent's goal is to elicit a latent target policy from the expert with as few queries as possible. To tackle this problem we propose a novel Bayesian model of the querying process and introduce two methods that exploit this model to actively select expert queries. Experimental results on four benchmark problems indicate that our model can effectively learn policies from trajectory preference queries and that active query selection can be substantially more efficient than random selection.",
    "authors": [
      "Wilson, Aaron",
      "Fern, Alan",
      "Tadepalli, Prasad"
    ]
  },
  {
    "id": "170c944978496731ba71f34c25826a34",
    "title": "Kernel Hyperalignment",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/170c944978496731ba71f34c25826a34-Paper.pdf",
    "abstract": "We offer a regularized, kernel extension of the multi-set, orthogonal Procrustes problem, or hyperalignment. Our new method, called Kernel Hyperalignment, expands the scope of hyperalignment to include nonlinear measures of similarity and enables the alignment of multiple datasets with a large number of base features. With direct application to fMRI data analysis, kernel hyperalignment is well-suited for multi-subject alignment of large ROIs, including the entire cortex. We conducted experiments using real-world, multi-subject fMRI data.",
    "authors": [
      "Lorbert, Alexander",
      "Ramadge, Peter J."
    ]
  },
  {
    "id": "1728efbda81692282ba642aafd57be3a",
    "title": "Multi-Task Averaging",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/1728efbda81692282ba642aafd57be3a-Paper.pdf",
    "abstract": "We present a multi-task learning approach to jointly estimate the means of multiple independent data sets. The proposed multi-task averaging (MTA) algorithm results in a convex combination of the single-task averages. We derive the optimal amount of regularization, and show that it can be effectively estimated. Simulations and real data experiments demonstrate that MTA  both maximum likelihood and James-Stein estimators, and that our approach to estimating the amount of regularization rivals cross-validation in performance but is more computationally efficient.",
    "authors": [
      "Feldman, Sergey",
      "Gupta, Maya",
      "Frigyik, Bela"
    ]
  },
  {
    "id": "17326d10d511828f6b34fa6d751739e2",
    "title": "A Unifying Perspective of Parametric Policy Search Methods for Markov Decision Processes",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/17326d10d511828f6b34fa6d751739e2-Paper.pdf",
    "abstract": "Parametric policy search algorithms are one of the methods of choice for the optimisation of Markov Decision Processes, with Expectation Maximisation and natural gradient ascent being considered the current state of the art in the field. In this article we provide a unifying perspective of these two algorithms by showing that their step-directions in the parameter space are closely related to the search direction of an approximate Newton method. This analysis leads naturally to the consideration of this approximate Newton method as an alternative gradient-based method for Markov Decision Processes. We are able show that the algorithm has numerous desirable properties, absent in the naive application of Newton's method, that make it a viable alternative to either Expectation Maximisation or natural gradient ascent. Empirical results suggest that the algorithm has excellent convergence and robustness properties, performing strongly in comparison to both Expectation Maximisation and natural gradient ascent.",
    "authors": [
      "Furmston, Thomas",
      "Barber, David"
    ]
  },
  {
    "id": "17c276c8e723eb46aef576537e9d56d0",
    "title": "Convergence and Energy Landscape for Cheeger Cut Clustering",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/17c276c8e723eb46aef576537e9d56d0-Paper.pdf",
    "abstract": "Unsupervised clustering of scattered, noisy and high-dimensional data points is an important and difficult problem. Continuous relaxations of balanced cut problems  yield excellent clustering results. This paper provides rigorous convergence results for two algorithms that solve the relaxed Cheeger Cut minimization.  The first algorithm is a new steepest descent algorithm and the second one is a slight modification of the Inverse Power Method algorithm \\cite{pro:HeinBuhler10OneSpec}. While the steepest descent algorithm has better theoretical convergence properties,  in practice both algorithm perform equally.  We also completely characterize the local minima of the relaxed problem in terms of the original balanced cut problem, and relate this characterization to the convergence of the algorithms.",
    "authors": [
      "Bresson, Xavier",
      "Laurent, Thomas",
      "Uminsky, David",
      "Brecht, James"
    ]
  },
  {
    "id": "184260348236f9554fe9375772ff966e",
    "title": "A Divide-and-Conquer Method for Sparse Inverse Covariance Estimation",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/184260348236f9554fe9375772ff966e-Paper.pdf",
    "abstract": "In this paper, we consider the $\\ell_1$ regularized sparse inverse covariance matrix estimation problem with a very large number of variables. Even in the face of this high dimensionality, and with limited number of samples, recent work has shown this estimator to have strong statistical guarantees in recovering the true structure of the sparse inverse covariance matrix, or alternatively the underlying graph structure of the corresponding Gaussian Markov Random Field. Our proposed algorithm divides the problem into  smaller sub-problems, and uses the solutions of the sub-problems to build a good approximation for the original problem. We derive a bound on the distance of the approximate solution to the true solution. Based on this bound, we propose a clustering algorithm that attempts to minimize this bound, and in practice, is able to find effective partitions of the variables. We further use the approximate solution, i.e., solution resulting from solving the sub-problems,  as an initial point to solve the original problem, and achieve a much faster computational procedure.  As an example, a recent state-of-the-art method, QUIC requires 10 hours to solve a problem (with 10,000 nodes) that arises from a climate application, while our proposed algorithm, Divide and Conquer QUIC (DC-QUIC) only requires one hour to solve the problem.",
    "authors": [
      "Hsieh, Cho-jui",
      "Banerjee, Arindam",
      "Dhillon, Inderjit",
      "Ravikumar, Pradeep"
    ]
  },
  {
    "id": "191c62d342811d1a0d3d0528ec35cd2d",
    "title": "Nonconvex Penalization Using Laplace Exponents and Concave Conjugates",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/191c62d342811d1a0d3d0528ec35cd2d-Paper.pdf",
    "abstract": "In this paper we study sparsity-inducing nonconvex penalty functions using L\u00b4evy processes. We de\ufb01ne such a penalty as the Laplace exponent of a subordina- tor. Accordingly, we propose a novel approach for the construction of sparsity- inducing nonconvex penalties. Particularly, we show that the nonconvex logarith- mic (LOG) and exponential (EXP) penalty functions are the Laplace exponents of Gamma and compound Poisson subordinators, respectively. Additionally, we explore the concave conjugate of nonconvex penalties. We \ufb01nd that the LOG and EXP penalties are the concave conjugates of negative Kullback-Leiber (KL) dis- tance functions. Furthermore, the relationship between these two penalties is due to asymmetricity of the KL distance.",
    "authors": [
      "Zhang, Zhihua",
      "Tu, Bojun"
    ]
  },
  {
    "id": "193002e668758ea9762904da1a22337c",
    "title": "How They Vote: Issue-Adjusted Models of Legislative Behavior",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/193002e668758ea9762904da1a22337c-Paper.pdf",
    "abstract": "We develop a probabilistic model of legislative data that uses the text of the bills to uncover lawmakers' positions on specific political issues.  Our model can be used to explore how a lawmaker's voting patterns deviate from what is expected and how that deviation depends on what is being voted on. We derive approximate posterior inference algorithms based on variational methods. Across 12 years of legislative data, we demonstrate both improvement in heldout predictive performance and the model's utility in interpreting an inherently multi-dimensional space.",
    "authors": [
      "Gerrish, Sean",
      "Blei, David"
    ]
  },
  {
    "id": "19f3cd308f1455b3fa09a282e0d496f4",
    "title": "Multiclass Learning Approaches: A Theoretical Comparison with Implications",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/19f3cd308f1455b3fa09a282e0d496f4-Paper.pdf",
    "abstract": "We theoretically analyze and compare the following five popular multiclass classification methods: One vs. All, All Pairs, Tree-based classifiers, Error Correcting Output Codes (ECOC) with randomly generated code matrices, and Multiclass SVM. In the first four methods, the classification is based on a reduction to binary classification. We consider the case where the binary classifier comes from a class of VC dimension $d$, and in particular from the class of halfspaces over $\\reals^d$. We analyze both the estimation error and the approximation error of these methods. Our analysis reveals interesting conclusions of practical relevance, regarding the success of the different approaches under various conditions. Our proof technique employs tools from VC theory to analyze the \\emph{approximation error} of hypothesis classes. This is in sharp contrast to most, if not all, previous uses of VC theory, which only deal with estimation error.",
    "authors": [
      "Daniely, Amit",
      "Sabato, Sivan",
      "Shwartz, Shai"
    ]
  },
  {
    "id": "1afa34a7f984eeabdbb0a7d494132ee5",
    "title": "Dynamical And-Or Graph Learning for Object Shape Modeling and Detection",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/1afa34a7f984eeabdbb0a7d494132ee5-Paper.pdf",
    "abstract": "This paper studies a novel discriminative part-based model to represent and recognize object shapes with an \u201cAnd-Or graph\u201d. We define this model consisting of three layers: the leaf-nodes with collaborative edges for localizing local parts, the or-nodes specifying the switch of leaf-nodes, and the root-node encoding the global verification. A discriminative learning algorithm, extended from the CCCP [23], is proposed to train the model in a dynamical manner: the model structure (e.g., the configuration of the leaf-nodes associated with the or-nodes) is automatically determined with optimizing the multi-layer parameters during the iteration. The advantages of our method are two-fold. (i) The And-Or graph model enables us to handle well large intra-class variance and background clutters for object shape detection from images. (ii) The proposed learning algorithm is able to obtain the And-Or graph representation without requiring elaborate supervision and initialization. We validate the proposed method on several challenging databases (e.g., INRIA-Horse, ETHZ-Shape, and UIUC-People), and it outperforms the state-of-the-arts approaches.",
    "authors": [
      "Wang, Xiaolong",
      "Lin, Liang"
    ]
  },
  {
    "id": "1be3bc32e6564055d5ca3e5a354acbef",
    "title": "Augmented-SVM: Automatic space partitioning for combining multiple non-linear dynamics",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/1be3bc32e6564055d5ca3e5a354acbef-Paper.pdf",
    "abstract": "Non-linear dynamical systems (DS) have been used extensively for building generative models of human behavior. Its applications range from modeling brain dynamics  to encoding motor commands. Many schemes have been proposed for encoding robot motions using dynamical systems with a single attractor placed at a predefined target in state space. Although these enable the robots to react against sudden perturbations without any re-planning, the motions are always directed towards a single target. In this work, we focus on combining several such DS with distinct attractors, resulting in a multi-stable DS. We show its applicability in reach-to-grasp tasks where the attractors represent several grasping points on the target object. While exploiting multiple attractors provides more flexibility in recovering from unseen perturbations, it also increases the complexity of the underlying learning problem. Here we present the Augmented-SVM (A-SVM) model which inherits region partitioning ability of the well known SVM classifier and is augmented with novel constraints derived from the individual DS. The new constraints modify the original SVM dual whose optimal solution then results in a new class of support vectors (SV). These new SV ensure that the resulting multi-stable DS incurs minimum deviation from the original dynamics and is stable at each of the attractors within a finite region of attraction. We show, via implementations on a simulated 10 degrees of freedom mobile robotic platform, that the model is capable of real-time motion generation and is able to adapt on-the-fly to perturbations.",
    "authors": [
      "Shukla, Ashwini",
      "Billard, Aude"
    ]
  },
  {
    "id": "1bf2efbbe0c49b9f567c2e40f645279a",
    "title": "3D Social Saliency from Head-mounted Cameras",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/1bf2efbbe0c49b9f567c2e40f645279a-Paper.pdf",
    "abstract": "Yaser Sheikh",
    "authors": [
      "Park, Hyun",
      "Jain, Eakta",
      "Sheikh, Yaser"
    ]
  },
  {
    "id": "1c383cd30b7c298ab50293adfecb7b18",
    "title": "Coupling Nonparametric Mixtures via Latent Dirichlet Processes",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/1c383cd30b7c298ab50293adfecb7b18-Paper.pdf",
    "abstract": "Mixture distributions are often used to model complex data. In this paper, we develop a new method that jointly estimates mixture models over multiple data sets by exploiting the statistical dependencies between them. Specifically, we introduce a set of latent Dirichlet processes as sources of component models (atoms), and for each data set, we construct a nonparametric mixture model by combining sub-sampled versions of the latent DPs. Each mixture model may acquire atoms from different latent DPs, while each atom may be shared by multiple mixtures. This multi-to-multi association distinguishes the proposed method from prior constructions that rely on tree or chain structures, allowing mixture models to be coupled more flexibly. In addition, we derive a sampling algorithm that jointly infers the model parameters and present experiments on both document analysis and image modeling.",
    "authors": [
      "Lin, Dahua",
      "Fisher, John"
    ]
  },
  {
    "id": "1cecc7a77928ca8133fa24680a88d2f9",
    "title": "Multiclass Learning with Simplex Coding",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf",
    "abstract": "In this paper we dicuss a novel  framework for multiclass learning,  defined by  a suitable coding/decoding strategy,  namely the simplex coding, that allows to generalize to multiple classes a relaxation approach commonly used in binary classification. In this framework a  relaxation error analysis can be developed avoiding constraints on the considered hypotheses class.  Moreover, we show that in this setting it is possible to derive the first provably consistent  regularized methods with training/tuning complexity which is {\\em independent} to the number of classes. Tools from convex analysis are introduced that can be used beyond the scope of this paper.",
    "authors": [
      "Mroueh, Youssef",
      "Poggio, Tomaso",
      "Rosasco, Lorenzo",
      "Slotine, Jean-jeacques"
    ]
  },
  {
    "id": "1e6e0a04d20f50967c64dac2d639a577",
    "title": "Clustering Sparse Graphs",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/1e6e0a04d20f50967c64dac2d639a577-Paper.pdf",
    "abstract": "We develop a new algorithm to cluster sparse unweighted graphs -- i.e. partition the nodes into disjoint clusters so that there is higher density within clusters, and low across clusters. By sparsity we mean the setting where both the in-cluster and across cluster edge densities are very small, possibly vanishing in the size of the graph. Sparsity makes the problem noisier, and hence more difficult to solve.   Any clustering involves a tradeoff between minimizing two kinds of errors: missing edges within clusters and present edges across clusters. Our insight is that in the sparse case, these must be {\\em penalized differently}. We analyze our algorithm's performance on the natural, classical and widely studied ``planted partition'' model (also called the stochastic block model); we show that our algorithm can cluster sparser graphs, and with smaller clusters, than all previous methods. This is seen empirically as well.",
    "authors": [
      "Chen, Yudong",
      "Sanghavi, Sujay",
      "Xu, Huan"
    ]
  },
  {
    "id": "1ecfb463472ec9115b10c292ef8bc986",
    "title": "On-line Reinforcement Learning Using Incremental Kernel-Based Stochastic Factorization",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/1ecfb463472ec9115b10c292ef8bc986-Paper.pdf",
    "abstract": "The ability to learn a policy for a sequential decision problem with continuous state space using on-line data is a long-standing challenge. This paper presents a new reinforcement-learning algorithm, called iKBSF, which extends the benefits of kernel-based learning to the on-line scenario. As a kernel-based method, the proposed algorithm is stable and has good convergence properties. However, unlike other similar algorithms,iKBSF's space complexity is independent of the number of sample transitions, and as a result it can process an arbitrary amount of data. We present theoretical results showing that iKBSF can approximate (to any level of accuracy) the value function that would be learned by an equivalent batch non-parametric kernel-based reinforcement learning approximator. In order to show the effectiveness of the proposed algorithm in practice, we apply iKBSF to the challenging three-pole balancing task, where the ability to process a large number of transitions is crucial for achieving a high success rate.",
    "authors": [
      "Barreto, Andre",
      "Precup, Doina",
      "Pineau, Joelle"
    ]
  },
  {
    "id": "1ee3dfcd8a0645a25a35977997223d22",
    "title": "Accelerated Training for Matrix-norm Regularization: A Boosting Approach",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/1ee3dfcd8a0645a25a35977997223d22-Paper.pdf",
    "abstract": "Sparse learning models typically combine a smooth loss with a nonsmooth penalty, such as trace norm. Although recent developments in sparse approximation have offered promising solution methods, current approaches either apply only to matrix-norm constrained problems or provide suboptimal convergence rates. In this paper, we propose a boosting method for regularized learning that guarantees $\\epsilon$ accuracy within $O(1/\\epsilon)$ iterations. Performance is further accelerated by interlacing boosting with fixed-rank local optimization---exploiting a simpler local objective than previous work. The proposed method yields state-of-the-art performance on large-scale problems. We also demonstrate an application to latent multiview learning for which we provide the first efficient weak-oracle.",
    "authors": [
      "Zhang, Xinhua",
      "Schuurmans, Dale",
      "Yu, Yao-liang"
    ]
  },
  {
    "id": "202cb962ac59075b964b07152d234b70",
    "title": "Supervised Learning with Similarity Functions",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/202cb962ac59075b964b07152d234b70-Paper.pdf",
    "abstract": "We address the problem of general supervised learning when data can only be accessed through an (indefinite) similarity function between data points. Existing work on learning with indefinite kernels has concentrated solely on binary/multiclass classification problems. We propose a model that is generic enough to handle any supervised learning task and also subsumes the model previously proposed for classification. We give a ''goodness'' criterion for similarity functions w.r.t. a given supervised learning task and then adapt a well-known landmarking technique to provide efficient algorithms for supervised learning using ''good'' similarity functions. We demonstrate the effectiveness of our model on three important supervised learning problems: a) real-valued regression, b) ordinal regression and c) ranking where we show that our method guarantees bounded generalization error. Furthermore, for the case of real-valued regression, we give a natural goodness definition that, when used in conjunction with a recent result in sparse vector recovery, guarantees a sparse predictor with bounded generalization error. Finally, we report results of our learning algorithms on regression and ordinal regression tasks using non-PSD similarity functions and demonstrate the effectiveness of our algorithms, especially that of the sparse landmark selection algorithm that achieves significantly higher accuracies than the baseline methods while offering reduced computational costs.",
    "authors": [
      "Kar, Purushottam",
      "Jain, Prateek"
    ]
  },
  {
    "id": "208e43f0e45c4c78cafadb83d2888cb6",
    "title": "A Simple and Practical Algorithm for Differentially Private Data Release",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/208e43f0e45c4c78cafadb83d2888cb6-Paper.pdf",
    "abstract": "We present a new algorithm for differentially private data release, based on a simple combination of the Exponential Mechanism with the Multiplicative Weights update rule.  Our MWEM algorithm achieves what are the best known and nearly optimal theoretical guarantees, while at the same time being simple to implement and experimentally more accurate on actual data sets than existing techniques.",
    "authors": [
      "Hardt, Moritz",
      "Ligett, Katrina",
      "Mcsherry, Frank"
    ]
  },
  {
    "id": "20aee3a5f4643755a79ee5f6a73050ac",
    "title": "Persistent Homology for Learning Densities with Bounded Support",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/20aee3a5f4643755a79ee5f6a73050ac-Paper.pdf",
    "abstract": "We present a novel method for learning densities with bounded support which enables us to incorporate `hard' topological constraints. In particular, we show how emerging techniques from computational algebraic topology and the notion of Persistent Homology can be combined with kernel based  methods from Machine Learning for the purpose of density estimation. The proposed formalism facilitates learning of models with bounded support in a principled way, and -- by incorporating Persistent Homology techniques in our approach -- we are able to encode algebraic-topological constraints which are not addressed in current state-of the art probabilistic models. We study the behaviour of our method on two synthetic examples for various sample sizes and exemplify the benefits of the proposed approach on a real-world data-set by learning a motion model for a racecar. We show how to learn a model which respects the underlying topological structure of the racetrack, constraining the trajectories of the car.",
    "authors": [
      "Pokorny, Florian",
      "Kjellstr\u00f6m, Hedvig",
      "Kragic, Danica",
      "Ek, Carl"
    ]
  },
  {
    "id": "217eedd1ba8c592db97d0dbe54c7adfc",
    "title": "Proper losses for learning from partial labels",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/217eedd1ba8c592db97d0dbe54c7adfc-Paper.pdf",
    "abstract": "This paper discusses the problem of calibrating posterior class probabilities from partially labelled data. Each instance is assumed to be labelled as belonging to one of several candidate categories, at most one of them being true. We generalize the concept of proper loss to this scenario, establish a necessary and sufficient condition for a loss function to be proper, and we show a direct procedure to construct a proper loss for partial labels from a conventional proper loss. The problem can be characterized by the mixing probability matrix relating the true class of the data and the observed labels. An interesting result is that the full knowledge of this matrix is not required, and losses can be constructed that are proper in a subset of the probability simplex.",
    "authors": [
      "Cid-sueiro, Jes\u00fas"
    ]
  },
  {
    "id": "21c52f533c0c585bab4f075bf08d7104",
    "title": "Predicting Action Content On-Line and in Real Time before Action Onset \u2013 an Intracranial Human Study",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/21c52f533c0c585bab4f075bf08d7104-Paper.pdf",
    "abstract": "The ability to predict action content from neural signals in real time before the ac- tion occurs has been long sought in the neuroscienti\ufb01c study of decision-making, agency and volition. On-line real-time (ORT) prediction is important for under- standing the relation between neural correlates of decision-making and conscious, voluntary action as well as for brain-machine interfaces. Here, epilepsy patients, implanted with intracranial depth microelectrodes or subdural grid electrodes for clinical purposes, participated in a \u201cmatching-pennies\u201d game against an opponent. In each trial, subjects were given a 5 s countdown, after which they had to raise their left or right hand immediately as the \u201cgo\u201d signal appeared on a computer screen. They won a \ufb01xed amount of money if they raised a different hand than their opponent and lost that amount otherwise. The question we here studied was the extent to which neural precursors of the subjects\u2019 decisions can be detected in intracranial local \ufb01eld potentials (LFP) prior to the onset of the action. We found that combined low-frequency (0.1\u20135 Hz) LFP signals from 10 electrodes were predictive of the intended left-/right-hand movements before the onset of the go signal. Our ORT system predicted which hand the patient would raise 0.5 s before the go signal with 68\u00b13% accuracy in two patients. Based on these results, we constructed an ORT system that tracked up to 30 electrodes simultaneously, and tested it on retrospective data from 7 patients. On average, we could predict the correct hand choice in 83% of the trials, which rose to 92% if we let the system drop 3/10 of the trials on which it was less con\ufb01dent. Our system demonstrates\u2014 for the \ufb01rst time\u2014the feasibility of accurately predicting a binary action on single trials in real time for patients with intracranial recordings, well before the action occurs.",
    "authors": [
      "Maoz, Uri",
      "Ye, Shengxuan",
      "Ross, Ian",
      "Mamelak, Adam",
      "Koch, Christof"
    ]
  },
  {
    "id": "24146db4eb48c718b84cae0a0799dcfc",
    "title": "Classification Calibration Dimension for General Multiclass Losses",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/24146db4eb48c718b84cae0a0799dcfc-Paper.pdf",
    "abstract": "We study consistency properties of surrogate loss functions for general multiclass classification problems, defined by a general loss matrix. We extend the notion of classification calibration, which has been studied for binary and multiclass 0-1 classification problems (and for certain other specific learning problems), to the general multiclass setting, and derive necessary and sufficient conditions for a surrogate loss to be classification calibrated with respect to a loss matrix in this setting. We then introduce the notion of \\emph{classification calibration dimension} of a multiclass loss matrix, which measures the smallest size' of a prediction space for which it is possible to design a convex surrogate that is classification calibrated with respect to the loss matrix. We derive both upper and lower bounds on this quantity, and use these results to analyze various loss matrices. In particular, as one application, we provide a different route from the recent result of Duchi et al.\\ (2010) for analyzing the difficulty of designinglow-dimensional' convex surrogates that are consistent with respect to pairwise subset ranking losses. We anticipate the classification calibration dimension may prove to be a useful tool in the study and design of surrogate losses for general multiclass learning problems.",
    "authors": [
      "Ramaswamy, Harish G.",
      "Agarwal, Shivani"
    ]
  },
  {
    "id": "250cf8b51c773f3f8dc8b4be867a9a02",
    "title": "Analog readout for optical reservoir computers",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf",
    "abstract": "Reservoir computing is a new, powerful and flexible machine learning technique that is easily implemented in hardware. Recently, by using a time-multiplexed architecture, hardware reservoir computers have reached performance comparable to digital implementations. Operating speeds allowing for real time information operation have been reached using optoelectronic systems. At present the main performance bottleneck is the readout layer which uses slow, digital postprocessing. We have designed an analog readout suitable for time-multiplexed optoelectronic reservoir computers, capable of working in real time. The readout has been built and tested experimentally on a standard benchmark task. Its performance is better than non-reservoir methods, with ample room for further improvement. The present work thereby overcomes one of the major limitations for the future development of hardware reservoir computers.",
    "authors": [
      "Smerieri, Anteo",
      "Duport, Fran\u00e7ois",
      "Paquot, Yvon",
      "Schrauwen, Benjamin",
      "Haelterman, Marc",
      "Massar, Serge"
    ]
  },
  {
    "id": "26337353b7962f533d78c762373b3318",
    "title": "Perfect Dimensionality Recovery by Variational Bayesian PCA",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/26337353b7962f533d78c762373b3318-Paper.pdf",
    "abstract": "The variational Bayesian (VB) approach is one of the best tractable approximations to the Bayesian estimation, and it was demonstrated to perform well in many applications. However, its good performance was not fully understood theoretically. For example, VB sometimes produces a sparse solution, which is regarded as a practical advantage of VB, but such sparsity is hardly observed in the rigorous Bayesian estimation. In this paper, we focus on probabilistic PCA and give more theoretical insight into the empirical success of VB. More specifically, for the situation where the noise variance is unknown, we derive a sufficient condition for perfect recovery of the true PCA dimensionality in the large-scale limit when the size of an observed matrix goes to infinity. In our analysis, we obtain bounds for a noise variance estimator and simple closed-form solutions for other parameters, which themselves are actually very useful for better implementation of VB-PCA.",
    "authors": [
      "Nakajima, Shinichi",
      "Tomioka, Ryota",
      "Sugiyama, Masashi",
      "Babacan, S."
    ]
  },
  {
    "id": "26408ffa703a72e8ac0117e74ad46f33",
    "title": "Compressive neural representation of sparse, high-dimensional probabilities",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/26408ffa703a72e8ac0117e74ad46f33-Paper.pdf",
    "abstract": "This paper shows how sparse, high-dimensional probability distributions could be represented by neurons with exponential compression. The representation is a novel application of compressive sensing to sparse probability distributions rather than to the usual sparse signals. The compressive measurements correspond to expected values of nonlinear functions of the probabilistically distributed variables. When these expected values are estimated by sampling, the quality of the compressed representation is limited only by the quality of sampling. Since the compression preserves the geometric structure of the space of sparse probability distributions, probabilistic computation can be performed in the compressed domain. Interestingly, functions satisfying the requirements of compressive sensing can be implemented as simple perceptrons. If we use perceptrons as a simple model of feedforward computation by neurons, these results show that the mean activity of a relatively small number of neurons can accurately represent a high-dimensional joint distribution implicitly, even without accounting for any noise correlations. This comprises a novel hypothesis for how neurons could encode probabilities in the brain.",
    "authors": [
      "Pitkow, Zachary"
    ]
  },
  {
    "id": "26e359e83860db1d11b6acca57d8ea88",
    "title": "Shifting Weights: Adapting Object Detectors from Image to Video",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/26e359e83860db1d11b6acca57d8ea88-Paper.pdf",
    "abstract": "Typical object detectors trained on images perform poorly on video, as there is a clear distinction in domain between the two types of data. In this paper, we tackle the problem of adapting object detectors learned from images to work well on videos. We treat the problem as one of unsupervised domain adaptation, in which we are given labeled data from the source domain (image), but only unlabeled data from the target domain (video). Our approach, self-paced domain adaptation, seeks to iteratively adapt the detector by re-training the detector with automatically discovered target domain examples, starting with the easiest first. At each iteration, the algorithm adapts by considering an increased number of target domain examples, and a decreased number of source domain examples. To discover target domain examples from the vast amount of video data, we introduce a simple, robust approach that scores trajectory tracks instead of bounding boxes. We also show how rich and expressive features specific to the target domain can be incorporated under the same framework. We show promising results on the 2011 TRECVID Multimedia Event Detection and LabelMe Video datasets that illustrate the benefit of our approach to adapt object detectors to video.",
    "authors": [
      "Tang, Kevin",
      "Ramanathan, Vignesh",
      "Fei-fei, Li",
      "Koller, Daphne"
    ]
  },
  {
    "id": "2715518c875999308842e3455eda2fe3",
    "title": "Minimization of Continuous Bethe Approximations: A Positive Variation",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/2715518c875999308842e3455eda2fe3-Paper.pdf",
    "abstract": "We develop convergent minimization algorithms for Bethe variational approximations which explicitly constrain marginal estimates to families of valid distributions.  While existing message passing algorithms define fixed point iterations corresponding to stationary points of the Bethe free energy, their greedy dynamics do not distinguish between local minima and maxima, and can fail to converge. For continuous estimation problems, this instability is linked to the creation of invalid marginal estimates, such as Gaussians with negative variance. Conversely, our approach leverages multiplier methods with well-understood convergence properties, and uses bound projection methods to ensure that marginal approximations are valid at all iterations. We derive general algorithms for discrete and Gaussian pairwise Markov random fields, showing improvements over standard loopy belief propagation. We also apply our method to a hybrid model with both discrete and continuous variables, showing improvements over expectation propagation.",
    "authors": [
      "Pacheco, Jason",
      "Sudderth, Erik"
    ]
  },
  {
    "id": "274ad4786c3abca69fa097b85867d9a4",
    "title": "Optimal Regularized Dual Averaging Methods for Stochastic Optimization",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/274ad4786c3abca69fa097b85867d9a4-Paper.pdf",
    "abstract": "This paper considers a wide spectrum of regularized stochastic optimization problems where both the loss function and regularizer can be non-smooth.  We develop a novel algorithm based on the regularized dual averaging (RDA) method, that can simultaneously achieve the optimal convergence rates for both convex and strongly convex loss. In particular, for strongly convex loss, it achieves the optimal  rate of $O(\\frac{1}{N}+\\frac{1}{N^2})$ for $N$  iterations, which improves the best known rate $O(\\frac{\\log N }{N})$ of previous stochastic dual averaging algorithms. In addition, our method constructs the final solution directly from the proximal mapping instead of averaging of all previous iterates. For widely used sparsity-inducing regularizers (e.g., $\\ell_1$-norm), it has the advantage of encouraging sparser solutions. We further develop a multi-stage extension using the proposed algorithm as a subroutine, which achieves the uniformly-optimal rate $O(\\frac{1}{N}+\\exp\\{-N\\})$ for strongly convex loss.",
    "authors": [
      "Chen, Xi",
      "Lin, Qihang",
      "Pena, Javier"
    ]
  },
  {
    "id": "286674e3082feb7e5afb92777e48821f",
    "title": "Gradient Weights help Nonparametric Regressors",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/286674e3082feb7e5afb92777e48821f-Paper.pdf",
    "abstract": "In regression problems over $\\real^d$, the unknown function $f$ often varies more in some coordinates than in others. We show that weighting each coordinate $i$ with the estimated norm of the $i$th derivative of $f$  is an efficient way to significantly improve the performance of distance-based regressors, e.g. kernel and $k$-NN regressors.  We propose a simple estimator of these derivative norms and prove its consistency. Moreover, the proposed  estimator is efficiently learned online.",
    "authors": [
      "Kpotufe, Samory",
      "Boularias, Abdeslam"
    ]
  },
  {
    "id": "28f0b864598a1291557bed248a998d4e",
    "title": "Regularized Off-Policy TD-Learning",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/28f0b864598a1291557bed248a998d4e-Paper.pdf",
    "abstract": "We present a novel $l_1$ regularized off-policy convergent TD-learning method (termed RO-TD), which is able to learn sparse representations of value functions with low computational complexity. The algorithmic framework underlying RO-TD integrates two key ideas: off-policy convergent gradient TD methods, such as TDC, and a convex-concave saddle-point formulation of non-smooth convex optimization, which enables first-order solvers and feature selection using online convex regularization. A detailed theoretical and experimental analysis of RO-TD is presented. A variety of experiments are presented to illustrate the off-policy convergence, sparse feature selection capability and low computational cost of the RO-TD algorithm.",
    "authors": [
      "Liu, Bo",
      "Mahadevan, Sridhar",
      "Liu, Ji"
    ]
  },
  {
    "id": "299fb2142d7de959380f91c01c3a293c",
    "title": "Locating Changes in Highly Dependent Data with Unknown Number of Change Points",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/299fb2142d7de959380f91c01c3a293c-Paper.pdf",
    "abstract": "The problem of multiple change point estimation is  considered for sequences  with unknown number of change points.  A consistency framework is suggested that is suitable for highly dependent time-series, and an asymptotically consistent algorithm is proposed.   In order for the consistency to be established the only assumption  required is that the data is generated by stationary ergodic time-series distributions. No modeling, independence or parametric assumptions are made; the data are allowed to be dependent and the dependence can be of arbitrary form.  The theoretical results are complemented with experimental evaluations.",
    "authors": [
      "Khaleghi, Azadeh",
      "Ryabko, Daniil"
    ]
  },
  {
    "id": "2a50e9c2d6b89b95bcb416d6857f8b45",
    "title": "Controlled Recognition Bounds for Visual Learning and Exploration",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/2a50e9c2d6b89b95bcb416d6857f8b45-Paper.pdf",
    "abstract": "We describe the tradeoff between the performance in a visual recognition problem and the control authority that the agent can exercise on the sensing process. We focus on the problem of \u201cvisual search\u201d of an object in an otherwise known and static scene, propose a measure of control authority, and relate it to the expected risk and its proxy (conditional entropy of the posterior density). We show this analytically, as well as empirically by simulation using the simplest known model that captures the phenomenology of image formation, including scaling and occlusions. We show that a \u201cpassive\u201d agent given a training set can provide no guarantees on performance beyond what is afforded by the priors, and that an \u201comnipotent\u201d agent, capable of infinite control authority, can achieve arbitrarily good performance (asymptotically).",
    "authors": [
      "Karasev, Vasiliy",
      "Chiuso, Alessandro",
      "Soatto, Stefano"
    ]
  },
  {
    "id": "2ab56412b1163ee131e1246da0955bd1",
    "title": "Multi-Stage Multi-Task Feature Learning",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/2ab56412b1163ee131e1246da0955bd1-Paper.pdf",
    "abstract": "Multi-task sparse feature learning aims to improve the generalization performance by exploiting the shared features among tasks. It has been successfully applied to many applications including computer vision and biomedical informatics. Most of the existing multi-task sparse feature learning algorithms are formulated as a convex sparse regularization problem, which is usually suboptimal, due to its looseness for approximating an $\\ell_0$-type regularizer. In this paper, we propose a non-convex formulation for multi-task sparse feature learning based on a novel regularizer. To solve the non-convex optimization problem, we propose a Multi-Stage Multi-Task Feature Learning (MSMTFL) algorithm. Moreover, we present a detailed theoretical analysis showing that MSMTFL achieves a better parameter estimation error bound than the convex formulation. Empirical studies on both synthetic and real-world data sets demonstrate the effectiveness of MSMTFL in comparison with the state of the art multi-task sparse feature learning algorithms.",
    "authors": [
      "Gong, Pinghua",
      "Ye, Jieping",
      "Zhang, Chang-shui"
    ]
  },
  {
    "id": "2b24d495052a8ce66358eb576b8912c8",
    "title": "Fast Resampling Weighted v-Statistics",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/2b24d495052a8ce66358eb576b8912c8-Paper.pdf",
    "abstract": "In this paper, a novel, computationally fast, and alternative algorithm for com- puting weighted v-statistics in resampling both univariate and multivariate data is proposed. To avoid any real resampling, we have linked this problem with finite group action and converted it into a problem of orbit enumeration. For further computational cost reduction, an efficient method is developed to list all orbits by their symmetry order and calculate all index function orbit sums and data function orbit sums recursively. The computational complexity analysis shows reduction in the computational cost from n! or nn level to low-order polynomial level.",
    "authors": [
      "Zhou, Chunxiao",
      "Park, Jiseong",
      "Fu, Yun"
    ]
  },
  {
    "id": "2cfd4560539f887a5e420412b370b361",
    "title": "Rational inference of relative preferences",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/2cfd4560539f887a5e420412b370b361-Paper.pdf",
    "abstract": "Statistical decision theory axiomatically assumes that the relative desirability of different options that humans perceive is well described by assigning them option-specific scalar utility functions. However, this assumption is refuted by observed human behavior, including studies wherein preferences have been shown to change systematically simply through variation in the set of choice options presented. In this paper, we show that interpreting desirability as a relative comparison between available options at any particular decision instance results in a rational theory of value-inference that explains heretofore intractable violations of rational choice behavior in human subjects. Complementarily, we also characterize the conditions under which a rational agent selecting optimal options indicated by dynamic value inference in our framework will behave identically to one whose preferences are encoded using a static ordinal utility function.",
    "authors": [
      "Srivastava, Nisheeth",
      "Schrater, Paul R."
    ]
  },
  {
    "id": "2d6cc4b2d139a53512fb8cbb3086ae2e",
    "title": "Latent Graphical Model Selection: Efficient Methods for Locally Tree-like Graphs",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/2d6cc4b2d139a53512fb8cbb3086ae2e-Paper.pdf",
    "abstract": "Graphical model selection refers to the problem of estimating the unknown graph structure given observations at the nodes in the model. We consider a challenging instance of this problem when some of the nodes are latent or hidden.  We  characterize  conditions for tractable graph estimation and develop efficient methods with provable guarantees. We consider the class of Ising models Markov on  locally tree-like graphs, which are in the regime of correlation decay. We  propose an efficient method for graph estimation, and establish its structural consistency when the number of samples $n$ scales as $n = \\Omega(\\theta_{\\min}^{-\\delta \\eta(\\eta+1)-2}\\log p)$, where $\\theta_{\\min}$ is the minimum edge potential, $\\delta$ is the depth (i.e., distance from a hidden node to the nearest  observed nodes), and $\\eta$ is a parameter which depends on the minimum and maximum node and edge potentials in the Ising model. The proposed method is practical to implement and provides  flexibility to control  the number of latent variables and the cycle lengths in the output graph.  We also present necessary conditions for graph estimation by any method and show that our method nearly matches the lower bound  on sample requirements.",
    "authors": [
      "Anandkumar, Anima",
      "Valluvan, Ragupathyraj"
    ]
  },
  {
    "id": "2dace78f80bc92e6d7493423d729448e",
    "title": "On the connections between saliency and tracking",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/2dace78f80bc92e6d7493423d729448e-Paper.pdf",
    "abstract": "A model connecting visual tracking and saliency has recently been proposed. This model is based on the saliency hypothesis for tracking which postulates that tracking is achieved by the top-down tuning, based on target features, of discriminant center-surround saliency mechanisms over time. In this work, we identify three main predictions that must hold if the hypothesis were true: 1) tracking reliability should be larger for salient than for non-salient targets, 2) tracking reliability should have a dependence on the defining variables of saliency, namely feature contrast and distractor heterogeneity, and must replicate the dependence of saliency on these variables, and 3) saliency and tracking can be implemented with common low level neural mechanisms. We confirm that the first two predictions hold by reporting results from a set of human behavior studies on the connection between saliency and tracking. We also show that the third prediction holds by constructing a common neurophysiologically plausible architecture that can computationally solve both saliency and tracking. This architecture is fully compliant with the standard physiological models of V1 and MT, and with what is known about attentional control in area LIP, while explaining the results of the human behavior experiments.",
    "authors": [
      "Mahadevan, Vijay",
      "Vasconcelos, Nuno"
    ]
  },
  {
    "id": "2dea61eed4bceec564a00115c4d21334",
    "title": "Symbolic Dynamic Programming for Continuous State and Observation POMDPs",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/2dea61eed4bceec564a00115c4d21334-Paper.pdf",
    "abstract": "Partially-observable Markov decision processes (POMDPs) provide a powerful model for real-world sequential decision-making problems. In recent years, point- based value iteration methods have proven to be extremely effective techniques for \ufb01nding (approximately) optimal dynamic programming solutions to POMDPs when an initial set of belief states is known. However, no point-based work has provided exact point-based backups for both continuous state and observation spaces, which we tackle in this paper. Our key insight is that while there may be an in\ufb01nite number of possible observations, there are only a \ufb01nite number of observation partitionings that are relevant for optimal decision-making when a \ufb01nite, \ufb01xed set of reachable belief states is known. To this end, we make two important contributions: (1) we show how previous exact symbolic dynamic pro- gramming solutions for continuous state MDPs can be generalized to continu- ous state POMDPs with discrete observations, and (2) we show how this solution can be further extended via recently developed symbolic methods to continuous state and observations to derive the minimal relevant observation partitioning for potentially correlated, multivariate observation spaces. We demonstrate proof-of- concept results on uni- and multi-variate state and observation steam plant control.",
    "authors": [
      "Zamani, Zahra",
      "Sanner, Scott",
      "Poupart, Pascal",
      "Kersting, Kristian"
    ]
  },
  {
    "id": "2dffbc474aa176b6dc957938c15d0c8b",
    "title": "Imitation Learning by Coaching",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/2dffbc474aa176b6dc957938c15d0c8b-Paper.pdf",
    "abstract": "Imitation Learning has been shown to be successful in solving many challenging real-world problems. Some recent approaches give strong performance guarantees by training the policy iteratively. However, it is important to note that these guarantees depend on   how well the policy we found can imitate the oracle on the training data.  When there is a substantial difference between the oracle's ability  and the learner's policy space, we may fail to find a policy that has low error on the training set. In such cases, we propose to use a coach that demonstrates easy-to-learn actions for the learner  and gradually approaches the oracle. By a reduction of learning by demonstration to online learning,  we prove that coaching can yield a lower regret bound than using the oracle. We apply our algorithm to a novel cost-sensitive dynamic feature selection problem, a hard decision problem that considers a user-specified accuracy-cost trade-off.  Experimental results on UCI datasets show that our method outperforms state-of-the-art imitation learning methods in dynamic features selection and two static feature selection methods.",
    "authors": [
      "He, He",
      "Eisner, Jason",
      "Daume, Hal"
    ]
  },
  {
    "id": "310dcbbf4cce62f762a2aaa148d556bd",
    "title": "Learning about Canonical Views from Internet Image Collections",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/310dcbbf4cce62f762a2aaa148d556bd-Paper.pdf",
    "abstract": "Although human object recognition is supposedly robust to viewpoint, much research on human perception indicates that there is a preferred or \u201ccanonical\u201d view of objects. This phenomenon was discovered more than 30 years ago but the canonical view of only a small number of categories has been validated experimentally. Moreover, the explanation for why humans prefer the canonical view over other views remains elusive. In this paper we ask: Can we use Internet image collections to learn more about canonical views? We start by manually finding the most common view in the results returned by Internet search engines when queried with the objects used in psychophysical experiments. Our results clearly show that the most likely view in the search engine corresponds to the same view preferred by human subjects in experiments. We also present a simple method to find the most likely view in an image collection and apply it to hundreds of categories. Using the new data we have collected we present strong evidence against the two most prominent formal theories of canonical views and provide novel constraints for new theories.",
    "authors": [
      "Mezuman, Elad",
      "Weiss, Yair"
    ]
  },
  {
    "id": "31839b036f63806cba3f47b93af8ccb5",
    "title": "A lattice filter model of the visual pathway",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/31839b036f63806cba3f47b93af8ccb5-Paper.pdf",
    "abstract": "Early stages of visual processing are thought to decorrelate, or whiten, the incoming temporally varying signals. Because the typical correlation time of natural stimuli, as well as the extent of temporal receptive fields of lateral geniculate nucleus (LGN) neurons, is much greater than neuronal time constants, such decorrelation must be done in stages combining contributions of multiple neurons. We propose to model temporal decorrelation in the visual pathway with the lattice filter, a signal processing device for stage-wise decorrelation of temporal signals. The stage-wise architecture of the lattice filter maps naturally onto the visual pathway (photoreceptors -> bipolar cells -> retinal ganglion cells -> LGN) and its filter weights can be learned using Hebbian rules in a stage-wise sequential manner. Moreover, predictions of neural activity from the lattice filter model are consistent with physiological measurements in LGN neurons and fruit fly second-order visual neurons. Therefore, the lattice filter model is a useful abstraction that may help unravel visual system function.",
    "authors": [
      "Gregor, Karol",
      "Chklovskii, Dmitri"
    ]
  },
  {
    "id": "33e75ff09dd601bbe69f351039152189",
    "title": "Multi-scale Hyper-time Hardware Emulation of Human Motor Nervous System Based on Spiking Neurons using FPGA",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/33e75ff09dd601bbe69f351039152189-Paper.pdf",
    "abstract": "Our central goal is to quantify the long-term progression of pediatric neurological diseases, such as a typical 10-15 years progression of child dystonia. To this purpose, quantitative models are convincing only if they can provide multi-scale details ranging from neuron spikes to limb biomechanics. The models also need to be evaluated in hyper-time, i.e. significantly faster than real-time, for producing useful predictions. We designed a platform with digital VLSI hardware for multi-scale hyper-time emulations of human motor nervous systems. The platform is constructed on a scalable, distributed array of Field Programmable Gate Array (FPGA) devices. All devices operate asynchronously with 1 millisecond time granularity, and the overall system is accelerated to 365x real-time. Each physiological component is implemented using models from well documented studies and can be flexibly modified. Thus the validity of emulation can be easily advised by neurophysiologists and clinicians. For maximizing the speed of emulation, all calculations are implemented in combinational logic instead of clocked iterative circuits. This paper presents the methodology of building FPGA modules in correspondence to components of a monosynaptic spinal loop. Results of emulated activities are shown. The paper also discusses the rationale of approximating neural circuitry by organizing neurons with sparse interconnections. In conclusion, our platform allows introducing various abnormalities into the neural emulation such that the emerging motor symptoms can be analyzed. It compels us to test the origins of childhood motor disorders and predict their long-term progressions.",
    "authors": [
      "Niu, C.",
      "Nandyala, Sirish",
      "Sohn, Won",
      "Sanger, Terence"
    ]
  },
  {
    "id": "33e8075e9970de0cfea955afd4644bb2",
    "title": "Recognizing Activities by Attribute Dynamics",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/33e8075e9970de0cfea955afd4644bb2-Paper.pdf",
    "abstract": "In this work, we consider the problem of modeling the dynamic structure of human activities in  the attributes space. A video sequence is first represented in a semantic feature space, where each feature encodes the probability of  occurrence of an activity attribute at a given time. A generative model,  denoted the binary dynamic system (BDS), is proposed to learn both the distribution and dynamics of different activities in this  space. The BDS is a non-linear dynamic system, which extends both the binary  principal component analysis (PCA) and classical linear dynamic systems (LDS), by combining binary observation variables with a hidden Gauss-Markov state  process. In this way, it integrates the representation power of semantic  modeling with the ability of dynamic systems to capture the temporal structure of time-varying processes. An algorithm for learning BDS  parameters, inspired by a popular LDS  learning method from dynamic textures, is proposed. A similarity measure between BDSs, which generalizes  the Binet-Cauchy kernel for LDS, is then introduced and used to design  activity classifiers. The proposed method is shown to outperform similar classifiers  derived from the kernel dynamic system (KDS) and state-of-the-art approaches for  dynamics-based or attribute-based action recognition.",
    "authors": [
      "Li, Weixin",
      "Vasconcelos, Nuno"
    ]
  },
  {
    "id": "3435c378bb76d4357324dd7e69f3cd18",
    "title": "Statistical Consistency of Ranking Methods in A Rank-Differentiable Probability Space",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/3435c378bb76d4357324dd7e69f3cd18-Paper.pdf",
    "abstract": "This paper is concerned with the statistical consistency of ranking methods. Recently, it was proven that many commonly used pairwise ranking methods are inconsistent with the weighted pairwise disagreement loss (WPDL), which can be viewed as the true loss of ranking, even in a low-noise setting. This result is interesting but also surprising, given that the pairwise ranking methods have been shown very effective in practice. In this paper, we argue that the aforementioned result might not be conclusive, depending on what kind of assumptions are used. We give a new assumption that the labels of objects to rank lie in a rank-differentiable probability space (RDPS), and prove that the pairwise ranking methods become consistent with WPDL under this assumption. What is especially inspiring is that RDPS is actually not stronger than but similar to the low-noise setting. Our studies provide theoretical justifications of some empirical findings on pairwise ranking methods that are unexplained before, which bridge the gap between theory and applications.",
    "authors": [
      "Lan, Yanyan",
      "Guo, Jiafeng",
      "Cheng, Xueqi",
      "Liu, Tie-yan"
    ]
  },
  {
    "id": "34ed066df378efacc9b924ec161e7639",
    "title": "A Scalable CUR Matrix Decomposition Algorithm: Lower Time Complexity and Tighter Bound",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/34ed066df378efacc9b924ec161e7639-Paper.pdf",
    "abstract": "The CUR matrix decomposition is an important extension of Nystr\u00f6m approximation to a general matrix. It approximates any data matrix in terms of a small number of its columns and rows. In this paper we propose a novel randomized CUR algorithm with an expected relative-error bound. The proposed algorithm has the advantages over the existing relative-error CUR algorithms that it possesses tighter theoretical bound and lower time complexity, and that it can avoid maintaining the whole data matrix in main memory. Finally, experiments on several real-world datasets demonstrate significant improvement over the existing relative-error algorithms.",
    "authors": [
      "Wang, Shusen",
      "Zhang, Zhihua"
    ]
  },
  {
    "id": "35051070e572e47d2c26c241ab88307f",
    "title": "Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based Search",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/35051070e572e47d2c26c241ab88307f-Paper.pdf",
    "abstract": "Bayesian model-based reinforcement learning is a formally elegant approach to learning optimal behaviour under model uncertainty, trading off exploration and exploitation in an ideal way. Unfortunately, finding the resulting Bayes-optimal policies is notoriously taxing, since the search space becomes enormous. In this paper we introduce a tractable, sample-based method for approximate Bayes-optimal planning which exploits Monte-Carlo tree search. Our approach outperformed prior Bayesian model-based RL algorithms by a significant margin on several well-known benchmark problems -- because it avoids expensive applications of Bayes rule within the search tree by lazily sampling models from the current beliefs. We illustrate the advantages of our approach by showing it working in an infinite state space domain which is qualitatively out of reach of almost all previous work in Bayesian exploration.",
    "authors": [
      "Guez, Arthur",
      "Silver, David",
      "Dayan, Peter"
    ]
  },
  {
    "id": "362e80d4df43b03ae6d3f8540cd63626",
    "title": "Dual-Space Analysis of the Sparse Linear Model",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/362e80d4df43b03ae6d3f8540cd63626-Paper.pdf",
    "abstract": "Sparse linear (or generalized linear) models combine a standard likelihood function with a sparse prior on the unknown coefficients.  These priors can conveniently be expressed as a maximization over zero-mean Gaussians with different variance hyperparameters.  Standard MAP estimation (Type I) involves maximizing over both the hyperparameters and coefficients, while an empirical Bayesian alternative (Type II) first marginalizes the coefficients and then maximizes over the hyperparameters, leading to a tractable posterior approximation.  The underlying cost functions can be related via a dual-space framework from Wipf et al. (2011), which allows both the Type I or Type II objectives to be expressed in either coefficient or hyperparmeter space.  This perspective is useful because some analyses or extensions are more conducive to development in one space or the other.  Herein we consider the estimation of a trade-off parameter balancing sparsity and data fit.  As this parameter is effectively a variance, natural estimators exist by assessing the problem in hyperparameter (variance) space, transitioning natural ideas from Type II to solve what is much less intuitive for Type I.  In contrast, for analyses of update rules and sparsity properties of local and global solutions, as well as extensions to more general likelihood models, we can leverage coefficient-space techniques developed for Type I and apply them to Type II.  For example, this allows us to prove that Type II-inspired techniques can be successful recovering sparse coefficients when unfavorable restricted isometry properties (RIP) lead to failure of popular L1 reconstructions. It also facilitates the analysis of Type II when non-Gaussian likelihood models lead to intractable integrations.",
    "authors": [
      "Wu, Yi",
      "Wipf, David"
    ]
  },
  {
    "id": "36660e59856b4de58a219bcf4e27eba3",
    "title": "Neuronal Spike Generation Mechanism as an Oversampling, Noise-shaping A-to-D converter",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/36660e59856b4de58a219bcf4e27eba3-Paper.pdf",
    "abstract": "We explore the hypothesis that the neuronal spike generation mechanism is an analog-to-digital converter, which rectifies low-pass filtered summed synaptic currents and encodes them into spike trains linearly decodable in post-synaptic neurons. To digitally encode an analog current waveform, the sampling rate of the spike generation mechanism must exceed its Nyquist rate. Such oversampling is consistent with the experimental observation that the precision of the spike-generation mechanism is an order of magnitude greater than the cut-off frequency of dendritic low-pass filtering. To achieve additional reduction in the error of analog-to-digital conversion, electrical engineers rely on noise-shaping. If noise-shaping were used in neurons, it would introduce correlations in spike timing to reduce low-frequency (up to Nyquist) transmission error at the cost of high-frequency one (from Nyquist to sampling rate). Using experimental data from three different classes of neurons, we demonstrate that biological neurons utilize noise-shaping. We also argue that rectification by the spike-generation mechanism may improve energy efficiency and carry out de-noising. Finally, the zoo of ion channels in neurons may be viewed as a set of predictors, various subsets of which are activated depending on the statistics of the input current.",
    "authors": [
      "Chklovskii, Dmitri",
      "Soudry, Daniel"
    ]
  },
  {
    "id": "36a1694bce9815b7e38a9dad05ad42e0",
    "title": "Multiple Operator-valued Kernel Learning",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/36a1694bce9815b7e38a9dad05ad42e0-Paper.pdf",
    "abstract": "Positive definite operator-valued kernels generalize the well-known notion of reproducing kernels, and are naturally adapted to multi-output learning situations. This paper addresses the problem of learning a finite linear combination of infinite-dimensional operator-valued kernels which are suitable for extending functional data analysis methods to nonlinear contexts. We study this problem in the case of kernel ridge regression for functional responses with an lr-norm constraint on the combination coefficients. The resulting optimization problem is more involved than those of multiple scalar-valued kernel learning since operator-valued kernels pose more technical and theoretical issues. We propose a multiple operator-valued kernel learning algorithm based on solving a system of linear operator equations by using a block coordinate-descent procedure. We experimentally validate our approach on a functional regression task in the context of finger movement prediction in brain-computer interfaces.",
    "authors": [
      "Kadri, Hachem",
      "Rakotomamonjy, Alain",
      "Preux, Philippe",
      "Bach, Francis"
    ]
  },
  {
    "id": "38ca89564b2259401518960f7a06f94b",
    "title": "No-Regret Algorithms for Unconstrained Online Convex Optimization",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/38ca89564b2259401518960f7a06f94b-Paper.pdf",
    "abstract": "Some of the most compelling applications of online convex optimization, including online prediction and classification, are unconstrained: the natural feasible set is R^n.  Existing algorithms fail to achieve sub-linear regret in this setting unless constraints on the comparator point x* are known in advance.  We present an algorithm that, without such prior knowledge, offers near-optimal regret bounds with respect to any choice of x.  In particular, regret with respect to x = 0 is constant.  We then prove lower bounds showing that our algorithm's guarantees are optimal in this setting up to constant factors.",
    "authors": [
      "Mcmahan, Brendan",
      "Streeter, Matthew"
    ]
  },
  {
    "id": "39461a19e9eddfb385ea76b26521ea48",
    "title": "Learning Partially Observable Models Using Temporally Abstract Decision Trees",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/39461a19e9eddfb385ea76b26521ea48-Paper.pdf",
    "abstract": "This paper introduces timeline trees, which are partial models of partially observable environments. Timeline trees are given some specific predictions to make and learn a decision tree over history. The main idea of timeline trees is to use temporally abstract features to identify and split on features of key events, spread arbitrarily far apart in the past (whereas previous decision-tree-based methods have been limited to a finite suffix of history). Experiments demonstrate that timeline trees can learn to make high quality predictions in complex, partially observable environments with high-dimensional observations (e.g. an arcade game).",
    "authors": [
      "Talvitie, Erik"
    ]
  },
  {
    "id": "39e4973ba3321b80f37d9b55f63ed8b8",
    "title": "Emergence of Object-Selective Features in Unsupervised Feature Learning",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/39e4973ba3321b80f37d9b55f63ed8b8-Paper.pdf",
    "abstract": "Recent work in unsupervised feature learning has focused on the goal   of discovering high-level features from unlabeled images.  Much   progress has been made in this direction, but in most cases it is   still standard to use a large amount of labeled data in order to   construct detectors sensitive to object classes or other complex   patterns in the data.  In this paper, we aim to test the hypothesis   that unsupervised feature learning methods, provided with only   unlabeled data, can learn high-level, invariant features that are   sensitive to commonly-occurring objects.  Though a handful of prior   results suggest that this is possible when each object class   accounts for a large fraction of the data (as in many labeled   datasets), it is unclear whether something similar can be   accomplished when dealing with completely unlabeled data.  A major   obstacle to this test, however, is scale: we cannot expect to   succeed with small datasets or with small numbers of learned   features.  Here, we propose a large-scale feature learning system   that enables us to carry out this experiment, learning 150,000   features from tens of millions of unlabeled images.  Based on two   scalable clustering algorithms (K-means and agglomerative   clustering), we find that our simple system can discover features   sensitive to a commonly occurring object class (human faces) and can   also combine these into detectors invariant to significant global   distortions like large translations and scale.",
    "authors": [
      "Coates, Adam",
      "Karpathy, Andrej",
      "Ng, Andrew"
    ]
  },
  {
    "id": "3a066bda8c96b9478bb0512f0a43028c",
    "title": "CPRL -- An Extension of Compressive Sensing to the Phase Retrieval Problem",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/3a066bda8c96b9478bb0512f0a43028c-Paper.pdf",
    "abstract": "While compressive sensing (CS) has been one of the most vibrant and active research fields in the past few years, most development only applies to linear models. This limits its application and excludes many areas where CS ideas could make a difference. This paper presents a novel extension of CS to the phase retrieval problem, where intensity measurements of a linear system are used to recover a complex sparse signal. We propose a novel solution using a lifting technique -- CPRL, which relaxes the NP-hard problem to a nonsmooth semidefinite program. Our analysis shows that CPRL inherits many desirable properties from CS, such as guarantees for exact recovery. We further provide scalable numerical solvers to accelerate its implementation. The source code of our algorithms will be provided to the public.",
    "authors": [
      "Ohlsson, Henrik",
      "Yang, Allen",
      "Dong, Roy",
      "Sastry, Shankar"
    ]
  },
  {
    "id": "3a15c7d0bbe60300a39f76f8a5ba6896",
    "title": "Learning optimal spike-based representations",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/3a15c7d0bbe60300a39f76f8a5ba6896-Paper.pdf",
    "abstract": "How do neural networks learn to represent information? Here, we address this question by assuming that neural networks seek to generate an optimal population representation for a fixed linear decoder. We define a loss function for the quality of the population read-out and derive the dynamical equations for both neurons and synapses from the requirement to minimize this loss. The dynamical equations yield a network of integrate-and-fire neurons undergoing Hebbian plasticity. We show that, through learning, initially regular and highly correlated spike trains evolve towards Poisson-distributed and independent spike trains with much lower firing rates. The learning rule drives the network into an asynchronous, balanced regime where all inputs to the network are represented optimally for the given decoder. We show that the network dynamics and synaptic plasticity jointly balance the excitation and inhibition received by each unit as tightly as possible and, in doing so, minimize the prediction error between the inputs and the decoded outputs. In turn, spikes are only signalled whenever this prediction error exceeds a certain value, thereby implementing a predictive coding scheme. Our work  suggests that several of the features reported in cortical networks, such as the high trial-to-trial variability, the balance between excitation and inhibition, and spike-timing dependent plasticity, are simply signatures of an efficient, spike-based code.",
    "authors": [
      "Bourdoukan, Ralph",
      "Barrett, David",
      "Deneve, Sophie",
      "Machens, Christian K."
    ]
  },
  {
    "id": "3b712de48137572f3849aabd5666a4e3",
    "title": "Collaborative Ranking With 17 Parameters",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/3b712de48137572f3849aabd5666a4e3-Paper.pdf",
    "abstract": "The primary application of collaborate filtering (CF) is to recommend a small set of items to a user, which entails ranking. Most approaches, however, formulate the CF problem as rating prediction, overlooking the ranking perspective. In this work we present a method for collaborative ranking that leverages the strengths of the two main CF approaches, neighborhood- and model-based. Our novel method is highly efficient, with only seventeen parameters to optimize and a single hyperparameter to tune, and beats the state-of-the-art collaborative ranking methods. We also show that parameters learned on one dataset yield excellent results on a very different dataset, without any retraining.",
    "authors": [
      "Volkovs, Maksims",
      "Zemel, Richard"
    ]
  },
  {
    "id": "3bbfdde8842a5c44a0323518eec97cbe",
    "title": "Small-Variance Asymptotics for Exponential Family Dirichlet Process Mixture Models",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/3bbfdde8842a5c44a0323518eec97cbe-Paper.pdf",
    "abstract": "Links between probabilistic and non-probabilistic learning algorithms can arise by performing small-variance asymptotics, i.e., letting the variance of particular distributions in a graphical model go to zero. For instance, in the context of clustering, such an approach yields precise connections between the k-means and EM algorithms.  In this paper, we explore small-variance asymptotics for exponential family Dirichlet process (DP) and hierarchical Dirichlet process (HDP) mixture models.  Utilizing connections between exponential family distributions and Bregman divergences, we derive novel clustering algorithms from the asymptotic limit of the DP and HDP mixtures that feature the scalability of existing hard clustering methods as well as the flexibility of Bayesian nonparametric models.  We focus on special cases of our analysis for discrete-data problems, including topic modeling, and we demonstrate the utility of our results by applying variants of our algorithms to problems arising in vision and document analysis.",
    "authors": [
      "Jiang, Ke",
      "Kulis, Brian",
      "Jordan, Michael"
    ]
  },
  {
    "id": "3c7781a36bcd6cf08c11a970fbe0e2a6",
    "title": "Deep Representations and Codes for Image Auto-Annotation",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/3c7781a36bcd6cf08c11a970fbe0e2a6-Paper.pdf",
    "abstract": "The task of assigning a set of relevant tags to an image is challenging due to the size and variability of tag vocabularies. Consequently, most existing algorithms focus on tag assignment and fix an often large number of hand-crafted features to describe image characteristics. In this paper we introduce a hierarchical model for learning representations of full sized color images from the pixel level, removing the need for engineered feature representations and subsequent feature selection. We benchmark our model on the STL-10 recognition dataset, achieving state-of-the-art performance. When our features are combined with TagProp (Guillaumin et al.), we outperform or compete with existing annotation approaches that use over a dozen distinct image descriptors. Furthermore, using 256-bit codes and Hamming distance for training TagProp, we exchange only a small reduction in performance for efficient storage and fast comparisons. In our experiments, using deeper architectures always outperform shallow ones.",
    "authors": [
      "Kiros, Ryan",
      "Szepesv\u00e1ri, Csaba"
    ]
  },
  {
    "id": "3cec07e9ba5f5bb252d13f5f431e4bbb",
    "title": "Patient Risk Stratification for Hospital-Associated C. diff as a Time-Series Classification Task",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/3cec07e9ba5f5bb252d13f5f431e4bbb-Paper.pdf",
    "abstract": "A patient's risk for adverse events is affected by temporal processes including the nature and timing of diagnostic and therapeutic activities, and the overall evolution of the patient's pathophysiology over time. Yet many investigators ignore this temporal aspect when modeling patient risk, considering only the patient's current or aggregate state. We explore representing patient risk as a time series. In doing so, patient risk stratification becomes a time-series classification task. The task differs from most applications of time-series analysis, like speech processing, since the time series itself must first be extracted. Thus, we begin by defining and extracting approximate \\textit{risk processes}, the evolving approximate daily risk of a patient. Once obtained, we use these signals to explore different approaches to time-series classification with the goal of identifying high-risk patterns. We apply the classification to the specific task of identifying patients at risk of testing positive for hospital acquired colonization with \\textit{Clostridium Difficile}. We achieve an area under the receiver operating characteristic curve of 0.79 on a held-out set of several hundred patients. Our two-stage approach to risk stratification outperforms classifiers that consider only a patient's current state (p$<$0.05).",
    "authors": [
      "Wiens, Jenna",
      "Horvitz, Eric",
      "Guttag, John"
    ]
  },
  {
    "id": "3cef96dcc9b8035d23f69e30bb19218a",
    "title": "Meta-Gaussian Information Bottleneck",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/3cef96dcc9b8035d23f69e30bb19218a-Paper.pdf",
    "abstract": "We present a reformulation of the information bottleneck (IB) problem in terms of copula, using the equivalence between mutual information and negative copula  entropy. Focusing on the Gaussian copula we extend the analytical IB solution available for the multivariate Gaussian case to distributions with a Gaussian dependence structure but arbitrary marginal densities, also called meta-Gaussian distributions. This opens new possibles applications of IB to continuous data and provides a solution more robust to outliers.",
    "authors": [
      "Rey, Melanie",
      "Roth, Volker"
    ]
  },
  {
    "id": "3df1d4b96d8976ff5986393e8767f5b2",
    "title": "Efficient Monte Carlo Counterfactual Regret Minimization in Games with Many Player Actions",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/3df1d4b96d8976ff5986393e8767f5b2-Paper.pdf",
    "abstract": "Counterfactual Regret Minimization (CFR) is a popular, iterative algorithm for computing strategies in extensive-form games. The Monte Carlo CFR (MCCFR) variants reduce the per iteration time cost of CFR by traversing a sampled portion of the tree. The previous most effective instances of MCCFR can still be very slow in games with many player actions since they sample every action for a given player. In this paper, we present a new MCCFR algorithm, Average Strategy Sampling (AS), that samples a subset of the player's actions according to the player's average strategy. Our new algorithm is inspired by a new, tighter bound on the number of iterations required by CFR to converge to a given solution quality. In addition, we prove a similar, tighter bound for AS and other popular MCCFR variants. Finally, we validate our work by demonstrating that AS converges faster than previous MCCFR algorithms in both no-limit poker and Bluff.",
    "authors": [
      "Burch, Neil",
      "Lanctot, Marc",
      "Szafron, Duane",
      "Gibson, Richard"
    ]
  },
  {
    "id": "3e313b9badf12632cdae5452d20e1af6",
    "title": "Fusion with Diffusion for Robust Visual Tracking",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/3e313b9badf12632cdae5452d20e1af6-Paper.pdf",
    "abstract": "A weighted graph is used as an underlying structure of many algorithms like semi-supervised learning and spectral clustering. The edge weights are usually deter-mined by a single similarity measure, but it often hard if not impossible to capture all relevant aspects of similarity when using a single similarity measure. In par-ticular, in the case of visual object matching it is beneficial to integrate different similarity measures that focus on different visual representations. In this paper, a novel approach to integrate multiple similarity measures is pro-posed. First pairs of similarity measures are combined with a diffusion process on their tensor product graph (TPG). Hence the diffused similarity of each pair of ob-jects becomes a function of joint diffusion of the two original similarities, which in turn depends on the neighborhood structure of the TPG. We call this process Fusion with Diffusion (FD). However, a higher order graph like the TPG usually means significant increase in time complexity. This is not the case in the proposed approach. A key feature of our approach is that the time complexity of the dif-fusion on the TPG is the same as the diffusion process on each of the original graphs, Moreover, it is not necessary to explicitly construct the TPG in our frame-work. Finally all diffused pairs of similarity measures are combined as a weighted sum. We demonstrate the advantages of the proposed approach on the task of visual tracking, where different aspects of the appearance similarity between the target object in frame t and target object candidates in frame t+1 are integrated. The obtained method is tested on several challenge video sequences and the experimental results show that it outperforms state-of-the-art tracking methods.",
    "authors": [
      "Zhou, Yu",
      "Bai, Xiang",
      "Liu, Wenyu",
      "Latecki, Longin"
    ]
  },
  {
    "id": "3eae62bba9ddf64f69d49dc48e2dd214",
    "title": "Convolutional-Recursive Deep Learning for 3D Object Classification",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/3eae62bba9ddf64f69d49dc48e2dd214-Paper.pdf",
    "abstract": "Recent advances in 3D sensing technologies make it possible to easily record color and depth images which together can improve object recognition. Most current methods rely on very well-designed features for this new 3D modality. We in- troduce a model based on a combination of convolutional and recursive neural networks (CNN and RNN) for learning features and classifying RGB-D images. The CNN layer learns low-level translationally invariant features which are then given as inputs to multiple, \ufb01xed-tree RNNs in order to compose higher order fea- tures. RNNs can be seen as combining convolution and pooling into one ef\ufb01cient, hierarchical operation. Our main result is that even RNNs with random weights compose powerful features. Our model obtains state of the art performance on a standard RGB-D object dataset while being more accurate and faster during train- ing and testing than comparable architectures such as two-layer CNNs.",
    "authors": [
      "Socher, Richard",
      "Huval, Brody",
      "Bath, Bharath",
      "Manning, Christopher D.",
      "Ng, Andrew"
    ]
  },
  {
    "id": "3f67fd97162d20e6fe27748b5b372509",
    "title": "Transferring Expectations in Model-based Reinforcement Learning",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/3f67fd97162d20e6fe27748b5b372509-Paper.pdf",
    "abstract": "We study how to automatically select and adapt multiple abstractions or representations of the world to support model-based reinforcement learning. We address the challenges of transfer learning in heterogeneous environments with varying tasks. We present an efficient, online framework that, through a sequence of tasks, learns a set of relevant representations to be used in future tasks. Without pre-defined mapping strategies, we introduce a general approach to support transfer learning across different state spaces. We demonstrate the potential impact of our system through improved jumpstart and faster convergence to near optimum policy in two benchmark domains.",
    "authors": [
      "Nguyen, Trung",
      "Silander, Tomi",
      "Leong, Tze"
    ]
  },
  {
    "id": "40cb228987243c91b2dd0b7c9c4a0856",
    "title": "Modelling Reciprocating Relationships with Hawkes Processes",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/40cb228987243c91b2dd0b7c9c4a0856-Paper.pdf",
    "abstract": "We present a Bayesian nonparametric model that discovers implicit social structure from interaction time-series data. Social groups are often formed implicitly, through actions among members of groups. Yet many models of social networks use explicitly declared relationships to infer social structure. We consider a particular class of Hawkes processes, a doubly stochastic point process, that is able to model reciprocity between groups of individuals. We then extend the Infinite Relational Model by using these reciprocating Hawkes processes to parameterise its edges, making events associated with edges co-dependent through time. Our model outperforms general, unstructured Hawkes processes as well as structured Poisson process-based models at predicting verbal and email turn-taking, and military conflicts among nations.",
    "authors": [
      "Blundell, Charles",
      "Beck, Jeff",
      "Heller, Katherine A."
    ]
  },
  {
    "id": "4122cb13c7a474c1976c9706ae36521d",
    "title": "Ancestor Sampling for Particle Gibbs",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/4122cb13c7a474c1976c9706ae36521d-Paper.pdf",
    "abstract": "We present a novel method in the family of particle MCMC methods that we refer to as particle Gibbs with ancestor sampling (PG-AS). Similarly to the existing PG with backward simulation (PG-BS) procedure, we use backward sampling to (considerably) improve the mixing of the PG kernel. Instead of using separate forward and backward sweeps as in PG-BS, however, we achieve the same effect in a single forward sweep. We apply the PG-AS framework to the challenging class of non-Markovian state-space models. We develop a truncation strategy of these models that is applicable in principle to any backward-simulation-based method, but which is particularly well suited to the PG-AS framework. In particular, as we show in a simulation study, PG-AS can yield an order-of-magnitude improved accuracy relative to PG-BS due to its robustness to the truncation error. Several application examples are discussed, including Rao-Blackwellized particle smoothing and inference in degenerate state-space models.",
    "authors": [
      "Lindsten, Fredrik",
      "Sch\u00f6n, Thomas",
      "Jordan, Michael"
    ]
  },
  {
    "id": "41a60377ba920919939d83326ebee5a1",
    "title": "Interpreting prediction markets: a stochastic approach",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/41a60377ba920919939d83326ebee5a1-Paper.pdf",
    "abstract": "We strengthen recent connections between prediction markets and learning by showing that a natural class of market makers can be understood as performing stochastic mirror descent when trader demands are sequentially drawn from a fixed distribution. This provides new insights into how market prices (and price paths) may be interpreted as a summary of the market's belief distribution by relating them to the optimization problem being solved. In particular, we show that the stationary point of the stochastic process of prices generated by the market is equal to the market's Walrasian equilibrium of classic market analysis. Together, these results suggest how traditional market making mechanisms might be replaced with general purpose learning algorithms while still retaining guarantees about their behaviour.",
    "authors": [
      "Frongillo, Rafael",
      "Della Penna, Nichol\u00e1s",
      "Reid, Mark D."
    ]
  },
  {
    "id": "41ae36ecb9b3eee609d05b90c14222fb",
    "title": "Nonparanormal Belief Propagation (NPNBP)",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/41ae36ecb9b3eee609d05b90c14222fb-Paper.pdf",
    "abstract": "The empirical success of the belief propagation approximate inference algorithm has inspired numerous theoretical and algorithmic advances. Yet, for continuous non-Gaussian domains performing belief propagation remains a challenging task: recent innovations such as nonparametric or kernel belief propagation, while useful, come with a substantial computational cost and offer little theoretical guarantees, even for tree structured models.  In this work we present Nonparanormal BP  for performing efficient inference on distributions parameterized by  a Gaussian copulas network and any univariate marginals. For  tree structured networks, our approach is guaranteed to be exact for  this powerful class of non-Gaussian models. Importantly, the method  is as efficient as standard Gaussian BP, and its convergence properties do not depend on the complexity of the univariate marginals, even when a nonparametric representation is used.",
    "authors": [
      "Elidan, Gal",
      "Cario, Cobi"
    ]
  },
  {
    "id": "42a0e188f5033bc65bf8d78622277c4e",
    "title": "Adaptive Stratified Sampling for Monte-Carlo integration of Differentiable functions",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/42a0e188f5033bc65bf8d78622277c4e-Paper.pdf",
    "abstract": "We consider the problem of adaptive stratified sampling for Monte Carlo integration of a differentiable function given a finite number of evaluations to the function. We construct a sampling scheme that samples more often in regions where the function oscillates more, while allocating the samples such that they are well spread on the domain (this notion shares similitude with low discrepancy). We prove that the estimate returned by the algorithm is almost as accurate as the estimate that an optimal oracle strategy (that would know the variations of the function everywhere) would return, and we provide a finite-sample analysis.",
    "authors": [
      "Carpentier, Alexandra",
      "Munos, R\u00e9mi"
    ]
  },
  {
    "id": "430c3626b879b4005d41b8a46172e0c0",
    "title": "Bayesian Nonparametric Modeling of Suicide Attempts",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/430c3626b879b4005d41b8a46172e0c0-Paper.pdf",
    "abstract": "The National Epidemiologic Survey on Alcohol and Related Conditions (NESARC) database contains a large amount of information, regarding the way of life, medical conditions, depression, etc., of a representative sample of the U.S. population. In the present paper, we are interested in seeking the hidden causes behind the suicide attempts, for which we propose to model the subjects using a nonparametric latent model based on the Indian Buffet Process (IBP). Due to the nature of the data, we need to adapt the observation model for discrete random variables. We propose a generative model in which the observations are drawn from a multinomial-logit distribution given the IBP matrix. The implementation of an efficient Gibbs sampler is accomplished using the Laplace approximation, which allows us to integrate out the weighting factors of the  multinomial-logit likelihood model. Finally, the experiments over the NESARC database show that our model properly captures some of the hidden causes that model suicide attempts.",
    "authors": [
      "Ruiz, Francisco",
      "Valera, Isabel",
      "Blanco, Carlos",
      "P\u00e9rez-Cruz, Fernando"
    ]
  },
  {
    "id": "43cca4b3de2097b9558efefd0ecc3588",
    "title": "Non-linear Metric Learning",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/43cca4b3de2097b9558efefd0ecc3588-Paper.pdf",
    "abstract": "In this paper, we introduce two novel metric learning algorithms, \u03c72-LMNN and GB-LMNN, which are explicitly designed to be non-linear and  easy-to-use. The two approaches achieve this goal in fundamentally different ways: \u03c72-LMNN inherits the computational benefits of a linear mapping from linear metric learning, but uses a non-linear \u03c72-distance to explicitly capture similarities within histogram data sets; GB-LMNN applies gradient-boosting to learn non-linear mappings directly in function space and takes advantage of this approach's robustness, speed, parallelizability and insensitivity towards the single additional hyper-parameter. On various benchmark data sets, we demonstrate these methods not only match the current state-of-the-art in terms of kNN classification error, but in the case of \u03c72-LMNN, obtain best results in 19 out of 20 learning settings.",
    "authors": [
      "Kedem, Dor",
      "Tyree, Stephen",
      "Sha, Fei",
      "Lanckriet, Gert",
      "Weinberger, Kilian Q."
    ]
  },
  {
    "id": "43ec517d68b6edd3015b3edc9a11367b",
    "title": "Putting Bayes to sleep",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/43ec517d68b6edd3015b3edc9a11367b-Paper.pdf",
    "abstract": "We consider sequential prediction algorithms that are given the predictions from a set of models as inputs. If the nature of the data is changing over time in that different models predict well on different segments of the data, then adaptivity is typically achieved by mixing into the weights in each round a bit of the initial prior (kind of like a weak restart). However, what if the favored models in each segment are from a small subset, i.e. the data is likely to be predicted well by models that predicted well before? Curiously, fitting such ''sparse composite models'' is achieved by mixing in a bit of all the past posteriors. This self-referential updating method is rather peculiar, but it is efficient and gives superior performance on many natural data sets. Also it is important because it introduces a long-term memory: any model that has done well in the past can be recovered quickly. While Bayesian interpretations can be found for mixing in a bit of the initial prior, no Bayesian interpretation is known for mixing in past posteriors.  We build atop the ''specialist'' framework from the online learning literature to give the Mixing Past Posteriors update a proper Bayesian foundation. We apply our method to a well-studied multitask learning problem and obtain a new intriguing efficient update that achieves a significantly better bound.",
    "authors": [
      "Adamskiy, Dmitry",
      "Warmuth, Manfred K. K.",
      "Koolen, Wouter M."
    ]
  },
  {
    "id": "4476b929e30dd0c4e8bdbcc82c6ba23a",
    "title": "Sparse Approximate Manifolds for Differential Geometric MCMC",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/4476b929e30dd0c4e8bdbcc82c6ba23a-Paper.pdf",
    "abstract": "One of the enduring challenges in Markov chain Monte Carlo methodology is the development of proposal mechanisms to make moves distant from the current point, that are accepted with high probability and at low computational cost. The recent introduction of locally adaptive MCMC methods based on the natural underlying Riemannian geometry of such models goes some way to alleviating these problems for certain classes of models for which the metric tensor is analytically tractable, however computational efficiency is not assured due to the necessity of potentially high-dimensional matrix operations at each iteration. In this paper we firstly investigate a sampling-based approach for approximating the metric tensor and suggest a valid MCMC algorithm that extends the applicability of Riemannian Manifold MCMC methods to statistical models that do not admit an analytically computable metric tensor. Secondly, we show how the approximation scheme we consider naturally motivates the use of l1 regularisation to improve estimates and obtain a sparse approximate inverse of the metric, which enables stable and sparse approximations of the local geometry to be made. We demonstrate the application of this algorithm for inferring the parameters of a realistic system of ordinary differential equations using a biologically motivated robust student-t error model, for which the expected Fisher Information is analytically intractable.",
    "authors": [
      "Calderhead, Ben",
      "Sustik, M\u00e1ty\u00e1s"
    ]
  },
  {
    "id": "4588e674d3f0faf985047d4c3f13ed0d",
    "title": "Bayesian active learning with localized priors for fast receptive field characterization",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/4588e674d3f0faf985047d4c3f13ed0d-Paper.pdf",
    "abstract": "Active learning can substantially improve the yield of neurophysiology experiments by adaptively selecting stimuli to probe a neuron's receptive field (RF) in real time. Bayesian active learning methods maintain a posterior distribution over the RF, and select stimuli to maximally reduce posterior entropy on each time step.  However, existing methods tend to rely on simple Gaussian priors, and do not exploit uncertainty at the level of hyperparameters when determining an optimal stimulus.  This uncertainty can play a substantial role in RF characterization, particularly when RFs are smooth, sparse, or local in space and time.  In this paper, we describe a novel framework for active learning under hierarchical, conditionally Gaussian priors.  Our algorithm uses sequential Markov Chain Monte Carlo sampling (''particle filtering'' with MCMC) over hyperparameters to construct a mixture-of-Gaussians representation of the RF posterior, and selects optimal stimuli using an approximate infomax criterion.  The core elements of this algorithm are parallelizable, making it computationally efficient for real-time experiments.  We apply our algorithm to simulated and real neural data, and show that it can provide highly accurate receptive field estimates from very limited data, even with a small number of hyperparameter samples.",
    "authors": [
      "Park, Mijung",
      "Pillow, Jonathan"
    ]
  },
  {
    "id": "459a4ddcb586f24efd9395aa7662bc7c",
    "title": "Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/459a4ddcb586f24efd9395aa7662bc7c-Paper.pdf",
    "abstract": "We address a central problem of neuroanatomy, namely, the automatic segmentation of neuronal structures  depicted in stacks of electron microscopy (EM) images. This is necessary to efficiently map 3D brain structure and connectivity. To segment {\\em biological} neuron membranes, we use a special type of deep {\\em artificial} neural network as a pixel classifier. The label of each pixel (membrane or non-membrane) is predicted from raw pixel values in a square window centered on it. The input layer maps each window pixel to a neuron. It is followed by a succession of convolutional and max-pooling layers which preserve 2D information and extract features with increasing levels of abstraction. The output layer produces a calibrated probability for each class. The classifier is trained by plain gradient descent on a $512 \\times 512 \\times 30$ stack with known ground truth, and tested on a stack of the same  size (ground truth unknown to the authors) by the organizers of the ISBI 2012 EM Segmentation Challenge.  Even without problem-specific post-processing, our approach outperforms competing techniques by a large margin in all three considered metrics, i.e. \\emph{rand error}, \\emph{warping error} and \\emph{pixel error}.  For pixel error, our approach is the only one outperforming a second human observer.",
    "authors": [
      "Ciresan, Dan",
      "Giusti, Alessandro",
      "Gambardella, Luca",
      "Schmidhuber, J\u00fcrgen"
    ]
  },
  {
    "id": "46489c17893dfdcf028883202cefd6d1",
    "title": "Learning from the Wisdom of Crowds by Minimax Entropy",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/46489c17893dfdcf028883202cefd6d1-Paper.pdf",
    "abstract": "An important way to make large training sets is to gather noisy labels from crowds of nonexperts. We propose a minimax entropy principle to improve the quality of these labels. Our method assumes that labels are generated by a probability distribution over workers, items, and labels. By maximizing the entropy of this distribution, the method naturally infers item confusability and worker expertise. We infer the ground truth by minimizing the entropy of this distribution, which we show minimizes the Kullback-Leibler (KL) divergence between the probability distribution and the unknown truth. We show that a simple coordinate descent scheme can optimize minimax entropy. Empirically, our results are substantially better than previously published methods for the same problem.",
    "authors": [
      "Zhou, Dengyong",
      "Basu, Sumit",
      "Mao, Yi",
      "Platt, John"
    ]
  },
  {
    "id": "470e7a4f017a5476afb7eeb3f8b96f9b",
    "title": "Parametric Local Metric Learning for Nearest Neighbor Classification",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/470e7a4f017a5476afb7eeb3f8b96f9b-Paper.pdf",
    "abstract": "We study the problem of learning local metrics for nearest neighbor classification. Most previous works on local metric learning learn a number of local unrelated metrics. While this ''independence'' approach delivers an increased flexibility its downside is the considerable risk of overfitting. We present a new parametric local metric learning method in which we learn a smooth metric matrix function over the data manifold. Using an approximation error bound of the metric matrix function we learn local metrics as linear combinations of basis metrics defined on anchor points over different regions of the instance space.  We constrain the metric matrix function by imposing on the linear combinations manifold regularization which makes the learned metric matrix function vary smoothly along the geodesics of the data manifold. Our metric learning method has excellent performance both in terms of predictive power and scalability. We experimented with several large-scale classification problems, tens of thousands of instances, and compared it with several state of the art metric learning methods, both global and local, as well as to SVM with automatic kernel selection, all of which it outperforms in a significant manner.",
    "authors": [
      "Wang, Jun",
      "Kalousis, Alexandros",
      "Woznica, Adam"
    ]
  },
  {
    "id": "47a658229eb2368a99f1d032c8848542",
    "title": "The topographic unsupervised learning of natural sounds in the auditory cortex",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/47a658229eb2368a99f1d032c8848542-Paper.pdf",
    "abstract": "The computational modelling of the primary auditory cortex (A1) has been less fruitful than that of the primary visual cortex (V1) due to the less organized properties of A1. Greater disorder has recently been demonstrated for the tonotopy of A1 that has traditionally been considered to be as ordered as the retinotopy of V1. This disorder appears to be incongruous, given the uniformity of the neocortex; however, we hypothesized that both A1 and V1 would adopt an efficient coding strategy and that the disorder in A1 reflects natural sound statistics. To provide a computational model of the tonotopic disorder in A1, we used a model that was originally proposed for the smooth V1 map. In contrast to natural images, natural sounds exhibit distant correlations, which were learned and reflected in the disordered map. The auditory model predicted harmonic relationships among neighbouring A1 cells; furthermore, the same mechanism used to model V1 complex cells reproduced nonlinear responses similar to the pitch selectivity. These results contribute to the understanding of the sensory cortices of different modalities in a novel and integrated manner.",
    "authors": [
      "Terashima, Hiroki",
      "Okada, Masato"
    ]
  },
  {
    "id": "4c27cea8526af8cfee3be5e183ac9605",
    "title": "Efficient Sampling for Bipartite Matching Problems",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/4c27cea8526af8cfee3be5e183ac9605-Paper.pdf",
    "abstract": "Bipartite matching problems characterize many situations, ranging from ranking in information retrieval to correspondence in vision. Exact inference in real-world applications of these problems is intractable, making efficient approximation methods essential for learning and inference. In this paper we propose a novel {\\it sequential matching} sampler based on the generalization of the Plackett-Luce model, which can effectively make large moves in the space of matchings. This allows the sampler to match the difficult target distributions common in these problems: highly multimodal distributions with well separated modes. We present experimental results with bipartite matching problems - ranking and image correspondence - which show that the sequential matching sampler efficiently approximates the target distribution, significantly outperforming other sampling approaches.",
    "authors": [
      "Volkovs, Maksims",
      "Zemel, Richard"
    ]
  },
  {
    "id": "4c5bde74a8f110656874902f07378009",
    "title": "Volume Regularization for Binary Classification",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/4c5bde74a8f110656874902f07378009-Paper.pdf",
    "abstract": "We introduce a large-volume box classification for binary   prediction, which maintains a subset of weight vectors, and   specifically axis-aligned boxes. Our learning algorithm seeks for a   box of large volume that contains ``simple'' weight vectors which   most of are accurate on the training set. Two versions of the   learning process are cast as convex optimization problems, and it   is shown how to solve them efficiently.  The formulation yields a   natural PAC-Bayesian performance bound and it is shown to minimize a   quantity directly aligned with it. The algorithm outperforms SVM and   the recently proposed AROW algorithm on a majority of $30$ NLP   datasets and binarized USPS optical character recognition datasets.",
    "authors": [
      "Crammer, Koby",
      "Wagner, Tal"
    ]
  },
  {
    "id": "4d5b995358e7798bc7e9d9db83c612a5",
    "title": "Causal discovery with scale-mixture model for spatiotemporal variance dependencies",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/4d5b995358e7798bc7e9d9db83c612a5-Paper.pdf",
    "abstract": "In conventional causal discovery, structural equation models (SEM) are directly applied to the observed variables, meaning that the causal effect can be represented as a function of the direct causes themselves. However, in many real world problems, there are significant dependencies in the variances or energies, which indicates that causality may possibly take place at the level of variances or energies. In this paper, we propose a probabilistic causal scale-mixture model with spatiotemporal variance dependencies to represent a specific type of generating mechanism of the observations. In particular, the causal mechanism including contemporaneous and temporal causal relations in variances or energies is represented by a Structural Vector AutoRegressive model (SVAR). We prove the identifiability of this model under the non-Gaussian assumption on the innovation processes. We also propose algorithms to estimate the involved parameters and discover the contemporaneous causal structure. Experiments on synthesis and real world data are conducted to show the applicability of the proposed model and algorithms.",
    "authors": [
      "Chen, Zhitang",
      "Zhang, Kun",
      "Chan, Laiwan"
    ]
  },
  {
    "id": "4daa3db355ef2b0e64b472968cb70f0d",
    "title": "Neurally Plausible Reinforcement Learning of Working Memory Tasks",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/4daa3db355ef2b0e64b472968cb70f0d-Paper.pdf",
    "abstract": "A key function of brains is undoubtedly the abstraction and maintenance of information from the environment for later use. Neurons in association cortex play an important role in this process: during learning these neurons become tuned to relevant features and represent the information that is required later as a persistent elevation of their activity. It is however not well known how these neurons acquire their task-relevant tuning. Here we introduce a biologically plausible learning scheme that explains how neurons become selective for relevant information when animals learn by trial and error. We propose that the action selection stage feeds back attentional signals to earlier processing levels. These feedback signals interact with feedforward signals to form synaptic tags at those connections that are responsible for the stimulus-response mapping. A globally released neuromodulatory signal interacts with these tagged synapses to determine the sign and strength of plasticity. The learning scheme is generic because it can train networks in different tasks, simply by varying inputs and rewards. It explains how neurons in association cortex learn to (1) temporarily store task-relevant information in non-linear stimulus-response mapping tasks and (2) learn to optimally integrate probabilistic evidence for perceptual decision making.",
    "authors": [
      "Rombouts, Jaldert",
      "Roelfsema, Pieter",
      "Bohte, Sander"
    ]
  },
  {
    "id": "4dcae38ee11d3a6606cc6cd636a3628b",
    "title": "Simultaneously Leveraging Output and Task Structures for Multiple-Output Regression",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/4dcae38ee11d3a6606cc6cd636a3628b-Paper.pdf",
    "abstract": "Multiple-output regression models require estimating multiple functions, one for each output. To improve parameter estimation in such models, methods based on structural regularization of the model parameters are usually needed. In this paper, we present a multiple-output regression model that leverages the covariance structure of the functions (i.e., how the multiple functions are related with each other) as well as the conditional covariance structure of the outputs. This is in contrast with existing methods that usually take into account only one of these structures. More importantly, unlike most of the other existing methods, none of these structures need be known a priori in our model, and are learned from the data. Several previously proposed structural regularization based  multiple-output regression models turn out to be special cases of our model. Moreover, in addition to being a rich model for multiple-output regression, our model can also be used in estimating the graphical model structure of a set of variables (multivariate outputs) conditioned on another set of variables (inputs). Experimental results on both synthetic and real datasets demonstrate the effectiveness of our method.",
    "authors": [
      "Rai, Piyush",
      "Kumar, Abhishek",
      "Daume, Hal"
    ]
  },
  {
    "id": "4e732ced3463d06de0ca9a15b6153677",
    "title": "Feature Clustering for Accelerating Parallel Coordinate Descent",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/4e732ced3463d06de0ca9a15b6153677-Paper.pdf",
    "abstract": "Large scale $\\ell_1$-regularized loss minimization problems arise in numerous applications such as compressed sensing and high dimensional supervised learning, including classification and regression problems.  High performance algorithms and implementations are critical to efficiently solving these problems.  Building upon previous work on coordinate descent algorithms for $\\ell_1$ regularized problems, we introduce a novel family of algorithms called block-greedy coordinate descent that includes, as special cases, several existing algorithms such as SCD, Greedy CD, Shotgun, and Thread-greedy.  We give a unified convergence analysis for the family of block-greedy algorithms.  The analysis suggests that block-greedy coordinate descent can better exploit parallelism if features are clustered so that the maximum inner product between features in different blocks is small.  Our theoretical convergence analysis is supported with experimental results using data from diverse real-world applications.  We hope that algorithmic approaches and convergence analysis we provide will not only advance the field, but will also encourage researchers to systematically explore the design space of algorithms for solving large-scale $\\ell_1$-regularization problems.",
    "authors": [
      "Scherrer, Chad",
      "Tewari, Ambuj",
      "Halappanavar, Mahantesh",
      "Haglin, David"
    ]
  },
  {
    "id": "4fac9ba115140ac4f1c22da82aa0bc7f",
    "title": "Phoneme Classification using Constrained Variational Gaussian Process Dynamical System",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/4fac9ba115140ac4f1c22da82aa0bc7f-Paper.pdf",
    "abstract": "This paper describes a new acoustic model based on variational Gaussian process dynamical system (VGPDS) for phoneme classification. The proposed model overcomes the limitations of the classical HMM in modeling the real speech data, by adopting a nonlinear and nonparametric model. In our model, the GP prior on the dynamics function enables representing the complex dynamic structure of speech, while the GP prior on the emission function successfully models the global dependency over the observations. Additionally, we introduce variance constraint to the original VGPDS for mitigating sparse approximation error of the kernel matrix. The effectiveness of the proposed model is demonstrated with extensive experimental results including parameter estimation, classification performance on the synthetic and benchmark datasets.",
    "authors": [
      "Park, Hyunsin",
      "Yun, Sungrack",
      "Park, Sanghyuk",
      "Kim, Jongmin",
      "Yoo, Chang"
    ]
  },
  {
    "id": "50905d7b2216bfeccb5b41016357176b",
    "title": "Fast Variational Inference in the Conjugate Exponential Family",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/50905d7b2216bfeccb5b41016357176b-Paper.pdf",
    "abstract": "We present a general method for deriving collapsed variational inference algorithms for probabilistic models in the conjugate exponential family. Our method unifies many existing approaches to collapsed variational inference. Our collapsed variational inference leads to a new lower bound on the marginal likelihood. We exploit the information geometry of the bound to derive much faster optimization methods based on conjugate gradients for these models. Our approach is very general and is easily applied to any model where the mean field update equations have been derived. Empirically we show significant speed-ups for probabilistic models optimized using our bound.",
    "authors": [
      "Hensman, James",
      "Rattray, Magnus",
      "Lawrence, Neil"
    ]
  },
  {
    "id": "50c3d7614917b24303ee6a220679dab3",
    "title": "Identifiability and Unmixing of Latent Parse Trees",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/50c3d7614917b24303ee6a220679dab3-Paper.pdf",
    "abstract": "This paper explores unsupervised learning of parsing models along two directions.  First, which models are identifiable from infinite data?  We use a general technique for numerically checking identifiability based on the rank of a Jacobian matrix, and apply it to several standard constituency and dependency parsing models.  Second, for identifiable models, how do we estimate the parameters efficiently?  EM suffers from local optima, while recent work using spectral methods cannot be directly applied since the topology of the parse tree varies across sentences.  We develop a strategy, unmixing, which deals with this additional complexity for restricted classes of parsing models.",
    "authors": [
      "Hsu, Daniel J.",
      "Kakade, Sham M.",
      "Liang, Percy S."
    ]
  },
  {
    "id": "50f3f8c42b998a48057e9d33f4144b8b",
    "title": "On the (Non-)existence of Convex, Calibrated Surrogate Losses for Ranking",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/50f3f8c42b998a48057e9d33f4144b8b-Paper.pdf",
    "abstract": "We study surrogate losses for learning to rank, in a framework where the rankings are induced by scores and the task is to learn the scoring function. We focus on the calibration of surrogate losses with respect to a ranking evaluation metric, where the calibration is equivalent to the guarantee that near-optimal values of the sur- rogate risk imply near-optimal values of the risk de\ufb01ned by the evaluation metric. We prove that if a surrogate loss is a convex function of the scores, then it is not calibrated with respect to two evaluation metrics widely used for search engine evaluation, namely the Average Precision and the Expected Reciprocal Rank. We also show that such convex surrogate losses cannot be calibrated with respect to the Pairwise Disagreement, an evaluation metric used when learning from pair- wise preferences. Our results cast lights on the intrinsic dif\ufb01culty of some ranking problems, as well as on the limitations of learning-to-rank algorithms based on the minimization of a convex surrogate risk.",
    "authors": [
      "Calauz\u00e8nes, Cl\u00e9ment",
      "Usunier, Nicolas",
      "Gallinari, Patrick"
    ]
  },
  {
    "id": "512c5cad6c37edb98ae91c8a76c3a291",
    "title": "Learning with Partially Absorbing Random Walks",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/512c5cad6c37edb98ae91c8a76c3a291-Paper.pdf",
    "abstract": "We propose a novel stochastic process that is with probability $\\alpha_i$ being absorbed at current state $i$, and with probability $1-\\alpha_i$ follows a random edge out of it. We analyze its properties and show its potential for exploring graph structures. We prove that under proper absorption rates, a random walk starting from a set $\\mathcal{S}$ of low conductance will be mostly absorbed in $\\mathcal{S}$. Moreover, the absorption probabilities vary slowly inside $\\mathcal{S}$, while dropping sharply outside $\\mathcal{S}$, thus implementing the desirable cluster assumption for graph-based learning. Remarkably, the partially absorbing process unifies many popular models arising in a variety of contexts, provides new insights into them, and makes it possible for transferring findings from one paradigm to another. Simulation results demonstrate its promising applications in graph-based learning.",
    "authors": [
      "Wu, Xiao-ming",
      "Li, Zhenguo",
      "So, Anthony",
      "Wright, John",
      "Chang, Shih-fu"
    ]
  },
  {
    "id": "53adaf494dc89ef7196d73636eb2451b",
    "title": "Relax and Randomize : From Value to Algorithms",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/53adaf494dc89ef7196d73636eb2451b-Paper.pdf",
    "abstract": "We show a principled way of deriving online learning algorithms from a minimax analysis. Various upper bounds on the minimax value, previously thought to be non-constructive, are shown to yield algorithms. This allows us to seamlessly recover known methods and to derive new ones, also capturing such ''unorthodox'' methods as Follow the Perturbed Leader and the R^2 forecaster. Understanding the inherent complexity of the learning problem thus leads to the development of algorithms. To illustrate our approach, we present several new algorithms, including a family of randomized methods that use the idea of a ''random play out''. New versions of the Follow-the-Perturbed-Leader algorithms are presented, as well as methods based on the Littlestone's dimension, efficient methods for matrix completion with trace norm, and algorithms for the problems of transductive learning and prediction with static experts.",
    "authors": [
      "Rakhlin, Sasha",
      "Shamir, Ohad",
      "Sridharan, Karthik"
    ]
  },
  {
    "id": "559cb990c9dffd8675f6bc2186971dc2",
    "title": "Inverse Reinforcement Learning through Structured Classification",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/559cb990c9dffd8675f6bc2186971dc2-Paper.pdf",
    "abstract": "This paper adresses the inverse reinforcement learning (IRL) problem, that is inferring a reward for which a demonstrated expert behavior is optimal. We introduce a new algorithm, SCIRL, whose principle is to use the so-called feature expectation of the expert as the parameterization of the score function of a multi-class classifier. This approach produces a reward function for which the expert policy is provably near-optimal. Contrary to most of existing IRL algorithms, SCIRL does not require solving the direct RL problem. Moreover, with an appropriate heuristic, it can succeed with only trajectories sampled according to the expert behavior. This is illustrated on a car driving simulator.",
    "authors": [
      "Klein, Edouard",
      "Geist, Matthieu",
      "Piot, Bilal",
      "Pietquin, Olivier"
    ]
  },
  {
    "id": "56468d5607a5aaf1604ff5e15593b003",
    "title": "Efficient and direct estimation of a neural subunit model for sensory coding",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/56468d5607a5aaf1604ff5e15593b003-Paper.pdf",
    "abstract": "Many visual and auditory neurons have response properties that are well explained by pooling the rectified responses of a set of self-similar linear filters. These filters cannot be found using spike-triggered averaging (STA), which estimates only a single filter. Other methods, like spike-triggered covariance (STC), define a multi-dimensional response subspace, but require substantial amounts of data and do not produce unique estimates of the linear filters. Rather, they provide a linear basis for the subspace in which the filters reside. Here, we define a 'subunit' model as an LN-LN cascade, in which the first linear stage is restricted to a set of shifted (\"convolutional\") copies of a common filter, and the first nonlinear stage consists of rectifying nonlinearities that are identical for all filter outputs; we refer to these initial LN elements as the 'subunits' of the receptive field. The second linear stage then computes a weighted sum of the responses of the rectified subunits. We present a method for directly fitting this model to spike data. The method performs well for both simulated and real data (from primate V1), and the resulting model outperforms STA and STC in terms of both cross-validated accuracy and efficiency.",
    "authors": [
      "Vintch, Brett",
      "Zaharia, Andrew",
      "Movshon, J",
      "Simoncelli, Eero"
    ]
  },
  {
    "id": "573f7f25b7b1eb79a4ec6ba896debefd",
    "title": "Discriminative Learning of Sum-Product Networks",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/573f7f25b7b1eb79a4ec6ba896debefd-Paper.pdf",
    "abstract": "Sum-product networks are a new deep architecture that can perform fast, exact in- ference on high-treewidth models. Only generative methods for training SPNs have been proposed to date. In this paper, we present the \ufb01rst discriminative training algorithms for SPNs, combining the high accuracy of the former with the representational power and tractability of the latter. We show that the class of tractable discriminative SPNs is broader than the class of tractable generative ones, and propose an ef\ufb01cient backpropagation-style algorithm for computing the gradient of the conditional log likelihood. Standard gradient descent suffers from the diffusion problem, but networks with many layers can be learned reliably us- ing \u201chard\u201d gradient descent, where marginal inference is replaced by MPE infer- ence (i.e., inferring the most probable state of the non-evidence variables). The resulting updates have a simple and intuitive form. We test discriminative SPNs on standard image classi\ufb01cation tasks. We obtain the best results to date on the CIFAR-10 dataset, using fewer features than prior methods with an SPN architec- ture that learns local image structure discriminatively. We also report the highest published test accuracy on STL-10 even though we only use the labeled portion of the dataset.",
    "authors": [
      "Gens, Robert",
      "Domingos, Pedro"
    ]
  },
  {
    "id": "5751ec3e9a4feab575962e78e006250d",
    "title": "Stochastic optimization and sparse statistical recovery: Optimal algorithms for high dimensions",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/5751ec3e9a4feab575962e78e006250d-Paper.pdf",
    "abstract": "We develop and analyze stochastic optimization algorithms for problems in which the expected loss is strongly convex, and the optimum is (approximately) sparse. Previous approaches are able to exploit only one of these two structures, yielding a $\\order(\\pdim/T)$ convergence rate for strongly convex objectives in $\\pdim$ dimensions and $\\order(\\sqrt{\\spindex( \\log\\pdim)/T})$ convergence rate when the optimum is $\\spindex$-sparse. Our algorithm is based on successively solving a series of $\\ell_1$-regularized optimization problems using Nesterov's dual averaging algorithm. We establish that the error of our solution after $T$ iterations is at most $\\order(\\spindex(\\log\\pdim)/T)$, with natural extensions to approximate sparsity. Our results apply to locally Lipschitz losses including the logistic, exponential, hinge and least-squares losses. By recourse to statistical minimax results, we show that our convergence rates are optimal up to constants. The effectiveness of our approach is also confirmed in numerical simulations where we compare to several baselines on a least-squares regression problem.",
    "authors": [
      "Agarwal, Alekh",
      "Negahban, Sahand",
      "Wainwright, Martin J."
    ]
  },
  {
    "id": "57aeee35c98205091e18d1140e9f38cf",
    "title": "Bandit Algorithms boost Brain Computer Interfaces for motor-task selection of a brain-controlled button",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/57aeee35c98205091e18d1140e9f38cf-Paper.pdf",
    "abstract": "A brain-computer interface (BCI) allows users to \u201ccommunicate\u201d with a computer without using their muscles. BCI based on sensori-motor rhythms use imaginary motor tasks, such as moving the right or left hand to send control signals. The performances of a BCI can vary greatly across users but also depend on the tasks used, making the problem of appropriate task selection an important issue. This study presents a new procedure to automatically select as fast as possible a discriminant motor task for a brain-controlled button. We develop for this purpose an adaptive algorithm UCB-classif based on the stochastic bandit theory. This shortens the training stage, thereby allowing the exploration of a greater variety of tasks. By not wasting time on inefficient tasks, and focusing on the most promising ones, this algorithm results in a faster task selection and a more efficient use of the BCI training session. Comparing the proposed method to the standard practice in task selection, for a fixed time budget, UCB-classif leads to an improve classification rate, and for a fix classification rate, to a reduction of the time spent in training by 50%.",
    "authors": [
      "Fruitet, Joan",
      "Carpentier, Alexandra",
      "Clerc, Maureen",
      "Munos, R\u00e9mi"
    ]
  },
  {
    "id": "58238e9ae2dd305d79c2ebc8c1883422",
    "title": "Localizing 3D cuboids in single-view images",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/58238e9ae2dd305d79c2ebc8c1883422-Paper.pdf",
    "abstract": "In this paper we seek to detect rectangular cuboids and localize their corners in uncalibrated single-view images depicting everyday scenes. In contrast to recent approaches that rely on detecting vanishing points of the scene and grouping line segments to form cuboids, we build a discriminative parts-based detector that models the appearance of the cuboid corners and internal edges while enforcing consistency to a 3D cuboid model. Our model is invariant to the different 3D viewpoints and aspect ratios and is able to detect cuboids across many different object categories. We introduce a database of images with cuboid annotations that spans a variety of indoor and outdoor scenes and show qualitative and quantitative results on our collected database. Our model out-performs baseline detectors that use 2D constraints alone on the task of localizing cuboid corners.",
    "authors": [
      "Xiao, Jianxiong",
      "Russell, Bryan",
      "Torralba, Antonio"
    ]
  },
  {
    "id": "58d4d1e7b1e97b258c9ed0b37e02d087",
    "title": "A Convex Formulation for Learning Scale-Free Networks via Submodular Relaxation",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/58d4d1e7b1e97b258c9ed0b37e02d087-Paper.pdf",
    "abstract": "A key problem in statistics and machine learning is the determination of network structure from data. We consider the case where the structure of the graph to be reconstructed is known to be scale-free. We show that in such cases it is natural to formulate structured sparsity inducing priors using submodular functions, and we use their Lovasz extension to obtain a convex relaxation. For tractable classes such as Gaussian graphical models, this leads to a convex optimization problem that can be efficiently solved. We show that our method results in an improvement in the accuracy of reconstructed networks for synthetic data. We also show how our prior encourages scale-free reconstructions on a bioinfomatics dataset.",
    "authors": [
      "Defazio, Aaron",
      "Caetano, Tib\u00e9rio"
    ]
  },
  {
    "id": "59b90e1005a220e2ebc542eb9d950b1e",
    "title": "Hamming Distance Metric Learning",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/59b90e1005a220e2ebc542eb9d950b1e-Paper.pdf",
    "abstract": "Motivated by large-scale multimedia applications we propose to learn mappings from high-dimensional data to binary codes that preserve semantic similarity. Binary codes are well suited to large-scale applications as they are storage efficient and permit exact sub-linear kNN search. The framework is applicable to broad families of mappings, and uses a flexible form of triplet ranking loss. We overcome discontinuous optimization of the discrete mappings by minimizing a piecewise-smooth upper bound on empirical loss, inspired by latent structural SVMs.  We develop a new loss-augmented inference algorithm that is quadratic in the code length.  We show strong retrieval performance on CIFAR-10 and MNIST, with promising classification results using no more than kNN on the binary codes.",
    "authors": [
      "Norouzi, Mohammad",
      "Fleet, David J.",
      "Salakhutdinov, Russ R."
    ]
  },
  {
    "id": "5c04925674920eb58467fb52ce4ef728",
    "title": "Co-Regularized Hashing for Multimodal Data",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/5c04925674920eb58467fb52ce4ef728-Paper.pdf",
    "abstract": "Hashing-based methods provide a very promising approach to large-scale similarity search.  To obtain compact hash codes, a recent trend seeks to learn the hash functions from data automatically.  In this paper, we study hash function learning in the context of multimodal data.  We propose a novel multimodal hash function learning method, called Co-Regularized Hashing (CRH), based on a boosted co-regularization framework.  The hash functions for each bit of the hash codes are learned by solving DC (difference of convex functions) programs, while the learning for multiple bits proceeds via a boosting procedure so that the bias introduced by the hash functions can be sequentially minimized.  We empirically compare CRH with two state-of-the-art multimodal hash function learning methods on two publicly available data sets.",
    "authors": [
      "Zhen, Yi",
      "Yeung, Dit-Yan"
    ]
  },
  {
    "id": "5c936263f3428a40227908d5a3847c0b",
    "title": "The Coloured Noise Expansion and Parameter Estimation of Diffusion Processes",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/5c936263f3428a40227908d5a3847c0b-Paper.pdf",
    "abstract": "Stochastic differential equations (SDE) are a natural tool for modelling systems that are inherently noisy or contain uncertainties that can be modelled as stochastic processes. Crucial to the process of using SDE to build mathematical models is the ability to estimate parameters of those models from observed data. Over the past few decades, significant progress has been made on this problem, but we are still far from having a definitive solution. We describe a novel method of approximating a diffusion process that we show to be useful in Markov chain Monte-Carlo (MCMC) inference algorithms. We take the \u2018white\u2019 noise that drives a diffusion process and decompose it into two terms. The first is a \u2018coloured noise\u2019 term that can be deterministically controlled by a set of auxilliary variables. The second term is small and enables us to form a linear Gaussian \u2018small noise\u2019 approximation. The decomposition allows us to take a diffusion process of interest and cast it in a form that is amenable to sampling by MCMC methods. We explain why many state-of-the-art inference methods fail on highly nonlinear inference problems. We demonstrate experimentally that our method performs well in such situations. Our results show that this method is a promising new tool for use in inference and parameter estimation problems.",
    "authors": [
      "Lyons, Simon",
      "Storkey, Amos J.",
      "S\u00e4rkk\u00e4, Simo"
    ]
  },
  {
    "id": "5d44ee6f2c3f71b73125876103c8f6c4",
    "title": "How Prior Probability Influences Decision Making: A Unifying Probabilistic Model",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/5d44ee6f2c3f71b73125876103c8f6c4-Paper.pdf",
    "abstract": "How does the brain combine prior knowledge with sensory evidence when making decisions under uncertainty? Two competing descriptive models have been proposed based on experimental data.  The first posits an additive offset to a decision variable, implying a static effect of the prior. However, this model is inconsistent with recent data from a motion discrimination task involving temporal integration of uncertain sensory evidence. To explain this data, a second model has been proposed which assumes a time-varying influence of the prior. Here we present a normative model of decision making that incorporates prior knowledge in a principled way.  We show that the additive offset model and the time-varying prior model emerge naturally when decision making is viewed within the framework of partially observable Markov decision processes (POMDPs).  Decision making in the model reduces to (1) computing beliefs given observations and prior information in a Bayesian manner, and (2) selecting actions based on these beliefs to maximize the  expected sum of future rewards. We show that the model can explain both  data previously explained using the additive offset model as well as more  recent data on the time-varying influence of prior knowledge on decision making.",
    "authors": [
      "Huang, Yanping",
      "Hanks, Timothy",
      "Shadlen, Mike",
      "Friesen, Abram L.",
      "Rao, Rajesh P."
    ]
  },
  {
    "id": "5e51eeda0422de44a7cc260b4239d4f9",
    "title": "Structured Learning of Gaussian Graphical Models",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/5e51eeda0422de44a7cc260b4239d4f9-Paper.pdf",
    "abstract": "We consider estimation of multiple high-dimensional Gaussian graphical models corresponding to a single set of nodes under several distinct conditions. We assume that most aspects of the networks are shared, but that there are some structured differences between them. Specifically, the network differences are generated from node perturbations: a few nodes are perturbed across networks, and most or all edges stemming from such nodes differ between networks. This corresponds to a simple model for the mechanism underlying many cancers, in which the gene regulatory network is disrupted  due to the aberrant activity of a few specific genes. We propose to solve this problem using the structured joint graphical lasso, a convex optimization problem that is based upon the use of a novel symmetric overlap norm penalty, which we solve using  an alternating directions method of multipliers algorithm. Our proposal is illustrated on synthetic data and on an application to brain cancer gene expression data.",
    "authors": [
      "Mohan, Karthik",
      "Chung, Mike",
      "Han, Seungyeop",
      "Witten, Daniela",
      "Lee, Su-in",
      "Fazel, Maryam"
    ]
  },
  {
    "id": "5e76bef6e019b2541ff53db39f407a98",
    "title": "Entropy Estimations Using Correlated Symmetric Stable Random Projections",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/5e76bef6e019b2541ff53db39f407a98-Paper.pdf",
    "abstract": "Methods for efficiently estimating the Shannon entropy of data streams have important applications in learning, data mining, and network anomaly detections (e.g., the DDoS attacks). For nonnegative data streams, the method of Compressed Counting (CC) based on maximally-skewed stable random projections can provide accurate estimates of the Shannon entropy using small storage. However, CC is no longer applicable when entries of data streams can be below zero, which is a common scenario when comparing two streams. In this paper, we propose an algorithm for entropy estimation in general data streams which allow negative entries. In our method, the Shannon entropy is approximated by the finite difference of two correlated frequency moments estimated from correlated samples of symmetric stable random variables. Our experiments confirm that this method is able to substantially better approximate the Shannon entropy compared to the prior state-of-the-art.",
    "authors": [
      "Li, Ping",
      "Zhang, Cun-hui"
    ]
  },
  {
    "id": "5fd0b37cd7dbbb00f97ba6ce92bf5add",
    "title": "Coding efficiency and detectability of rate fluctuations with non-Poisson neuronal firing",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/5fd0b37cd7dbbb00f97ba6ce92bf5add-Paper.pdf",
    "abstract": "Statistical features of neuronal spike trains are known to be non-Poisson. Here, we investigate the extent to which the non-Poissonian feature affects the efficiency of transmitting information on fluctuating firing rates. For this purpose, we introduce the Kullbuck-Leibler (KL) divergence as a measure of the efficiency of information encoding, and assume that spike trains are generated by time-rescaled renewal processes. We show that the KL divergence determines the lower bound of the degree of rate fluctuations below which the temporal variation of the firing rates is undetectable from sparse data. We also show that the KL divergence, as well as the lower bound, depends not only on the variability of spikes in terms of the coefficient of variation, but also significantly on the higher-order moments of interspike interval (ISI) distributions. We examine three specific models that are commonly used for describing the stochastic nature of spikes (the gamma, inverse Gaussian (IG) and lognormal ISI distributions), and find that the time-rescaled renewal process with the IG distribution achieves the largest KL divergence, followed by the lognormal and gamma distributions.",
    "authors": [
      "Koyama, Shinsuke"
    ]
  },
  {
    "id": "621bf66ddb7c962aa0d22ac97d69b793",
    "title": "Nystr\u00f6m Method vs Random Fourier Features: A Theoretical and Empirical Comparison",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/621bf66ddb7c962aa0d22ac97d69b793-Paper.pdf",
    "abstract": "Both random Fourier features and the Nystr\u00f6m method have been successfully applied to efficient kernel learning. In this work, we investigate the fundamental difference between these two approaches, and how the difference could affect their generalization performances. Unlike approaches based on random Fourier features  where the basis functions (i.e., cosine and sine functions) are sampled from a distribution  {\\it independent} from the training data, basis functions used by the Nystr\u00f6m method are randomly sampled from the training examples and are therefore {\\it data dependent}. By exploring this difference, we show that when there is a large gap in the eigen-spectrum of the kernel matrix, approaches based the Nystr\u00f6m method can yield  impressively  better generalization error bound than random Fourier features based approach. We empirically verify our theoretical findings on a wide range of large data sets.",
    "authors": [
      "Yang, Tianbao",
      "Li, Yu-feng",
      "Mahdavi, Mehrdad",
      "Jin, Rong",
      "Zhou, Zhi-Hua"
    ]
  },
  {
    "id": "63538fe6ef330c13a05a3ed7e599d5f7",
    "title": "Spiking and saturating dendrites differentially expand single neuron computation capacity",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/63538fe6ef330c13a05a3ed7e599d5f7-Paper.pdf",
    "abstract": "The integration of excitatory inputs in dendrites is non-linear: multiple excitatory inputs can produce a local depolarization departing from the arithmetic sum of each input's response taken separately. If this depolarization is bigger than the arithmetic sum, the dendrite is spiking; if the depolarization is smaller, the dendrite is saturating. Decomposing a dendritic tree into independent dendritic spiking units greatly extends its computational capacity, as the neuron then maps onto a two layer neural network, enabling it to compute linearly non-separable Boolean functions (lnBFs). How can these lnBFs be implemented by dendritic architectures in practise? And can saturating dendrites equally expand computational capacity? To adress these questions we use a binary neuron model and Boolean algebra. First, we confirm that spiking dendrites enable a neuron to compute lnBFs using an architecture based on the disjunctive normal form (DNF). Second, we prove that saturating dendrites as well as spiking dendrites also enable a neuron to compute lnBFs using an architecture based on the conjunctive normal form (CNF). Contrary to the DNF-based architecture, a CNF-based architecture leads to a dendritic unit tuning that does not imply the neuron tuning, as has been observed experimentally. Third, we show that one cannot use a DNF-based architecture with saturating dendrites. Consequently, we show that an important family of lnBFs implemented with a CNF-architecture can require an exponential number of saturating dendritic units, whereas the same family implemented with either a DNF-architecture or a CNF-architecture always require a linear number of spiking dendritic unit. This minimization could explain why a neuron spends energetic resources to make its dendrites spike.",
    "authors": [
      "Caz\u00e9, Romain",
      "Humphries, Mark",
      "Gutkin, Boris"
    ]
  },
  {
    "id": "6364d3f0f495b6ab9dcf8d3b5c6e0b01",
    "title": "Active Learning of Model Evidence Using Bayesian Quadrature",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/6364d3f0f495b6ab9dcf8d3b5c6e0b01-Paper.pdf",
    "abstract": "Numerical integration is an key component of many problems in scientific computing, statistical modelling, and machine learning. Bayesian Quadrature is a model-based method for numerical integration which, relative to standard Monte Carlo methods, offers increased sample efficiency and a more robust estimate of the uncertainty in the estimated integral. We propose a novel Bayesian Quadrature approach for numerical integration when the integrand is non-negative, such as the case of computing the marginal likelihood, predictive distribution, or normalising constant of a probabilistic model. Our approach approximately marginalises the quadrature model's hyperparameters in closed form, and introduces an active learning scheme to optimally select function evaluations, as opposed to using Monte Carlo samples. We demonstrate our method on both a number of synthetic benchmarks and a real scientific problem from astronomy.",
    "authors": [
      "Osborne, Michael",
      "Garnett, Roman",
      "Ghahramani, Zoubin",
      "Duvenaud, David K.",
      "Roberts, Stephen J.",
      "Rasmussen, Carl"
    ]
  },
  {
    "id": "6395ebd0f4b478145ecfbaf939454fa4",
    "title": "Diffusion Decision Making for Adaptive k-Nearest Neighbor Classification",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/6395ebd0f4b478145ecfbaf939454fa4-Paper.pdf",
    "abstract": "This paper sheds light on some fundamental connections of the diffusion decision making model of neuroscience and cognitive psychology with k-nearest neighbor classification.  We show that conventional k-nearest neighbor classification can be viewed as a special problem of the diffusion decision model in the asymptotic situation.  Applying the optimal strategy associated with the diffusion decision model, an adaptive rule is developed for determining appropriate values of k in k-nearest neighbor classification.  Making use of the sequential probability ratio test (SPRT) and Bayesian analysis, we propose five different criteria for adaptively acquiring nearest neighbors.  Experiments with both synthetic and real datasets demonstrate the effectivness of our classification criteria.",
    "authors": [
      "Noh, Yung-kyun",
      "Park, Frank",
      "Lee, Daniel"
    ]
  },
  {
    "id": "642e92efb79421734881b53e1e1b18b6",
    "title": "Bayesian Hierarchical Reinforcement Learning",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/642e92efb79421734881b53e1e1b18b6-Paper.pdf",
    "abstract": "We describe an approach to incorporating Bayesian priors in the maxq framework for hierarchical reinforcement learning (HRL). We define priors on the primitive environment model and on task pseudo-rewards. Since models for composite tasks can be complex, we use a mixed model-based/model-free learning approach to find an optimal hierarchical policy. We show empirically that (i) our approach results in improved convergence over non-Bayesian baselines, given sensible priors, (ii) task hierarchies and Bayesian priors can be complementary sources of information, and using both sources is better than either alone, (iii) taking advantage of the structural decomposition induced by the task hierarchy significantly reduces the computational cost of Bayesian reinforcement learning and (iv) in this framework, task pseudo-rewards can be learned instead of being manually specified, leading to automatic learning of hierarchically optimal rather than recursively optimal policies.",
    "authors": [
      "Cao, Feng",
      "Ray, Soumya"
    ]
  },
  {
    "id": "65658fde58ab3c2b6e5132a39fae7cb9",
    "title": "Compressive Sensing MRI with Wavelet Tree Sparsity",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/65658fde58ab3c2b6e5132a39fae7cb9-Paper.pdf",
    "abstract": "In Compressive Sensing Magnetic Resonance Imaging (CS-MRI), one can reconstruct a MR image with good quality from only a small number of measurements. This can significantly reduce MR scanning time. According to structured sparsity theory, the measurements can be further reduced to $\\mathcal{O}(K+\\log n)$ for tree-sparse data instead of $\\mathcal{O}(K+K\\log n)$ for standard $K$-sparse data with length $n$. However, few of existing algorithms has utilized this for CS-MRI, while most of them use Total Variation and wavelet sparse regularization. On the other side, some algorithms have been proposed for tree sparsity regularization, but few of them has validated   the benefit of tree structure in CS-MRI. In this paper, we propose a fast convex optimization algorithm to improve CS-MRI.  Wavelet sparsity, gradient sparsity and tree sparsity are all considered in our model for real MR images. The original complex problem is decomposed to three simpler subproblems then each of the subproblems can be efficiently solved with an iterative scheme. Numerous experiments have been conducted and show that the proposed algorithm outperforms the state-of-the-art CS-MRI algorithms, and gain better reconstructions results on real MR images than general tree based solvers or algorithms.",
    "authors": [
      "Chen, Chen",
      "Huang, Junzhou"
    ]
  },
  {
    "id": "670e8a43b246801ca1eaca97b3e19189",
    "title": "Mixability in Statistical Learning",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/670e8a43b246801ca1eaca97b3e19189-Paper.pdf",
    "abstract": "Statistical learning and sequential prediction are two different but related formalisms to study the quality of predictions. Mapping out their relations and transferring ideas is an active area of investigation. We provide another piece of the puzzle by showing that an important concept in sequential prediction, the mixability of a loss, has a natural counterpart in the statistical setting, which we call stochastic mixability. Just as ordinary mixability characterizes fast rates for the worst-case regret in sequential prediction, stochastic mixability characterizes fast rates in statistical learning. We show that, in the special case of log-loss, stochastic mixability reduces to a well-known (but usually unnamed) martingale condition, which is used in existing convergence theorems for minimum description length and Bayesian inference. In the case of 0/1-loss, it reduces to the margin condition of Mammen and Tsybakov, and in the case that the model under consideration contains all possible predictors, it is equivalent to ordinary mixability.",
    "authors": [
      "Erven, Tim",
      "Gr\u00fcnwald, Peter",
      "Reid, Mark D.",
      "Williamson, Robert C."
    ]
  },
  {
    "id": "6766aa2750c19aad2fa1b32f36ed4aee",
    "title": "Symmetric Correspondence Topic Models for Multilingual Text Analysis",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/6766aa2750c19aad2fa1b32f36ed4aee-Paper.pdf",
    "abstract": "Topic modeling is a widely used approach to analyzing large text collections. A small number of multilingual topic models have recently been explored to discover latent topics among parallel or comparable documents, such as in Wikipedia. Other topic models that were originally proposed for structured data are also applicable to multilingual documents. Correspondence Latent Dirichlet Allocation (CorrLDA) is one such model; however, it requires a pivot language to be specified in advance. We propose a new topic model, Symmetric Correspondence LDA (SymCorrLDA), that incorporates a hidden variable to control a pivot language, in an extension of CorrLDA. We experimented with two multilingual comparable datasets extracted from Wikipedia and demonstrate that SymCorrLDA is more effective than some other existing multilingual topic models.",
    "authors": [
      "Fukumasu, Kosuke",
      "Eguchi, Koji",
      "Xing, Eric"
    ]
  },
  {
    "id": "67f7fb873eaf29526a11a9b7ac33bfac",
    "title": "Learning Halfspaces with the Zero-One Loss: Time-Accuracy Tradeoffs",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/67f7fb873eaf29526a11a9b7ac33bfac-Paper.pdf",
    "abstract": "Given $\\alpha,\\epsilon$, we study the time complexity   required to improperly learn a halfspace with misclassification   error rate of at most $(1+\\alpha)\\,L^*_\\gamma + \\epsilon$, where   $L^*_\\gamma$ is the optimal $\\gamma$-margin error rate. For $\\alpha   = 1/\\gamma$, polynomial time and sample complexity is achievable   using the hinge-loss. For $\\alpha = 0$, \\cite{ShalevShSr11} showed   that $\\poly(1/\\gamma)$ time is impossible, while learning is   possible in time $\\exp(\\tilde{O}(1/\\gamma))$.  An immediate   question, which this paper tackles, is what is achievable if $\\alpha   \\in (0,1/\\gamma)$.  We derive positive results interpolating between   the polynomial time for $\\alpha = 1/\\gamma$ and the exponential   time for $\\alpha=0$. In particular, we show that there are cases in   which $\\alpha = o(1/\\gamma)$ but the problem is still solvable in   polynomial time. Our results naturally extend to the adversarial   online learning model and to the PAC learning with malicious noise   model.",
    "authors": [
      "Birnbaum, Aharon",
      "Shwartz, Shai"
    ]
  },
  {
    "id": "6855456e2fe46a9d49d3d3af4f57443d",
    "title": "Learning High-Density Regions for a Generalized Kolmogorov-Smirnov Test in High-Dimensional Data",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/6855456e2fe46a9d49d3d3af4f57443d-Paper.pdf",
    "abstract": "We propose an efficient, generalized, nonparametric, statistical Kolmogorov-Smirnov test for detecting distributional change in high-dimensional data. To implement the test, we introduce a novel, hierarchical, minimum-volume sets estimator to represent the distributions to be tested. Our work is motivated by the need to detect changes in data streams, and the test is especially efficient in this context. We provide the theoretical foundations of our test and show its superiority over existing methods.",
    "authors": [
      "Glazer, Assaf",
      "Lindenbaum, Michael",
      "Markovitch, Shaul"
    ]
  },
  {
    "id": "68d13cf26c4b4f4f932e3eff990093ba",
    "title": "Factorial LDA: Sparse Multi-Dimensional Text Models",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/68d13cf26c4b4f4f932e3eff990093ba-Paper.pdf",
    "abstract": "Multi-dimensional latent variable models can capture the many latent factors in a text corpus, such as topic, author perspective and sentiment. We introduce factorial LDA, a multi-dimensional latent variable model in which a document is influenced by K different factors, and each word token depends on a K-dimensional vector of latent variables. Our model incorporates structured word priors and learns a sparse product of factors. Experiments on research abstracts show that our model can learn latent factors such as research topic, scientific discipline, and focus (e.g. methods vs. applications.) Our modeling improvements reduce test perplexity and improve human interpretability of the discovered factors.",
    "authors": [
      "Paul, Michael",
      "Dredze, Mark"
    ]
  },
  {
    "id": "6a61d423d02a1c56250dc23ae7ff12f3",
    "title": "Augment-and-Conquer Negative Binomial Processes",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/6a61d423d02a1c56250dc23ae7ff12f3-Paper.pdf",
    "abstract": "By developing data augmentation methods unique to the negative binomial (NB) distribution, we unite seemingly disjoint count and mixture models  under the  NB process framework. We develop fundamental properties of the models and derive efficient Gibbs sampling inference. We show that the  gamma-NB process can be reduced to the hierarchical Dirichlet process with normalization, highlighting its unique theoretical, structural and computational advantages. A variety of NB processes with distinct sharing mechanisms are constructed and applied to topic modeling, with connections to existing algorithms, showing the importance of inferring both the NB dispersion and probability parameters.",
    "authors": [
      "Zhou, Mingyuan",
      "Carin, Lawrence"
    ]
  },
  {
    "id": "6aca97005c68f1206823815f66102863",
    "title": "Large Scale Distributed Deep Networks",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/6aca97005c68f1206823815f66102863-Paper.pdf",
    "abstract": "Recent work in unsupervised feature learning and deep learning has shown that  being able to train large models can dramatically improve performance.  In this  paper, we consider the problem of training a deep network with billions of  parameters using tens of thousands of CPU cores.  We have developed a  software framework called DistBelief that can utilize computing clusters  with thousands of machines to train large models.  Within this framework, we  have developed two algorithms for large-scale distributed training: (i) Downpour  SGD, an asynchronous stochastic gradient descent procedure supporting a  large number of model replicas, and (ii) Sandblaster, a framework that supports  for a variety of distributed batch optimization procedures, including a distributed  implementation of L-BFGS.  Downpour SGD and Sandblaster L-BFGS both  increase the scale and speed of deep network training.  We have successfully  used our system to train a deep network 100x larger than previously reported in  the literature, and achieves state-of-the-art performance on ImageNet, a visual  object recognition task with 16 million images and 21k categories.  We show that  these same techniques dramatically accelerate the training of a more modestly  sized deep network for a commercial speech recognition service. Although we  focus on and report performance of these methods as applied to training large  neural networks, the underlying algorithms are applicable to any gradient-based  machine learning algorithm.",
    "authors": [
      "Dean, Jeffrey",
      "Corrado, Greg",
      "Monga, Rajat",
      "Chen, Kai",
      "Devin, Matthieu",
      "Mao, Mark",
      "Ranzato, Marc'aurelio",
      "Senior, Andrew",
      "Tucker, Paul",
      "Yang, Ke",
      "Le, Quoc",
      "Ng, Andrew"
    ]
  },
  {
    "id": "6ba1085b788407963fe0e89c699a7396",
    "title": "Structure estimation for discrete graphical models: Generalized covariance matrices and their inverses",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/6ba1085b788407963fe0e89c699a7396-Paper.pdf",
    "abstract": "We investigate a curious relationship between the structure of a discrete graphical model and the support of the inverse of a generalized covariance matrix. We show that for certain graph structures, the support of the inverse covariance matrix of indicator variables on the vertices of a graph re\ufb02ects the conditional independence structure of the graph. Our work extends results that have previously been es- tablished only in the context of multivariate Gaussian graphical models, thereby addressing an open question about the signi\ufb01cance of the inverse covariance ma- trix of a non-Gaussian distribution. Based on our population-level results, we show how the graphical Lasso may be used to recover the edge structure of cer- tain classes of discrete graphical models, and present simulations to verify our theoretical results.",
    "authors": [
      "Loh, Po-ling",
      "Wainwright, Martin J."
    ]
  },
  {
    "id": "6c29793a140a811d0c45ce03c1c93a28",
    "title": "Joint Modeling of a Matrix with Associated Text via Latent Binary Features",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/6c29793a140a811d0c45ce03c1c93a28-Paper.pdf",
    "abstract": "A new methodology is developed for joint analysis of a matrix and accompanying documents, with the documents associated with the matrix rows/columns. The documents are modeled with a focused topic model, inferring latent binary features (topics) for each document. A new matrix decomposition is developed, with latent binary features associated with the rows/columns, and with imposition of a low-rank constraint. The matrix decomposition and topic model are coupled by sharing the latent binary feature vectors associated with each. The model is applied to roll-call data, with the associated documents defined by the legislation. State-of-the-art results are manifested for prediction of votes on a new piece of legislation, based only on the observed text legislation. The coupling of the text and legislation is also demonstrated to yield insight into the properties of the matrix decomposition for roll-call data.",
    "authors": [
      "Zhang, Xianxing",
      "Carin, Lawrence"
    ]
  },
  {
    "id": "6c8dba7d0df1c4a79dd07646be9a26c8",
    "title": "Near-Optimal MAP Inference for Determinantal Point Processes",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/6c8dba7d0df1c4a79dd07646be9a26c8-Paper.pdf",
    "abstract": "Determinantal point processes (DPPs) have recently been proposed as   computationally efficient probabilistic models of diverse sets for a   variety of applications, including document summarization, image   search, and pose estimation.  Many DPP inference operations,   including normalization and sampling, are tractable; however,   finding the most likely configuration (MAP), which is often required   in practice for decoding, is NP-hard, so we must resort to   approximate inference.  Because DPP probabilities are   log-submodular, greedy algorithms have been used in the past with   some empirical success; however, these methods only give   approximation guarantees in the special case of DPPs with monotone   kernels.  In this paper we propose a new algorithm for approximating   the MAP problem based on continuous techniques for submodular   function maximization.  Our method involves a novel continuous   relaxation of the log-probability function, which, in contrast to   the multilinear extension used for general submodular functions, can   be evaluated and differentiated exactly and efficiently.  We obtain   a practical algorithm with a 1/4-approximation guarantee for a   general class of non-monotone DPPs.  Our algorithm also extends to   MAP inference under complex polytope constraints, making it possible   to combine DPPs with Markov random fields, weighted matchings, and   other models.  We demonstrate that our approach outperforms greedy   methods on both synthetic and real-world data.",
    "authors": [
      "Gillenwater, Jennifer",
      "Kulesza, Alex",
      "Taskar, Ben"
    ]
  },
  {
    "id": "6cdd60ea0045eb7a6ec44c54d29ed402",
    "title": "Image Denoising and Inpainting with Deep Neural Networks",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/6cdd60ea0045eb7a6ec44c54d29ed402-Paper.pdf",
    "abstract": "We present a novel approach to low-level vision problems that combines sparse coding and deep networks pre-trained with denoising auto-encoder (DA). We propose an alternative training scheme that successfully adapts DA, originally designed for unsupervised feature learning, to the tasks of image denoising and blind inpainting. Our method achieves state-of-the-art performance in the image denoising task. More importantly, in blind image inpainting task, the proposed method provides solutions to some complex problems that have not been tackled before. Specifically, we can automatically remove complex patterns like superimposed text from an image, rather than simple patterns like pixels missing at random. Moreover, the proposed method does not need the information regarding the region that requires inpainting to be given a priori. Experimental results demonstrate the effectiveness of the proposed method in the tasks of image denoising and blind inpainting. We also show that our new training scheme for DA is more effective and can improve the performance of unsupervised feature learning.",
    "authors": [
      "Xie, Junyuan",
      "Xu, Linli",
      "Chen, Enhong"
    ]
  },
  {
    "id": "6d70cb65d15211726dcce4c0e971e21c",
    "title": "Strategic Impatience in Go/NoGo versus Forced-Choice Decision-Making",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/6d70cb65d15211726dcce4c0e971e21c-Paper.pdf",
    "abstract": "Two-alternative forced choice (2AFC) and Go/NoGo (GNG) tasks are behavioral choice paradigms commonly used to study sensory and cognitive processing in choice behavior. While GNG is thought to isolate the sensory/decisional component by removing the need for response selection, a consistent bias towards the Go response (higher hits and false alarm rates) in the GNG task suggests possible fundamental differences in the sensory or cognitive processes engaged in the two tasks. Existing mechanistic models of these choice tasks, mostly variants of the drift-diffusion model (DDM; [1,2]) and the related leaky competing accumulator models [3,4] capture various aspects of behavior but do not address the provenance of the Go bias.  We postulate that this ``impatience'' to go is a strategic adjustment in response to the implicit asymmetry in the cost structure of GNG: the NoGo response requires waiting until the response deadline, while a Go response immediately terminates the current trial. We show that a Bayes-risk minimizing decision policy that minimizes both error rate and average decision delay naturally exhibits the experimentally observed bias.  The optimal decision policy is formally equivalent to a DDM with a time-varying threshold that initially rises after stimulus onset, and collapses again near the response deadline. The initial rise is due to the fading temporal advantage of choosing the Go response over the fixed-delay NoGo response. We show that fitting a simpler, fixed-threshold DDM to the optimal model reproduces the counterintuitive result of a higher threshold in GNG than 2AFC decision-making, previously observed in direct DDM fit to behavioral data [2], although such approximations cannot reproduce the Go bias. Thus, observed discrepancies between GNG and 2AFC decision-making may arise from rational strategic adjustments to the cost structure, and need not imply additional differences in the underlying sensory and cognitive processes.",
    "authors": [
      "Shenoy, Pradeep",
      "Yu, Angela J."
    ]
  },
  {
    "id": "6d9c547cf146054a5a720606a7694467",
    "title": "Cost-Sensitive Exploration in Bayesian Reinforcement Learning",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/6d9c547cf146054a5a720606a7694467-Paper.pdf",
    "abstract": "In this paper, we consider Bayesian reinforcement learning (BRL) where actions incur costs in addition to rewards, and thus exploration has to be constrained in terms of the expected total cost while learning to maximize the expected long-term total reward. In order to formalize cost-sensitive exploration, we use the constrained Markov decision process (CMDP) as the model of the environment, in which we can naturally encode exploration requirements using the cost function. We extend BEETLE, a model-based BRL method, for learning in the environment with cost constraints. We demonstrate the cost-sensitive exploration behaviour in a number of simulated problems.",
    "authors": [
      "Kim, Dongho",
      "Kim, Kee-eung",
      "Poupart, Pascal"
    ]
  },
  {
    "id": "6da37dd3139aa4d9aa55b8d237ec5d4a",
    "title": "MCMC for continuous-time discrete-state systems",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/6da37dd3139aa4d9aa55b8d237ec5d4a-Paper.pdf",
    "abstract": "We propose a simple and novel framework for MCMC inference in continuous-time discrete-state systems with pure jump trajectories. We construct an exact MCMC sampler for such systems by alternately sampling a random discretization of time given a trajectory of the system, and then a new trajectory given the discretization.  The first step can be performed efficiently using properties of the Poisson process, while the second step can avail of discrete-time MCMC techniques based on the forward-backward algorithm. We compare our approach to particle MCMC and a uniformization-based sampler, and show its advantages.",
    "authors": [
      "Rao, Vinayak",
      "Teh, Yee"
    ]
  },
  {
    "id": "700fdb2ba62d4554dc268c65add4b16e",
    "title": "Spectral Learning of General Weighted Automata via Constrained Matrix Completion",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/700fdb2ba62d4554dc268c65add4b16e-Paper.pdf",
    "abstract": "Many tasks in text and speech processing and computational biology require es- timating functions mapping strings to real numbers. A broad class of such func- tions can be de\ufb01ned by weighted automata. Spectral methods based on the sin- gular value decomposition of a Hankel matrix have been recently proposed for learning a probability distribution represented by a weighted automaton from a training sample drawn according to this same target distribution. In this paper, we show how spectral methods can be extended to the problem of learning a general weighted automaton from a sample generated by an arbitrary distribution. The main obstruction to this approach is that, in general, some entries of the Hankel matrix may be missing. We present a solution to this problem based on solving a constrained matrix completion problem. Combining these two ingredients, matrix completion and spectral method, a whole new family of algorithms for learning general weighted automata is obtained. We present generalization bounds for a particular algorithm in this family. The proofs rely on a joint stability analysis of matrix completion and spectral learning.",
    "authors": [
      "Balle, Borja",
      "Mohri, Mehryar"
    ]
  },
  {
    "id": "70222949cc0db89ab32c9969754d4758",
    "title": "Learning with Recursive Perceptual Representations",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/70222949cc0db89ab32c9969754d4758-Paper.pdf",
    "abstract": "Linear Support Vector Machines (SVMs) have become very popular in vision as part of state-of-the-art object recognition and other classification tasks but require high dimensional feature spaces for good performance. Deep learning methods can find more compact representations but current methods employ multilayer perceptrons that require solving a difficult, non-convex optimization problem. We propose a deep non-linear classifier whose layers are SVMs and which incorporates random projection as its core stacking element. Our method learns layers of linear SVMs recursively transforming the original data manifold through a random projection of the weak prediction computed from each layer. Our method scales as linear SVMs, does not rely on any kernel computations or nonconvex optimization, and exhibits better generalization ability than kernel-based SVMs. This is especially true when the number of training samples is smaller than the dimensionality of data, a common scenario in many real-world applications. The use of random projections is key to our method, as we show in the experiments section, in which we observe a consistent improvement over previous --often more complicated-- methods on several vision and speech benchmarks.",
    "authors": [
      "Vinyals, Oriol",
      "Jia, Yangqing",
      "Deng, Li",
      "Darrell, Trevor"
    ]
  },
  {
    "id": "71a3cb155f8dc89bf3d0365288219936",
    "title": "Scaled Gradients on Grassmann Manifolds for Matrix Completion",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/71a3cb155f8dc89bf3d0365288219936-Paper.pdf",
    "abstract": "This paper describes gradient methods based on a scaled metric on the Grassmann manifold for low-rank matrix completion. The proposed methods significantly improve canonical gradient methods especially on ill-conditioned matrices, while maintaining established global convegence and exact recovery guarantees. A connection between a form of subspace iteration for matrix completion and the scaled gradient descent procedure is also established. The proposed conjugate gradient method based on the scaled gradient outperforms several existing algorithms for matrix completion and is competitive with recently proposed methods.",
    "authors": [
      "Ngo, Thanh",
      "Saad, Yousef"
    ]
  },
  {
    "id": "71f6278d140af599e06ad9bf1ba03cb0",
    "title": "Link Prediction in Graphs with Autoregressive Features",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/71f6278d140af599e06ad9bf1ba03cb0-Paper.pdf",
    "abstract": "In the paper, we consider the problem of link prediction in time-evolving graphs. We assume that certain graph features, such as the node degree, follow a vector autoregressive (VAR) model and we propose to use this information to improve the accuracy of prediction. Our strategy involves a joint optimization procedure over the space of adjacency matrices and VAR matrices which takes into account both sparsity and low rank properties of the matrices. Oracle inequalities are derived and illustrate the trade-offs in the choice of smoothing parameters when modeling the joint effect of sparsity and low rank property. The estimate is computed efficiently using proximal methods through a generalized forward-backward agorithm.",
    "authors": [
      "Richard, Emile",
      "Gaiffas, Stephane",
      "Vayatis, Nicolas"
    ]
  },
  {
    "id": "72b32a1f754ba1c09b3695e0cb6cde7f",
    "title": "A Generative Model for Parts-based Object Segmentation",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/72b32a1f754ba1c09b3695e0cb6cde7f-Paper.pdf",
    "abstract": "The Shape Boltzmann Machine (SBM) has recently been introduced as a state-of-the-art model of foreground/background object shape. We extend the SBM to account for the foreground object's parts. Our model, the Multinomial SBM (MSBM), can capture both local and global statistics of part shapes accurately. We combine the MSBM with an appearance model to form a fully generative model of images of objects. Parts-based image segmentations are obtained simply by performing probabilistic inference in the model. We apply the model to two challenging datasets which exhibit significant shape and appearance variability, and find that it obtains results that are comparable to the state-of-the-art.",
    "authors": [
      "Eslami, S.",
      "Williams, Christopher"
    ]
  },
  {
    "id": "7380ad8a673226ae47fce7bff88e9c33",
    "title": "Dimensionality Dependent PAC-Bayes Margin Bound",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/7380ad8a673226ae47fce7bff88e9c33-Paper.pdf",
    "abstract": "Margin is one of the most important concepts in machine learning. Previous margin bounds, both for SVM and for boosting, are dimensionality independent. A major advantage of this dimensionality independency is that it can explain the excellent performance of SVM whose feature spaces are often of high or infinite dimension. In this paper we address the problem whether such dimensionality independency is intrinsic for the margin bounds. We prove a dimensionality dependent PAC-Bayes margin bound. The bound is monotone increasing with respect to the dimension when keeping all other factors fixed. We show that our bound is strictly sharper than a previously well-known PAC-Bayes margin bound if the feature space is of finite dimension; and the two bounds tend to be equivalent as the dimension goes to infinity. In addition, we show that the VC bound for linear classifiers can be recovered from our bound under mild conditions. We conduct extensive experiments on benchmark datasets and find that the new bound is useful for model selection and is significantly sharper than the dimensionality independent PAC-Bayes margin bound as well as the VC bound for linear classifiers.",
    "authors": [
      "Jin, Chi",
      "Wang, Liwei"
    ]
  },
  {
    "id": "7634ea65a4e6d9041cfd3f7de18e334a",
    "title": "MAP Inference in Chains using Column Generation",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/7634ea65a4e6d9041cfd3f7de18e334a-Paper.pdf",
    "abstract": "Linear chains and trees are basic building blocks in many applications of graphical models.  Although exact inference in these models can be performed by dynamic programming, this computation can still be prohibitively expensive with non-trivial target variable domain sizes due to the quadratic dependence on this size.  Standard message-passing algorithms for these problems are inefficient because they compute scores on hypotheses for which there is strong negative local evidence.  For this reason there has been significant previous interest in beam search and its variants; however, these methods provide only approximate inference.  This paper presents new efficient exact inference algorithms based on the combination of it column generation and pre-computed bounds on the model's cost structure.  Improving worst-case performance is impossible. However, our method substantially speeds real-world, typical-case inference in chains and trees.  Experiments show our method to be twice as fast as exact Viterbi for Wall Street Journal part-of-speech tagging and over thirteen times faster for a joint part-of-speed and named-entity-recognition task.  Our algorithm is also extendable to new techniques for approximate inference, to faster two-best inference, and new opportunities for connections between inference and learning.",
    "authors": [
      "Belanger, David",
      "Passos, Alexandre",
      "Riedel, Sebastian",
      "McCallum, Andrew"
    ]
  },
  {
    "id": "76dc611d6ebaafc66cc0879c71b5db5c",
    "title": "Cocktail Party Processing via Structured Prediction",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/76dc611d6ebaafc66cc0879c71b5db5c-Paper.pdf",
    "abstract": "While human listeners excel at selectively attending to a conversation in a cocktail party, machine performance is still far inferior by comparison. We show that the cocktail party problem, or the speech separation problem, can be effectively approached via structured prediction. To account for temporal dynamics in speech, we employ conditional random fields (CRFs) to classify speech dominance within each time-frequency unit for a sound mixture. To capture complex, nonlinear relationship between input and output, both state and transition feature functions in CRFs are learned by deep neural networks. The formulation of the problem as classification allows us to directly optimize a measure that is well correlated with human speech intelligibility. The proposed system substantially outperforms existing ones in a variety of noises.",
    "authors": [
      "Wang, Yuxuan",
      "Wang, Deliang"
    ]
  },
  {
    "id": "7750ca3559e5b8e1f44210283368fc16",
    "title": "Fused sparsity and robust estimation for linear models with unknown variance",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/7750ca3559e5b8e1f44210283368fc16-Paper.pdf",
    "abstract": "In this paper, we develop a novel approach to the problem of learning sparse representations in the context of fused sparsity and unknown noise level. We propose an algorithm, termed Scaled Fused Dantzig Selector (SFDS), that accomplishes the aforementioned learning task by means of a second-order cone program. A special  emphasize is put on the particular instance of fused sparsity corresponding to  the learning in presence of outliers. We establish finite sample risk bounds and  carry out an experimental evaluation on both synthetic and real data.",
    "authors": [
      "Dalalyan, Arnak",
      "Chen, Yin"
    ]
  },
  {
    "id": "78b9cab19959e4af8ff46156ee460c74",
    "title": "On the Sample Complexity of Robust PCA",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/78b9cab19959e4af8ff46156ee460c74-Paper.pdf",
    "abstract": "We estimate the sample complexity of a recent robust estimator for a generalized version of the inverse covariance matrix. This estimator is used in a convex algorithm for robust subspace recovery (i.e., robust PCA). Our model assumes a sub-Gaussian underlying distribution and an i.i.d.~sample from it. Our main result shows with high probability that the norm of the difference between the generalized inverse covariance of the underlying distribution and its estimator from an i.i.d.~sample of size $N$ is of order $O(N^{-0.5+\\eps})$ for arbitrarily small $\\eps>0$ (affecting the probabilistic estimate); this rate of convergence is close to one of direct covariance and inverse covariance estimation, i.e., $O(N^{-0.5})$. Our precise probabilistic estimate implies for some natural settings that the sample complexity of the generalized inverse covariance estimation when using the Frobenius norm is $O(D^{2+\\delta})$ for arbitrarily small $\\delta>0$ (whereas the sample complexity of direct covariance estimation with Frobenius norm is $O(D^{2})$). These results provide similar rates of convergence and sample complexity for the corresponding robust subspace recovery algorithm, which are close to those of PCA. To the best of our knowledge, this is the only work analyzing the sample complexity of any robust PCA algorithm.",
    "authors": [
      "Coudron, Matthew",
      "Lerman, Gilad"
    ]
  },
  {
    "id": "7a614fd06c325499f1680b9896beedeb",
    "title": "Learning to Discover Social Circles in Ego Networks",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/7a614fd06c325499f1680b9896beedeb-Paper.pdf",
    "abstract": "Our personal social networks are big and cluttered, and currently there is no good way to organize them. Social networking sites allow users to manually categorize their friends into social circles (e.g. circles' on Google+, andlists' on Facebook and Twitter), however they are laborious to construct and must be updated whenever a user's network grows. We define a novel machine learning task of identifying users' social circles. We pose the problem as a node clustering problem on a user's ego-network, a network of connections between her friends. We develop a model for detecting circles that combines network structure as well as user profile information. For each circle we learn its members and the circle-specific user profile similarity metric. Modeling node membership to multiple circles allows us to detect overlapping as well as hierarchically nested circles. Experiments show that our model accurately identifies circles on a diverse set of data from Facebook, Google+, and Twitter for all of which we obtain hand-labeled ground-truth data.",
    "authors": [
      "Leskovec, Jure",
      "Mcauley, Julian"
    ]
  },
  {
    "id": "7bccfde7714a1ebadf06c5f4cea752c1",
    "title": "Exact and Stable Recovery of Sequences of Signals with Sparse Increments via Differential _1-Minimization",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/7bccfde7714a1ebadf06c5f4cea752c1-Paper.pdf",
    "abstract": "We consider the problem of recovering a sequence of vectors, $(x_k)_{k=0}^K$, for which the increments $x_k-x_{k-1}$ are $S_k$-sparse (with $S_k$ typically smaller than $S_1$), based on linear measurements $(y_k = A_k x_k + e_k)_{k=1}^K$, where $A_k$ and $e_k$ denote the measurement matrix and noise, respectively. Assuming each $A_k$ obeys the restricted isometry property (RIP) of a certain order---depending only on $S_k$---we show that in the absence of noise a convex program, which minimizes the weighted sum of the $\\ell_1$-norm of successive differences subject to the linear measurement constraints, recovers the sequence $(x_k)_{k=1}^K$ \\emph{exactly}. This is an interesting result because this convex program is equivalent to a standard compressive sensing problem with a highly-structured aggregate measurement matrix which does not satisfy the RIP requirements in the standard sense, and yet we can achieve exact recovery. In the presence of bounded noise, we propose a quadratically-constrained convex program for recovery and derive bounds on the reconstruction error of the sequence. We supplement our theoretical analysis with simulations and an application to real video data. These further support the validity of the proposed approach for acquisition and recovery of signals with time-varying sparsity.",
    "authors": [
      "Ba, Demba",
      "Babadi, Behtash",
      "Purdon, Patrick",
      "Brown, Emery"
    ]
  },
  {
    "id": "7c33e57e3dbd8a52940fa1a963aa4a4a",
    "title": "Tight Bounds on Profile Redundancy and Distinguishability",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/7c33e57e3dbd8a52940fa1a963aa4a4a-Paper.pdf",
    "abstract": "The minimax KL-divergence of any distribution from all distributions in a collection P has several practical implications. In compression, it is called redundancy and represents the least additional number of bits over the entropy needed to encode the output of any distribution in P. In online es- timation and learning, it is the lowest expected log-loss regret when guessing a sequence of random values generated by a distribution in P. In hypothesis testing, it upper bounds the largest number of distinguishable distributions in P. Motivated by problems ranging from population estimation to text classi\ufb01cation and speech recognition, several machine-learning and information-theory researchers have recently considered label-invariant observations and properties induced by i.i.d. distributions. A suf\ufb01cient statistic for all these properties is the data\u2019s pro\ufb01le, the multiset of the number of times each data element appears. Improving on a sequence of previous works, we show that the redun- dancy of the collection of distributions induced over pro\ufb01les by length-n i.i.d. sequences is between 0.3 \u00b7 n1/3 and n1/3 log2 n, in particular, establishing its exact growth power.",
    "authors": [
      "Acharya, Jayadev",
      "Das, Hirakendu",
      "Orlitsky, Alon"
    ]
  },
  {
    "id": "7cce53cf90577442771720a370c3c723",
    "title": "On Triangular versus Edge Representations --- Towards Scalable Modeling of Networks",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/7cce53cf90577442771720a370c3c723-Paper.pdf",
    "abstract": "In this paper, we argue for representing networks as a bag of {\\it triangular motifs}, particularly for important network problems that current model-based approaches handle poorly due to computational bottlenecks incurred by using edge representations. Such approaches require both 1-edges and 0-edges (missing edges) to be provided as input, and as a consequence, approximate inference algorithms for these models usually require $\\Omega(N^2)$ time per iteration, precluding their application to larger real-world networks. In contrast, triangular modeling requires less computation, while providing equivalent or better inference quality. A triangular motif is a vertex triple containing 2 or 3 edges, and the number of such motifs is $\\Theta(\\sum_{i}D_{i}^{2})$ (where $D_i$ is the degree of vertex $i$), which is much smaller than $N^2$ for low-maximum-degree networks. Using this representation, we develop a novel mixed-membership network model and approximate inference algorithm suitable for large networks with low max-degree. For networks with high maximum degree, the triangular motifs can be naturally subsampled in a {\\it node-centric} fashion, allowing for much faster inference at a small cost in accuracy. Empirically, we demonstrate that our approach, when compared to that of an edge-based model, has faster runtime and improved accuracy for mixed-membership community detection. We conclude with a large-scale demonstration on an $N\\approx 280,000$-node network, which is infeasible for network models with $\\Omega(N^2)$ inference cost.",
    "authors": [
      "Ho, Qirong",
      "Yin, Junming",
      "Xing, Eric"
    ]
  },
  {
    "id": "7d771e0e8f3633ab54856925ecdefc5d",
    "title": "A Better Way to Pretrain Deep Boltzmann Machines",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/7d771e0e8f3633ab54856925ecdefc5d-Paper.pdf",
    "abstract": "We describe how the pre-training algorithm for Deep Boltzmann Machines (DBMs) is related to the pre-training algorithm for Deep Belief Networks and we show that under certain conditions, the pre-training procedure improves the variational lower bound of a two-hidden-layer DBM. Based on this analysis, we develop a different method of pre-training DBMs that distributes the modelling work more evenly over the hidden layers. Our results on the MNIST and NORB datasets demonstrate that the new pre-training algorithm allows us to learn better generative models.",
    "authors": [
      "Hinton, Geoffrey E.",
      "Salakhutdinov, Russ R."
    ]
  },
  {
    "id": "7e7e69ea3384874304911625ac34321c",
    "title": "Semi-supervised Eigenvectors for Locally-biased Learning",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/7e7e69ea3384874304911625ac34321c-Paper.pdf",
    "abstract": "In many applications, one has information, e.g., labels that are  provided in a semi-supervised manner, about a specific target region of a  large data set, and one wants to perform machine learning and data analysis  tasks nearby that pre-specified target region.   Locally-biased problems of this sort are particularly challenging for  popular eigenvector-based machine learning and data analysis tools. At root, the reason is that eigenvectors are inherently global quantities. In this paper, we address this issue by providing a methodology to construct  semi-supervised eigenvectors of a graph Laplacian, and we illustrate  how these locally-biased eigenvectors can be used to perform  locally-biased machine learning. These semi-supervised eigenvectors capture successively-orthogonalized  directions of maximum variance, conditioned on being well-correlated with an  input seed set of nodes that is assumed to be provided in a semi-supervised  manner. We also provide several empirical examples demonstrating how these  semi-supervised eigenvectors can be used to perform locally-biased learning.",
    "authors": [
      "Hansen, Toke",
      "Mahoney, Michael W."
    ]
  },
  {
    "id": "7eabe3a1649ffa2b3ff8c02ebfd5659f",
    "title": "The variational hierarchical EM algorithm for clustering hidden Markov models",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/7eabe3a1649ffa2b3ff8c02ebfd5659f-Paper.pdf",
    "abstract": "In this paper, we derive a novel algorithm to cluster  hidden Markov models (HMMs) according to their probability distributions. We propose a variational hierarchical EM algorithm that i) clusters a given collection of HMMs into groups of HMMs that are similar, in terms of the distributions they represent, and ii) characterizes each group by a ``cluster center'', i.e., a novel HMM that is representative for the group. We illustrate the benefits of the proposed algorithm on hierarchical clustering of motion capture sequences as well as on automatic music tagging.",
    "authors": [
      "Coviello, Emanuele",
      "Lanckriet, Gert",
      "Chan, Antoni"
    ]
  },
  {
    "id": "7f100b7b36092fb9b06dfb4fac360931",
    "title": "Scalable nonconvex inexact proximal splitting",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/7f100b7b36092fb9b06dfb4fac360931-Paper.pdf",
    "abstract": "We study large-scale, nonsmooth, nonconconvex optimization problems. In particular, we focus on nonconvex problems with \\emph{composite} objectives. This class of problems includes the extensively studied convex, composite objective problems as a special case. To tackle composite nonconvex problems, we introduce a powerful new framework based on asymptotically \\emph{nonvanishing} errors, avoiding the common convenient assumption of eventually vanishing errors. Within our framework we derive both batch and incremental nonconvex proximal splitting algorithms. To our knowledge, our framework is first to develop and analyze incremental \\emph{nonconvex} proximal-splitting algorithms, even if we disregard the ability to handle nonvanishing errors. We illustrate our theoretical framework by showing how it applies to difficult large-scale, nonsmooth, and nonconvex problems.",
    "authors": [
      "Sra, Suvrit"
    ]
  },
  {
    "id": "7f1171a78ce0780a2142a6eb7bc4f3c8",
    "title": "Bayesian nonparametric models for ranked data",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/7f1171a78ce0780a2142a6eb7bc4f3c8-Paper.pdf",
    "abstract": "We develop a Bayesian nonparametric extension of the popular Plackett-Luce choice model that can handle an infinite number of choice items.   Our framework is based on the theory of random atomic measures, with the prior specified by a gamma process.  We derive a posterior characterization and a simple and effective Gibbs sampler for posterior simulation.  We then develop a time-varying extension of our model, and apply our model to the New York Times lists of weekly bestselling books.",
    "authors": [
      "Caron, Francois",
      "Teh, Yee"
    ]
  },
  {
    "id": "7f24d240521d99071c93af3917215ef7",
    "title": "GenDeR: A Generic Diversified Ranking Algorithm",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/7f24d240521d99071c93af3917215ef7-Paper.pdf",
    "abstract": "Diversified ranking is a fundamental task in machine learning. It is broadly applicable in many real world problems, e.g., information retrieval, team assembling, product search, etc. In this paper, we consider a generic setting where we aim to diversify the top-k ranking list based on an arbitrary relevance function and an arbitrary similarity function among all the examples. We formulate it as an optimization problem and show that in general it is NP-hard. Then, we show that for a large volume of the parameter space, the proposed objective function enjoys the diminishing returns property, which enables us to design a scalable, greedy algorithm to find the near-optimal solution. Experimental results on real data sets demonstrate the effectiveness of the proposed algorithm.",
    "authors": [
      "He, Jingrui",
      "Tong, Hanghang",
      "Mei, Qiaozhu",
      "Szymanski, Boleslaw"
    ]
  },
  {
    "id": "7fe1f8abaad094e0b5cb1b01d712f708",
    "title": "Accuracy at the Top",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/7fe1f8abaad094e0b5cb1b01d712f708-Paper.pdf",
    "abstract": "We introduce a new notion of classification accuracy based on the top $\\tau$-quantile values of a scoring function, a relevant criterion in a number of problems arising for search engines. We define an algorithm optimizing a convex surrogate of the corresponding loss, and show how its solution can be obtained by solving several convex optimization problems. We also present margin-based guarantees for this algorithm based on the $\\tau$-quantile of the functions in the hypothesis set. Finally, we report the results of several experiments evaluating the performance of our algorithm. In a comparison in a bipartite setting with several algorithms seeking high precision at the top, our algorithm achieves a better performance in precision at the top.",
    "authors": [
      "Boyd, Stephen",
      "Cortes, Corinna",
      "Mohri, Mehryar",
      "Radovanovic, Ana"
    ]
  },
  {
    "id": "801c14f07f9724229175b8ef8b4585a8",
    "title": "Approximating Equilibria in Sequential Auctions with Incomplete Information and Multi-Unit Demand",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/801c14f07f9724229175b8ef8b4585a8-Paper.pdf",
    "abstract": "In many large economic markets, goods are sold through sequential auctions. Such domains include eBay, online ad auctions, wireless spectrum auctions, and the Dutch flower auctions. Bidders in these domains face highly complex decision-making problems, as their preferences for outcomes in one auction often depend on the outcomes of other auctions, and bidders have limited information about factors that drive outcomes, such as other bidders' preferences and past actions.   In this work, we formulate the bidder's problem as one of price prediction (i.e., learning) and optimization. We define the concept of stable price predictions and show that (approximate) equilibrium in sequential auctions can be characterized as a profile of strategies that (approximately) optimize with respect to such (approximately) stable price predictions. We show how equilibria found with our formulation compare to known theoretical equilibria for simpler auction domains, and we find new approximate equilibria for a more complex auction domain where analytical solutions were heretofore unknown.",
    "authors": [
      "Greenwald, Amy",
      "Li, Jiacui",
      "Sodomka, Eric"
    ]
  },
  {
    "id": "819f46e52c25763a55cc642422644317",
    "title": "Multiresolution Gaussian Processes",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/819f46e52c25763a55cc642422644317-Paper.pdf",
    "abstract": "We propose a multiresolution Gaussian process to capture long-range, non-Markovian dependencies while allowing for abrupt changes.  The multiresolution GP hierarchically couples a collection of smooth GPs, each defined over an element of a random nested partition.  Long-range dependencies are captured by the top-level GP while the partition points define the abrupt changes.  Due to the inherent conjugacy of the GPs, one can analytically marginalize the GPs and compute the conditional likelihood of the observations given the partition tree.  This allows for efficient inference of the partition itself, for which we employ graph-theoretic techniques.  We apply the multiresolution GP to the analysis of Magnetoencephalography (MEG) recordings of brain activity.",
    "authors": [
      "Fox, Emily",
      "Dunson, David"
    ]
  },
  {
    "id": "81e5f81db77c596492e6f1a5a792ed53",
    "title": "Burn-in, bias, and the rationality of anchoring",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/81e5f81db77c596492e6f1a5a792ed53-Paper.pdf",
    "abstract": "Bayesian inference provides a unifying framework for addressing problems in machine learning, artificial intelligence, and robotics, as well as the problems facing the human mind. Unfortunately, exact Bayesian inference is intractable in all but the simplest models. Therefore minds and machines have to approximate Bayesian inference. Approximate inference algorithms can achieve a wide range of time-accuracy tradeoffs, but what is the optimal tradeoff? We investigate time-accuracy tradeoffs using the Metropolis-Hastings algorithm as a metaphor for the mind's inference algorithm(s). We find that reasonably accurate decisions are possible long before the Markov chain has converged to the posterior distribution, i.e. during the period known as burn-in. Therefore the strategy that is optimal subject to the mind's bounded processing speed and opportunity costs may perform so few iterations that the resulting samples are biased towards the initial value. The resulting cognitive process model provides a rational basis for the anchoring-and-adjustment heuristic. The model's quantitative predictions are tested against published data on anchoring in numerical estimation tasks. Our theoretical and empirical results suggest that the anchoring bias is consistent with approximate Bayesian inference.",
    "authors": [
      "Lieder, Falk",
      "Griffiths, Tom",
      "Goodman, Noah"
    ]
  },
  {
    "id": "838e8afb1ca34354ac209f53d90c3a43",
    "title": "Truly Nonparametric Online Variational Inference for Hierarchical Dirichlet Processes",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/838e8afb1ca34354ac209f53d90c3a43-Paper.pdf",
    "abstract": "Variational methods provide a computationally scalable alternative to Monte Carlo methods for large-scale, Bayesian nonparametric learning.  In practice, however, conventional batch and online variational methods quickly become trapped in local optima. In this paper, we consider a nonparametric topic model based on the hierarchical Dirichlet process (HDP), and develop a novel online variational inference algorithm based on split-merge topic updates. We derive a simpler and faster variational approximation of the HDP, and show that by intelligently splitting and merging components of the variational posterior, we can achieve substantially better predictions of test data than conventional online and batch variational algorithms. For streaming analysis of large datasets where batch analysis is infeasible, we show that our split-merge updates better capture the nonparametric properties of the underlying model, allowing continual learning of new topics.",
    "authors": [
      "Bryant, Michael",
      "Sudderth, Erik"
    ]
  },
  {
    "id": "839ab46820b524afda05122893c2fe8e",
    "title": "3D Object Detection and Viewpoint Estimation with a Deformable 3D Cuboid Model",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/839ab46820b524afda05122893c2fe8e-Paper.pdf",
    "abstract": "This paper addresses the problem of category-level 3D object detection. Given a monocular image, our aim is to localize the objects  in 3D by enclosing them with tight  oriented 3D bounding boxes. We propose a novel approach that extends the well-acclaimed deformable part-based model[Felz.] to reason in 3D. Our model represents an object class as a deformable 3D cuboid composed of faces and parts, which are both allowed to deform with respect to their anchors on the 3D box. We model the appearance of each face in fronto-parallel coordinates, thus effectively factoring out the appearance variation induced by viewpoint. Our model reasons about face visibility patters called aspects. We train the cuboid model jointly and discriminatively and share weights across all aspects to attain efficiency. Inference then entails sliding and rotating the box in 3D and scoring object hypotheses. While for inference we discretize the search space, the variables are  continuous in our model. We demonstrate the effectiveness of our approach in indoor and outdoor scenarios, and show that our approach outperforms the state-of-the-art in both 2D[Felz09] and 3D object detection[Hedau12].",
    "authors": [
      "Fidler, Sanja",
      "Dickinson, Sven",
      "Urtasun, Raquel"
    ]
  },
  {
    "id": "83f2550373f2f19492aa30fbd5b57512",
    "title": "Risk-Aversion in Multi-armed Bandits",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/83f2550373f2f19492aa30fbd5b57512-Paper.pdf",
    "abstract": "In stochastic multi--armed bandits the objective is to solve the exploration--exploitation dilemma and ultimately maximize the expected reward. Nonetheless, in many practical problems, maximizing the expected reward is not the most desirable objective. In this paper, we introduce a novel setting based on the principle of risk--aversion where the objective is to compete against the arm with the best risk--return trade--off. This setting proves to be intrinsically more difficult than the standard multi-arm bandit setting due in part to an exploration risk which introduces a regret associated to the variability of an algorithm. Using variance as a measure of risk, we introduce two new algorithms, we investigate their theoretical guarantees, and we report preliminary empirical results.",
    "authors": [
      "Sani, Amir",
      "Lazaric, Alessandro",
      "Munos, R\u00e9mi"
    ]
  },
  {
    "id": "83f97f4825290be4cb794ec6a234595f",
    "title": "Approximate Message Passing with Consistent Parameter Estimation and Applications to Sparse Learning",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/83f97f4825290be4cb794ec6a234595f-Paper.pdf",
    "abstract": "We consider the estimation of an i.i.d.\\ vector $\\xbf \\in \\R^n$ from measurements $\\ybf \\in \\R^m$ obtained by a general cascade model consisting of a known linear transform followed by a probabilistic componentwise (possibly nonlinear) measurement channel. We present a method, called adaptive generalized approximate message passing (Adaptive GAMP), that enables joint learning of the statistics of the prior and measurement channel along with estimation of the unknown vector $\\xbf$. The proposed algorithm is a generalization of a recently-developed method by Vila and Schniter that uses expectation-maximization (EM) iterations where the posteriors in the E-steps are computed via approximate message passing. The techniques can be applied to a large class of learning problems including the learning of sparse priors in compressed sensing or identification of linear-nonlinear cascade models in dynamical systems and neural spiking processes.  We prove that for large i.i.d.\\ Gaussian transform matrices the asymptotic componentwise behavior of the adaptive GAMP algorithm is predicted by a simple set of scalar state evolution equations.  This analysis shows that the adaptive GAMP method can yield asymptotically consistent parameter estimates, which implies that the algorithm achieves a reconstruction quality equivalent to the oracle algorithm that knows the correct parameter values.  The adaptive GAMP methodology thus provides a systematic, general and computationally efficient method applicable to a large range of complex linear-nonlinear models with provable guarantees.",
    "authors": [
      "Kamilov, Ulugbek",
      "Rangan, Sundeep",
      "Unser, Michael",
      "Fletcher, Alyson K."
    ]
  },
  {
    "id": "83fa5a432ae55c253d0e60dbfa716723",
    "title": "Gradient-based kernel method for feature extraction and variable selection",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/83fa5a432ae55c253d0e60dbfa716723-Paper.pdf",
    "abstract": "We propose a novel kernel approach to dimension reduction for supervised learning: feature extraction and variable selection; the former constructs a small number of features from predictors, and the latter finds a subset of predictors. First, a method of linear feature extraction is proposed using the gradient of regression function, based on the recent development of the kernel method.  In comparison with other existing methods, the proposed one has wide applicability without strong assumptions on the regressor or type of variables, and uses computationally simple eigendecomposition, thus applicable to large data sets.  Second, in combination of a sparse penalty, the method is extended to variable selection, following the approach by Chen et al. (2010).  Experimental results show that the proposed methods successfully find effective features and variables without parametric models.",
    "authors": [
      "Fukumizu, Kenji",
      "Leng, Chenlei"
    ]
  },
  {
    "id": "84438b7aae55a0638073ef798e50b4ef",
    "title": "Scalable imputation of genetic data with a discrete fragmentation-coagulation process",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/84438b7aae55a0638073ef798e50b4ef-Paper.pdf",
    "abstract": "We present a Bayesian nonparametric model for genetic sequence data in which a set of genetic sequences is modelled using a Markov model of  partitions. The partitions at consecutive locations in the genome are related by their clusters first splitting and then merging.  Our model can be thought of as a discrete time analogue of continuous time fragmentation-coagulation processes [Teh et al 2011], preserving the important properties of projectivity, exchangeability and reversibility, while being more scalable. We apply this model to the problem of genotype imputation, showing improved computational efficiency while maintaining the same accuracies as in [Teh et al 2011].",
    "authors": [
      "Elliott, Lloyd",
      "Teh, Yee"
    ]
  },
  {
    "id": "84d9ee44e457ddef7f2c4f25dc8fa865",
    "title": "Non-parametric Approximate Dynamic Programming via the Kernel Method",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/84d9ee44e457ddef7f2c4f25dc8fa865-Paper.pdf",
    "abstract": "This paper presents a novel non-parametric approximate dynamic programming (ADP) algorithm that enjoys graceful, dimension-independent approximation and sample complexity guarantees. In particular, we establish both theoretically and computationally that our proposal can serve as a viable alternative to state-of-the-art parametric ADP algorithms, freeing the designer from carefully specifying an approximation architecture. We accomplish this by developing a kernel-based mathematical program for ADP. Via a computational study on a controlled queueing network, we show that our non-parametric procedure is competitive with parametric ADP approaches.",
    "authors": [
      "Bhat, Nikhil",
      "Farias, Vivek",
      "Moallemi, Ciamac C."
    ]
  },
  {
    "id": "84fdbc3ac902561c00871c9b0c226756",
    "title": "Probabilistic n-Choose-k Models for Classification and Ranking",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/84fdbc3ac902561c00871c9b0c226756-Paper.pdf",
    "abstract": "In categorical data there is often structure in the number of variables that take on each label. For example, the total number of objects in an image and the number of highly relevant documents per query in web search both tend to follow a structured distribution. In this paper, we study a probabilistic model that explicitly includes a prior distribution over such counts, along with a count-conditional likelihood that de\ufb01nes probabilities over all subsets of a given size. When labels are binary and the prior over counts is a Poisson-Binomial distribution, a standard logistic regression model is recovered, but for other count distributions, such priors induce global dependencies and combinatorics that appear to complicate learning and inference. However, we demonstrate that simple, ef\ufb01cient learning procedures can be derived for more general forms of this model. We illustrate the utility of the formulation by exploring applications to multi-object classi\ufb01cation, learning to rank, and top-K classi\ufb01cation.",
    "authors": [
      "Swersky, Kevin",
      "Frey, Brendan J.",
      "Tarlow, Daniel",
      "Zemel, Richard",
      "Adams, Ryan P."
    ]
  },
  {
    "id": "85422afb467e9456013a2a51d4dff702",
    "title": "Delay Compensation with Dynamical Synapses",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/85422afb467e9456013a2a51d4dff702-Paper.pdf",
    "abstract": "Time delay is pervasive in neural information processing. To achieve real-time tracking, it is critical to compensate the transmission and processing delays in a neural system. In the present study we show that dynamical synapses with short-term depression can enhance the mobility of a continuous attractor network to the extent that the system tracks time-varying stimuli in a timely manner. The state of the network can either track the instantaneous position of a moving stimulus perfectly (with zero-lag) or lead it with an effectively constant time, in agreement with experiments on the head-direction systems in rodents. The parameter regions for delayed, perfect and anticipative tracking correspond to network states that are static, ready-to-move and spontaneously moving, respectively, demonstrating the strong correlation between tracking performance and the intrinsic dynamics of the network. We also find that when the speed of the stimulus coincides with the natural speed of the network state, the delay becomes effectively independent of the stimulus amplitude.",
    "authors": [
      "Fung, Chi",
      "Wong, K.",
      "Wu, Si"
    ]
  },
  {
    "id": "85d8ce590ad8981ca2c8286f79f59954",
    "title": "Visual Recognition using Embedded Feature Selection for Curvature Self-Similarity",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/85d8ce590ad8981ca2c8286f79f59954-Paper.pdf",
    "abstract": "Category-level object detection has a crucial need for informative object representations. This demand has led to feature descriptors of ever increasing dimensionality  like co-occurrence statistics and self-similarity. In this paper we propose a new object representation based on curvature self-similarity that goes beyond the currently popular  approximation of objects using straight lines. However, like all descriptors using second order statistics, ours also exhibits a high dimensionality. Although improving discriminability,  the high dimensionality becomes a critical issue due to lack of generalization ability and curse of dimensionality. Given only a limited amount of training data, even sophisticated  learning algorithms such as the popular kernel methods are not able to suppress noisy or superfluous dimensions of such high-dimensional data. Consequently, there is a natural need for  feature selection when using present-day informative features and, particularly, curvature self-similarity. We therefore suggest an embedded feature selection method for SVMs that  reduces complexity and improves generalization capability of object models. By successfully integrating the proposed curvature self-similarity representation together with the embedded  feature selection in a widely used state-of-the-art object detection framework we show the general pertinence of the approach.",
    "authors": [
      "Eigenstetter, Angela",
      "Ommer, Bjorn"
    ]
  },
  {
    "id": "85fc37b18c57097425b52fc7afbb6969",
    "title": "High-Order Multi-Task Feature Learning to Identify Longitudinal Phenotypic Markers for Alzheimer's Disease Progression Prediction",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/85fc37b18c57097425b52fc7afbb6969-Paper.pdf",
    "abstract": "Alzheimer disease (AD) is a neurodegenerative disorder characterized by progressive impairment of memory and other cognitive functions. Regression analysis has been studied to relate neuroimaging measures to cognitive status. However, whether these measures have further predictive power to infer a trajectory of cognitive performance over time is still an under-explored but important topic in AD research. We propose a novel high-order multi-task learning model to address this issue. The proposed model explores the temporal correlations existing in data features and regression tasks by the structured sparsity-inducing norms. In addition, the sparsity of the model enables the selection of a small number of MRI measures while maintaining high prediction accuracy. The empirical studies, using the baseline MRI and serial cognitive data of the ADNI cohort, have yielded promising results.",
    "authors": [
      "Wang, Hua",
      "Nie, Feiping",
      "Huang, Heng",
      "Yan, Jingwen",
      "Kim, Sungeun",
      "Risacher, Shannon",
      "Saykin, Andrew",
      "Shen, Li"
    ]
  },
  {
    "id": "86b122d4358357d834a87ce618a55de0",
    "title": "Multiresolution analysis on the symmetric group",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/86b122d4358357d834a87ce618a55de0-Paper.pdf",
    "abstract": "There is no generally accepted way to define wavelets on permutations. We address this issue by introducing the notion of coset based multiresolution analysis (CMRA) on the symmetric group; find the corresponding wavelet functions; and describe a fast wavelet transform of O(n^p) complexity with small p for sparse signals (in contrast to the O(n^q n!) complexity typical of FFTs). We discuss potential applications in ranking, sparse approximation, and multi-object tracking.",
    "authors": [
      "Kondor, Risi",
      "Dempsey, Walter"
    ]
  },
  {
    "id": "884d247c6f65a96a7da4d1105d584ddd",
    "title": "Learned Prioritization for Trading Off Accuracy and Speed",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/884d247c6f65a96a7da4d1105d584ddd-Paper.pdf",
    "abstract": "Users want natural language processing (NLP) systems to be both fast and accurate, but quality often comes at the cost of speed. The field has been manually exploring various speed-accuracy tradeoffs (for particular problems and datasets).  We aim to explore this space automatically, focusing here on the case of agenda-based syntactic parsing \\cite{kay-1986}. Unfortunately, off-the-shelf reinforcement learning techniques fail to learn good policies: the state space is simply too large to explore naively. An attempt to counteract this by applying imitation learning algorithms also fails: the ``teacher'' is far too good to successfully imitate with our inexpensive features.  Moreover, it is not specifically tuned for the known reward function.  We propose a hybrid reinforcement/apprenticeship learning algorithm that, even with only a few inexpensive features, can automatically learn weights that achieve competitive accuracies at significant improvements in speed over state-of-the-art baselines.",
    "authors": [
      "Jiang, Jiarong",
      "Teichert, Adam",
      "Eisner, Jason",
      "Daume, Hal"
    ]
  },
  {
    "id": "89fcd07f20b6785b92134bd6c1d0fa42",
    "title": "Learning as MAP Inference in Discrete Graphical Models",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/89fcd07f20b6785b92134bd6c1d0fa42-Paper.pdf",
    "abstract": "We present a new formulation for attacking binary classification problems. Instead of relying on convex losses and regularisers such as in SVMs, logistic regression and boosting, or instead non-convex but continuous formulations such as those encountered in neural networks and deep belief networks, our framework entails a non-convex but \\emph{discrete} formulation, where estimation amounts to finding a MAP configuration in a graphical model whose potential functions are low-dimensional discrete surrogates for the misclassification loss. We argue that such a discrete formulation can naturally account for a number of issues that are typically encountered in either the convex or the continuous non-convex paradigms, or both. By reducing the learning problem to a MAP inference problem, we can immediately translate the guarantees available for many inference settings to the learning problem itself. We empirically demonstrate in a number of experiments that this approach is promising in dealing with issues such as severe label noise, while still having global optimality guarantees. Due to the discrete nature of the formulation, it also allows for \\emph{direct} regularisation through cardinality-based penalties, such as the $\\ell_0$ pseudo-norm, thus providing the ability to perform feature selection and trade-off interpretability and predictability in a principled manner. We also outline a number of open problems arising from the formulation.",
    "authors": [
      "Liu, Xianghang",
      "Petterson, James",
      "Caetano, Tib\u00e9rio"
    ]
  },
  {
    "id": "8a0e1141fd37fa5b98d5bb769ba1a7cc",
    "title": "Hierarchical Optimistic Region Selection driven by Curiosity",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/8a0e1141fd37fa5b98d5bb769ba1a7cc-Paper.pdf",
    "abstract": "This paper aims to take a step forwards making the term ``intrinsic motivation'' from reinforcement learning theoretically well founded, focusing on curiosity-driven learning. To that end, we consider the setting where, a fixed partition P of a continuous space X being given, and a process \\nu defined on X being unknown, we are asked to sequentially decide which cell of the partition to select as well as where to sample \\nu in that cell, in order to minimize a loss function that is inspired from previous work on curiosity-driven learning. The loss on each cell consists of one term measuring a simple worst case quadratic sampling error, and a penalty term proportional to the range of the variance in that cell. The corresponding problem formulation extends the setting known as active learning for multi-armed bandits to the case when each arm is a continuous region, and we show how an adaptation of recent algorithms for that problem and of hierarchical optimistic sampling algorithms for optimization can be used in order to solve this problem. The resulting procedure, called Hierarchical Optimistic Region SElection driven by Curiosity (HORSE.C) is provided together with a finite-time regret analysis.",
    "authors": [
      "Maillard, Odalric-ambrym"
    ]
  },
  {
    "id": "8a146f1a3da4700cbf03cdc55e2daae6",
    "title": "Trajectory-Based Short-Sighted Probabilistic Planning",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/8a146f1a3da4700cbf03cdc55e2daae6-Paper.pdf",
    "abstract": "Probabilistic planning captures the uncertainty of plan execution by probabilistically modeling the effects of actions in the environment, and therefore the probability of reaching different states from a given state and action. In order to compute a solution for a probabilistic planning problem, planners need to manage the uncertainty associated with the different paths from the initial state to a goal state. Several approaches to manage uncertainty were proposed, e.g., consider all paths at once, perform determinization of actions, and sampling. In this paper, we introduce trajectory-based short-sighted Stochastic Shortest Path Problems (SSPs), a novel approach to manage uncertainty for probabilistic planning problems in which states reachable with low probability are substituted by artificial goals that heuristically estimate their cost to reach a goal state. We also extend the theoretical results of Short-Sighted Probabilistic Planner (SSiPP) [ref] by proving that SSiPP always finishes and is asymptotically optimal under sufficient conditions on the structure of short-sighted SSPs.  We empirically compare SSiPP using trajectory-based short-sighted SSPs with the winners of the previous probabilistic planning competitions and other state-of-the-art planners in the triangle tireworld problems. Trajectory-based SSiPP outperforms all the competitors and is the only planner able to scale up to problem number 60, a problem in which the optimal solution contains approximately $10^{70}$ states.",
    "authors": [
      "Trevizan, Felipe",
      "Veloso, Manuela"
    ]
  },
  {
    "id": "8b0d268963dd0cfb808aac48a549829f",
    "title": "Best Arm Identification: A Unified Approach to Fixed Budget and Fixed Confidence",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/8b0d268963dd0cfb808aac48a549829f-Paper.pdf",
    "abstract": "We study the problem of identifying the best arm(s) in the stochastic multi-armed bandit setting. This problem has been studied in the literature from two different perspectives: fixed budget and fixed confidence. We propose a unifying approach that leads to a meta-algorithm called unified gap-based exploration (UGapE), with a common structure and similar theoretical analysis for these two settings. We prove a performance bound for the two versions of the algorithm showing that the two problems are characterized by the same notion of complexity. We also show how the UGapE algorithm as well as its theoretical analysis can be extended to take into account the variance of the arms and to multiple bandits. Finally, we evaluate the performance of UGapE and compare it with a number of existing fixed budget and fixed confidence algorithms.",
    "authors": [
      "Gabillon, Victor",
      "Ghavamzadeh, Mohammad",
      "Lazaric, Alessandro"
    ]
  },
  {
    "id": "8b6dd7db9af49e67306feb59a8bdc52c",
    "title": "On the Use of Non-Stationary Policies for Stationary Infinite-Horizon Markov Decision Processes",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/8b6dd7db9af49e67306feb59a8bdc52c-Paper.pdf",
    "abstract": "We consider infinite-horizon stationary $\\gamma$-discounted Markov   Decision Processes, for which it is known that there exists a   stationary optimal policy. Using Value and Policy Iteration with   some error $\\epsilon$ at each iteration, it is well-known that one   can compute stationary policies that are $\\frac{2\\gamma{(1-\\gamma)^2}\\epsilon$-optimal. After arguing that this   guarantee is tight, we develop variations of Value and Policy   Iteration for computing non-stationary policies that can be up to   $\\frac{2\\gamma}{1-\\gamma}\\epsilon$-optimal, which constitutes a significant   improvement in the usual situation when $\\gamma$ is close to   $1$. Surprisingly, this shows that the problem of ``computing near-optimal non-stationary policies'' is much simpler than that   of ``computing near-optimal stationary policies''.",
    "authors": [
      "Scherrer, Bruno",
      "Lesner, Boris"
    ]
  },
  {
    "id": "8c19f571e251e61cb8dd3612f26d5ecf",
    "title": "Deep Spatio-Temporal Architectures and Learning for Protein Structure Prediction",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/8c19f571e251e61cb8dd3612f26d5ecf-Paper.pdf",
    "abstract": "Residue-residue contact prediction is a fundamental problem in protein structure prediction. Hower, despite considerable research efforts, contact prediction methods are still largely unreliable. Here we introduce a novel deep machine-learning architecture which consists of a multidimensional stack of learning modules. For contact prediction, the idea is implemented as a three-dimensional stack of Neural Networks NN^k_{ij}, where i and j index the spatial coordinates of the contact map and k indexes ''time''. The temporal dimension is introduced to capture the fact that protein folding is not an instantaneous process, but rather a progressive refinement. Networks at level k in the stack can be trained in supervised fashion to refine the predictions produced by the previous level, hence addressing the problem of vanishing gradients, typical of deep architectures. Increased accuracy and generalization capabilities of this approach are established by rigorous comparison with other classical machine learning approaches for contact prediction. The deep approach leads to an accuracy for difficult long-range contacts of about 30%, roughly 10% above the state-of-the-art. Many variations in the architectures and the training algorithms are possible, leaving room for further improvements. Furthermore, the approach is applicable to other problems with strong underlying spatial and temporal components.",
    "authors": [
      "Lena, Pietro",
      "Nagata, Ken",
      "Baldi, Pierre"
    ]
  },
  {
    "id": "8c6744c9d42ec2cb9e8885b54ff744d0",
    "title": "Isotropic Hashing",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/8c6744c9d42ec2cb9e8885b54ff744d0-Paper.pdf",
    "abstract": "Most existing hashing methods adopt some projection functions to project the original data into several dimensions of real values, and then each of these projected dimensions is quantized into one bit (zero or one) by thresholding. Typically, the variances of different projected dimensions are different for existing projection functions such as principal component analysis (PCA). Using the same number of bits for different projected dimensions is unreasonable because larger-variance dimensions will carry more information. Although this viewpoint has been widely accepted by many researchers, it is still not verified by either theory or experiment because no methods have been proposed to find a projection with equal variances for different dimensions. In this paper, we propose a novel method, called isotropic hashing (IsoHash), to learn projection functions which can produce projected dimensions with isotropic variances (equal variances). Experimental results on real data sets show that IsoHash can outperform its counterpart with different variances for different dimensions, which verifies the viewpoint that projections with isotropic variances will be better than those with anisotropic variances.",
    "authors": [
      "Kong, Weihao",
      "Li, Wu-jun"
    ]
  },
  {
    "id": "8d6dc35e506fc23349dd10ee68dabb64",
    "title": "Repulsive Mixtures",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/8d6dc35e506fc23349dd10ee68dabb64-Paper.pdf",
    "abstract": "Discrete mixtures are used routinely in broad sweeping applications ranging from unsupervised settings to fully supervised multi-task learning.  Indeed, finite mixtures and infinite mixtures, relying on Dirichlet processes and modifications, have become a standard tool.  One important issue that arises in using discrete mixtures is low separation in the components; in particular, different components can be introduced that are very similar and hence redundant.   Such redundancy leads to too many clusters that are too similar, degrading performance in unsupervised learning and leading to computational problems and an unnecessarily complex model in supervised settings.  Redundancy can arise in the absence of a penalty on components placed close together even when a Bayesian approach is used to learn the number of components.  To solve this problem, we propose a novel prior that generates components from a repulsive process, automatically penalizing redundant components.  We characterize this repulsive prior theoretically and propose a Markov chain Monte Carlo sampling algorithm for posterior computation.  The methods are illustrated using synthetic examples and an iris data set.",
    "authors": [
      "Petralia, Francesca",
      "Rao, Vinayak",
      "Dunson, David"
    ]
  },
  {
    "id": "8df707a948fac1b4a0f97aa554886ec8",
    "title": "Forward-Backward Activation Algorithm for Hierarchical Hidden Markov Models",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/8df707a948fac1b4a0f97aa554886ec8-Paper.pdf",
    "abstract": "Hierarchical Hidden Markov Models (HHMMs) are sophisticated stochastic models that enable us to capture a hierarchical context characterization of sequence data. However, existing HHMM parameter estimation methods require large computations of time complexity O(TN^{2D}) at least for model inference, where D is the depth of the hierarchy, N is the number of states in each level, and T is the sequence length. In this paper, we propose a new inference method of HHMMs for which the time complexity is O(TN^{D+1}). A key idea of our algorithm is application of the forward-backward algorithm to ''state activation probabilities''. The notion of a state activation, which offers a simple formalization of the hierarchical transition behavior of HHMMs, enables us to conduct model inference efficiently. We present some experiments to demonstrate that our proposed method works more efficiently to estimate HHMM parameters than do some existing methods such as the flattening method and Gibbs sampling method.",
    "authors": [
      "Wakabayashi, Kei",
      "Miura, Takao"
    ]
  },
  {
    "id": "8e296a067a37563370ded05f5a3bf3ec",
    "title": "Finding Exemplars from Pairwise Dissimilarities via Simultaneous Sparse Recovery",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/8e296a067a37563370ded05f5a3bf3ec-Paper.pdf",
    "abstract": "Given pairwise dissimilarities between data points, we consider the problem of finding a subset of data points called representatives or exemplars that can efficiently describe the data collection. We formulate the problem as a row-sparsity regularized trace minimization problem which can be solved efficiently using convex programming. The solution of the proposed optimization program finds the representatives and the probability that each data point is associated to each one of the representatives. We obtain the range of the regularization parameter for which the solution of the proposed optimization program changes from selecting one representative to selecting all data points as the representatives. When data points are distributed around multiple clusters according to the dissimilarities, we show that the data in each cluster select only representatives from that cluster. Unlike metric-based methods, our algorithm does not require that the pairwise dissimilarities be metrics and can be applied to dissimilarities that are asymmetric or violate the triangle inequality. We demonstrate the effectiveness of the proposed algorithm on synthetic data as well as real-world datasets of images and text.",
    "authors": [
      "Elhamifar, Ehsan",
      "Sapiro, Guillermo",
      "Vidal, Ren\u00e9"
    ]
  },
  {
    "id": "8e6b42f1644ecb1327dc03ab345e618b",
    "title": "Mirror Descent Meets Fixed Share (and feels no regret)",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/8e6b42f1644ecb1327dc03ab345e618b-Paper.pdf",
    "abstract": "Mirror descent with an entropic regularizer is known to achieve shifting regret bounds that are logarithmic in the dimension. This is done using either a carefully designed projection or by a weight sharing technique. Via a novel unified analysis, we show that these two approaches deliver essentially equivalent bounds on a notion of regret generalizing shifting, adaptive, discounted, and other related regrets. Our analysis also captures and extends the generalized weight sharing technique of Bousquet and Warmuth, and can be refined in several ways, including improvements for small losses and adaptive tuning of parameters.",
    "authors": [
      "Cesa-bianchi, Nicol\u00f2",
      "Gaillard, Pierre",
      "Lugosi, Gabor",
      "Stoltz, Gilles"
    ]
  },
  {
    "id": "8e98d81f8217304975ccb23337bb5761",
    "title": "Semi-Supervised Domain Adaptation with Non-Parametric Copulas",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/8e98d81f8217304975ccb23337bb5761-Paper.pdf",
    "abstract": "A new framework based on the theory of copulas is proposed to address semi-supervised domain adaptation problems. The presented method factorizes any multivariate density into a product of marginal distributions and bivariate copula functions. Therefore, changes in each of these factors can be detected and corrected to adapt a density model across different learning domains. Importantly, we introduce a novel vine copula model, which allows for this factorization in a non-parametric manner. Experimental results on regression problems with real-world data illustrate the efficacy of the proposed approach when compared to state-of-the-art techniques.",
    "authors": [
      "Lopez-paz, David",
      "Hern\u00e1ndez-lobato, Jose",
      "Sch\u00f6lkopf, Bernhard"
    ]
  },
  {
    "id": "8eefcfdf5990e441f0fb6f3fad709e21",
    "title": "The Lov\u00e1sz \u03d1 function, SVMs and finding large dense subgraphs",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/8eefcfdf5990e441f0fb6f3fad709e21-Paper.pdf",
    "abstract": "The Lovasz $\\theta$ function of a graph, is a fundamental tool in combinatorial optimization and approximation algorithms.  Computing $\\theta$ involves solving a SDP  and is extremely expensive even for moderately sized graphs.  In this paper we establish that the Lovasz $\\theta$ function is equivalent to  a kernel learning problem related to one class SVM. This interesting connection opens up many opportunities  bridging graph theoretic algorithms and machine learning.   We show that there exist graphs, which we call $SVM-\\theta$ graphs, on which the Lovasz $\\theta$ function can be approximated well by a one-class  SVM.    This leads to a novel use of SVM techniques to solve algorithmic problems in large graphs e.g. identifying a planted clique  of size $\\Theta({\\sqrt{n}})$ in a random graph $G(n,\\frac{1}{2})$. A classic approach for this problem involves computing  the $\\theta$ function, however it is not scalable due to SDP computation. We show that the random graph with a planted clique is an example of $SVM-\\theta$ graph, and as a consequence a SVM based approach  easily identifies the clique in large graphs and is competitive with the  state-of-the-art.    Further, we introduce  the notion of a ''common orthogonal labeling'' which extends the notion  of a ''orthogonal labelling of a single  graph (used in defining the $\\theta$ function)  to multiple graphs.  The problem of finding the optimal common orthogonal labelling is cast as a  Multiple Kernel Learning problem and is used to identify a large common dense region in multiple graphs.  The proposed algorithm achieves an order of magnitude scalability compared to the state of the art.",
    "authors": [
      "Jethava, Vinay",
      "Martinsson, Anders",
      "Bhattacharyya, Chiranjib",
      "Dubhashi, Devdatt"
    ]
  },
  {
    "id": "8f1d43620bc6bb580df6e80b0dc05c48",
    "title": "Slice sampling normalized kernel-weighted completely random measure mixture models",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/8f1d43620bc6bb580df6e80b0dc05c48-Paper.pdf",
    "abstract": "A number of dependent nonparametric processes have been proposed to model non-stationary data with unknown latent dimensionality.  However, the inference algorithms are often slow and unwieldy, and are in general highly specific to a given model formulation. In this paper, we describe a wide class of nonparametric processes, including several existing models, and present a slice sampler that allows efficient inference across this class of models.",
    "authors": [
      "Foti, Nick",
      "Williamson, Sinead"
    ]
  },
  {
    "id": "8f85517967795eeef66c225f7883bdcb",
    "title": "Automatic Feature Induction for Stagewise Collaborative Filtering",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/8f85517967795eeef66c225f7883bdcb-Paper.pdf",
    "abstract": "Recent approaches to collaborative filtering have concentrated on estimating an algebraic or statistical model, and using the model for predicting missing ratings. In this paper we observe that different models have relative advantages in different regions of the input space. This motivates our approach of using stagewise linear combinations of collaborative filtering algorithms, with non-constant combination coefficients based on kernel smoothing. The resulting stagewise model is computationally scalable and outperforms a wide selection of state-of-the-art collaborative filtering algorithms.",
    "authors": [
      "Lee, Joonseok",
      "Sun, Mingxuan",
      "Kim, Seungyeon",
      "Lebanon, Guy"
    ]
  },
  {
    "id": "905056c1ac1dad141560467e0a99e1cf",
    "title": "A Stochastic Gradient Method with an Exponential Convergence _Rate for Finite Training Sets",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/905056c1ac1dad141560467e0a99e1cf-Paper.pdf",
    "abstract": "We propose a new stochastic gradient method for optimizing the sum of\u2029 a finite set of smooth functions, where the sum is strongly convex.\u2029 While standard stochastic gradient methods\u2029 converge at sublinear rates for this problem, the proposed method incorporates a memory of previous gradient values in order to achieve a linear convergence \u2029rate.  In a machine learning context, numerical experiments indicate that the new algorithm can dramatically outperform standard\u2029 algorithms, both in terms of optimizing the training error and reducing the test error quickly.",
    "authors": [
      "Roux, Nicolas",
      "Schmidt, Mark",
      "Bach, Francis"
    ]
  },
  {
    "id": "912d2b1c7b2826caf99687388d2e8f7c",
    "title": "Monte Carlo Methods for Maximum Margin Supervised Topic Models",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/912d2b1c7b2826caf99687388d2e8f7c-Paper.pdf",
    "abstract": "An effective strategy to exploit the supervising side information for discovering predictive topic representations is to impose discriminative constraints induced by such information on the posterior distributions under a topic model. This strategy has been adopted by a number of supervised topic models, such as MedLDA, which employs max-margin posterior constraints. However, unlike the likelihood-based supervised topic models, of which posterior inference can be carried out using the Bayes' rule, the max-margin posterior constraints have made Monte Carlo methods infeasible or at least not directly applicable, thereby limited the choice of inference algorithms to be based on variational approximation with strict mean field assumptions. In this paper, we develop two efficient Monte Carlo methods under much weaker assumptions for max-margin supervised topic models based on an importance sampler and a collapsed Gibbs sampler, respectively, in a convex dual formulation. We report thorough experimental results that compare our approach favorably against existing alternatives in both accuracy and efficiency.",
    "authors": [
      "Jiang, Qixia",
      "Zhu, Jun",
      "Sun, Maosong",
      "Xing, Eric"
    ]
  },
  {
    "id": "918317b57931b6b7a7d29490fe5ec9f9",
    "title": "Analyzing 3D Objects in Cluttered Images",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/918317b57931b6b7a7d29490fe5ec9f9-Paper.pdf",
    "abstract": "We present an approach to detecting and analyzing the 3D configuration of objects in real-world images with heavy occlusion and clutter. We focus on the application of finding and analyzing cars. We do so with a two-stage model; the first stage reasons about 2D shape and appearance variation due to within-class variation(station wagons look different than sedans) and changes in viewpoint. Rather than using a view-based model, we describe a compositional representation that models a large number of effective views and shapes using a small number of local view-based templates. We use this model to propose candidate detections and 2D estimates of shape. These estimates are then refined by our second stage, using an explicit 3D model of shape and viewpoint. We use a morphable model to capture 3D within-class variation, and use a weak-perspective camera model to capture viewpoint. We learn all model parameters from 2D annotations. We demonstrate state-of-the-art accuracy for detection, viewpoint estimation, and 3D shape reconstruction on challenging images from the PASCAL VOC 2011 dataset.",
    "authors": [
      "Hejrati, Mohsen",
      "Ramanan, Deva"
    ]
  },
  {
    "id": "92977ae4d2ba21425a59afb269c2a14e",
    "title": "A mechanistic model of early sensory processing based on subtracting sparse representations",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/92977ae4d2ba21425a59afb269c2a14e-Paper.pdf",
    "abstract": "Early stages of sensory systems face the challenge of compressing information from numerous receptors onto a much smaller number of projection neurons, a so called communication bottleneck. To make more efficient use of limited bandwidth, compression may be achieved using predictive coding, whereby predictable, or redundant, components of the stimulus are removed. In the case of the retina, Srinivasan et al. (1982) suggested that feedforward inhibitory connections subtracting a linear prediction generated from nearby receptors implement such compression, resulting in biphasic center-surround receptive fields. However, feedback inhibitory circuits are common in early sensory circuits and furthermore their dynamics may be nonlinear. Can such circuits implement predictive coding as well? Here, solving the transient dynamics of nonlinear reciprocal feedback circuits through analogy to a signal-processing algorithm called linearized Bregman iteration we show that nonlinear predictive coding can be implemented in an inhibitory feedback circuit. In response to a step stimulus, interneuron activity in time constructs progressively less sparse but more accurate representations of the stimulus, a temporally evolving prediction. This analysis provides a powerful theoretical framework to interpret and understand the dynamics of early sensory processing in a variety of physiological experiments and yields novel predictions regarding the relation between activity and stimulus statistics.",
    "authors": [
      "Druckmann, Shaul",
      "Hu, Tao",
      "Chklovskii, Dmitri"
    ]
  },
  {
    "id": "92c8c96e4c37100777c7190b76d28233",
    "title": "Ensemble weighted kernel estimators for multivariate entropy estimation",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/92c8c96e4c37100777c7190b76d28233-Paper.pdf",
    "abstract": "The problem of estimation of entropy functionals of probability densities has received much attention in the information theory, machine learning and statistics communities. Kernel density plug-in estimators are simple, easy to implement and widely used for estimation of entropy. However, kernel plug-in estimators suffer from the curse of dimensionality, wherein the MSE rate of convergence is glacially slow - of order  $O(T^{-{\\gamma}/{d}})$, where $T$ is the number of samples, and $\\gamma>0$ is a rate parameter. In this paper, it is shown that for sufficiently smooth densities, an ensemble of kernel plug-in estimators can be combined via a weighted convex combination, such that the resulting weighted estimator has a superior parametric MSE rate of convergence of order $O(T^{-1})$. Furthermore, it is shown that these optimal weights can be determined by solving a convex optimization problem which does not require training data or knowledge of the underlying density, and therefore can be performed offline. This novel result is remarkable in that, while each of the individual kernel plug-in estimators belonging to the ensemble suffer from the curse of dimensionality, by appropriate ensemble averaging we can achieve parametric convergence rates.",
    "authors": [
      "Sricharan, Kumar",
      "Hero, Alfred"
    ]
  },
  {
    "id": "92fb0c6d1758261f10d052e6e2c1123c",
    "title": "Active Comparison of Prediction Models",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/92fb0c6d1758261f10d052e6e2c1123c-Paper.pdf",
    "abstract": "We address the problem of comparing the risks of two given predictive models - for instance, a baseline model and a challenger - as confidently as possible on a fixed labeling budget. This problem occurs whenever models cannot be compared on held-out training data, possibly because the training data are unavailable or do not reflect the desired test distribution. In this case, new test instances have to be drawn and labeled at a cost. We devise an active comparison method that selects instances according to an instrumental sampling distribution. We derive the sampling distribution that maximizes the power of a statistical test applied to the observed empirical risks, and thereby minimizes the likelihood of choosing the inferior model. Empirically, we investigate model selection problems on several classification and regression tasks and study the accuracy of the resulting p-values.",
    "authors": [
      "Sawade, Christoph",
      "Landwehr, Niels",
      "Scheffer, Tobias"
    ]
  },
  {
    "id": "93d65641ff3f1586614cf2c1ad240b6c",
    "title": "Reducing statistical time-series problems to binary classification",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/93d65641ff3f1586614cf2c1ad240b6c-Paper.pdf",
    "abstract": "We  show how binary classification methods developed to work on i.i.d. data can be  used for solving  statistical problems that are seemingly unrelated to classification and concern highly-dependent time series.  Specifically, the problems of time-series  clustering, homogeneity testing and the three-sample problem  are addressed. The algorithms that we construct for solving  these problems are based on a new metric between time-series distributions, which can be evaluated using binary classification methods.  Universal consistency of the  proposed algorithms  is proven under most general assumptions. The theoretical results are illustrated with experiments on synthetic and real-world data.",
    "authors": [
      "Ryabko, Daniil",
      "Mary, Jeremie"
    ]
  },
  {
    "id": "9872ed9fc22fc182d371c3e9ed316094",
    "title": "Max-Margin Structured Output Regression for Spatio-Temporal Action Localization",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/9872ed9fc22fc182d371c3e9ed316094-Paper.pdf",
    "abstract": "Structured output learning has been successfully applied to object localization, where the mapping between an image and an object bounding box can be well captured. Its extension to action localization in videos, however, is much more challenging, because one needs to predict the locations of the action patterns both spatially and temporally, i.e., identifying a sequence of bounding boxes that track the action in video. The problem becomes intractable due to the exponentially large size of the structured video space where actions could occur. We propose a novel structured learning approach for spatio-temporal action localization. The mapping between a video and a spatio-temporal action trajectory is learned. The intractable inference and learning problems are addressed by leveraging an efficient Max-Path search method, thus makes it feasible to optimize the model over the whole structured space. Experiments on two challenging benchmark datasets show that our proposed method outperforms the state-of-the-art methods.",
    "authors": [
      "Tran, Du",
      "Yuan, Junsong"
    ]
  },
  {
    "id": "98b297950041a42470269d56260243a1",
    "title": "Minimizing Sparse High-Order Energies by Submodular Vertex-Cover",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/98b297950041a42470269d56260243a1-Paper.pdf",
    "abstract": "Inference on high-order graphical models has become increasingly important in recent years.  We consider energies with simple 'sparse' high-order potentials.  Previous work in this area uses either specialized message-passing  or transforms each high-order potential to the pairwise case. We take a fundamentally different approach, transforming the entire original problem into a comparatively small instance of a submodular vertex-cover problem. These vertex-cover instances can then be attacked by standard pairwise methods, where they run much faster (4--15 times) and are often more effective than on the original problem. We evaluate our approach on synthetic data, and we show that our algorithm can be useful in a fast hierarchical clustering and model estimation framework.",
    "authors": [
      "Delong, Andrew",
      "Veksler, Olga",
      "Osokin, Anton",
      "Boykov, Yuri"
    ]
  },
  {
    "id": "98dce83da57b0395e163467c9dae521b",
    "title": "A new metric on the manifold of kernel matrices with application to matrix geometric means",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/98dce83da57b0395e163467c9dae521b-Paper.pdf",
    "abstract": "Symmetric positive definite (spd) matrices are remarkably pervasive in a multitude of scientific disciplines, including machine learning and optimization. We consider the fundamental task of measuring distances between two spd matrices; a task that is often nontrivial whenever an application demands the distance function to respect the non-Euclidean geometry of spd matrices. Unfortunately, typical non-Euclidean distance measures such as the Riemannian metric $\\riem(X,Y)=\\frob{\\log(X\\inv{Y})}$, are computationally demanding and also complicated to use. To allay some of these difficulties, we introduce a new metric on spd matrices: this metric not only respects non-Euclidean geometry, it also offers faster computation than $\\riem$ while being less complicated to use. We support our claims theoretically via a series of theorems that relate our metric to $\\riem(X,Y)$, and experimentally by studying the nonconvex problem of computing matrix geometric means based on squared distances.",
    "authors": [
      "Sra, Suvrit"
    ]
  },
  {
    "id": "996a7fa078cc36c46d02f9af3bef918b",
    "title": "Wavelet based multi-scale shape features on arbitrary surfaces for cortical thickness discrimination",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/996a7fa078cc36c46d02f9af3bef918b-Paper.pdf",
    "abstract": "Hypothesis testing on signals de\ufb01ned on surfaces (such as the cortical surface) is a fundamental component of a variety of studies in Neuroscience. The goal here is to identify regions that exhibit changes as a function of the clinical condition under study. As the clinical questions of interest move towards identifying very early signs of diseases, the corresponding statistical differences at the group level invariably become weaker and increasingly hard to identify. Indeed, after a multiple comparisons correction is adopted (to account for correlated statistical tests over all surface points), very few regions may survive. In contrast to hypothesis tests on point-wise measurements, in this paper, we make the case for performing statistical analysis on multi-scale shape descriptors that characterize the local topological context of the signal around each surface vertex. Our descriptors are based on recent results from harmonic analysis, that show how wavelet theory extends to non-Euclidean settings (i.e., irregular weighted graphs). We provide strong evidence that these descriptors successfully pick up group-wise differences, where traditional methods either fail or yield unsatisfactory results. Other than this primary application, we show how the framework allows performing cortical surface smoothing in the native space without mappint to a unit sphere.",
    "authors": [
      "Kim, Won",
      "Pachauri, Deepti",
      "Hatt, Charles",
      "Chung, Moo.",
      "Johnson, Sterling",
      "Singh, Vikas"
    ]
  },
  {
    "id": "99adff456950dd9629a5260c4de21858",
    "title": "Cardinality Restricted Boltzmann Machines",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/99adff456950dd9629a5260c4de21858-Paper.pdf",
    "abstract": "The Restricted Boltzmann Machine (RBM) is a popular density model that is also good for extracting features. A main source of tractability in RBM models is the model's assumption that given an input, hidden units activate independently from one another. Sparsity and competition in the hidden representation is believed to be beneficial, and while an RBM with competition among its hidden units would acquire some of the attractive properties of sparse coding, such constraints are not added  due to the widespread belief that the resulting model would become intractable.  In this work, we show how a dynamic programming algorithm developed in 1981 can be used to implement exact sparsity in the RBM's hidden units. We then expand on this and show how to pass derivatives through a layer of exact sparsity, which makes it possible to fine-tune a deep belief network (DBN) consisting of RBMs with sparse hidden layers.  We show that sparsity in the RBM's hidden layer improves the performance of both the pre-trained representations and of the fine-tuned model.",
    "authors": [
      "Swersky, Kevin",
      "Sutskever, Ilya",
      "Tarlow, Daniel",
      "Zemel, Richard",
      "Salakhutdinov, Russ R.",
      "Adams, Ryan P."
    ]
  },
  {
    "id": "99bcfcd754a98ce89cb86f73acc04645",
    "title": "Sparse Prediction with the $k$-Support Norm",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/99bcfcd754a98ce89cb86f73acc04645-Paper.pdf",
    "abstract": "We derive a novel norm that corresponds to the tightest convex   relaxation of sparsity combined with an $\\ell_2$ penalty. We show   that this new norm provides a tighter relaxation than the elastic   net, and is thus a good replacement for the Lasso or the elastic net   in sparse prediction problems.  But through studying our new norm,   we also bound the looseness of the elastic net, thus shedding new   light on it and providing justification for its use.",
    "authors": [
      "Argyriou, Andreas",
      "Foygel, Rina",
      "Srebro, Nathan"
    ]
  },
  {
    "id": "9ad6aaed513b73148b7d49f70afcfb32",
    "title": "A Marginalized Particle Gaussian Process Regression",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/9ad6aaed513b73148b7d49f70afcfb32-Paper.pdf",
    "abstract": "We present a novel marginalized particle Gaussian process (MPGP) regression, which provides a fast, accurate online Bayesian filtering framework to model the latent function. Using a state space model established by the data construction procedure, our MPGP recursively filters out the estimation of hidden function values by a Gaussian mixture. Meanwhile, it provides a new online method for training hyperparameters with a number of weighted particles. We demonstrate the estimated performance of our MPGP on both simulated and real large data sets. The results show that our MPGP is a robust estimation algorithm with high computational efficiency, which outperforms other state-of-art sparse GP methods.",
    "authors": [
      "Wang, Yali",
      "Chaib-draa, Brahim"
    ]
  },
  {
    "id": "9adeb82fffb5444e81fa0ce8ad8afe7a",
    "title": "Iterative ranking from pair-wise comparisons",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/9adeb82fffb5444e81fa0ce8ad8afe7a-Paper.pdf",
    "abstract": "The question of aggregating pairwise comparisons to obtain a global ranking over a collection of objects has been of interest for a very long time: be it ranking of online gamers (e.g. MSR\u2019s TrueSkill system) and chess players, aggregating social opinions, or deciding which product to sell based on transactions. In most settings, in addition to obtaining ranking, finding \u2018scores\u2019 for each object (e.g. player\u2019s rating) is of interest to understanding the intensity of the preferences.   In this paper, we propose a novel iterative rank aggregation algorithm for discovering scores for objects from pairwise comparisons. The algorithm has a natural random walk interpretation over the graph of objects with edges present between two objects if they are compared; the scores turn out to be the stationary probability of this random walk. The algorithm is model independent. To establish the efficacy of our method, however, we consider the popular Bradley-Terry-Luce (BTL) model in which each object has an associated score which determines the probabilistic outcomes of pairwise comparisons between objects. We bound the finite sample error rates between the scores assumed by the BTL model and those estimated by our algorithm. This, in essence, leads to order-optimal dependence on the number of samples required to learn the scores well by our algorithm. Indeed, the experimental evaluation shows that our (model independent) algorithm performs as well as the Maximum Likelihood Estimator of the BTL model and outperforms a recently proposed algorithm by Ammar and Shah [1].",
    "authors": [
      "Negahban, Sahand",
      "Oh, Sewoong",
      "Shah, Devavrat"
    ]
  },
  {
    "id": "9b72e31dac81715466cd580a448cf823",
    "title": "Training sparse natural image models with a fast Gibbs sampler of an extended state space",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/9b72e31dac81715466cd580a448cf823-Paper.pdf",
    "abstract": "We present a new learning strategy based on an efficient blocked Gibbs sampler for sparse overcomplete linear models. Particular emphasis is placed on statistical image modeling, where overcomplete models have played an important role in discovering sparse representations. Our Gibbs sampler is faster than general purpose sampling schemes while also requiring no tuning as it is free of parameters. Using the Gibbs sampler and a persistent variant of expectation maximization, we are able to extract highly sparse distributions over latent sources from data. When applied to natural images, our algorithm learns source distributions which resemble spike-and-slab distributions. We evaluate the likelihood and quantitatively compare the performance of the overcomplete linear model to its complete counterpart as well as a product of experts model, which represents another overcomplete generalization of the complete linear model. In contrast to previous claims, we find that overcomplete representations lead to significant improvements, but that the overcomplete linear model still underperforms other models.",
    "authors": [
      "Theis, Lucas",
      "Sohl-dickstein, Jascha",
      "Bethge, Matthias"
    ]
  },
  {
    "id": "9bf31c7ff062936a96d3c8bd1f8f2ff3",
    "title": "Learning from Distributions via Support Measure Machines",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/9bf31c7ff062936a96d3c8bd1f8f2ff3-Paper.pdf",
    "abstract": "This paper presents a kernel-based discriminative learning framework on probability measures. Rather than relying on large collections of vectorial training examples, our framework learns using a collection of probability distributions that have been constructed to meaningfully represent training data. By representing these probability distributions as mean embeddings in the reproducing kernel Hilbert space (RKHS), we are able to apply many standard kernel-based learning techniques in straightforward fashion. To accomplish this, we construct a generalization of the support vector machine (SVM) called a support measure machine (SMM). Our analyses of SMMs provides several insights into their relationship to traditional SVMs. Based on such insights, we propose a flexible SVM (Flex-SVM) that places different kernel functions on each training example. Experimental results on both synthetic and real-world data demonstrate the effectiveness of our proposed framework.",
    "authors": [
      "Muandet, Krikamol",
      "Fukumizu, Kenji",
      "Dinuzzo, Francesco",
      "Sch\u00f6lkopf, Bernhard"
    ]
  },
  {
    "id": "9c82c7143c102b71c593d98d96093fde",
    "title": "Learning Multiple Tasks using Shared Hypotheses",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/9c82c7143c102b71c593d98d96093fde-Paper.pdf",
    "abstract": "In this work we consider a setting where we have a very large number   of related tasks with few examples from each individual task. Rather   than either learning each task individually (and having a large   generalization error) or learning all the tasks together using a   single hypothesis (and suffering a potentially large inherent   error), we consider learning a small pool of {\\em shared     hypotheses}. Each task is then mapped to a single hypothesis in   the pool (hard association). We derive VC dimension generalization   bounds for our model, based on the number of tasks, shared   hypothesis and the VC dimension of the hypotheses   class. We conducted experiments with both synthetic problems and   sentiment of reviews, which strongly support our approach.",
    "authors": [
      "Crammer, Koby",
      "Mansour, Yishay"
    ]
  },
  {
    "id": "9cb67ffb59554ab1dabb65bcb370ddd9",
    "title": "Minimizing Uncertainty in Pipelines",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/9cb67ffb59554ab1dabb65bcb370ddd9-Paper.pdf",
    "abstract": "In this paper, we consider the problem of debugging large pipelines by human labeling. We represent the execution of a pipeline using a directed acyclic graph of AND and OR nodes, where each node represents a data item produced by some operator in the pipeline. We assume that each operator assigns a confidence to each of its output data. We want to reduce the uncertainty in the output by issuing queries to a human expert, where a query consists of checking if a given data item is correct. In this paper, we consider the problem of asking the optimal set of queries to minimize the resulting output uncertainty. We perform a detailed evaluation of the complexity of the problem for various classes of graphs. We give efficient algorithms for the problem for trees, and show that, for a general dag, the problem is intractable.",
    "authors": [
      "Dalvi, Nilesh",
      "Parameswaran, Aditya",
      "Rastogi, Vibhor"
    ]
  },
  {
    "id": "9e7ba617ad9e69b39bd0c29335b79629",
    "title": "An Integer Optimization Approach to Associative Classification",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/9e7ba617ad9e69b39bd0c29335b79629-Paper.pdf",
    "abstract": "Abstract Unavailable",
    "authors": [
      "Bertsimas, Dimitris",
      "Chang, Allison",
      "Rudin, Cynthia"
    ]
  },
  {
    "id": "9f36407ead0629fc166f14dde7970f68",
    "title": "Algorithms for Learning Markov Field Policies",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/9f36407ead0629fc166f14dde7970f68-Paper.pdf",
    "abstract": "We present a new graph-based approach for incorporating domain knowledge in reinforcement learning applications. The domain knowledge is given as a weighted graph, or a kernel matrix, that loosely indicates which states should have similar optimal actions. We first introduce a bias into the policy search process by deriving a distribution on policies such that policies that disagree with the provided graph have low probabilities. This distribution corresponds to a Markov Random Field. We then present a reinforcement and an apprenticeship learning algorithms for finding such policy distributions. We also illustrate the advantage of the proposed approach on three problems: swing-up cart-balancing with nonuniform and smooth frictions, gridworlds, and teaching a robot to grasp new objects.",
    "authors": [
      "Boularias, Abdeslam",
      "Peters, Jan",
      "Kroemer, Oliver"
    ]
  },
  {
    "id": "9f396fe44e7c05c16873b05ec425cbad",
    "title": "Bayesian Probabilistic Co-Subspace Addition",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/9f396fe44e7c05c16873b05ec425cbad-Paper.pdf",
    "abstract": "For modeling data matrices, this paper introduces Probabilistic Co-Subspace Addition (PCSA) model by simultaneously capturing the dependent structures among both rows and columns. Briefly, PCSA assumes that each entry of a matrix is generated by the additive combination of the linear mappings of two features, which distribute in the row-wise and column-wise latent subspaces. Consequently, it captures the dependencies among entries intricately, and is able to model the non-Gaussian and heteroscedastic density. Variational inference is proposed on PCSA for  approximate Bayesian learning, where the updating for posteriors is formulated into the problem of solving Sylvester equations. Furthermore, PCSA is extended to tackling and filling missing values, to adapting its sparseness, and to modelling tensor data. In comparison with several state-of-art approaches, experiments demonstrate the effectiveness and efficiency of Bayesian (sparse) PCSA on modeling matrix (tensor) data and filling missing values.",
    "authors": [
      "Shi, Lei"
    ]
  },
  {
    "id": "a0a080f42e6f13b3a2df133f073095dd",
    "title": "Exploration in Model-based Reinforcement Learning by Empirically Estimating Learning Progress",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/a0a080f42e6f13b3a2df133f073095dd-Paper.pdf",
    "abstract": "Formal exploration approaches in model-based reinforcement learning estimate the accuracy of the currently learned model without consideration of the empirical prediction error. For example, PAC-MDP approaches such as Rmax base their model certainty on the amount of collected data, while Bayesian approaches assume a prior over the transition dynamics. We propose extensions to such approaches which drive exploration solely based on empirical estimates of the learner's accuracy and learning progress. We provide a ``sanity check'' theoretical analysis, discussing the behavior of our extensions in the standard stationary finite state-action case. We then provide experimental studies demonstrating the robustness of these exploration measures in cases of non-stationary environments or where original approaches are misled by wrong domain assumptions.",
    "authors": [
      "Lopes, Manuel",
      "Lang, Tobias",
      "Toussaint, Marc",
      "Oudeyer, Pierre-yves"
    ]
  },
  {
    "id": "a1140a3d0df1c81e24ae954d935e8926",
    "title": "From Deformations to Parts: Motion-based Segmentation of 3D Objects",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/a1140a3d0df1c81e24ae954d935e8926-Paper.pdf",
    "abstract": "We develop a method for discovering the parts of an articulated object from aligned meshes capturing various three-dimensional (3D) poses.  We adapt the distance dependent Chinese restaurant process (ddCRP) to allow nonparametric discovery of a potentially unbounded number of parts, while simultaneously guaranteeing a spatially connected segmentation.  To allow analysis of datasets in which object instances have varying shapes, we model part variability across poses via affine transformations.  By placing a matrix normal-inverse-Wishart prior on these affine transformations, we develop a ddCRP Gibbs sampler which tractably marginalizes over transformation uncertainty.  Analyzing a dataset of humans captured in dozens of poses, we infer parts which provide quantitatively better motion predictions than conventional clustering methods.",
    "authors": [
      "Ghosh, Soumya",
      "Loper, Matthew",
      "Sudderth, Erik",
      "Black, Michael"
    ]
  },
  {
    "id": "a3fb4fbf9a6f9cf09166aa9c20cbc1ad",
    "title": "Bayesian models for Large-scale Hierarchical Classification",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/a3fb4fbf9a6f9cf09166aa9c20cbc1ad-Paper.pdf",
    "abstract": "A challenging problem in hierarchical classification is to leverage the hierarchical relations among classes for  improving classification performance. An even greater challenge is to do so in a manner that is computationally feasible for the large scale problems usually encountered in practice. This paper proposes a set of Bayesian methods to model hierarchical dependencies among class labels using multivari- ate logistic regression. Specifically, the parent-child relationships are modeled by placing a hierarchical prior over the children nodes centered around the parame- ters of their parents; thereby encouraging classes nearby in the hierarchy to share similar model parameters. We present new, efficient variational algorithms for tractable posterior inference in these models, and provide a parallel implementa- tion that can comfortably handle large-scale problems with hundreds of thousands of dimensions and tens of thousands of classes. We run a comparative evaluation on multiple large-scale benchmark datasets that highlights the scalability of our approach, and shows a significant performance advantage over the other state-of- the-art hierarchical methods.",
    "authors": [
      "Gopal, Siddharth",
      "Yang, Yiming",
      "Bai, Bing",
      "Niculescu-mizil, Alexandru"
    ]
  },
  {
    "id": "a4300b002bcfb71f291dac175d52df94",
    "title": "Efficient Spike-Coding with Multiplicative Adaptation in a Spike Response Model",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/a4300b002bcfb71f291dac175d52df94-Paper.pdf",
    "abstract": "Neural adaptation underlies the ability of neurons to maximize encoded information over a wide dynamic range of input stimuli. While adaptation is an intrinsic feature of neuronal models like the Hodgkin-Huxley model, the challenge is to integrate adaptation in models of neural computation.  Recent computational models like the Adaptive Spike Response Model implement adaptation as spike-based addition of fixed-size fast spike-triggered threshold dynamics and slow spike-triggered currents. Such adaptation has been shown to accurately model neural spiking behavior over a limited dynamic range. Taking a cue from kinetic models of adaptation, we propose a multiplicative Adaptive Spike Response Model where the spike-triggered adaptation dynamics are scaled multiplicatively by the adaptation state at the time of spiking. We show that unlike the additive adaptation model, the firing rate in the multiplicative adaptation model saturates to a maximum spike-rate. When simulating variance switching experiments, the model also quantitatively fits the experimental data over a wide dynamic range. Furthermore, dynamic threshold models of adaptation suggest a straightforward interpretation of neural activity in terms of dynamic signal encoding with shifted and weighted exponential kernels. We show that when thus encoding rectified filtered stimulus signals, the multiplicative Adaptive Spike Response Model achieves a high coding efficiency and maintains this efficiency over changes in the dynamic signal range of several orders of magnitude, without changing model parameters.",
    "authors": [
      "Bohte, Sander"
    ]
  },
  {
    "id": "a50abba8132a77191791390c3eb19fe7",
    "title": "Forging The Graphs: A Low Rank and Positive Semidefinite Graph Learning Approach",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/a50abba8132a77191791390c3eb19fe7-Paper.pdf",
    "abstract": "In many graph-based machine learning and data mining approaches, the quality of the graph is critical. However, in real-world applications, especially in semi-supervised learning and unsupervised learning, the evaluation of the quality of a graph is often expensive and sometimes even impossible, due the cost or the unavailability of ground truth. In this paper, we proposed a robust approach with convex optimization to ``forge'' a graph: with an input of a graph, to learn a graph with higher quality. Our major concern is that an ideal graph shall satisfy all the following constraints: non-negative, symmetric, low rank, and positive semidefinite. We develop a graph learning algorithm by solving a convex optimization problem and further develop an efficient optimization to obtain global optimal solutions with theoretical guarantees. With only one non-sensitive parameter, our method is shown by experimental results to be robust and achieve higher accuracy in semi-supervised learning and clustering under various settings. As a preprocessing of graphs, our method has a wide range of potential applications machine learning and data mining.",
    "authors": [
      "Luo, Dijun",
      "Huang, Heng",
      "Nie, Feiping",
      "Ding, Chris"
    ]
  },
  {
    "id": "a512294422de868f8474d22344636f16",
    "title": "Random Utility Theory for Social Choice",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/a512294422de868f8474d22344636f16-Paper.pdf",
    "abstract": "Random utility theory models an agents preferences on alternatives by drawing a real-valued score on each alternative (typically independently) from a parameterized distribution, and then ranking the alternatives according to scores. A special case that has received signicant attention is the Plackett-Luce model, for which fast inference methods for maximum likelihood estimators are available. This paper develops conditions on general random utility models that enable fast inference within a Bayesian framework through MC-EM, providing concave loglikelihood functions and bounded sets of global maxima solutions. Results on both real-world and simulated data provide support for the scalability of the approach and capability for model selection among general random utility models including Plackett-Luce.",
    "authors": [
      "Azari, Hossein",
      "Parks, David",
      "Xia, Lirong"
    ]
  },
  {
    "id": "a58149d355f02887dfbe55ebb2b64ba3",
    "title": "Tensor Decomposition for Fast Parsing with Latent-Variable PCFGs",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/a58149d355f02887dfbe55ebb2b64ba3-Paper.pdf",
    "abstract": "We describe an approach to speed-up inference with latent variable PCFGs, which have been shown to  be highly effective for natural language parsing. Our approach is based on a tensor formulation recently introduced for spectral estimation of latent-variable PCFGs coupled with a tensor decomposition algorithm well-known in the multilinear algebra literature.  We also describe an error bound for this approximation, which bounds the difference between the probabilities calculated by the algorithm and the true probabilities that the approximated model gives. Empirical evaluation on real-world natural language parsing data demonstrates a significant speed-up at minimal cost for parsing performance.",
    "authors": [
      "Collins, Michael",
      "Cohen, Shay"
    ]
  },
  {
    "id": "a597e50502f5ff68e3e25b9114205d4a",
    "title": "Action-Model Based Multi-agent Plan Recognition",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/a597e50502f5ff68e3e25b9114205d4a-Paper.pdf",
    "abstract": "Multi-Agent Plan Recognition (MAPR) aims to recognize dynamic team structures and team behaviors from the observed team traces (activity sequences) of a set of intelligent agents. Previous MAPR approaches required a library of team activity sequences (team plans) be given as input. However, collecting a library of team plans to ensure adequate coverage is often difficult and costly. In this paper, we relax this constraint, so that team plans are not required to be provided beforehand. We assume instead that a set of action models are available. Such models are often already created to describe domain physics; i.e., the  preconditions and effects of effects actions. We propose a novel approach for recognizing multi-agent team plans based on such action models rather than libraries of team plans. We encode the resulting MAPR problem as a \\emph{satisfiability problem} and solve the problem using a state-of-the-art weighted MAX-SAT solver. Our approach also allows for incompleteness in the observed plan traces. Our empirical studies demonstrate that our algorithm is both effective and efficient in comparison to state-of-the-art MAPR methods based on plan libraries.",
    "authors": [
      "Zhuo, Hankz",
      "Yang, Qiang",
      "Kambhampati, Subbarao"
    ]
  },
  {
    "id": "a5bad363fc47f424ddf5091c8471480a",
    "title": "Semiparametric Principal Component Analysis",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/a5bad363fc47f424ddf5091c8471480a-Paper.pdf",
    "abstract": "We propose two new principal component analysis methods in this paper utilizing a semiparametric model. The according methods are named Copula Component Analysis (COCA) and Copula PCA. The semiparametric model assumes that, af- ter unspeci\ufb01ed marginally monotone transformations, the distributions are multi- variate Gaussian. The COCA and Copula PCA accordingly estimate the leading eigenvectors of the correlation and covariance matrices of the latent Gaussian dis- tribution. The robust nonparametric rank-based correlation coef\ufb01cient estimator, Spearman\u2019s rho, is exploited in estimation. We prove that, under suitable condi- tions, although the marginal distributions can be arbitrarily continuous, the COCA and Copula PCA estimators obtain fast estimation rates and are feature selection consistent in the setting where the dimension is nearly exponentially large relative to the sample size. Careful numerical experiments on the synthetic and real data are conducted to back up the theoretical results. We also discuss the relationship with the transelliptical component analysis proposed by Han and Liu (2012).",
    "authors": [
      "Han, Fang",
      "Liu, Han"
    ]
  },
  {
    "id": "a5e00132373a7031000fd987a3c9f87b",
    "title": "Multi-task Vector Field Learning",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/a5e00132373a7031000fd987a3c9f87b-Paper.pdf",
    "abstract": "Multi-task learning (MTL) aims to improve generalization performance by learning multiple related tasks simultaneously and identifying the shared information among tasks. Most of existing MTL methods focus on learning linear models under the supervised setting. We propose a novel semi-supervised and nonlinear approach for MTL using vector fields. A vector field is a smooth mapping from the manifold to the tangent spaces which can be viewed as a directional derivative of functions on the manifold. We argue that vector fields provide a natural way to exploit the geometric structure of data as well as the shared differential structure of tasks, both are crucial for semi-supervised multi-task learning. In this paper, we develop multi-task vector field learning (MTVFL) which learns the prediction functions and the vector fields simultaneously. MTVFL has the following key properties: (1) the vector fields we learned are close to the gradient fields of the prediction functions; (2) within each task, the vector field is required to be as parallel as possible which is expected to span a low dimensional subspace; (3) the vector fields from all tasks share a low dimensional subspace. We formalize our idea in a regularization framework and also provide a convex relaxation method to solve the original non-convex problem. The experimental results on synthetic and real data demonstrate the effectiveness of our proposed approach.",
    "authors": [
      "Lin, Binbin",
      "Yang, Sen",
      "Zhang, Chiyuan",
      "Ye, Jieping",
      "He, Xiaofei"
    ]
  },
  {
    "id": "a684eceee76fc522773286a895bc8436",
    "title": "Local Supervised Learning through Space Partitioning",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/a684eceee76fc522773286a895bc8436-Paper.pdf",
    "abstract": "We develop a novel approach for supervised learning based on adaptively partitioning the feature space into different regions and learning local region-specific classifiers. We formulate an empirical risk minimization problem that incorporates both partitioning and classification in to a single global objective. We show that space partitioning can be equivalently reformulated as a supervised learning problem and consequently any discriminative learning method can be utilized in conjunction with our approach. Nevertheless, we consider locally linear schemes by learning linear partitions and linear region classifiers. Locally linear schemes can not only approximate complex decision boundaries and ensure low training error but also provide tight control on over-fitting and generalization error. We train locally linear classifiers by using LDA, logistic regression and perceptrons, and so our scheme is scalable to large data sizes and high-dimensions. We present experimental results demonstrating improved performance over state of the art classification techniques on benchmark datasets. We also show improved robustness to label noise.",
    "authors": [
      "Wang, Joseph",
      "Saligrama, Venkatesh"
    ]
  },
  {
    "id": "a8240cb8235e9c493a0c30607586166c",
    "title": "Dip-means: an incremental clustering method for estimating the number of clusters",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/a8240cb8235e9c493a0c30607586166c-Paper.pdf",
    "abstract": "Learning the number of clusters is a key problem in data clustering. We present dip-means, a novel robust incremental method to learn the number of data clusters that may be used as a wrapper around any iterative clustering algorithm of the k-means family. In contrast to many popular methods which make assumptions about the underlying cluster distributions, dip-means only assumes a fundamental cluster property: each cluster to admit a unimodal distribution. The proposed algorithm considers each cluster member as a ''viewer'' and applies a univariate statistic hypothesis test for unimodality (dip-test) on the distribution of the distances between the viewer and the cluster members. Two important advantages are: i) the unimodality test is applied on univariate distance vectors, ii) it can be directly applied with kernel-based methods, since only the pairwise distances are involved in the computations. Experimental results on artificial and real datasets indicate the effectiveness of our method and its superiority over analogous approaches.",
    "authors": [
      "Kalogeratos, Argyris",
      "Likas, Aristidis"
    ]
  },
  {
    "id": "a8f8f60264024dca151f164729b76c0b",
    "title": "Unsupervised Template Learning for Fine-Grained Object Recognition",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/a8f8f60264024dca151f164729b76c0b-Paper.pdf",
    "abstract": "Fine-grained recognition refers to a subordinate level of recognition, such are recognizing different species of birds, animals or plants. It differs from recognition of basic categories, such as humans, tables, and computers, in that there are global similarities in shape or structure shared within a category, and the differences are in the details of the object parts. We suggest that the key to identifying the fine-grained differences lies in finding the right alignment of image regions that contain the same object parts. We propose a template model for the purpose, which captures common shape patterns of object parts, as well as the co-occurence relation of the shape patterns. Once the image regions are aligned, extracted features are used for classification. Learning of the template model is efficient, and the recognition results we achieve significantly outperform the state-of-the-art algorithms.",
    "authors": [
      "Yang, Shulin",
      "Bo, Liefeng",
      "Wang, Jue",
      "Shapiro, Linda"
    ]
  },
  {
    "id": "a9be4c2a4041cadbf9d61ae16dd1389e",
    "title": "A Nonparametric Conjugate Prior Distribution for the Maximizing Argument of a Noisy Function",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/a9be4c2a4041cadbf9d61ae16dd1389e-Paper.pdf",
    "abstract": "We propose a novel Bayesian approach to solve stochastic optimization problems that involve \ufb01nding extrema of noisy, nonlinear functions. Previous work has focused on representing possible functions explicitly, which leads to a two-step procedure of \ufb01rst, doing inference over the function space and second, \ufb01nding the extrema of these functions. Here we skip the representation step and directly model the distribution over extrema. To this end, we devise a non-parametric conjugate prior where the natural parameter corresponds to a given kernel function and the suf\ufb01cient statistic is composed of the observed function values. The resulting posterior distribution directly captures the uncertainty over the maximum of the unknown function.",
    "authors": [
      "Ortega, Pedro",
      "Grau-moya, Jordi",
      "Genewein, Tim",
      "Balduzzi, David",
      "Braun, Daniel"
    ]
  },
  {
    "id": "a9eb812238f753132652ae09963a05e9",
    "title": "Efficient Reinforcement Learning for High Dimensional Linear Quadratic Systems",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/a9eb812238f753132652ae09963a05e9-Paper.pdf",
    "abstract": "We study the problem of adaptive control of a high dimensional linear quadratic (LQ) system.  Previous work established the asymptotic convergence to an optimal controller for various adaptive control schemes. More recently, an asymptotic regret bound of $\\tilde{O}(\\sqrt{T})$ was shown for $T \\gg p$ where $p$ is the dimension of the state space. In this work we consider the case where the matrices describing the dynamic of the LQ system are sparse and their dimensions are large. We present an adaptive control scheme that for $p \\gg 1$ and $T \\gg \\polylog(p)$ achieves a regret bound of $\\tilde{O}(p \\sqrt{T})$. In particular, our algorithm has an average cost of $(1+\\eps)$ times the optimum cost after $T = \\polylog(p) O(1/\\eps^2)$. This is in comparison to previous work on the dense dynamics where the algorithm needs $\\Omega(p)$ samples before it can estimate the unknown dynamic with any significant accuracy. We believe our result has prominent applications in the emerging area of computational advertising, in particular targeted online advertising and advertising in social networks.",
    "authors": [
      "Ibrahimi, Morteza",
      "Javanmard, Adel",
      "Roy, Benjamin"
    ]
  },
  {
    "id": "aaebdb8bb6b0e73f6c3c54a0ab0c6415",
    "title": "A Conditional Multinomial Mixture Model for Superset Label Learning",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/aaebdb8bb6b0e73f6c3c54a0ab0c6415-Paper.pdf",
    "abstract": "In the superset label learning problem (SLL), each training instance provides a set of candidate labels of which one is the true label of the instance. As in ordinary regression, the candidate label set is a noisy version of the true label. In this work, we solve the problem by maximizing the likelihood of the candidate label sets of training instances. We propose a probabilistic model, the Logistic Stick- Breaking Conditional Multinomial Model (LSB-CMM), to do the job. The LSB- CMM is derived from the logistic stick-breaking process. It \ufb01rst maps data points to mixture components and then assigns to each mixture component a label drawn from a component-speci\ufb01c multinomial distribution. The mixture components can capture underlying structure in the data, which is very useful when the model is weakly supervised. This advantage comes at little cost, since the model introduces few additional parameters. Experimental tests on several real-world problems with superset labels show results that are competitive or superior to the state of the art. The discovered underlying structures also provide improved explanations of the classi\ufb01cation predictions.",
    "authors": [
      "Liu, Liping",
      "Dietterich, Thomas"
    ]
  },
  {
    "id": "ab233b682ec355648e7891e66c54191b",
    "title": "Value Pursuit Iteration",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/ab233b682ec355648e7891e66c54191b-Paper.pdf",
    "abstract": "Value Pursuit Iteration (VPI) is an approximate value iteration algorithm that finds a close to optimal policy for  reinforcement learning and planning problems with large state spaces. VPI has two main features: First, it is a nonparametric algorithm that finds a good sparse approximation of the optimal value function given a dictionary of features. The algorithm is almost insensitive to the number of irrelevant features. Second, after each iteration of VPI, the algorithm adds a set of functions based on the currently learned value function to the dictionary. This increases the representation power of the dictionary in a way that is directly relevant to the goal of having a good approximation of the optimal value function. We theoretically study VPI and provide a finite-sample error upper bound for it.",
    "authors": [
      "Farahmand, Amir",
      "Precup, Doina"
    ]
  },
  {
    "id": "ab541d874c7bc19ab77642849e02b89f",
    "title": "Latent Coincidence Analysis: A Hidden Variable Model for Distance Metric Learning",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/ab541d874c7bc19ab77642849e02b89f-Paper.pdf",
    "abstract": "We describe a latent variable model for supervised dimensionality reduction and distance metric learning. The model discovers linear projections of high dimensional data that shrink the distance between similarly labeled inputs and expand the distance between differently labeled ones. The model\u2019s continuous latent variables locate pairs of examples in a latent space of lower dimensionality. The model differs significantly from classical factor analysis in that the posterior distribution over these latent variables is not always multivariate Gaussian. Nevertheless we show that inference is completely tractable and derive an Expectation-Maximization (EM) algorithm for parameter estimation. We also compare the model to other approaches in distance metric learning. The model\u2019s main advantage is its simplicity: at each iteration of the EM algorithm, the distance metric is re-estimated by solving an unconstrained least-squares problem. Experiments show that these simple updates are highly effective.",
    "authors": [
      "Der, Matthew",
      "Saul, Lawrence"
    ]
  },
  {
    "id": "ad13a2a07ca4b7642959dc0c4c740ab6",
    "title": "Identification of Recurrent Patterns in the Activation of Brain Networks",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/ad13a2a07ca4b7642959dc0c4c740ab6-Paper.pdf",
    "abstract": "Identifying patterns from the neuroimaging recordings of brain activity  related to the unobservable psychological or mental state of an individual can be treated as a unsupervised pattern recognition problem. The main challenges, however, for such an analysis of fMRI data are: a) defining a physiologically meaningful feature-space for representing the spatial patterns across time; b) dealing with the high-dimensionality of the data; and c) robustness to the various artifacts and confounds in the fMRI time-series.  In this paper, we present a network-aware feature-space to represent the states of a general network, that enables  comparing and clustering such states in a manner that is a) meaningful in terms of the network connectivity structure; b)computationally efficient; c) low-dimensional; and d) relatively robust to structured and random noise artifacts. This feature-space is obtained from a spherical relaxation of the transportation distance metric which measures the cost of transporting ``mass'' over the network to transform one function into another. Through theoretical and empirical assessments, we demonstrate the accuracy and efficiency of the approximation, especially for large problems.  While the application presented here is for identifying distinct brain activity patterns from fMRI, this feature-space can be applied to the problem of identifying recurring patterns and detecting outliers in measurements on many different types of networks, including sensor, control and social networks.",
    "authors": [
      "Janoos, Firdaus",
      "Li, Weichang",
      "Subrahmanya, Niranjan",
      "Morocz, Istvan",
      "Wells, William"
    ]
  },
  {
    "id": "ad3019b856147c17e82a5bead782d2a8",
    "title": "Hierarchical spike coding of sound",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/ad3019b856147c17e82a5bead782d2a8-Paper.pdf",
    "abstract": "We develop a probabilistic generative model for representing acoustic event structure at multiple scales via a two-stage hierarchy. The first stage consists of a spiking representation which encodes a sound with a sparse set of kernels at different frequencies positioned precisely in time. The coarse time and frequency statistical structure of the first-stage spikes is encoded by a second stage spiking representation, while fine-scale statistical regularities are encoded by recurrent interactions within the first-stage.  When fitted to speech data, the model encodes acoustic features such as harmonic stacks, sweeps, and frequency modulations, that can be composed to represent complex acoustic events. The model is also able to synthesize sounds from the higher-level representation and provides significant improvement over wavelet thresholding techniques on a denoising task.",
    "authors": [
      "Karklin, Yan",
      "Ekanadham, Chaitanya",
      "Simoncelli, Eero"
    ]
  },
  {
    "id": "ae5e3ce40e0404a45ecacaaf05e5f735",
    "title": "A Polynomial-time Form of Robust Regression",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/ae5e3ce40e0404a45ecacaaf05e5f735-Paper.pdf",
    "abstract": "Despite the variety of robust regression methods that have been developed, current regression formulations are either NP-hard, or allow unbounded response to even a single leverage point. We present a general formulation for robust regression --Variational M-estimation--that unifies a number of robust regression methods while allowing a tractable approximation strategy. We develop an estimator that requires only polynomial-time, while achieving certain robustness and consistency guarantees. An experimental evaluation demonstrates the effectiveness of the new estimation approach  compared to standard methods.",
    "authors": [
      "Yu, Yao-liang",
      "Aslan, \u00d6zlem",
      "Schuurmans, Dale"
    ]
  },
  {
    "id": "af21d0c97db2e27e13572cbf59eb343d",
    "title": "Multimodal Learning with Deep Boltzmann Machines",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/af21d0c97db2e27e13572cbf59eb343d-Paper.pdf",
    "abstract": "We propose a Deep Boltzmann Machine for learning a generative model of multimodal data. We show how to use the model to extract a meaningful representation of multimodal data. We find that the learned representation is useful for classification and information retreival tasks, and hence conforms to some notion of semantic similarity. The model defines a probability density over the space of multimodal inputs. By sampling from the conditional distributions over each data modality, it possible to create the representation even when some data modalities are missing. Our experimental results on bi-modal data consisting of images and text show that the Multimodal DBM can learn a good generative model of the joint space of image and text inputs that is useful for information retrieval from both unimodal and multimodal queries. We further demonstrate that our model can significantly outperform SVMs and LDA on discriminative tasks. Finally, we compare our model to other deep learning methods, including autoencoders and deep belief networks, and show that it achieves significant gains.",
    "authors": [
      "Srivastava, Nitish",
      "Salakhutdinov, Russ R."
    ]
  },
  {
    "id": "af4732711661056eadbf798ba191272a",
    "title": "A nonparametric variable clustering model",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/af4732711661056eadbf798ba191272a-Paper.pdf",
    "abstract": "Factor analysis models effectively summarise the covariance structure of high dimensional data, but the solutions are typically hard to interpret. This motivates attempting to find a disjoint partition, i.e. a clustering, of observed variables so that variables in a cluster are highly correlated. We introduce a Bayesian non-parametric approach to this problem, and demonstrate advantages over heuristic methods proposed to date.",
    "authors": [
      "Palla, Konstantina",
      "Ghahramani, Zoubin",
      "Knowles, David"
    ]
  },
  {
    "id": "af8d1eb220186400c494db7091e402b0",
    "title": "Transelliptical Graphical Models",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/af8d1eb220186400c494db7091e402b0-Paper.pdf",
    "abstract": "We advocate the use of a new distribution family\u2014the transelliptical\u2014for robust inference of high dimensional graphical models. The transelliptical family is an extension of the nonparanormal family proposed by Liu et al. (2009). Just as the nonparanormal extends the normal by transforming the variables using univariate functions, the transelliptical extends the elliptical family in the same way. We propose a nonparametric rank-based regularization estimator which achieves the parametric rates of convergence for both graph recovery and parameter estima- tion. Such a result suggests that the extra robustness and \ufb02exibility obtained by the semiparametric transelliptical modeling incurs almost no ef\ufb01ciency loss. We also discuss the relationship between this work with the transelliptical component analysis proposed by Han and Liu (2012).",
    "authors": [
      "Liu, Han",
      "Han, Fang",
      "Zhang, Cun-hui"
    ]
  },
  {
    "id": "afdec7005cc9f14302cd0474fd0f3c96",
    "title": "Collaborative Gaussian Processes for Preference Learning",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/afdec7005cc9f14302cd0474fd0f3c96-Paper.pdf",
    "abstract": "We present a new model based on Gaussian processes (GPs) for learning pairwise preferences expressed by multiple users. Inference is simplified by using a \\emph{preference kernel} for GPs which allows us to combine supervised GP learning of user preferences with unsupervised dimensionality reduction for multi-user systems. The model not only exploits collaborative information from the shared structure in user behavior, but may also incorporate user features if they are available. Approximate inference is implemented using a combination of expectation propagation and variational Bayes. Finally, we present an efficient active learning strategy for querying preferences. The proposed technique performs favorably on real-world data against state-of-the-art multi-user preference learning algorithms.",
    "authors": [
      "Houlsby, Neil",
      "Huszar, Ferenc",
      "Ghahramani, Zoubin",
      "Hern\u00e1ndez-lobato, Jose"
    ]
  },
  {
    "id": "b112ca4087d668785e947a57493d1740",
    "title": "Smooth-projected Neighborhood Pursuit for High-dimensional Nonparanormal Graph Estimation",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/b112ca4087d668785e947a57493d1740-Paper.pdf",
    "abstract": "Many statistical methods gain robustness and exibility by sacricing convenient computational structure. In this paper, we illustrate this fundamental tradeoff by studying a semiparametric graphical model estimation problem. We explain how new computational techniques help to solve this type of problem. In particularly, we propose a smooth-projected neighborhood pursuit method for efciently estimating high dimensional nonparanormal graphs with theoretical guarantees. Besides new computational and theoretical analysis, we also provide an alternative view to analyze the tradeoff between computational efciency and statistical error under a smoothing optimization framework. We also report experimental results on text and stock datasets.",
    "authors": [
      "Zhao, Tuo",
      "Roeder, Kathryn",
      "Liu, Han"
    ]
  },
  {
    "id": "b20bb95ab626d93fd976af958fbc61ba",
    "title": "Learning Manifolds with K-Means and K-Flats",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/b20bb95ab626d93fd976af958fbc61ba-Paper.pdf",
    "abstract": "We study the problem of estimating a manifold from random samples. In particular, we consider piecewise constant and piecewise linear estimators induced by  k-means and k-\ufb02ats, and analyze their performance. We extend previous results  for k-means in two separate directions. First, we provide new results for k-means reconstruction on manifolds and, secondly, we prove reconstruction bounds for higher-order approximation (k-\ufb02ats), for which no known results were previously available. While the results for k-means are novel, some of the technical tools are well-established in the literature. In the case of k-\ufb02ats, both the results and the mathematical tools are  new.",
    "authors": [
      "Canas, Guillermo",
      "Poggio, Tomaso",
      "Rosasco, Lorenzo"
    ]
  },
  {
    "id": "b3967a0e938dc2a6340e258630febd5a",
    "title": "Newton-Like Methods for Sparse Inverse Covariance Estimation",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/b3967a0e938dc2a6340e258630febd5a-Paper.pdf",
    "abstract": "We propose two classes of second-order optimization methods for solving the sparse inverse covariance estimation problem. The first approach, which we call the Newton-LASSO method, minimizes a piecewise quadratic model of the objective function at every iteration to generate a step. We employ the fast iterative shrinkage thresholding method (FISTA) to solve this subproblem. The second approach, which we call the Orthant-Based Newton method, is a two-phase algorithm that first identifies an orthant face and then minimizes a smooth quadratic approximation of the objective function using the conjugate gradient method.  These methods exploit the structure of the Hessian to efficiently compute the search direction and to avoid explicitly storing the Hessian.  We show that quasi-Newton methods are also effective in this context, and describe a limited memory BFGS variant of the orthant-based Newton method.  We present numerical results that suggest that all the techniques described in this paper have attractive properties and constitute useful tools for solving the sparse inverse covariance estimation problem. Comparisons with the method implemented in the QUIC software package are presented.",
    "authors": [
      "Oztoprak, Figen",
      "Nocedal, Jorge",
      "Rennie, Steven",
      "Olsen, Peder A."
    ]
  },
  {
    "id": "b495ce63ede0f4efc9eec62cb947c162",
    "title": "A Neural Autoregressive Topic Model",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/b495ce63ede0f4efc9eec62cb947c162-Paper.pdf",
    "abstract": "We describe a new model for learning meaningful representations of text documents from an unlabeled collection of documents. This model is inspired by the recently proposed Replicated Softmax, an undirected graphical model of word counts that was shown to learn a better generative model and more meaningful document representations. Specifically, we take inspiration from the conditional mean-field recursive equations of the Replicated Softmax in order to define a neural network architecture that estimates the probability of observing a new word in a given document given the previously observed words. This paradigm also allows us to replace the expensive softmax distribution over words with a hierarchical distribution over paths in a binary tree of words. The end result is a model whose training complexity scales logarithmically with the vocabulary size instead of linearly as in the Replicated Softmax. Our experiments show that our model is competitive both as a generative model of documents and as a document representation learning algorithm.",
    "authors": [
      "Larochelle, Hugo",
      "Lauly, Stanislas"
    ]
  },
  {
    "id": "b4a528955b84f584974e92d025a75d1f",
    "title": "Active Learning of Multi-Index Function Models",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/b4a528955b84f584974e92d025a75d1f-Paper.pdf",
    "abstract": "We consider the problem of actively learning \\textit{multi-index} functions of the form $f(\\vecx) = g(\\matA\\vecx)= \\sum_{i=1}^k g_i(\\veca_i^T\\vecx)$ from point evaluations of $f$. We assume that the function $f$ is defined on an $\\ell_2$-ball in $\\Real^d$, $g$ is twice continuously differentiable almost everywhere, and $\\matA \\in \\mathbb{R}^{k \\times d}$ is a rank $k$ matrix, where $k \\ll d$.  We propose a randomized, active sampling scheme for estimating such functions with uniform approximation guarantees. Our theoretical developments leverage recent techniques from low rank matrix recovery, which enables us to derive an estimator of the function $f$ along with sample complexity bounds. We also characterize the noise robustness of the scheme, and provide empirical evidence that the high-dimensional scaling of our sample complexity bounds are quite accurate.",
    "authors": [
      "Hemant, Tyagi",
      "Cevher, Volkan"
    ]
  },
  {
    "id": "b51a15f382ac914391a58850ab343b00",
    "title": "Probabilistic Low-Rank Subspace Clustering",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/b51a15f382ac914391a58850ab343b00-Paper.pdf",
    "abstract": "In this paper, we consider the problem of clustering data points into low-dimensional subspaces in the presence of outliers. We pose the problem using a density estimation formulation with an associated generative model. Based on this probability model, we first develop an iterative expectation-maximization (EM) algorithm and then derive its global solution. In addition, we develop two Bayesian methods based on variational Bayesian (VB) approximation, which are capable of automatic dimensionality selection. While the first method is based on an alternating optimization scheme for all unknowns, the second method makes use of recent results in VB matrix factorization leading to fast and effective estimation. Both methods are extended to handle sparse outliers for robustness and can handle missing values. Experimental results suggest that proposed methods are very effective in clustering and identifying outliers.",
    "authors": [
      "Babacan, S.",
      "Nakajima, Shinichi",
      "Do, Minh"
    ]
  },
  {
    "id": "b55ec28c52d5f6205684a473a2193564",
    "title": "Fully Bayesian inference for neural models with negative-binomial spiking",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/b55ec28c52d5f6205684a473a2193564-Paper.pdf",
    "abstract": "Characterizing the information carried by neural populations in the brain requires accurate statistical models of neural spike responses.  The negative-binomial distribution provides a convenient model for over-dispersed spike counts, that is, responses with greater-than-Poisson variability.  Here we describe a powerful data-augmentation framework for fully Bayesian inference in neural models with negative-binomial spiking. Our approach relies on a recently described latent-variable representation of the negative-binomial distribution, which equates it to a Polya-gamma mixture of normals.  This framework provides a tractable, conditionally Gaussian representation of the posterior that can be used to design efficient EM and Gibbs sampling based algorithms for inference in regression and dynamic factor models.  We apply the model to neural data from primate retina and show that it substantially outperforms Poisson regression on held-out data, and reveals latent structure underlying spike count correlations in simultaneously recorded spike trains.",
    "authors": [
      "Pillow, Jonathan",
      "Scott, James"
    ]
  },
  {
    "id": "b571ecea16a9824023ee1af16897a582",
    "title": "Adaptive Learning of Smoothing Functions: Application to Electricity Load Forecasting",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/b571ecea16a9824023ee1af16897a582-Paper.pdf",
    "abstract": "This paper proposes an efficient online learning algorithm to track the smoothing functions of Additive Models. The key idea is to combine the linear representation of Additive Models with a Recursive Least Squares (RLS) filter. In order to quickly track changes in the model and put more weight on recent data, the RLS filter uses a forgetting factor which exponentially weights down observations by the order of their arrival. The tracking behaviour is further enhanced by using an adaptive forgetting factor which is updated based on the gradient of the a priori errors. Using results from Lyapunov stability theory, upper bounds for the learning rate are analyzed. The proposed algorithm is applied to 5 years of electricity load data provided by the French utility company Electricite de France (EDF). Compared to state-of-the-art methods, it achieves a superior performance in terms of model tracking and prediction accuracy.",
    "authors": [
      "Ba, Amadou",
      "Sinn, Mathieu",
      "Goude, Yannig",
      "Pompey, Pascal"
    ]
  },
  {
    "id": "b7087c1f4f89e63af8d46f3b20271153",
    "title": "Complex Inference in Neural Circuits with Probabilistic Population Codes and Topic Models",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/b7087c1f4f89e63af8d46f3b20271153-Paper.pdf",
    "abstract": "Recent experiments have demonstrated that humans and animals typically reason probabilistically about their environment. This ability requires a neural code that represents probability distributions and neural circuits that are capable of implementing the operations of probabilistic inference. The proposed probabilistic population coding (PPC) framework provides a statistically efficient neural representation of probability distributions that is both broadly consistent with physiological measurements and capable of implementing some of the basic operations of probabilistic inference in a biologically plausible way. However, these experiments and the corresponding neural models have largely focused on simple (tractable) probabilistic computations such as cue combination, coordinate transformations, and decision making. As a result it remains unclear how to generalize this framework to more complex probabilistic computations. Here we address this short coming by showing that a very general approximate inference algorithm known as Variational Bayesian Expectation Maximization can be implemented within the linear PPC framework. We apply this approach to a generic problem faced by any given layer of cortex, namely the identification of latent causes of complex mixtures of spikes. We identify a formal equivalent between this spike pattern demixing problem and topic models used for document classification, in particular Latent Dirichlet Allocation (LDA). We then construct a neural network implementation of variational inference and learning for LDA that utilizes a linear PPC. This network relies critically on two non-linear operations: divisive normalization and super-linear facilitation, both of which are ubiquitously observed in neural circuits. We also demonstrate how online learning can be achieved using a variation of Hebb\u2019s rule and describe an extesion of this work which allows us to deal with time varying and correlated latent causes.",
    "authors": [
      "Beck, Jeff",
      "Pouget, Alexandre",
      "Heller, Katherine A."
    ]
  },
  {
    "id": "ba2fd310dcaa8781a9a652a31baf3c68",
    "title": "Clustering by Nonnegative Matrix Factorization Using Graph Random Walk",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/ba2fd310dcaa8781a9a652a31baf3c68-Paper.pdf",
    "abstract": "Nonnegative Matrix Factorization (NMF) is a promising relaxation technique for clustering analysis.  However, conventional NMF methods that directly approximate the pairwise similarities using the least square error often yield mediocre performance for data in curved manifolds because they can capture only the immediate similarities between data samples.  Here we propose a new NMF clustering method which replaces the approximated matrix with its smoothed version using random walk.  Our method can thus accommodate farther relationships between data samples.  Furthermore, we introduce a novel regularization in the proposed objective function in order to improve over spectral clustering.  The new learning objective is optimized by a multiplicative Majorization-Minimization algorithm with a scalable implementation for learning the factorizing matrix.  Extensive experimental results on real-world datasets show that our method has strong performance in terms of cluster purity.",
    "authors": [
      "Yang, Zhirong",
      "Hao, Tele",
      "Dikmen, Onur",
      "Chen, Xi",
      "Oja, Erkki"
    ]
  },
  {
    "id": "ba3866600c3540f67c1e9575e213be0a",
    "title": "Graphical Gaussian Vector for Image Categorization",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/ba3866600c3540f67c1e9575e213be0a-Paper.pdf",
    "abstract": "This paper proposes a novel image representation called a Graphical Gaussian Vector, which is a counterpart of the codebook and local feature matching approaches. In our method, we model the distribution of local features as a Gaussian Markov Random Field (GMRF) which can efficiently represent the spatial relationship among local features. We consider the parameter of GMRF as a feature vector of the image. Using concepts of information geometry, proper parameters and a metric from the GMRF can be obtained. Finally we define a new image feature by embedding the metric into the parameters, which can be directly applied to scalable linear classifiers. Our method obtains superior performance over the state-of-the-art methods in the standard object recognition datasets and comparable performance in the scene dataset. As the proposed method simply calculates the local auto-correlations of local features, it is able to achieve both high classification accuracy and high efficiency.",
    "authors": [
      "Harada, Tatsuya",
      "Kuniyoshi, Yasuo"
    ]
  },
  {
    "id": "bad5f33780c42f2588878a9d07405083",
    "title": "Convergence Rate Analysis of MAP Coordinate Minimization Algorithms",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/bad5f33780c42f2588878a9d07405083-Paper.pdf",
    "abstract": "Finding maximum aposteriori (MAP) assignments in graphical models is an important task in many applications. Since the problem is generally hard, linear programming (LP) relaxations are often used. Solving these relaxations efficiently is thus an important practical problem. In recent years, several authors have proposed message passing updates corresponding to coordinate descent in the dual LP. However,these are generally not guaranteed to converge to a global optimum. One approach to remedy this is to smooth the LP, and perform coordinate descent on the smoothed dual. However, little is known about the convergence rate of this procedure. Here we perform a thorough rate analysis of such schemes and derive primal and dual convergence rates. We also provide a simple dual to primal mapping that yields feasible primal solutions with a guaranteed rate of convergence. Empirical evaluation supports our theoretical claims and shows that the method is highly competitive with state of the art approaches that yield global optima.",
    "authors": [
      "Meshi, Ofer",
      "Globerson, Amir",
      "Jaakkola, Tommi"
    ]
  },
  {
    "id": "bb7946e7d85c81a9e69fee1cea4a087c",
    "title": "Distributed Probabilistic Learning for Camera Networks with Missing Data",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/bb7946e7d85c81a9e69fee1cea4a087c-Paper.pdf",
    "abstract": "Probabilistic approaches to computer vision typically assume a centralized setting, with the algorithm granted access to all observed data points.  However, many problems in wide-area surveillance can benefit from distributed modeling, either because of physical or computational constraints.  Most distributed models to date use algebraic approaches (such as distributed SVD) and as a result cannot explicitly deal with missing data.  In this work we present an approach to estimation and learning of generative probabilistic models in a distributed context where certain sensor data can be missing.  In particular, we show how traditional centralized models, such as probabilistic PCA and missing-data PPCA, can be learned when the data is distributed across a network of sensors.  We demonstrate the utility of this approach on the problem of distributed affine structure from motion.  Our experiments suggest that the accuracy of the learned probabilistic structure and motion models rivals that of traditional centralized factorization methods while being able to handle challenging situations such as missing or noisy observations.",
    "authors": [
      "Yoon, Sejong",
      "Pavlovic, Vladimir"
    ]
  },
  {
    "id": "bc7316929fe1545bf0b98d114ee3ecb8",
    "title": "Confusion-Based Online Learning and a Passive-Aggressive Scheme",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/bc7316929fe1545bf0b98d114ee3ecb8-Paper.pdf",
    "abstract": "This paper provides the first ---to the best of our knowledge--- analysis of online learning algorithms for multiclass problems when the {\\em confusion} matrix is taken as a performance measure. The work builds upon recent and elegant results on noncommutative concentration inequalities, i.e. concentration inequalities that apply to matrices, and more precisely to matrix martingales.  We do establish generalization bounds for online learning algorithm and show how the theoretical study motivate the proposition of a new confusion-friendly learning procedure. This learning algorithm, called \\copa (for COnfusion Passive-Aggressive) is a passive-aggressive learning algorithm; it is shown that the update equations for \\copa can be computed analytically, thus allowing the user from having to recours to any optimization package to implement it.",
    "authors": [
      "Ralaivola, Liva"
    ]
  },
  {
    "id": "bcbe3365e6ac95ea2c0343a2395834dd",
    "title": "Context-Sensitive Decision Forests for Object Detection",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/bcbe3365e6ac95ea2c0343a2395834dd-Paper.pdf",
    "abstract": "In this paper we introduce Context-Sensitive Decision Forests - A new perspective to exploit contextual information in the popular decision forest framework for the object detection problem. They are tree-structured classifiers with the ability to access intermediate prediction (here: classification and regression) information during training and inference time. This intermediate prediction is available to each sample, which allows us to develop context-based decision criteria, used for refining the prediction process. In addition, we introduce a novel split criterion which in combination with a priority based way of constructing the trees, allows more accurate regression mode selection and hence improves the current context information. In our experiments, we demonstrate improved results for the task of pedestrian detection on the challenging TUD data set when compared to state-of-the-art methods.",
    "authors": [
      "Kontschieder, Peter",
      "Bul\u00f2, Samuel",
      "Criminisi, Antonio",
      "Kohli, Pushmeet",
      "Pelillo, Marcello",
      "Bischof, Horst"
    ]
  },
  {
    "id": "bdb106a0560c4e46ccc488ef010af787",
    "title": "Approximating Concavely Parameterized Optimization Problems",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/bdb106a0560c4e46ccc488ef010af787-Paper.pdf",
    "abstract": "We consider an abstract class of optimization problems that are parameterized concavely in a single parameter, and show that the solution path along the parameter can always be approximated with accuracy $\\varepsilon >0$ by a set of size $O(1/\\sqrt{\\varepsilon})$. A lower bound of size $\\Omega (1/\\sqrt{\\varepsilon})$ shows that the upper bound is tight up to a constant factor. We also devise an algorithm that calls a step-size oracle and computes an approximate path of size $O(1/\\sqrt{\\varepsilon})$. Finally, we provide an implementation of the oracle for soft-margin support vector machines, and a parameterized semi-definite program for matrix completion.",
    "authors": [
      "Giesen, Joachim",
      "Mueller, Jens",
      "Laue, Soeren",
      "Swiercy, Sascha"
    ]
  },
  {
    "id": "beed13602b9b0e6ecb5b568ff5058f07",
    "title": "Kernel Latent SVM for Visual Recognition",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/beed13602b9b0e6ecb5b568ff5058f07-Paper.pdf",
    "abstract": "Latent SVMs (LSVMs) are a class of powerful tools that have been successfully applied to many applications in computer vision. However, a limitation of LSVMs is that they rely on linear models. For many computer vision tasks, linear models are suboptimal and nonlinear models learned with kernels typically perform much better. Therefore it is desirable to develop the kernel version of LSVM. In this paper, we propose kernel latent SVM (KLSVM) -- a new learning framework that combines latent SVMs and kernel methods. We develop an iterative training algorithm to learn the model parameters. We demonstrate the effectiveness of KLSVM using three different applications in visual recognition. Our KLSVM formulation is very general and can be applied to solve a wide range of applications in computer vision and machine learning.",
    "authors": [
      "Yang, Weilong",
      "Wang, Yang",
      "Vahdat, Arash",
      "Mori, Greg"
    ]
  },
  {
    "id": "bf62768ca46b6c3b5bea9515d1a1fc45",
    "title": "A Linear Time Active Learning Algorithm for Link Classification",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/bf62768ca46b6c3b5bea9515d1a1fc45-Paper.pdf",
    "abstract": "We present very efficient active learning algorithms for link classification in signed networks. Our algorithms are motivated by a stochastic model in which edge labels are obtained through perturbations of a initial sign assignment consistent with a two-clustering of the nodes. We provide a theoretical analysis within this model, showing that we can achieve an optimal (to whithin a constant factor) number of mistakes on any graph $G = (V,E)$ such that $|E|$ is at least order of $|V|^{3/2}$ by querying at most order of $|V|^{3/2}$ edge labels. More generally, we show an algorithm that achieves optimality to within a factor of order $k$ by querying at most order of $|V| + (|V|/k)^{3/2}$ edge labels. The running time of this algorithm is at most of order $|E| + |V|\\log|V|$.",
    "authors": [
      "Cesa-bianchi, Nicol\u00f2",
      "Gentile, Claudio",
      "Vitale, Fabio",
      "Zappella, Giovanni"
    ]
  },
  {
    "id": "c042f4db68f23406c6cecf84a7ebb0fe",
    "title": "A P300 BCI for the Masses: Prior Information Enables Instant Unsupervised Spelling",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/c042f4db68f23406c6cecf84a7ebb0fe-Paper.pdf",
    "abstract": "The usability of Brain Computer Interfaces (BCI) based on the P300 speller is severely hindered by the need for long training times and many repetitions of the same stimulus. In this contribution we introduce a set of unsupervised hierarchical probabilistic models that tackle both problems simultaneously by incorporating prior knowledge from two sources: information from other training subjects (through transfer learning) and information about the words being spelled (through language models). We show, that due to this prior knowledge, the performance of the unsupervised models parallels and in some cases even surpasses that of supervised models, while eliminating the tedious training session.",
    "authors": [
      "Kindermans, Pieter-jan",
      "Verschore, Hannes",
      "Verstraeten, David",
      "Schrauwen, Benjamin"
    ]
  },
  {
    "id": "c0c7c76d30bd3dcaefc96f40275bdc0a",
    "title": "Dynamic Pruning of Factor Graphs for Maximum Marginal Prediction",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/c0c7c76d30bd3dcaefc96f40275bdc0a-Paper.pdf",
    "abstract": "We study the problem of maximum marginal prediction (MMP) in probabilistic graphical models, a task that occurs, for example, as the Bayes optimal decision rule under a Hamming loss. MMP is typically performed as a two-stage procedure: one estimates each variable's marginal probability and then forms a prediction from the states of maximal probability. In this work we propose a simple yet effective technique for accelerating MMP when inference is sampling-based: instead of the above two-stage procedure we directly estimate the posterior probability of each decision variable. This allows us to identify the point of time when we are sufficiently certain about any individual decision. Whenever this is the case, we dynamically prune the variable we are confident about from the underlying factor graph. Consequently, at any time only samples of variable whose decision is still uncertain need to be created. Experiments in two prototypical scenarios, multi-label classification and image inpainting, shows that adaptive sampling can drastically accelerate MMP without sacrificing prediction accuracy.",
    "authors": [
      "Lampert, Christoph H."
    ]
  },
  {
    "id": "c20ad4d76fe97759aa27a0c99bff6710",
    "title": "Locally Uniform Comparison Image Descriptor",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/c20ad4d76fe97759aa27a0c99bff6710-Paper.pdf",
    "abstract": "Keypoint matching between pairs of images using popular descriptors like SIFT or a faster variant called SURF is at the heart of many computer vision algorithms including recognition, mosaicing, and structure from motion. For real-time mobile applications, very fast but less accurate descriptors like BRIEF and related methods use a random sampling of pairwise comparisons of pixel intensities in an image patch. Here, we introduce Locally Uniform Comparison Image Descriptor (LUCID), a simple description method based on permutation distances between the ordering of intensities of RGB values between two patches. LUCID is computable in linear time with respect to patch size and does not require floating point computation. An analysis reveals an underlying issue that limits the potential of BRIEF and related approaches compared to LUCID. Experiments demonstrate that LUCID is faster than BRIEF, and its accuracy is directly comparable to SURF while being more than an order of magnitude faster.",
    "authors": [
      "Ziegler, Andrew",
      "Christiansen, Eric",
      "Kriegman, David",
      "Belongie, Serge"
    ]
  },
  {
    "id": "c26820b8a4c1b3c2aa868d6d57e14a79",
    "title": "Priors for Diversity in Generative Latent Variable Models",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/c26820b8a4c1b3c2aa868d6d57e14a79-Paper.pdf",
    "abstract": "Probabilistic latent variable models are one of the cornerstones of machine learning.  They offer a convenient and coherent way to specify prior distributions over unobserved structure in data, so that these unknown properties can be inferred via posterior inference.  Such models are useful for exploratory analysis and visualization, for building density models of data, and for   providing features that can be used for later discriminative tasks. A significant limitation of these models, however, is that draws from the prior are often highly redundant due to i.i.d. assumptions   on internal parameters.  For example, there is no preference in the prior of a mixture model to make components non-overlapping, or in topic model to ensure that co-ocurring words only appear in a small number of topics.  In this work, we revisit these independence assumptions for probabilistic latent variable models, replacing the   underlying i.i.d.\\ prior with a determinantal point process (DPP). The DPP allows us to specify a preference for diversity in our latent variables using a positive definite kernel function.  Using a kernel between probability distributions, we are able to define a DPP on probability measures.  We show how to perform MAP inference   with DPP priors in latent Dirichlet allocation and in mixture models, leading to better intuition for the latent variable representation and quantitatively improved unsupervised feature extraction, without compromising the generative aspects of the model.",
    "authors": [
      "Kwok, James",
      "Adams, Ryan P."
    ]
  },
  {
    "id": "c2aee86157b4a40b78132f1e71a9e6f1",
    "title": "Mixing Properties of Conditional Markov Chains with Unbounded Feature Functions",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/c2aee86157b4a40b78132f1e71a9e6f1-Paper.pdf",
    "abstract": "Conditional Markov Chains (also known as Linear-Chain Conditional Random Fields  in the literature) are a versatile class of discriminative models for the distribution of a sequence of hidden states conditional on a sequence of observable variables. Large-sample properties of Conditional Markov Chains have been first studied by Sinn and Poupart [1]. The paper extends this work in two directions: first, mixing properties of models with unbounded feature functions are being established; second, necessary conditions for model identifiability and the uniqueness of maximum likelihood estimates are being given.",
    "authors": [
      "Sinn, Mathieu",
      "Chen, Bei"
    ]
  },
  {
    "id": "c399862d3b9d6b76c8436e924a68c45b",
    "title": "ImageNet Classification with Deep Convolutional Neural Networks",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf",
    "abstract": "We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7\\% and 18.9\\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.",
    "authors": [
      "Krizhevsky, Alex",
      "Sutskever, Ilya",
      "Hinton, Geoffrey E."
    ]
  },
  {
    "id": "c52f1bd66cc19d05628bd8bf27af3ad6",
    "title": "Stochastic Gradient Descent with Only One Projection",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/c52f1bd66cc19d05628bd8bf27af3ad6-Paper.pdf",
    "abstract": "Although many variants of stochastic gradient descent have been proposed for large-scale convex optimization, most of them require projecting the solution at {\\it each} iteration to ensure that the obtained solution stays within the feasible domain. For complex domains (e.g., positive semidefinite cone), the projection step can be computationally expensive, making stochastic gradient descent unattractive for large-scale optimization problems. We address this limitation by developing a novel stochastic gradient descent algorithm that does not need intermediate projections. Instead, only one projection at the last iteration is needed to obtain a feasible solution in the given domain. Our theoretical analysis shows that with a high probability, the proposed algorithms achieve an $O(1/\\sqrt{T})$ convergence rate for general convex optimization, and an $O(\\ln T/T)$  rate for  strongly convex optimization under mild conditions about the domain and the objective function.",
    "authors": [
      "Mahdavi, Mehrdad",
      "Yang, Tianbao",
      "Jin, Rong",
      "Zhu, Shenghuo",
      "Yi, Jinfeng"
    ]
  },
  {
    "id": "c54e7837e0cd0ced286cb5995327d1ab",
    "title": "Learning Probability Measures with respect to Optimal Transport Metrics",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/c54e7837e0cd0ced286cb5995327d1ab-Paper.pdf",
    "abstract": "We study the problem of estimating, in the sense of optimal transport metrics, a measure which is assumed supported on a manifold embedded in a Hilbert space. By establishing a precise connection between optimal transport metrics, optimal quantization, and learning theory, we derive new probabilistic bounds for the performance of a classic algorithm in unsupervised learning (k-means), when used to produce a probability measure derived from the data. In the course of the analysis, we arrive at new lower bounds, as well as probabilistic bounds on the convergence rate of the empirical law of large numbers, which, unlike existing bounds, are applicable to a wide class of measures.",
    "authors": [
      "Canas, Guillermo",
      "Rosasco, Lorenzo"
    ]
  },
  {
    "id": "c5ab0bc60ac7929182aadd08703f1ec6",
    "title": "Effective Split-Merge Monte Carlo Methods for Nonparametric Models of Sequential Data",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/c5ab0bc60ac7929182aadd08703f1ec6-Paper.pdf",
    "abstract": "Applications of Bayesian nonparametric methods require learning and inference algorithms which efficiently explore models of unbounded complexity. We develop new Markov chain Monte Carlo methods for the beta process hidden Markov model (BP-HMM), enabling discovery of shared activity patterns in large video and motion capture databases. By introducing split-merge moves based on sequential allocation, we allow large global changes in the shared feature structure. We also develop data-driven reversible jump moves which more reliably discover rare or unique behaviors. Our proposals apply to any choice of conjugate likelihood for observed data, and we show success with multinomial, Gaussian, and autoregressive emission models. Together, these innovations allow tractable analysis of hundreds of time series, where previous inference required clever initialization and at least ten thousand burn-in iterations for just six sequences.",
    "authors": [
      "Hughes, Michael C.",
      "Fox, Emily",
      "Sudderth, Erik"
    ]
  },
  {
    "id": "c5d736809766d46260d816d8dbc9eb44",
    "title": "Scaling MPE Inference for Constrained Continuous Markov Random Fields with Consensus Optimization",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/c5d736809766d46260d816d8dbc9eb44-Paper.pdf",
    "abstract": "Probabilistic graphical models are powerful tools for analyzing constrained, continuous domains. However, finding most-probable explanations (MPEs) in these models can be computationally expensive. In this paper, we improve the scalability of MPE inference in a class of graphical models with piecewise-linear and piecewise-quadratic dependencies and linear constraints over continuous domains. We derive algorithms based on a consensus-optimization framework and demonstrate their superior performance over state of the art. We show empirically that in a large-scale voter-preference modeling problem our algorithms scale linearly in the number of dependencies and constraints.",
    "authors": [
      "Bach, Stephen",
      "Broecheler, Matthias",
      "Getoor, Lise",
      "O'leary, Dianne"
    ]
  },
  {
    "id": "c60d060b946d6dd6145dcbad5c4ccf6f",
    "title": "A systematic approach to extracting semantic information from functional MRI data",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/c60d060b946d6dd6145dcbad5c4ccf6f-Paper.pdf",
    "abstract": "This paper introduces a novel classification method for functional magnetic resonance imaging datasets with tens of classes. The method is designed to make predictions using information from as many brain locations as possible, instead of resorting to feature selection, and does this by decomposing the pattern of brain activation into differently informative sub-regions. We provide results over a complex semantic processing dataset that show that the method is competitive with state-of-the-art feature selection and also suggest how the method may be used to perform group or exploratory analyses of complex class structure.",
    "authors": [
      "Pereira, Francisco",
      "Botvinick, Matthew"
    ]
  },
  {
    "id": "c667d53acd899a97a85de0c201ba99be",
    "title": "Sketch-Based Linear Value Function Approximation",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/c667d53acd899a97a85de0c201ba99be-Paper.pdf",
    "abstract": "Hashing is a common method to reduce large, potentially infinite feature vectors to a fixed-size table. In reinforcement learning, hashing is often used in conjunction with tile coding to represent states in continuous spaces. Hashing is also a promising approach to value function approximation in large discrete domains such as Go and Hearts, where feature vectors can be constructed by exhaustively combining a set of atomic features. Unfortunately, the typical use of hashing in value function approximation results in biased value estimates due to the possibility of collisions. Recent work in data stream summaries has led to the development of the tug-of-war sketch, an unbiased estimator for approximating inner products. Our work investigates the application of this new data structure to linear value function approximation. Although in the reinforcement learning setting the use of the tug-of-war sketch leads to biased value estimates, we show that this bias can be orders of magnitude less than that of standard hashing. We provide empirical results on two RL benchmark domains and fifty-five Atari 2600 games to highlight the superior learning performance of tug-of-war hashing.",
    "authors": [
      "Bellemare, Marc",
      "Veness, Joel",
      "Bowling, Michael"
    ]
  },
  {
    "id": "c6e19e830859f2cb9f7c8f8cacb8d2a6",
    "title": "Unsupervised Structure Discovery for Semantic Analysis of Audio",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/c6e19e830859f2cb9f7c8f8cacb8d2a6-Paper.pdf",
    "abstract": "Approaches to audio classification and retrieval tasks largely rely on detection-based discriminative models. We submit that such models make a simplistic assumption in mapping acoustics directly to semantics, whereas the actual process is likely more complex. We present a generative model that maps acoustics in a hierarchical manner to increasingly higher-level semantics. Our model has 2 layers with the first being generic sound units with no clear semantic associations, while the second layer attempts to find patterns over the generic sound units. We evaluate our model on a large-scale retrieval task from TRECVID 2011, and report significant improvements over standard baselines.",
    "authors": [
      "Chaudhuri, Sourish",
      "Raj, Bhiksha"
    ]
  },
  {
    "id": "c73dfe6c630edb4c1692db67c510f65c",
    "title": "The Time-Marginalized Coalescent Prior for Hierarchical Clustering",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/c73dfe6c630edb4c1692db67c510f65c-Paper.pdf",
    "abstract": "We introduce a new prior for use in Nonparametric Bayesian Hierarchical Clustering. The prior is constructed by marginalizing out the time information of Kingman\u2019s coalescent, providing a prior over tree structures which we call the Time-Marginalized Coalescent (TMC). This allows for models which factorize the tree structure and times, providing two benefits: more flexible priors may be constructed and more efficient Gibbs type inference can be used. We demonstrate this on an example model for density estimation and show the TMC achieves competitive experimental results.",
    "authors": [
      "Boyles, Levi",
      "Welling, Max"
    ]
  },
  {
    "id": "c78c347465f4775425c059ea101c131f",
    "title": "Nonparametric Max-Margin Matrix Factorization for Collaborative Prediction",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/c78c347465f4775425c059ea101c131f-Paper.pdf",
    "abstract": "We present a probabilistic formulation of max-margin matrix factorization and build accordingly a nonparametric Bayesian model which automatically resolves the unknown number of latent factors. Our work demonstrates a successful example that integrates Bayesian nonparametrics and max-margin learning, which are conventionally two separate paradigms and enjoy complementary advantages. We develop an efcient variational algorithm for posterior inference, and our extensive empirical studies on large-scale MovieLens and EachMovie data sets appear to justify the aforementioned dual advantages.",
    "authors": [
      "Xu, Minjie",
      "Zhu, Jun",
      "Zhang, Bo"
    ]
  },
  {
    "id": "c8ba76c279269b1c6bc8a07e38e78fa4",
    "title": "Exponential Concentration for Mutual Information Estimation with Application to Forests",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/c8ba76c279269b1c6bc8a07e38e78fa4-Paper.pdf",
    "abstract": "We prove a new exponential concentration inequality for a plug-in estimator of the Shannon mutual information. Previous results on mutual information estimation only bounded expected error. The advantage of having the exponential inequality is that, combined with the union bound, we can guarantee accurate estimators of the mutual information for many pairs of random variables simultaneously. As an application, we show how to use such a result to optimally estimate the density function and graph of a distribution which is Markov to a forest graph.",
    "authors": [
      "Liu, Han",
      "Wasserman, Larry",
      "Lafferty, John"
    ]
  },
  {
    "id": "c8c41c4a18675a74e01c8a20e8a0f662",
    "title": "Slice Normalized Dynamic Markov Logic Networks",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/c8c41c4a18675a74e01c8a20e8a0f662-Paper.pdf",
    "abstract": "Markov logic is a widely used tool in statistical relational learning, which uses a weighted first-order logic knowledge base to specify a Markov random field (MRF) or a conditional random field (CRF). In many applications, a Markov logic network (MLN) is trained in one domain, but used in a different one. This paper focuses on dynamic Markov logic networks, where the domain of time points typically varies between training and testing. It has been previously pointed out that the marginal probabilities of truth assignments to ground atoms can change if one extends or reduces the domains of predicates in an MLN. We show that in addition to this problem, the standard way of unrolling a Markov logic theory into a MRF may result in time-inhomogeneity of the underlying Markov chain. Furthermore, even if these representational problems are not significant for a given domain, we show that the more practical problem of generating samples in a sequential conditional random field for the next slice relying on the samples from the previous slice has high computational cost in the general case, due to the need to estimate a normalization factor for each sample. We propose a new discriminative model, slice normalized dynamic Markov logic networks (SN-DMLN), that suffers from none of these issues. It supports efficient online inference, and can directly model influences between variables within a time slice that do not have a causal direction, in contrast with fully directed models (e.g., DBNs). Experimental results show an improvement in accuracy over previous approaches to online inference in dynamic Markov logic networks.",
    "authors": [
      "Papai, Tivadar",
      "Kautz, Henry",
      "Stefankovic, Daniel"
    ]
  },
  {
    "id": "c913303f392ffc643f7240b180602652",
    "title": "Continuous Relaxations for Discrete Hamiltonian Monte Carlo",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/c913303f392ffc643f7240b180602652-Paper.pdf",
    "abstract": "Continuous relaxations play an important role in discrete optimization, but have not seen much use in approximate probabilistic inference. Here we show that a general form of the Gaussian Integral Trick makes it possible to transform a wide class of discrete variable undirected models into fully continuous systems. The continuous representation allows the use of gradient-based Hamiltonian Monte Carlo for inference,  results in new ways of estimating normalization constants (partition functions), and in general opens up a number of new avenues for inference in difficult discrete systems. We demonstrate some of these continuous relaxation inference algorithms on a number of illustrative problems.",
    "authors": [
      "Zhang, Yichuan",
      "Ghahramani, Zoubin",
      "Storkey, Amos J.",
      "Sutton, Charles"
    ]
  },
  {
    "id": "c9f95a0a5af052bffce5c89917335f67",
    "title": "Learning with Target Prior",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/c9f95a0a5af052bffce5c89917335f67-Paper.pdf",
    "abstract": "In the conventional approaches for supervised parametric learning, relations between data and target variables are provided through training sets consisting of pairs of corresponded data and target variables. In this work, we describe a new learning scheme for parametric learning, in which the target variables $\\y$ can be modeled with a prior model $p(\\y)$ and the relations between data and target variables are estimated through $p(\\y)$ and a set of uncorresponded data $\\x$ in training. We term this method as learning with target priors (LTP). Specifically, LTP learning seeks parameter $\\t$ that maximizes the log likelihood of $f_\\t(\\x)$ on a uncorresponded training set with regards to $p(\\y)$. Compared to the conventional (semi)supervised learning approach, LTP can make efficient use of prior knowledge of the target variables in the form of probabilistic distributions, and thus removes/reduces the reliance on training data in learning. Compared to the Bayesian approach, the learned parametric regressor in LTP can be more efficiently implemented and deployed in tasks where running efficiency is critical, such as on-line BCI signal decoding. We demonstrate the effectiveness of the proposed approach on parametric regression tasks for BCI signal decoding and pose estimation from video.",
    "authors": [
      "Wang, Zuoguan",
      "Lyu, Siwei",
      "Schalk, Gerwin",
      "Ji, Qiang"
    ]
  },
  {
    "id": "ca8155f4d27f205953f9d3d7974bdd70",
    "title": "Generalization Bounds for Domain Adaptation",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/ca8155f4d27f205953f9d3d7974bdd70-Paper.pdf",
    "abstract": "In this paper, we provide a new framework to study the generalization bound of the learning process for domain adaptation. Without loss of generality, we consider two kinds of representative domain adaptation settings: one is domain adaptation with multiple sources and the other is domain adaptation combining source and target data. In particular, we introduce two quantities that capture the inherent characteristics of domains. For either kind of domain adaptation, based on the two quantities, we then develop the specific Hoeffding-type deviation inequality and symmetrization inequality to achieve the corresponding generalization bound based on the uniform entropy number. By using the resultant generalization bound, we analyze the asymptotic convergence and the rate of convergence of the learning process for such kind of domain adaptation. Meanwhile, we discuss the factors that affect the asymptotic behavior of the learning process. The numerical experiments support our results.",
    "authors": [
      "Zhang, Chao",
      "Zhang, Lei",
      "Ye, Jieping"
    ]
  },
  {
    "id": "cb70ab375662576bd1ac5aaf16b3fca4",
    "title": "Multiplicative Forests for Continuous-Time Processes",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/cb70ab375662576bd1ac5aaf16b3fca4-Paper.pdf",
    "abstract": "Learning temporal dependencies between variables over continuous time is an important and challenging task. Continuous-time Bayesian networks effectively model such processes but are limited by the number of conditional intensity matrices, which grows exponentially in the number of parents per variable. We develop a partition-based representation using regression trees and forests whose parameter spaces grow linearly in the number of node splits. Using a multiplicative assumption we show how to update the forest likelihood in closed form, producing efficient model updates. Our results show multiplicative forests can be learned from few temporal trajectories with large gains in performance and scalability.",
    "authors": [
      "Weiss, Jeremy",
      "Natarajan, Sriraam",
      "Page, David"
    ]
  },
  {
    "id": "cd00692c3bfe59267d5ecfac5310286c",
    "title": "Variational Inference for Crowdsourcing",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/cd00692c3bfe59267d5ecfac5310286c-Paper.pdf",
    "abstract": "Crowdsourcing has become a popular paradigm for labeling large datasets. However, it has given rise to the computational task of aggregating the crowdsourced labels provided by a collection of unreliable annotators. We approach this problem by transforming it into a standard inference problem in graphical models, and applying approximate variational methods, including belief propagation (BP) and mean field (MF). We show that our BP algorithm generalizes both majority voting and a recent algorithm by Karger et al, while our MF method is closely related to a commonly used EM algorithm. In both cases, we find that the performance of the algorithms critically depends on the choice of a prior distribution on the workers' reliability; by choosing the prior properly, both BP and MF (and EM) perform surprisingly well on both simulated and real-world datasets, competitive with state-of-the-art algorithms based on more complicated modeling assumptions.",
    "authors": [
      "Liu, Qiang",
      "Peng, Jian",
      "Ihler, Alexander T."
    ]
  },
  {
    "id": "cd61a580392a70389e27b0bc2b439f49",
    "title": "Why MCA? Nonlinear sparse coding with spike-and-slab prior for neurally plausible image encoding",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/cd61a580392a70389e27b0bc2b439f49-Paper.pdf",
    "abstract": "Modelling natural images with sparse coding (SC) has faced two main challenges: \ufb02exibly representing varying pixel intensities and realistically representing low- level image components. This paper proposes a novel multiple-cause generative model of low-level image statistics that generalizes the standard SC model in two crucial points: (1) it uses a spike-and-slab prior distribution for a more realistic representation of component absence/intensity, and (2) the model uses the highly nonlinear combination rule of maximal causes analysis (MCA) instead of a lin- ear combination. The major challenge is parameter optimization because a model with either (1) or (2) results in strongly multimodal posteriors. We show for the \ufb01rst time that a model combining both improvements can be trained ef\ufb01ciently while retaining the rich structure of the posteriors. We design an exact piece- wise Gibbs sampling method and combine this with a variational method based on preselection of latent dimensions. This combined training scheme tackles both analytical and computational intractability and enables application of the model to a large number of observed and hidden dimensions. Applying the model to image patches we study the optimal encoding of images by simple cells in V1 and compare the model\u2019s predictions with in vivo neural recordings. In contrast to standard SC, we \ufb01nd that the optimal prior favors asymmetric and bimodal ac- tivity of simple cells. Testing our model for consistency we \ufb01nd that the average posterior is approximately equal to the prior. Furthermore, we \ufb01nd that the model predicts a high percentage of globular receptive \ufb01elds alongside Gabor-like \ufb01elds. Similarly high percentages are observed in vivo. Our results thus argue in favor of improvements of the standard sparse coding model for simple cells by using \ufb02exible priors and nonlinear combinations.",
    "authors": [
      "Sterne, Philip",
      "Bornschein, Joerg",
      "Sheikh, Abdul-saboor",
      "L\u00fccke, J\u00f6rg",
      "Shelton, Jacquelyn"
    ]
  },
  {
    "id": "ce5140df15d046a66883807d18d0264b",
    "title": "Tractable Objectives for Robust Policy Optimization",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/ce5140df15d046a66883807d18d0264b-Paper.pdf",
    "abstract": "Robust policy optimization acknowledges that risk-aversion plays a vital role in real-world decision-making. When faced with uncertainty about the effects of actions, the policy that maximizes expected utility over the unknown parameters of the system may also carry with it a risk of intolerably poor performance.  One might prefer to accept lower utility in expectation in order to avoid, or reduce the likelihood of, unacceptable levels of utility under harmful parameter realizations.   In this paper, we take a Bayesian approach to parameter uncertainty, but unlike other methods avoid making any distributional assumptions about the form of this uncertainty.  Instead we focus on identifying optimization objectives for which solutions can be efficiently approximated.  We introduce percentile measures: a very general class of objectives for robust policy optimization, which encompasses most existing approaches, including ones known to be intractable. We then introduce a broad subclass of this family for which robust policies can be approximated efficiently. Finally, we frame these objectives in the context of a two-player, zero-sum, extensive-form game and employ a no-regret algorithm to approximate an optimal policy, with computation only polynomial in the number of states and actions of the MDP.",
    "authors": [
      "Chen, Katherine",
      "Bowling, Michael"
    ]
  },
  {
    "id": "cfbce4c1d7c425baf21d6b6f2babe6be",
    "title": "Multiple Choice Learning: Learning to Produce Multiple Structured Outputs",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/cfbce4c1d7c425baf21d6b6f2babe6be-Paper.pdf",
    "abstract": "The paper addresses the problem of generating multiple hypotheses for prediction tasks that involve interaction with users or successive components in a cascade. Given a set of multiple hypotheses, such components/users have the ability to automatically rank the results and thus retrieve the best one. The standard approach for handling this scenario is to learn a single model and then produce M-best Maximum a Posteriori (MAP) hypotheses from this model. In contrast, we formulate this multiple {\\em choice} learning task as a multiple-output structured-output prediction problem with a loss function that captures the natural setup of the problem. We present a max-margin formulation  that minimizes an upper-bound on this loss-function. Experimental results on the problems of image co-segmentation and protein side-chain prediction show that our method outperforms conventional approaches used for this  scenario and leads to substantial improvements in prediction accuracy.",
    "authors": [
      "Guzm\u00e1n-rivera, Abner",
      "Batra, Dhruv",
      "Kohli, Pushmeet"
    ]
  },
  {
    "id": "d1f491a404d6854880943e5c3cd9ca25",
    "title": "Robustness and risk-sensitivity in Markov decision processes",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/d1f491a404d6854880943e5c3cd9ca25-Paper.pdf",
    "abstract": "We uncover relations between robust MDPs and risk-sensitive MDPs.  The objective of a robust MDP is to minimize a function, such as the expectation of cumulative cost, for the worst case when the parameters have uncertainties.  The objective of a risk-sensitive MDP is to minimize a risk measure of the cumulative cost when the parameters are known.  We show that a risk-sensitive MDP of minimizing the expected exponential utility is equivalent to a robust MDP of minimizing the worst-case expectation with a penalty for the deviation of the uncertain parameters from their nominal values, which is measured with the Kullback-Leibler divergence.  We also show that a risk-sensitive MDP of minimizing an iterated risk measure that is composed of certain coherent risk measures is equivalent to a robust MDP of minimizing the worst-case expectation when the possible deviations of uncertain parameters from their nominal values are characterized with a concave function.",
    "authors": [
      "Osogami, Takayuki"
    ]
  },
  {
    "id": "d240e3d38a8882ecad8633c8f9c78c9b",
    "title": "Communication/Computation Tradeoffs in Consensus-Based Distributed Optimization",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/d240e3d38a8882ecad8633c8f9c78c9b-Paper.pdf",
    "abstract": "We study the scalability of consensus-based distributed optimization algorithms by considering two questions: How many processors should we use for a given problem, and how often should they communicate when communication is not free? Central to our analysis is a problem-specific value $r$ which quantifies the communication/computation tradeoff. We show that organizing the communication among nodes as a $k$-regular expander graph~\\cite{kRegExpanders} yields speedups, while when all pairs of nodes communicate (as in a complete graph), there is an optimal number of processors that depends on $r$. Surprisingly, a speedup can be obtained, in terms of the time to reach a fixed level of accuracy, by communicating less and less frequently as the computation progresses. Experiments on a real cluster solving metric learning and non-smooth convex minimization tasks demonstrate strong agreement between theory and practice.",
    "authors": [
      "Tsianos, Konstantinos",
      "Lawlor, Sean",
      "Rabbat, Michael"
    ]
  },
  {
    "id": "d296c101daa88a51f6ca8cfc1ac79b50",
    "title": "A Polylog Pivot Steps Simplex Algorithm for Classification",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/d296c101daa88a51f6ca8cfc1ac79b50-Paper.pdf",
    "abstract": "We present a simplex algorithm for linear programming in a linear classification formulation. The paramount complexity parameter in linear classification problems is called the margin. We prove that for margin values of practical interest our simplex variant performs a polylogarithmic number of pivot steps in the worst case, and its overall running time is near linear. This is in contrast to general linear programming, for which no sub-polynomial pivot rule is known.",
    "authors": [
      "Hazan, Elad",
      "Karnin, Zohar"
    ]
  },
  {
    "id": "d2ed45a52bc0edfa11c2064e9edee8bf",
    "title": "Fiedler Random Fields: A Large-Scale Spectral Approach to Statistical Network Modeling",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/d2ed45a52bc0edfa11c2064e9edee8bf-Paper.pdf",
    "abstract": "Statistical models for networks have been typically committed to strong prior assumptions concerning the form of the modeled distributions. Moreover, the vast majority of currently available models are explicitly designed for capturing some specific graph properties (such as power-law degree distributions), which makes them unsuitable for application to domains where the behavior of the target quantities is not known a priori. The key contribution of this paper is twofold. First, we introduce the Fiedler delta statistic, based on the Laplacian spectrum of graphs, which allows to dispense with any parametric assumption concerning the modeled network properties. Second, we use the defined statistic to develop the Fiedler random field model, which allows for efficient estimation of edge distributions over large-scale random networks. After analyzing the dependence structure involved in Fiedler random fields, we estimate them over several real-world networks, showing that they achieve a much higher modeling accuracy than other well-known statistical approaches.",
    "authors": [
      "Freno, Antonino",
      "Keller, Mikaela",
      "Tommasi, Marc"
    ]
  },
  {
    "id": "d395771085aab05244a4fb8fd91bf4ee",
    "title": "Majorization for CRFs and Latent Likelihoods",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/d395771085aab05244a4fb8fd91bf4ee-Paper.pdf",
    "abstract": "The partition function plays a key role in probabilistic modeling including conditional random fields, graphical models, and maximum likelihood estimation. To optimize partition functions, this article introduces a quadratic variational upper bound. This inequality facilitates majorization methods: optimization of complicated functions through the iterative solution of simpler sub-problems. Such bounds remain efficient to compute even when the partition function involves a graphical model (with small tree-width) or in latent likelihood settings. For large-scale problems, low-rank versions of the bound are provided and outperform LBFGS as well as first-order methods. Several learning applications are shown and reduce to fast and convergent update rules. Experimental results show advantages over state-of-the-art optimization methods.",
    "authors": [
      "Jebara, Tony",
      "Choromanska, Anna"
    ]
  },
  {
    "id": "d4c2e4a3297fe25a71d030b67eb83bfc",
    "title": "Feature-aware Label Space Dimension Reduction for Multi-label Classification",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/d4c2e4a3297fe25a71d030b67eb83bfc-Paper.pdf",
    "abstract": "Label space dimension reduction (LSDR) is an efficient and effective paradigm for multi-label classification with many classes. Existing approaches to LSDR, such as compressive sensing and principal label space transformation, exploit only the label part of the dataset, but not the feature part. In this paper, we propose a novel approach to LSDR that considers both the label and the feature parts. The approach, called conditional principal label space transformation, is based on minimizing an upper bound of the popular Hamming loss. The minimization step of the approach can be carried out efficiently by a simple use of singular value decomposition. In addition, the approach can be extended to a kernelized version that allows the use of sophisticated feature combinations to assist LSDR. The experimental results verify that the proposed approach is more effective than existing ones to LSDR across many real-world datasets.",
    "authors": [
      "Chen, Yao-nan",
      "Lin, Hsuan-tien"
    ]
  },
  {
    "id": "d554f7bb7be44a7267068a7df88ddd20",
    "title": "Semantic Kernel Forests from Multiple Taxonomies",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/d554f7bb7be44a7267068a7df88ddd20-Paper.pdf",
    "abstract": "When learning features for complex visual recognition problems, labeled image exemplars alone can be insufficient.  While an \\emph{object taxonomy} specifying the categories' semantic relationships could bolster the learning process, not all relationships are relevant to a given visual classification task, nor does a single taxonomy capture all ties that \\emph{are} relevant.  In light of these issues, we propose a discriminative feature learning approach that leverages \\emph{multiple} hierarchical taxonomies representing different semantic views of the object categories (e.g., for animal classes, one taxonomy could reflect their phylogenic ties, while another could reflect their habitats).  For each taxonomy, we first learn a tree of semantic kernels, where each node has a Mahalanobis kernel optimized to distinguish between the classes in its children nodes.  Then, using the resulting \\emph{semantic kernel forest}, we learn class-specific kernel combinations to select only those relationships relevant to recognize each object class.  To learn the weights, we introduce a novel hierarchical regularization term that further exploits the taxonomies' structure.  We demonstrate our method on challenging object recognition datasets, and show that interleaving multiple taxonomic views yields significant accuracy improvements.",
    "authors": [
      "Hwang, Sung",
      "Grauman, Kristen",
      "Sha, Fei"
    ]
  },
  {
    "id": "d58072be2820e8682c0a27c0518e805e",
    "title": "Spectral learning of linear dynamics from generalised-linear observations with application to neural population data",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/d58072be2820e8682c0a27c0518e805e-Paper.pdf",
    "abstract": "Latent linear dynamical systems with generalised-linear observation models arise in a variety of applications, for example when modelling the spiking activity of populations of neurons.  Here, we show how  spectral learning methods for linear systems with Gaussian observations   (usually called subspace identification in this context) can be extended to estimate the parameters of dynamical system models observed through non-Gaussian noise models. We use this approach to obtain estimates of parameters for a dynamical model of neural population data, where the observed spike-counts are Poisson-distributed with log-rates determined by the latent dynamical process, possibly driven by external inputs. We show that the extended system identification algorithm is consistent and accurately recovers the correct parameters on large simulated data sets with much smaller computational cost than approximate expectation-maximisation (EM) due to the non-iterative nature of subspace identification. Even on smaller data sets, it provides an effective initialization for EM, leading to more robust performance and faster convergence. These benefits are shown to extend to real neural data.",
    "authors": [
      "Buesing, Lars",
      "Macke, Jakob H.",
      "Sahani, Maneesh"
    ]
  },
  {
    "id": "d6baf65e0b240ce177cf70da146c8dc8",
    "title": "Assessing Blinding in Clinical Trials",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/d6baf65e0b240ce177cf70da146c8dc8-Paper.pdf",
    "abstract": "The interaction between the patient's expected outcome of an intervention and the inherent effects of that intervention can have extraordinary effects. Thus in clinical trials an effort is made to conceal the nature of the administered intervention from the participants in the trial i.e. to blind it. Yet, in practice perfect blinding is impossible to ensure or even verify. The current standard is follow up the trial with an auxiliary questionnaire, which allows trial participants to express their belief concerning the assigned intervention and which is used to compute a measure of the extent of blinding in the trial. If the estimated extent of blinding exceeds a threshold the trial is deemed sufficiently blinded; otherwise, the trial is deemed to have failed. In this paper we make several important contributions. Firstly, we identify a series of fundamental problems of the aforesaid practice and discuss them in context of the most commonly used blinding measures. Secondly, motivated by the highlighted problems, we formulate a novel method for handling imperfectly blinded trials. We too adopt a post-trial feedback questionnaire but interpret the collected data using an original approach, fundamentally different from those previously proposed. Unlike previous approaches, ours is void of any ad hoc free parameters, is robust to small changes in auxiliary data and is not predicated on any strong assumptions used to interpret participants' feedback.",
    "authors": [
      "Arandjelovic, Ognjen"
    ]
  },
  {
    "id": "d6ef5f7fa914c19931a55bb262ec879c",
    "title": "Scalable Inference of Overlapping Communities",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/d6ef5f7fa914c19931a55bb262ec879c-Paper.pdf",
    "abstract": "We develop a scalable algorithm for posterior inference of overlapping communities in large networks.  Our algorithm is based on stochastic variational inference in the mixed-membership stochastic blockmodel. It naturally interleaves subsampling the network with estimating its community structure.  We apply our algorithm on ten large, real-world networks with up to 60,000 nodes. It converges several orders of magnitude faster than the state-of-the-art algorithm for MMSB, finds hundreds of communities in large real-world networks, and detects the true communities in 280 benchmark networks with equal or better accuracy compared to other scalable algorithms.",
    "authors": [
      "Gopalan, Prem K.",
      "Gerrish, Sean",
      "Freedman, Michael",
      "Blei, David",
      "Mimno, David"
    ]
  },
  {
    "id": "d759175de8ea5b1d9a2660e45554894f",
    "title": "Learning Networks of Heterogeneous Influence",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/d759175de8ea5b1d9a2660e45554894f-Paper.pdf",
    "abstract": "Information, disease, and influence diffuse over networks of entities in both natural systems and human society. Analyzing these transmission networks plays an important role in understanding the diffusion processes and predicting events in the future. However, the underlying transmission networks are often hidden and incomplete, and we observe only the time stamps when cascades of events happen. In this paper, we attempt to address the challenging problem of uncovering the hidden network only from the cascades.  The structure discovery problem is complicated by the fact that the influence among different entities in a network are heterogeneous, which can not be described by a simple parametric model. Therefore, we propose a kernel-based method which can capture a diverse range of different types of influence without any prior assumption. In both synthetic and real cascade data, we show that our model can better recover the underlying diffusion network and drastically improve the estimation of the influence functions between networked entities.",
    "authors": [
      "Du, Nan",
      "Song, Le",
      "Yuan, Ming",
      "Smola, Alex"
    ]
  },
  {
    "id": "d81f9c1be2e08964bf9f24b15f0e4900",
    "title": "Learning to Align from Scratch",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/d81f9c1be2e08964bf9f24b15f0e4900-Paper.pdf",
    "abstract": "Unsupervised joint alignment of images has been demonstrated to   improve performance on recognition tasks such as face verification.   Such alignment reduces undesired variability due to factors such as   pose, while only requiring weak supervision in the form of poorly   aligned examples.  However, prior work on unsupervised alignment of   complex, real world images has required the careful selection of   feature representation based on hand-crafted image descriptors, in   order to achieve an appropriate, smooth optimization landscape.    In this paper, we instead propose a novel combination of   unsupervised joint alignment with unsupervised feature learning.   Specifically, we incorporate deep learning into the {\\em congealing}   alignment framework.  Through deep learning, we obtain features that   can represent the image at differing resolutions based on network   depth, and that are tuned to the statistics of the specific data   being aligned.  In addition, we modify the learning algorithm for   the restricted Boltzmann machine by incorporating a group sparsity   penalty, leading to a topographic organization on the learned   filters and improving subsequent alignment results.    We apply our method to the Labeled Faces in the Wild database   (LFW). Using the aligned images produced by our proposed   unsupervised algorithm, we achieve a significantly higher accuracy   in face verification than obtained using the original face images,   prior work in unsupervised alignment, and prior work in supervised   alignment.  We also match the accuracy for the best available, but   unpublished method.",
    "authors": [
      "Huang, Gary",
      "Mattar, Marwan",
      "Lee, Honglak",
      "Learned-miller, Erik"
    ]
  },
  {
    "id": "d840cc5d906c3e9c84374c8919d2074e",
    "title": "Bayesian Warped Gaussian Processes",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/d840cc5d906c3e9c84374c8919d2074e-Paper.pdf",
    "abstract": "Warped Gaussian processes (WGP) [1] model output observations in regression tasks as a parametric nonlinear transformation of a Gaussian process (GP). The use of this nonlinear transformation, which is included as part of the probabilistic model, was shown to enhance performance by providing a better prior model on several data sets. In order to learn its parameters, maximum likelihood was used. In this work we show that it is possible to use a non-parametric nonlinear transformation in WGP and variationally integrate it out. The resulting Bayesian WGP is then able to work in scenarios in which the maximum likelihood WGP failed: Low data regime, data with censored values, classification, etc. We demonstrate the superior performance of Bayesian warped GPs on several real data sets.",
    "authors": [
      "L\u00e1zaro-Gredilla, Miguel"
    ]
  },
  {
    "id": "d91d1b4d82419de8a614abce9cc0e6d4",
    "title": "Affine Independent Variational Inference",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/d91d1b4d82419de8a614abce9cc0e6d4-Paper.pdf",
    "abstract": "We present a method for approximate inference for a broad class of non-conjugate probabilistic models. In particular, for the family of generalized linear model target densities we describe a rich class of variational approximating densities which can be best fit to the target by minimizing the Kullback-Leibler divergence.  Our approach is based on using the Fourier representation which we show results in efficient and scalable inference.",
    "authors": [
      "Challis, Edward",
      "Barber, David"
    ]
  },
  {
    "id": "dba1cdfcf6359389d170caadb3223ad2",
    "title": "Submodular-Bregman and the Lov\u00e1sz-Bregman Divergences with Applications",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/dba1cdfcf6359389d170caadb3223ad2-Paper.pdf",
    "abstract": "We introduce a class of discrete divergences on sets (equivalently binary vectors) that we call the submodular-Bregman divergences. We consider two kinds, de\ufb01ned either from tight modular upper or tight modular lower bounds of a submodular function. We show that the properties of these divergences are analogous to the (standard continuous) Bregman divergence. We demonstrate how they generalize many useful divergences, including the weighted Hamming distance, squared weighted Hamming, weighted precision, recall, conditional mutual information, and a generalized KL-divergence on sets. We also show that the generalized Bregman divergence on the Lov\u00b4asz extension of a submodular function, which we call the Lov\u00b4asz-Bregman divergence, is a continuous extension of a submodular Bregman divergence. We point out a number of applications, and in particular show that a proximal algorithm de\ufb01ned through the submodular Bregman divergence pro- vides a framework for many mirror-descent style algorithms related to submodular function optimization. We also show that a generalization of the k-means algorithm using the Lov\u00b4asz Bregman divergence is natural in clustering scenarios where ordering is important. A unique property of this algorithm is that computing the mean ordering is extremely ef\ufb01cient unlike other order based distance measures.",
    "authors": [
      "Iyer, Rishabh",
      "Bilmes, Jeff A."
    ]
  },
  {
    "id": "dbe272bab69f8e13f14b405e038deb64",
    "title": "Optimal kernel choice for large-scale two-sample tests",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/dbe272bab69f8e13f14b405e038deb64-Paper.pdf",
    "abstract": "Abstract Given samples from distributions $p$ and $q$, a two-sample test determines whether to reject the null hypothesis that $p=q$, based on the value of a test statistic measuring the distance between the samples. One choice of test statistic is the maximum mean discrepancy (MMD), which is a distance between embeddings of the probability distributions in a reproducing kernel Hilbert space. The kernel used in obtaining these embeddings is thus critical in ensuring the test has high power, and correctly distinguishes unlike distributions with high probability. A means of parameter selection for the two-sample test based on the MMD is proposed. For a given test level (an upper bound on the probability of making a Type I error), the kernel is chosen so as to maximize the test power, and minimize the probability of making a Type II error. The test statistic, test threshold, and optimization over the kernel parameters are obtained with cost linear in the sample size. These properties make the kernel selection and test procedures suited to data streams, where the observations cannot all be stored in memory. In experiments, the new kernel selection approach yields a more powerful test than earlier kernel selection heuristics.",
    "authors": [
      "Gretton, Arthur",
      "Sejdinovic, Dino",
      "Strathmann, Heiko",
      "Balakrishnan, Sivaraman",
      "Pontil, Massimiliano",
      "Fukumizu, Kenji",
      "Sriperumbudur, Bharath K."
    ]
  },
  {
    "id": "dc4c44f624d600aa568390f1f1104aa0",
    "title": "Entangled Monte Carlo",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/dc4c44f624d600aa568390f1f1104aa0-Paper.pdf",
    "abstract": "We propose a novel method for scalable parallelization of SMC algorithms, Entangled Monte Carlo simulation (EMC).  EMC avoids the transmission of particles between  nodes, and instead reconstructs them from the particle genealogy. In particular, we show that we can reduce the communication to the particle weights for each machine while efficiently maintaining implicit global coherence of the parallel simulation. We explain methods to efficiently maintain a genealogy of particles from which any particle can be reconstructed. We demonstrate using examples from Bayesian phylogenetic that the computational gain from parallelization using EMC significantly outweighs the cost of particle reconstruction. The timing experiments show that reconstruction of particles is indeed much more efficient as compared to transmission of particles.",
    "authors": [
      "Jun, Seong-hwan",
      "Wang, Liangliang",
      "Bouchard-c\u00f4t\u00e9, Alexandre"
    ]
  },
  {
    "id": "dc58e3a306451c9d670adcd37004f48f",
    "title": "Minimax Multi-Task Learning and a Generalized Loss-Compositional Paradigm for MTL",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/dc58e3a306451c9d670adcd37004f48f-Paper.pdf",
    "abstract": "Since its inception, the modus operandi of multi-task learning (MTL) has been to minimize the task-wise mean of the empirical risks.   We introduce a generalized loss-compositional paradigm for MTL that includes a spectrum of formulations as a subfamily. One endpoint of this spectrum is minimax MTL: a new MTL formulation that minimizes the maximum of the tasks' empirical risks. Via a certain relaxation of minimax MTL, we obtain a continuum of MTL formulations spanning minimax MTL and classical MTL. The full paradigm itself is loss-compositional, operating on the vector of empirical risks. It incorporates minimax MTL, its relaxations, and many new MTL formulations as special cases. We show theoretically that minimax MTL tends to avoid worst case outcomes on newly drawn test tasks in the learning to learn (LTL) test setting. The results of several MTL formulations on synthetic and real problems in the MTL and LTL test settings are encouraging.",
    "authors": [
      "Mehta, Nishant",
      "Lee, Dongryeol",
      "Gray, Alexander"
    ]
  },
  {
    "id": "dd45045f8c68db9f54e70c67048d32e8",
    "title": "Semi-Crowdsourced Clustering: Generalizing Crowd Labeling by Robust Distance Metric Learning",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/dd45045f8c68db9f54e70c67048d32e8-Paper.pdf",
    "abstract": "One of the main challenges in data clustering is to define an appropriate similarity measure between two objects. Crowdclustering addresses this challenge by defining the pairwise similarity based on the manual annotations obtained through crowdsourcing. Despite its encouraging results, a key limitation of crowdclustering is that it can only cluster objects when their manual annotations are available. To address this limitation, we propose a new approach for clustering, called \\textit{semi-crowdsourced clustering} that effectively combines the low-level features of objects with the manual annotations of a subset of the objects obtained via crowdsourcing. The key idea is to learn an appropriate similarity measure, based on the low-level features of objects, from the manual annotations of only a small portion of the data to be clustered. One difficulty in learning the pairwise similarity measure is that there is a significant amount of noise and inter-worker variations in the manual annotations obtained via crowdsourcing. We address this difficulty by developing a metric learning algorithm based on the matrix completion method. Our empirical study with two real-world image data sets shows that the proposed algorithm outperforms state-of-the-art distance metric learning algorithms in both clustering accuracy and computational efficiency.",
    "authors": [
      "Yi, Jinfeng",
      "Jin, Rong",
      "Jain, Shaili",
      "Yang, Tianbao",
      "Jain, Anil"
    ]
  },
  {
    "id": "dd77279f7d325eec933f05b1672f6a1f",
    "title": "Online L1-Dictionary Learning with Application to Novel Document Detection",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/dd77279f7d325eec933f05b1672f6a1f-Paper.pdf",
    "abstract": "Given their pervasive use, social media, such as Twitter, have become a leading source of breaking news. A key task in the automated identification of such news is the detection of novel documents from a voluminous stream of text documents in a scalable manner. Motivated by this challenge, we introduce the problem of online L1-dictionary learning where unlike traditional dictionary learning, which uses squared loss, the L1-penalty is used for measuring the reconstruction error. We present an efficient online algorithm for this problem based on alternating directions method of multipliers, and establish a sublinear regret bound for this algorithm. Empirical results on news-stream and Twitter data, shows that this online L1-dictionary learning algorithm for novel document detection gives more than an order of magnitude speedup over the previously known batch algorithm, without any significant loss in quality of results. Our algorithm for online L1-dictionary learning could be of independent interest.",
    "authors": [
      "Kasiviswanathan, Shiva",
      "Wang, Huahua",
      "Banerjee, Arindam",
      "Melville, Prem"
    ]
  },
  {
    "id": "dd8eb9f23fbd362da0e3f4e70b878c16",
    "title": "Learning curves for multi-task Gaussian process regression",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/dd8eb9f23fbd362da0e3f4e70b878c16-Paper.pdf",
    "abstract": "We study the average case performance of multi-task Gaussian process (GP)   regression as captured in the learning curve, i.e.\\ the average Bayes error   for a chosen task versus the total number of examples $n$ for all   tasks. For GP covariances that are the product of an   input-dependent covariance function and a free-form inter-task   covariance matrix, we   show that accurate approximations for the learning curve can be   obtained for an arbitrary number of tasks $T$.  We use   these to study the asymptotic learning behaviour for large   $n$. Surprisingly, multi-task learning can be asymptotically essentially   useless: examples from other tasks only help when the   degree of inter-task correlation, $\\rho$, is near its maximal value   $\\rho=1$. This effect is most extreme for learning of smooth target   functions as described by e.g.\\ squared exponential kernels. We also   demonstrate that when learning {\\em many} tasks, the learning curves   separate into an initial phase, where the Bayes error on each task   is reduced down to a plateau value by ``collective learning''    even though most tasks have not seen examples,   and a final decay that occurs only once the number of examples is   proportional to the number of tasks.",
    "authors": [
      "Sollich, Peter",
      "Ashton, Simon"
    ]
  },
  {
    "id": "de3f712d1a02c5fb481a7a99b0da7fa3",
    "title": "Transelliptical Component Analysis",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/de3f712d1a02c5fb481a7a99b0da7fa3-Paper.pdf",
    "abstract": "We propose a high dimensional semiparametric scale-invariant principle compo- nent analysis, named TCA, by utilize the natural connection between the ellipti- cal distribution family and the principal component analysis. Elliptical distribu- tion family includes many well-known multivariate distributions like multivari- ate Gaussian, t and logistic and it is extended to the meta-elliptical by Fang et.al (2002) using the copula techniques. In this paper we extend the meta-elliptical distribution family to a even larger family, called transelliptical. We prove that",
    "authors": [
      "Han, Fang",
      "Liu, Han"
    ]
  },
  {
    "id": "df0aab058ce179e4f7ab135ed4e641a9",
    "title": "Learning the Dependency Structure of Latent Factors",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/df0aab058ce179e4f7ab135ed4e641a9-Paper.pdf",
    "abstract": "In this paper, we study latent factor models with the dependency structure in the latent space.  We propose a general learning framework which induces sparsity on the undirected graphical model imposed on the vector of latent factors. A novel latent factor model SLFA is then proposed as a matrix factorization problem with a special regularization term that encourages collaborative reconstruction. The main benefit (novelty) of the model is that we can simultaneously learn the lower-dimensional representation for data and model the pairwise relationships between latent factors explicitly. An on-line learning algorithm is devised to make the model feasible for large-scale learning problems. Experimental results on two synthetic data and two real-world data sets demonstrate that pairwise relationships and latent factors learned by our model provide a more structured way of exploring high-dimensional data,  and the learned representations achieve the state-of-the-art classification performance.",
    "authors": [
      "He, Yunlong",
      "Qi, Yanjun",
      "Kavukcuoglu, Koray",
      "Park, Haesun"
    ]
  },
  {
    "id": "df6c9756b2334cc5008c115486124bfe",
    "title": "Random function priors for exchangeable arrays with applications to graphs and relational data",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/df6c9756b2334cc5008c115486124bfe-Paper.pdf",
    "abstract": "A fundamental problem in the analysis of structured relational data like graphs, networks, databases, and matrices is to extract a summary of the common struc- ture underlying relations between individual entities. Relational data are typically encoded in the form of arrays; invariance to the ordering of rows and columns corresponds to exchangeable arrays. Results in probability theory due to Aldous, Hoover and Kallenberg show that exchangeable arrays can be represented in terms of a random measurable function which constitutes the natural model parameter in a Bayesian model. We obtain a \ufb02exible yet simple Bayesian nonparametric model by placing a Gaussian process prior on the parameter function. Ef\ufb01cient inference utilises elliptical slice sampling combined with a random sparse approximation to the Gaussian process. We demonstrate applications of the model to network data and clarify its relation to models in the literature, several of which emerge as special cases.",
    "authors": [
      "Lloyd, James",
      "Orbanz, Peter",
      "Ghahramani, Zoubin",
      "Roy, Daniel M."
    ]
  },
  {
    "id": "dfd7468ac613286cdbb40872c8ef3b06",
    "title": "Bayesian Pedigree Analysis using Measure Factorization",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/dfd7468ac613286cdbb40872c8ef3b06-Paper.pdf",
    "abstract": "Pedigrees, or family trees, are directed graphs used to identify sites of the genome that are correlated with the presence or absence of a disease.  With the advent of genotyping and sequencing technologies, there has been an explosion in the amount of data available, both in the number of individuals and in the number of sites.  Some pedigrees number in the thousands of individuals.  Meanwhile, analysis methods have remained limited to pedigrees of <100 individuals which limits analyses to many small independent pedigrees.  Disease models, such those used for the linkage analysis log-odds (LOD) estimator, have similarly been limited.  This is because linkage anlysis was originally designed with a different task in mind, that of ordering the sites in the genome, before there were technologies that could reveal the order.  LODs are difficult to interpret and nontrivial to extend to consider interactions among sites.  These developments and difficulties call for the creation of modern methods of pedigree analysis.  Drawing from recent advances in graphical model inference and transducer theory, we introduce a simple yet powerful formalism for expressing genetic disease models.   We show that these disease models can be turned into accurate and efficient estimators.  The technique we use for constructing the variational approximation has potential applications to inference in other large-scale graphical models.  This method allows inference on larger pedigrees than previously analyzed in the literature, which improves disease site prediction.",
    "authors": [
      "Kirkpatrick, Bonnie",
      "Bouchard-c\u00f4t\u00e9, Alexandre"
    ]
  },
  {
    "id": "e00406144c1e7e35240afed70f34166a",
    "title": "Density Propagation and Improved Bounds on the Partition Function",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/e00406144c1e7e35240afed70f34166a-Paper.pdf",
    "abstract": "Given a probabilistic graphical model, its density of states is a function that, for any likelihood value, gives the number of configurations with that probability. We introduce a novel message-passing algorithm called Density Propagation (DP) for estimating this function. We show that DP is exact for tree-structured graphical models and is, in general, a strict generalization of both sum-product and max-product algorithms. Further, we use density of states and tree decomposition to introduce a new family of upper and lower bounds on the partition function. For any tree decompostion, the new upper bound based on finer-grained density of state information is provably at least as tight as previously known bounds based on convexity of the log-partition function, and strictly stronger if a general condition holds. We conclude with empirical evidence of improvement over convex relaxations and mean-field based bounds.",
    "authors": [
      "Ermon, Stefano",
      "Sabharwal, Ashish",
      "Selman, Bart",
      "Gomes, Carla P."
    ]
  },
  {
    "id": "e034fb6b66aacc1d48f445ddfb08da98",
    "title": "A quasi-Newton proximal splitting method",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/e034fb6b66aacc1d48f445ddfb08da98-Paper.pdf",
    "abstract": "We describe efficient implementations of the proximity calculation for a useful class of functions; the implementations exploit the piece-wise linear nature of the dual problem. The second part of the paper applies the previous result to acceleration of convex minimization problems, and leads to an elegant quasi-Newton method. The optimization method compares favorably against state-of-the-art alternatives. The algorithm has extensive applications including signal processing, sparse regression and recovery, and machine learning and classification.",
    "authors": [
      "Becker, Stephen",
      "Fadili, Jalal"
    ]
  },
  {
    "id": "e0cf1f47118daebc5b16269099ad7347",
    "title": "Waveform Driven Plasticity in BiFeO3 Memristive Devices: Model and Implementation",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/e0cf1f47118daebc5b16269099ad7347-Paper.pdf",
    "abstract": "Memristive devices have recently been proposed as efficient implementations of plastic synapses in neuromorphic systems. The plasticity in these memristive devices, i.e. their resistance change, is defined by the applied waveforms. This behavior resembles biological synapses, whose plasticity is also triggered by mechanisms that are determined by local waveforms. However, learning in memristive devices has so far been approached mostly on a pragmatic technological level. The focus seems to be on finding any waveform that achieves spike-timing-dependent plasticity (STDP), without regard to the biological veracity of said waveforms or to further important forms of plasticity. Bridging this gap, we make use of a plasticity model driven by neuron waveforms that explains a large number of experimental observations and adapt it to the characteristics of the recently introduced BiFeO$_3$ memristive material. Based on this approach, we show STDP for the first time for this material, with learning window replication superior to previous memristor-based STDP implementations. We also demonstrate in measurements that it is possible to overlay short and long term plasticity at a memristive device in the form of the well-known triplet plasticity. To the best of our knowledge, this is the first implementations of triplet plasticity on any physical memristive device.",
    "authors": [
      "Mayr, Christian",
      "St\u00e4rke, Paul",
      "Partzsch, Johannes",
      "Cederstroem, Love",
      "Sch\u00fcffny, Rene",
      "Shuai, Yao",
      "Du, Nan",
      "Schmidt, Heidemarie"
    ]
  },
  {
    "id": "e1d5be1c7f2f456670de3d53c7b54f4a",
    "title": "Multilabel Classification using Bayesian Compressed Sensing",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/e1d5be1c7f2f456670de3d53c7b54f4a-Paper.pdf",
    "abstract": "In this paper, we present a Bayesian framework for multilabel classification using compressed sensing. The key idea in compressed sensing for multilabel classification is to first project the label vector to a lower dimensional space using a random transformation and then learn regression functions over these projections. Our approach considers both of these components in a single probabilistic model, thereby jointly optimizing over compression as well as learning tasks. We then derive an efficient variational inference scheme that provides joint posterior distribution over all the unobserved labels. The two key benefits of the model are that a) it can naturally handle datasets that have  missing labels and b) it can also measure uncertainty in prediction. The uncertainty estimate provided by the model naturally allows for active learning paradigms where an oracle provides information about labels that promise to be maximally informative for the prediction task. Our experiments show significant boost over prior methods in terms of prediction performance over benchmark datasets, both in the fully labeled and the missing labels case.  Finally, we also highlight various useful active learning scenarios that are enabled by the probabilistic model.",
    "authors": [
      "Kapoor, Ashish",
      "Viswanathan, Raajay",
      "Jain, Prateek"
    ]
  },
  {
    "id": "e2c4a40d50b47094f571e40efead3900",
    "title": "Online Sum-Product Computation Over Trees",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/e2c4a40d50b47094f571e40efead3900-Paper.pdf",
    "abstract": "Abstract Unavailable",
    "authors": [
      "Herbster, Mark",
      "Pasteris, Stephen",
      "Vitale, Fabio"
    ]
  },
  {
    "id": "e2f374c3418c50bc30d67d5f7454a5b4",
    "title": "Risk Aversion in Markov Decision Processes via Near Optimal Chernoff Bounds",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/e2f374c3418c50bc30d67d5f7454a5b4-Paper.pdf",
    "abstract": "The expected return is a widely used objective in decision making under uncer- tainty. Many algorithms, such as value iteration, have been proposed to optimize it. In risk-aware settings, however, the expected return is often not an appropriate objective to optimize. We propose a new optimization objective for risk-aware planning and show that it has desirable theoretical properties. We also draw con- nections to previously proposed objectives for risk-aware planing: minmax, ex- ponential utility, percentile and mean minus variance. Our method applies to an extended class of Markov decision processes: we allow costs to be stochastic as long as they are bounded. Additionally, we present an ef\ufb01cient algorithm for op- timizing the proposed objective. Synthetic and real-world experiments illustrate the effectiveness of our method, at scale.",
    "authors": [
      "Moldovan, Teodor",
      "Abbeel, Pieter"
    ]
  },
  {
    "id": "e46de7e1bcaaced9a54f1e9d0d2f800d",
    "title": "Calibrated Elastic Regularization in Matrix Completion",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/e46de7e1bcaaced9a54f1e9d0d2f800d-Paper.pdf",
    "abstract": "This paper concerns the problem of matrix completion, which is to estimate a matrix from observations in a small subset of indices. We propose a calibrated spectrum elastic net method with a sum of the nuclear and Frobenius penalties and develop an iterative algorithm to solve the convex minimization problem. The iterative algorithm alternates between imputing the missing entries in the incomplete matrix by the current guess and estimating the matrix by a scaled soft-thresholding singular value decomposition of the imputed matrix until the resulting matrix converges. A calibration step follows to correct the bias caused by the Frobenius penalty. Under proper coherence conditions and for suitable penalties levels, we prove that the proposed estimator achieves an error bound of nearly optimal order and in proportion to the noise level. This provides a unified analysis of the noisy and noiseless matrix completion problems. Simulation results are presented to compare our proposal with previous ones.",
    "authors": [
      "Sun, Tingni",
      "Zhang, Cun-hui"
    ]
  },
  {
    "id": "e53a0a2978c28872a4505bdb51db06dc",
    "title": "Expectation Propagation in Gaussian Process Dynamical Systems",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/e53a0a2978c28872a4505bdb51db06dc-Paper.pdf",
    "abstract": "Rich and complex time-series data, such as those generated from engineering sys- tems, financial markets, videos or neural recordings are now a common feature of modern data analysis. Explaining the phenomena underlying these diverse data sets requires flexible and accurate models. In this paper, we promote Gaussian process dynamical systems as a rich model class appropriate for such analysis. In particular, we present a message passing algorithm for approximate inference in GPDSs based on expectation propagation. By phrasing inference as a general mes- sage passing problem, we iterate forward-backward smoothing. We obtain more accurate posterior distributions over latent structures, resulting in improved pre- dictive performance compared to state-of-the-art GPDS smoothers, which are spe- cial cases of our general iterative message passing algorithm. Hence, we provide a unifying approach within which to contextualize message passing in GPDSs.",
    "authors": [
      "Deisenroth, Marc",
      "Mohamed, Shakir"
    ]
  },
  {
    "id": "e555ebe0ce426f7f9b2bef0706315e0c",
    "title": "Finite Sample Convergence Rates of Zero-Order Stochastic Optimization Methods",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/e555ebe0ce426f7f9b2bef0706315e0c-Paper.pdf",
    "abstract": "We consider derivative-free algorithms for stochastic optimization problems that use only noisy function values rather than gradients, analyzing their finite-sample convergence rates. We show that if pairs of function values are available, algorithms that use gradient estimates based on random perturbations suffer a factor of at most $\\sqrt{\\dim}$ in convergence rate over traditional stochastic gradient methods, where $\\dim$ is the dimension of the problem. We complement our algorithmic development with information-theoretic lower bounds on the minimax convergence rate of such problems, which show that our bounds are sharp with respect to all problem-dependent quantities: they cannot be improved by more than constant factors.",
    "authors": [
      "Wibisono, Andre",
      "Wainwright, Martin J.",
      "Jordan, Michael",
      "Duchi, John C."
    ]
  },
  {
    "id": "e6d8545daa42d5ced125a4bf747b3688",
    "title": "Query Complexity of Derivative-Free Optimization",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/e6d8545daa42d5ced125a4bf747b3688-Paper.pdf",
    "abstract": "Derivative Free Optimization (DFO) is attractive when the objective function's derivatives are not available and evaluations are costly.   Moreover, if the function evaluations are noisy, then approximating gradients by finite differences is difficult.  This paper gives quantitative lower bounds on the performance of DFO with noisy function evaluations, exposing a fundamental and unavoidable gap between optimization performance based on noisy evaluations versus noisy gradients. This challenges the conventional wisdom that the method of finite differences is comparable to a stochastic gradient.  However, there are situations in which DFO is unavoidable, and for such situations we propose a new DFO algorithm that is proved to be near optimal for the class of strongly convex objective functions.  A distinctive feature of the algorithm is that it only uses Boolean-valued function comparisons, rather than evaluations.  This makes the algorithm useful in an even wider range of applications, including optimization based on paired comparisons from human subjects, for example.  Remarkably, we show that regardless of whether DFO is based on noisy function evaluations or Boolean-valued function comparisons, the convergence rate is the same.",
    "authors": [
      "Jamieson, Kevin G.",
      "Nowak, Robert",
      "Recht, Ben"
    ]
  },
  {
    "id": "e7f8a7fb0b77bcb3b283af5be021448f",
    "title": "Communication-Efficient Algorithms for Statistical Optimization",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/e7f8a7fb0b77bcb3b283af5be021448f-Paper.pdf",
    "abstract": "We study two communication-efficient algorithms for distributed statistical optimization on large-scale data. The first algorithm is an averaging method that distributes the $N$ data samples evenly to $m$ machines, performs separate minimization on each subset, and then averages the estimates.  We provide a sharp analysis of this average mixture algorithm, showing that under a reasonable set of conditions, the combined parameter achieves mean-squared error that decays as $\\order(N^{-1}+(N/m)^{-2})$. Whenever $m \\le \\sqrt{N}$, this guarantee matches the best possible rate achievable by a centralized algorithm having access to all $N$ samples.  The second algorithm is a novel method, based on an appropriate form of the bootstrap.  Requiring only a single round of communication, it has mean-squared error that decays as $\\order(N^{-1}+(N/m)^{-3})$, and so is more robust to the amount of parallelization. We complement our theoretical results with experiments on large-scale problems from the Microsoft Learning to Rank dataset.",
    "authors": [
      "Zhang, Yuchen",
      "Wainwright, Martin J.",
      "Duchi, John C."
    ]
  },
  {
    "id": "e94550c93cd70fe748e6982b3439ad3b",
    "title": "Selecting Diverse Features via Spectral Regularization",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/e94550c93cd70fe748e6982b3439ad3b-Paper.pdf",
    "abstract": "We study the problem of diverse feature selection in linear regression: selecting a small subset of diverse features that can predict a given objective. Diversity is useful for several reasons such as interpretability, robustness to noise, etc.  We propose several spectral regularizers that capture a notion of diversity of features and show that these are all submodular set functions. These regularizers, when added to the objective function for linear regression, result in approximately submodular functions, which can then be maximized approximately by efficient greedy and local search algorithms, with provable guarantees.  We compare our algorithms to traditional greedy and $\\ell_1$-regularization schemes and show that we obtain a more diverse set of features that result in the regression problem being stable under perturbations.",
    "authors": [
      "Das, Abhimanyu",
      "Dasgupta, Anirban",
      "Kumar, Ravi"
    ]
  },
  {
    "id": "e94f63f579e05cb49c05c2d050ead9c0",
    "title": "Fast Bayesian Inference for Non-Conjugate Gaussian Process Regression",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/e94f63f579e05cb49c05c2d050ead9c0-Paper.pdf",
    "abstract": "We present a new variational inference algorithm for Gaussian processes with non-conjugate likelihood functions. This includes binary and multi-class classification, as well as ordinal regression. Our method constructs a convex lower bound, which can be optimized by using an efficient fixed point update method. We then show empirically that our new approach is much faster than existing methods without any degradation in performance.",
    "authors": [
      "Khan, Emtiyaz",
      "Mohamed, Shakir",
      "Murphy, Kevin P."
    ]
  },
  {
    "id": "e97ee2054defb209c35fe4dc94599061",
    "title": "Natural Images, Gaussian Mixtures and Dead Leaves",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/e97ee2054defb209c35fe4dc94599061-Paper.pdf",
    "abstract": "Simple Gaussian Mixture Models (GMMs) learned from pixels of natural image patches have been recently shown to be surprisingly strong performers in modeling the statistics of natural images. Here we provide an in depth analysis of this simple yet rich model. We show that such a GMM model is able to compete with even the most successful models of natural images in log likelihood scores, denoising performance and sample quality. We provide an analysis of what such a model learns from natural images as a function of number of mixture components --- including covariance structure, contrast variation and intricate structures such as textures, boundaries and more. Finally, we show that the salient properties of the GMM learned from natural images can be derived from a simplified Dead Leaves model which explicitly models occlusion, explaining its surprising success relative to other models.",
    "authors": [
      "Zoran, Daniel",
      "Weiss, Yair"
    ]
  },
  {
    "id": "e9dae45ec08b498f7e1af247757c9b35",
    "title": "Memorability of Image Regions",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/e9dae45ec08b498f7e1af247757c9b35-Paper.pdf",
    "abstract": "While long term human visual memory can store a remarkable amount of visual information, it tends to degrade over time. Recent works have shown that image memorability is an intrinsic property of an image that can be reliably estimated using state-of-the-art image features and machine learning algorithms. However, the class of features and image information that is forgotten has not been explored yet. In this work, we propose a probabilistic framework that models how and which local regions from an image may be forgotten using a data-driven approach that combines local and global images features. The model automatically discov- ers memorability maps of individual images without any human annotation. We incorporate multiple image region attributes in our algorithm, leading to improved memorability prediction of images as compared to previous works.",
    "authors": [
      "Khosla, Aditya",
      "Xiao, Jianxiong",
      "Torralba, Antonio",
      "Oliva, Aude"
    ]
  },
  {
    "id": "ea8fcd92d59581717e06eb187f10666d",
    "title": "Projection Retrieval for Classification",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/ea8fcd92d59581717e06eb187f10666d-Paper.pdf",
    "abstract": "In many applications classification systems often require in the loop human intervention. In such cases the decision process must be transparent and comprehensible simultaneously requiring minimal assumptions on the underlying data distribution. To tackle this problem, we formulate it as an axis-alligned subspacefinding task under the assumption that query specific information dictates the complementary use of the subspaces. We develop a regression-based approach called RECIP that efficiently solves this problem by finding projections that minimize a nonparametric conditional entropy estimator. Experiments show that the method is accurate in identifying the informative projections of the dataset, picking the correct ones to classify query points and facilitates visual evaluation by users.",
    "authors": [
      "Fiterau, Madalina",
      "Dubrawski, Artur"
    ]
  },
  {
    "id": "eaa32c96f620053cf442ad32258076b9",
    "title": "One Permutation Hashing",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/eaa32c96f620053cf442ad32258076b9-Paper.pdf",
    "abstract": "While minwise hashing is promising for large-scale learning in massive binary data, the preprocessing cost is prohibitive as it requires applying (e.g.,) $k=500$ permutations on the data. The testing time is also  expensive if a new data point (e.g., a new document or a new image) has not been processed.  In this paper, we develop a simple \\textbf{one permutation hashing} scheme to address this important issue. While it is true that the preprocessing step can be parallelized, it comes at the cost of additional hardware and implementation. Also, reducing $k$ permutations to just one  would be much more \\textbf{energy-efficient}, which might be an important perspective as minwise hashing is commonly deployed in the search industry. While the theoretical probability analysis is  interesting, our experiments on similarity estimation and   SVM \\& logistic regression also confirm the theoretical results.",
    "authors": [
      "Li, Ping",
      "Owen, Art",
      "Zhang, Cun-hui"
    ]
  },
  {
    "id": "eb160de1de89d9058fcb0b968dbbbd68",
    "title": "The representer theorem for Hilbert spaces: a necessary and sufficient condition",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/eb160de1de89d9058fcb0b968dbbbd68-Paper.pdf",
    "abstract": "The representer theorem is a property that lies at the foundation of regularization theory and kernel methods. A class of regularization functionals is said to admit a linear representer theorem if every member of the class admits minimizers that lie in the finite dimensional subspace spanned by the representers of the data. A recent characterization states that certain classes of regularization functionals with differentiable regularization term admit a linear representer theorem for any choice of the data if and only if the regularization term is a radial nondecreasing function. In this paper, we extend such result by weakening the assumptions on the regularization term. In particular, the main result of this paper implies that, for a sufficiently large family of regularization functionals, radial nondecreasing functions are the only lower semicontinuous regularization terms that guarantee existence of a representer theorem for any choice of the data.",
    "authors": [
      "Dinuzzo, Francesco",
      "Sch\u00f6lkopf, Bernhard"
    ]
  },
  {
    "id": "ec5aa0b7846082a2415f0902f0da88f2",
    "title": "A Geometric take on Metric Learning",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/ec5aa0b7846082a2415f0902f0da88f2-Paper.pdf",
    "abstract": "Multi-metric learning techniques learn local metric tensors in different parts of a feature space. With such an approach, even simple classifiers can be competitive with the state-of-the-art because the distance measure locally adapts to the structure of the data. The learned distance measure is, however, non-metric, which has prevented multi-metric learning from generalizing to tasks such as dimensionality reduction and regression in a principled way. We prove that, with appropriate changes, multi-metric learning corresponds to learning the structure of a Riemannian manifold. We then show that this structure gives us a principled way to perform dimensionality reduction and regression according to the learned metrics. Algorithmically, we provide the first practical algorithm for computing geodesics according to the learned metrics, as well as algorithms for computing exponential and logarithmic maps on the Riemannian manifold. Together, these tools let many Euclidean algorithms take advantage of multi-metric learning. We illustrate the approach on regression and dimensionality reduction tasks that involve predicting measurements of the human body from shape data.",
    "authors": [
      "Hauberg, S\u00f8ren",
      "Freifeld, Oren",
      "Black, Michael"
    ]
  },
  {
    "id": "edfbe1afcf9246bb0d40eb4d8027d90f",
    "title": "Iterative Thresholding Algorithm for Sparse Inverse Covariance Estimation",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/edfbe1afcf9246bb0d40eb4d8027d90f-Paper.pdf",
    "abstract": "Sparse graphical modelling/inverse covariance selection is an important problem in machine learning and has seen significant advances in recent years. A major focus has been on methods which perform model selection in high dimensions. To this end, numerous convex $\\ell_1$ regularization approaches have been proposed in the literature. It is not however clear which of these methods are optimal in any well-defined sense. A major gap in this regard pertains to the rate of convergence of proposed optimization methods. To address this, an iterative thresholding algorithm for numerically solving the $\\ell_1$-penalized maximum likelihood problem for sparse inverse covariance estimation is presented. The proximal gradient method considered in this paper is shown to converge at a linear rate, a result which is the first of its kind for numerically solving the sparse inverse covariance estimation problem. The convergence rate is provided in closed form, and is related to the condition number of the optimal point. Numerical results demonstrating the proven rate of convergence are presented.",
    "authors": [
      "Rolfs, Benjamin",
      "Rajaratnam, Bala",
      "Guillot, Dominique",
      "Wong, Ian",
      "Maleki, Arian"
    ]
  },
  {
    "id": "eeb69a3cb92300456b6a5f4162093851",
    "title": "Online allocation and homogeneous partitioning for piecewise constant mean-approximation",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/eeb69a3cb92300456b6a5f4162093851-Paper.pdf",
    "abstract": "In the setting of active learning for the multi-armed bandit, where the goal of a learner is to estimate with equal precision the mean of a finite number of arms, recent results show that it is possible to derive strategies based on finite-time confidence bounds that are competitive with the best possible strategy. We here consider an extension of this problem to the case when the arms are the cells of a finite partition P of a continuous sampling space X \\subset \\Real^d. Our goal is now to build a piecewise constant approximation of a noisy function (where each piece is one region of P and P is fixed beforehand) in order to maintain the local quadratic error of approximation on each cell equally low. Although this extension is not trivial, we show that a simple algorithm based on upper confidence bounds can be proved to be adaptive to the function itself in a near-optimal way, when |P| is chosen to be of minimax-optimal order  on the class of \\alpha-H\u00f6lder functions.",
    "authors": [
      "Carpentier, Alexandra",
      "Maillard, Odalric-ambrym"
    ]
  },
  {
    "id": "ef0eff6088e2ed94f6caf720239f40d5",
    "title": "Efficient coding provides a direct link between prior and likelihood in perceptual Bayesian inference",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/ef0eff6088e2ed94f6caf720239f40d5-Paper.pdf",
    "abstract": "A common challenge for Bayesian models of perception is the fact that the two fundamental Bayesian components, the prior distribution and the likelihood func- tion, are formally unconstrained. Here we argue that a neural system that emulates Bayesian inference is naturally constrained by the way it represents sensory infor- mation in populations of neurons. More speci\ufb01cally, we show that an ef\ufb01cient coding principle creates a direct link between prior and likelihood based on the underlying stimulus distribution. The resulting Bayesian estimates can show bi- ases away from the peaks of the prior distribution, a behavior seemingly at odds with the traditional view of Bayesian estimation, yet one that has been reported in human perception. We demonstrate that our framework correctly accounts for the repulsive biases previously reported for the perception of visual orientation, and show that the predicted tuning characteristics of the model neurons match the reported orientation tuning properties of neurons in primary visual cortex. Our results suggest that ef\ufb01cient coding is a promising hypothesis in constrain- ing Bayesian models of perceptual inference.",
    "authors": [
      "Wei, Xue-xin",
      "Stocker, Alan A."
    ]
  },
  {
    "id": "ef50c335cca9f340bde656363ebd02fd",
    "title": "Pointwise Tracking the Optimal Regression Function",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/ef50c335cca9f340bde656363ebd02fd-Paper.pdf",
    "abstract": "This paper examines the possibility of a `reject option' in the context of least squares regression. It is shown that using rejection it is theoretically possible to learn `selective' regressors that can $\\epsilon$-pointwise track the best regressor in hindsight from the same hypothesis class, while rejecting only a bounded portion of the domain. Moreover, the rejected volume vanishes with the training set size, under certain conditions. We then develop efficient and exact implementation of these selective regressors for the case of linear regression. Empirical evaluation over a suite of real-world datasets corroborates the theoretical analysis and indicates that our selective regressors can provide substantial advantage by reducing estimation error.",
    "authors": [
      "Wiener, Yair",
      "El-Yaniv, Ran"
    ]
  },
  {
    "id": "f197002b9a0853eca5e046d9ca4663d5",
    "title": "Globally Convergent Dual MAP LP Relaxation Solvers using Fenchel-Young Margins",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/f197002b9a0853eca5e046d9ca4663d5-Paper.pdf",
    "abstract": "While finding the exact solution for the MAP inference problem is intractable for many real-world tasks, MAP LP relaxations have been shown to be very effective in practice.  However, the most efficient methods that perform block coordinate descent can get stuck in sub-optimal points as  they are not globally convergent. In this work we propose to augment these algorithms with an $\\epsilon$-descent approach and present a method to efficiently optimize for a descent direction in the subdifferential using a margin-based extension of the Fenchel-Young duality theorem. Furthermore, the presented approach provides a methodology to construct a primal optimal solution from its dual optimal counterpart. We demonstrate the efficiency of the presented approach on spin glass models and protein interactions problems and show that our approach outperforms state-of-the-art solvers.",
    "authors": [
      "Schwing, Alex",
      "Hazan, Tamir",
      "Pollefeys, Marc",
      "Urtasun, Raquel"
    ]
  },
  {
    "id": "f2201f5191c4e92cc5af043eebfd0946",
    "title": "Nonparametric Reduced Rank Regression",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/f2201f5191c4e92cc5af043eebfd0946-Paper.pdf",
    "abstract": "We propose an approach to multivariate nonparametric regression that generalizes reduced rank regression for linear models.  An additive model is estimated for each dimension of a $q$-dimensional response, with a shared $p$-dimensional predictor variable.  To control the complexity of the model, we employ a functional form of the Ky-Fan or nuclear norm, resulting in a set of function estimates that have low rank.  Backfitting algorithms are derived and justified using a nonparametric form of the nuclear norm subdifferential.  Oracle inequalities on excess risk are derived that exhibit the scaling behavior of the procedure in the high dimensional setting.  The methods are illustrated on gene expression data.",
    "authors": [
      "Foygel, Rina",
      "Horrell, Michael",
      "Drton, Mathias",
      "Lafferty, John"
    ]
  },
  {
    "id": "f2fc990265c712c49d51a18a32b39f0c",
    "title": "Density-Difference Estimation",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/f2fc990265c712c49d51a18a32b39f0c-Paper.pdf",
    "abstract": "We address the problem of estimating the difference between two probability densities. A naive approach  is a two-step procedure of first estimating two densities separately and then computing their difference. However, such a two-step procedure does not necessarily work well because the first step is performed without regard to the second step and thus a small estimation error incurred in the first stage can cause a big error in the second stage. In this paper, we propose a single-shot procedure  for directly estimating the density difference without separately estimating two densities. We derive a non-parametric finite-sample error bound for the proposed single-shot density-difference estimator and show that it achieves the optimal convergence rate. We then show how the proposed density-difference estimator can be utilized in L2-distance approximation. Finally, we experimentally demonstrate the usefulness of the proposed method in robust distribution comparison such as class-prior estimation and change-point detection.",
    "authors": [
      "Sugiyama, Masashi",
      "Kanamori, Takafumi",
      "Suzuki, Taiji",
      "Plessis, Marthinus",
      "Liu, Song",
      "Takeuchi, Ichiro"
    ]
  },
  {
    "id": "f33ba15effa5c10e873bf3842afb46a6",
    "title": "Learning the Architecture of Sum-Product Networks Using Clustering on Variables",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/f33ba15effa5c10e873bf3842afb46a6-Paper.pdf",
    "abstract": "The sum-product network (SPN) is a recently-proposed deep model consisting of a network of sum and product nodes, and has been shown to be competitive with state-of-the-art deep models on certain difficult tasks such as image completion. Designing an SPN network architecture that is suitable for the task at hand is an open question. We propose an algorithm for learning the SPN architecture from data. The idea is to cluster variables (as opposed to data instances) in order to identify variable subsets that strongly interact with one another. Nodes in the SPN network are then allocated towards explaining these interactions. Experimental evidence shows that learning the SPN architecture significantly improves its performance compared to using a previously-proposed static architecture.",
    "authors": [
      "Dennis, Aaron",
      "Ventura, Dan"
    ]
  },
  {
    "id": "f47330643ae134ca204bf6b2481fec47",
    "title": "Towards a learning-theoretic analysis of spike-timing dependent plasticity",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/f47330643ae134ca204bf6b2481fec47-Paper.pdf",
    "abstract": "This paper suggests a learning-theoretic perspective on how synaptic plasticity benefits global brain functioning. We introduce a model, the selectron, that (i) arises as the fast time constant limit of leaky integrate-and-fire neurons equipped with spiking timing dependent plasticity (STDP) and (ii) is amenable to theoretical analysis. We show that the selectron encodes reward estimates into spikes and that an error bound on spikes is controlled by a spiking margin and the sum of synaptic weights. Moreover, the efficacy of spikes (their usefulness to other reward maximizing selectrons) also depends on total synaptic strength. Finally, based on our analysis, we propose a regularized version of STDP, and show the regularization improves the robustness of neuronal learning when faced with multiple stimuli.",
    "authors": [
      "Balduzzi, David",
      "Besserve, Michel"
    ]
  },
  {
    "id": "f5deaeeae1538fb6c45901d524ee2f98",
    "title": "Angular Quantization-based Binary Codes for Fast Similarity Search",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/f5deaeeae1538fb6c45901d524ee2f98-Paper.pdf",
    "abstract": "This paper focuses on the problem of learning binary embeddings for efficient retrieval of high-dimensional non-negative data. Such data typically arises in a large number of vision and text applications where counts or frequencies are used as features.  Also, cosine distance is commonly used as a measure of dissimilarity between such vectors. In this work, we introduce a novel spherical quantization scheme to generate binary embedding of such data and analyze its properties. The number of quantization landmarks in this scheme grows exponentially with data dimensionality resulting in low-distortion quantization.  We propose a very efficient method for computing the binary embedding using such large number of landmarks. Further, a linear transformation is learned to minimize the quantization error by adapting the method to the input data resulting in improved embedding.  Experiments on image and text retrieval applications show superior performance of the proposed method over other existing state-of-the-art methods.",
    "authors": [
      "Gong, Yunchao",
      "Kumar, Sanjiv",
      "Verma, Vishal",
      "Lazebnik, Svetlana"
    ]
  },
  {
    "id": "f5f8590cd58a54e94377e6ae2eded4d9",
    "title": "Matrix reconstruction with the local max norm",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/f5f8590cd58a54e94377e6ae2eded4d9-Paper.pdf",
    "abstract": "We introduce a new family of matrix norms, the ''local max'' norms, generalizing existing methods such as the max norm, the trace norm (nuclear norm), and the weighted or smoothed weighted trace norms, which have been extensively used in the literature as regularizers for matrix reconstruction problems. We show that this new family can be used to interpolate between the (weighted or unweighted) trace norm and the more conservative max norm. We test this interpolation on simulated data and on the large-scale Netflix and MovieLens ratings data, and find improved accuracy relative to the existing matrix norms. We also provide theoretical results showing learning guarantees for some of the new norms.",
    "authors": [
      "Foygel, Rina",
      "Srebro, Nathan",
      "Salakhutdinov, Russ R."
    ]
  },
  {
    "id": "f770b62bc8f42a0b66751fe636fc6eb0",
    "title": "Near-optimal Differentially Private Principal Components",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/f770b62bc8f42a0b66751fe636fc6eb0-Paper.pdf",
    "abstract": "Principal components analysis (PCA) is a standard tool for identifying good low-dimensional approximations to data sets in high dimension.  Many current data sets of interest contain private or sensitive information about individuals.  Algorithms which operate on such data should be sensitive to the privacy risks in publishing their outputs.  Differential privacy is a framework for developing tradeoffs between privacy and the utility of these outputs.  In this paper we investigate the theory and empirical performance of differentially private approximations to PCA and propose a new method which explicitly optimizes the utility of the output.  We demonstrate that on real data, there this a large performance gap between the existing methods and our method.  We show that the sample complexity for the two procedures differs in the scaling with the data dimension, and that our method is nearly optimal in terms of this scaling.",
    "authors": [
      "Chaudhuri, Kamalika",
      "Sarwate, Anand",
      "Sinha, Kaushik"
    ]
  },
  {
    "id": "f899139df5e1059396431415e770c6dd",
    "title": "Mandatory Leaf Node Prediction in Hierarchical Multilabel Classification",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/f899139df5e1059396431415e770c6dd-Paper.pdf",
    "abstract": "In hierarchical classification, the prediction paths may be required to always end at leaf nodes. This is called mandatory leaf node prediction (MLNP) and is particularly useful when the leaf nodes have much stronger semantic meaning than the internal nodes. However, while there have been a lot of MLNP methods in hierarchical multiclass classification, performing MLNP in hierarchical multilabel classification is much more difficult. In this paper, we propose a novel MLNP algorithm that (i) considers the global hierarchy structure; and (ii) can  be used on hierarchies of both trees and DAGs. We show that one can efficiently maximize the joint posterior probability of all the node labels by a simple greedy algorithm. Moreover, this can be further extended to the minimization of the expected symmetric loss. Experiments are performed on a number of real-world data sets with tree- and DAG-structured label hierarchies. The  proposed method consistently outperforms other hierarchical and flat multilabel classification methods.",
    "authors": [
      "Bi, Wei",
      "Kwok, James"
    ]
  },
  {
    "id": "f8c1f23d6a8d8d7904fc0ea8e066b3bb",
    "title": "Synchronization can Control Regularization in Neural Systems via Correlated Noise Processes",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/f8c1f23d6a8d8d7904fc0ea8e066b3bb-Paper.pdf",
    "abstract": "To learn reliable rules that can generalize to novel situations, the brain must be capable of imposing some form of regularization. Here we suggest, through theoretical and computational arguments, that the combination of noise with synchronization provides a plausible mechanism for regularization in the nervous system. The functional role of regularization is considered in a general context in which coupled computational systems receive inputs corrupted by correlated noise. Noise on the inputs is shown to impose regularization, and when synchronization upstream induces time-varying correlations across noise variables, the degree of regularization can be calibrated over time. The resulting qualitative behavior matches experimental data from visual cortex.",
    "authors": [
      "Bouvrie, Jake",
      "Slotine, Jean-jeacques"
    ]
  },
  {
    "id": "f9a40a4780f5e1306c46f1c8daecee3b",
    "title": "Online Regret Bounds for Undiscounted Continuous Reinforcement Learning",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/f9a40a4780f5e1306c46f1c8daecee3b-Paper.pdf",
    "abstract": "We derive sublinear regret bounds for undiscounted reinforcement learning in continuous state space. The proposed algorithm combines state aggregation with the use of upper confidence bounds for implementing optimism in the face of uncertainty.  Beside the existence of an optimal policy which satisfies the Poisson equation, the only assumptions made are Hoelder continuity of rewards and transition probabilities.",
    "authors": [
      "Ortner, Ronald",
      "Ryabko, Daniil"
    ]
  },
  {
    "id": "fb60d411a5c5b72b2e7d3527cfc84fd0",
    "title": "Perceptron Learning of SAT",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/fb60d411a5c5b72b2e7d3527cfc84fd0-Paper.pdf",
    "abstract": "Boolean satisfiability (SAT) as a canonical NP-complete decision problem is one of the most important problems in computer science.  In practice, real-world SAT sentences are drawn from a distribution that may result in efficient algorithms for their solution. Such SAT instances are likely to have shared characteristics and substructures. This work approaches the exploration of a family of SAT solvers as a learning problem.  In particular, we relate polynomial time solvability of a SAT subset to a notion of margin between sentences mapped by a feature function into a Hilbert space.  Provided this mapping is based on polynomial time computable statistics of a sentence, we show that the existance of a margin between these data points implies the existance of a polynomial time solver for that SAT subset based on the Davis-Putnam-Logemann-Loveland algorithm.  Furthermore, we show that a simple perceptron-style learning rule will find an optimal SAT solver with a bounded number of training updates.  We derive a linear time computable set of features and show analytically that margins exist for important polynomial special cases of SAT.  Empirical results show an order of magnitude improvement over a state-of-the-art SAT solver on a hardware verification task.",
    "authors": [
      "Flint, Alex",
      "Blaschko, Matthew"
    ]
  },
  {
    "id": "fc8001f834f6a5f0561080d134d53d29",
    "title": "On Lifting the Gibbs Sampling Algorithm",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/fc8001f834f6a5f0561080d134d53d29-Paper.pdf",
    "abstract": "Statistical relational learning models combine the power of first-order logic, the de facto tool for handling relational structure, with that of probabilistic graphical models, the de facto tool for handling uncertainty. Lifted probabilistic inference algorithms for them have been the subject of much recent research. The main idea in these algorithms is to improve the speed, accuracy and scalability of existing graphical models' inference algorithms by exploiting symmetry in the first-order representation. In this paper, we consider blocked Gibbs sampling, an advanced variation of the classic Gibbs sampling algorithm and lift it to the first-order level. We propose to achieve this by partitioning the first-order atoms in the relational model into a set of disjoint clusters such that exact lifted inference is polynomial in each cluster given an assignment to all other atoms not in the cluster. We propose an approach for constructing such clusters and determining their complexity and show how it can be used to trade accuracy with computational complexity in a principled manner. Our experimental evaluation shows that lifted Gibbs sampling is superior to the propositional algorithm in terms of accuracy and convergence.",
    "authors": [
      "Venugopal, Deepak",
      "Gogate, Vibhav"
    ]
  },
  {
    "id": "fccb3cdc9acc14a6e70a12f74560c026",
    "title": "Q-MKL: Matrix-induced Regularization in Multi-Kernel Learning with Applications to Neuroimaging",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/fccb3cdc9acc14a6e70a12f74560c026-Paper.pdf",
    "abstract": "Multiple Kernel Learning (MKL) generalizes SVMs to the setting where one simultaneously trains a linear classifier and chooses an optimal combination of given base kernels. Model complexity is typically controlled using various norm regularizations on the vector of base kernel mixing coefficients. Existing methods, however, neither regularize nor exploit potentially useful information pertaining to how kernels in the input set 'interact'; that is, higher order kernel-pair relationships that can be easily obtained via unsupervised (similarity, geodesics), supervised (correlation in errors), or domain knowledge driven mechanisms (which features were used to construct the kernel?). We show that by substituting the norm penalty with an arbitrary quadratic function Q \\succeq 0, one can impose a desired covariance structure on mixing coefficient selection, and use this as an inductive bias when learning the concept. This formulation significantly generalizes the widely used 1- and 2-norm MKL objectives. We explore the model\u2019s utility via experiments on a challenging Neuroimaging problem, where the goal is to predict a subject\u2019s conversion to Alzheimer\u2019s Disease (AD) by exploiting aggregate information from several distinct imaging modalities. Here, our new model outperforms the state of the art (p-values << 10\u22123 ). We briefly discuss ramifications in terms of learning bounds (Rademacher complexity).",
    "authors": [
      "Hinrichs, Chris",
      "Singh, Vikas",
      "Peng, Jiming",
      "Johnson, Sterling"
    ]
  },
  {
    "id": "fe2d010308a6b3799a3d9c728ee74244",
    "title": "Label Ranking with Partial Abstention based on Thresholded Probabilistic Models",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/fe2d010308a6b3799a3d9c728ee74244-Paper.pdf",
    "abstract": "Several machine learning methods allow for abstaining from uncertain predictions. While being common for settings like conventional classification, abstention has been studied much less in learning to rank. We address abstention for the label ranking setting, allowing the learner to declare certain pairs of labels as being incomparable and, thus, to predict partial instead of total orders. In our method, such predictions are produced via thresholding the probabilities of pairwise preferences between labels, as induced by a predicted probability distribution on the set of all rankings. We formally analyze this approach for the Mallows and the Plackett-Luce model, showing that it produces proper partial orders as predictions and characterizing the expressiveness of the induced class of partial orders. These theoretical results are complemented by experiments demonstrating the practical usefulness of the approach.",
    "authors": [
      "Cheng, Weiwei",
      "H\u00fcllermeier, Eyke",
      "Waegeman, Willem",
      "Welker, Volkmar"
    ]
  },
  {
    "id": "feab05aa91085b7a8012516bc3533958",
    "title": "Weighted Likelihood Policy Search with Model Selection",
    "year": "2012",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/feab05aa91085b7a8012516bc3533958-Paper.pdf",
    "abstract": "Reinforcement learning (RL) methods based on direct policy search (DPS) have been actively discussed to achieve an efficient approach to complicated Markov decision processes (MDPs). Although they have brought much progress in practical applications of RL, there still remains an unsolved problem in DPS related to model  selection for the policy. In this paper, we propose a novel DPS method, {\\it  weighted likelihood policy search (WLPS)}, where a policy is efficiently learned through the weighted likelihood estimation. WLPS naturally connects DPS to the statistical inference problem and thus various sophisticated techniques in statistics can be applied to DPS problems directly. Hence, by following the idea of the {\\it information criterion}, we develop a new measurement for model comparison in DPS based on the weighted log-likelihood.",
    "authors": [
      "Ueno, Tsuyoshi",
      "Hayashi, Kohei",
      "Washio, Takashi",
      "Kawahara, Yoshinobu"
    ]
  },
  {
    "id": "01386bd6d8e091c2ab4c7c7de644d37b",
    "title": "Inferring neural population dynamics from multiple partial recordings of the same neural circuit",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/01386bd6d8e091c2ab4c7c7de644d37b-Paper.pdf",
    "abstract": "Simultaneous recordings of the activity of large neural populations are extremely valuable as they can be used to infer the dynamics and interactions of neurons in a local circuit, shedding light on the computations performed. It is now possible to measure the activity of hundreds of neurons  using 2-photon calcium imaging. However, many computations are thought to involve circuits consisting of thousands of neurons, such as cortical barrels in rodent somatosensory cortex. Here we contribute a statistical method for stitching\" together sequentially imaged sets of neurons into one model  by phrasing the problem as fitting a latent dynamical system with missing observations. This method allows us to substantially expand the population-sizes for which population dynamics can be characterized---beyond the number of simultaneously imaged neurons. In particular, we demonstrate using recordings in mouse somatosensory cortex that this method makes it possible to predict noise correlations between non-simultaneously recorded neuron pairs.\"",
    "authors": [
      "Turaga, Srini",
      "Buesing, Lars",
      "Packer, Adam M.",
      "Dalgleish, Henry",
      "Pettit, Noah",
      "Hausser, Michael",
      "Macke, Jakob H."
    ]
  },
  {
    "id": "021bbc7ee20b71134d53e20206bd6feb",
    "title": "Approximate Gaussian process inference for the drift function in stochastic differential equations",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/021bbc7ee20b71134d53e20206bd6feb-Paper.pdf",
    "abstract": "We introduce a nonparametric approach for estimating drift functions in systems of stochastic differential equations from incomplete observations of the state vector. Using a Gaussian process prior over the drift as a function of the state vector, we develop an approximate EM algorithm to deal with the unobserved, latent dynamics between observations. The posterior over states is approximated by a piecewise linearized process and the MAP estimation of the drift is facilitated by a sparse Gaussian process regression.",
    "authors": [
      "Ruttor, Andreas",
      "Batz, Philipp",
      "Opper, Manfred"
    ]
  },
  {
    "id": "024d7f84fff11dd7e8d9c510137a2381",
    "title": "Third-Order Edge Statistics: Contour Continuation, Curvature, and Cortical Connections",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/024d7f84fff11dd7e8d9c510137a2381-Paper.pdf",
    "abstract": "Association field models have been used to explain human contour grouping performance and to explain the mean frequency of long-range horizontal connections across cortical columns in V1. However, association fields essentially depend on pairwise statistics of edges in natural scenes. We develop a spectral test of the sufficiency of pairwise statistics and show that there is significant higher-order structure.  An analysis using a probabilistic spectral embedding reveals curvature-dependent components to the association field, and reveals a challenge for biological learning algorithms.",
    "authors": [
      "Lawlor, Matthew",
      "Zucker, Steven W."
    ]
  },
  {
    "id": "02522a2b2726fb0a03bb19f2d8d9524d",
    "title": "Transportability from Multiple Environments with Limited Experiments",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/02522a2b2726fb0a03bb19f2d8d9524d-Paper.pdf",
    "abstract": "This paper considers the problem of transferring experimental findings learned from multiple heterogeneous domains to a target environment, in which only limited experiments can be performed. We reduce questions of transportability from multiple domains and with limited scope to symbolic derivations in the do-calculus, thus extending the treatment of transportability from full experiments introduced in Pearl and Bareinboim (2011). We further provide different graphical and algorithmic conditions for computing the transport formula for this setting, that is, a way of fusing the observational and experimental information scattered throughout different domains to synthesize a consistent estimate of the desired effects.",
    "authors": [
      "Bareinboim, Elias",
      "Lee, Sanghack",
      "Honavar, Vasant",
      "Pearl, Judea"
    ]
  },
  {
    "id": "0266e33d3f546cb5436a10798e657d97",
    "title": "On model selection consistency of penalized M-estimators: a geometric theory",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/0266e33d3f546cb5436a10798e657d97-Paper.pdf",
    "abstract": "Penalized M-estimators are used in diverse areas of science and engineering to fit high-dimensional models with some low-dimensional structure. Often, the penalties are \\emph{geometrically decomposable}, \\ie\\ can be expressed as a sum of (convex) support functions. We generalize the notion of irrepresentable to geometrically decomposable penalties and develop a general framework for establishing consistency and model selection consistency of M-estimators with such penalties. We then use this framework to derive results for some special cases of interest in bioinformatics and statistical learning.",
    "authors": [
      "Lee, Jason D.",
      "Sun, Yuekai",
      "Taylor, Jonathan E."
    ]
  },
  {
    "id": "043c3d7e489c69b48737cc0c92d0f3a2",
    "title": "Robust Bloom Filters for Large MultiLabel Classification Tasks",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/043c3d7e489c69b48737cc0c92d0f3a2-Paper.pdf",
    "abstract": "This  paper presents an approach to multilabel classification (MLC) with a large number of labels. Our approach is a reduction to binary classification in which label sets are represented by low dimensional binary vectors. This representation follows the principle of Bloom filters, a space-efficient data structure originally designed for approximate membership testing. We show that a naive application of Bloom filters in MLC is not robust to individual binary classifiers' errors. We then present an approach that exploits a specific feature of real-world datasets when the number of labels is large: many labels (almost) never appear together.  Our approch is provably robust, has sublinear training and inference complexity with respect to the number of labels, and compares favorably to state-of-the-art algorithms on two large scale multilabel datasets.",
    "authors": [
      "Cisse, Moustapha M.",
      "Usunier, Nicolas",
      "Arti\u00e8res, Thierry",
      "Gallinari, Patrick"
    ]
  },
  {
    "id": "05311655a15b75fab86956663e1819cd",
    "title": "On the Relationship Between Binary Classification, Bipartite Ranking, and Binary Class Probability Estimation",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/05311655a15b75fab86956663e1819cd-Paper.pdf",
    "abstract": "We investigate the relationship between three fundamental problems in machine learning: binary classification, bipartite ranking, and binary class probability estimation (CPE). It is known that a good binary CPE model can be used to obtain a good binary classification model (by thresholding at 0.5), and also to obtain a good bipartite ranking model (by using the CPE model directly as a ranking model); it is also known that a binary classification model does not necessarily yield a CPE model. However, not much is known about other directions. Formally, these relationships involve regret transfer bounds. In this paper, we introduce the notion of weak regret transfer bounds, where the mapping needed to transform a model from one problem to another depends on the underlying probability distribution (and in practice, must be estimated from data). We then show that, in this weaker sense, a good bipartite ranking model can be used to construct a good classification model (by thresholding at a suitable point), and more surprisingly, also to construct a good binary CPE model (by calibrating the scores of the ranking model).",
    "authors": [
      "Narasimhan, Harikrishna",
      "Agarwal, Shivani"
    ]
  },
  {
    "id": "062ddb6c727310e76b6200b7c71f63b5",
    "title": "Sequential Transfer in Multi-armed Bandit with Finite Set of Models",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/062ddb6c727310e76b6200b7c71f63b5-Paper.pdf",
    "abstract": "Learning from prior tasks and transferring that experience to improve future performance is critical for building lifelong learning agents. Although results in supervised and reinforcement learning show that transfer may significantly improve the learning performance, most of the literature on transfer is focused on batch learning tasks. In this paper we study the problem of sequential transfer in online learning, notably in the multi-arm bandit framework, where the objective is to minimize the cumulative regret over a sequence of tasks by incrementally transferring knowledge from prior tasks.  We introduce a novel bandit algorithm based on a method-of-moments approach for the estimation of the possible tasks and derive regret bounds for it.",
    "authors": [
      "Gheshlaghi azar, Mohammad",
      "Lazaric, Alessandro",
      "Brunskill, Emma"
    ]
  },
  {
    "id": "0768281a05da9f27df178b5c39a51263",
    "title": "A Graphical Transformation for Belief Propagation: Maximum Weight Matchings and Odd-Sized Cycles",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/0768281a05da9f27df178b5c39a51263-Paper.pdf",
    "abstract": "Max-product \u2018belief propagation\u2019 (BP) is a popular distributed heuristic for finding the Maximum A Posteriori (MAP) assignment in a joint probability distribution represented by a Graphical Model (GM). It was recently shown that BP converges to the correct MAP assignment for a class of loopy GMs with the following common feature: the Linear Programming (LP) relaxation to the MAP problem is tight (has no integrality gap). Unfortunately, tightness of the LP relaxation does not, in general, guarantee convergence and correctness of the BP algorithm. The failure of BP in such cases motivates reverse engineering a solution \u2013 namely, given a tight LP, can we design a \u2018good\u2019 BP algorithm.  In this paper, we design a BP algorithm for the Maximum Weight Matching (MWM) problem over general graphs. We prove that the algorithm converges to the correct optimum if the respective LP relaxation, which may include inequalities associated with non-intersecting odd-sized cycles, is tight. The most significant part of our approach is the introduction of a novel graph transformation designed to force convergence of BP. Our theoretical result suggests an efficient BP-based heuristic for the MWM problem, which consists of making sequential, \u201ccutting plane\u201d, modifications to the underlying GM. Our experiments show that this heuristic performs as well as traditional cutting-plane algorithms using LP solvers on MWM problems.",
    "authors": [
      "Shin, Jinwoo",
      "Gelfand, Andrew E.",
      "Chertkov, Misha"
    ]
  },
  {
    "id": "076a0c97d09cf1a0ec3e19c7f2529f2b",
    "title": "A Kernel Test for Three-Variable Interactions",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/076a0c97d09cf1a0ec3e19c7f2529f2b-Paper.pdf",
    "abstract": "We introduce kernel nonparametric tests for Lancaster three-variable interaction and for total independence, using embeddings of signed measures into a reproducing kernel Hilbert space. The resulting test statistics are straightforward to compute, and are used in powerful three-variable interaction tests, which are consistent against all alternatives for a large family of reproducing kernels. We show the Lancaster test to be sensitive to cases where two independent causes individually have weak influence on a third dependent variable, but their combined effect has a strong influence. This makes the Lancaster test especially suited to finding structure in directed graphical models, where it outperforms competing nonparametric tests in detecting such V-structures.",
    "authors": [
      "Sejdinovic, Dino",
      "Gretton, Arthur",
      "Bergsma, Wicher"
    ]
  },
  {
    "id": "077e29b11be80ab57e1a2ecabb7da330",
    "title": "Accelerated Mini-Batch Stochastic Dual Coordinate Ascent",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/077e29b11be80ab57e1a2ecabb7da330-Paper.pdf",
    "abstract": "Stochastic dual coordinate ascent (SDCA) is an effective technique for solving regularized loss minimization problems in machine learning. This paper considers an extension of SDCA under the mini-batch setting that is often used in practice. Our main contribution is to introduce an accelerated mini-batch version of SDCA and prove a fast convergence rate for this method. We discuss an implementation of our method over a parallel computing system, and compare the results to both the vanilla stochastic dual coordinate ascent and to the accelerated deterministic gradient descent method of Nesterov [2007].",
    "authors": [
      "Shalev-Shwartz, Shai",
      "Zhang, Tong"
    ]
  },
  {
    "id": "07cdfd23373b17c6b337251c22b7ea57",
    "title": "A Scalable Approach to Probabilistic Latent Space Inference of Large-Scale Networks",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/07cdfd23373b17c6b337251c22b7ea57-Paper.pdf",
    "abstract": "We propose a scalable approach for making inference about latent spaces of large networks. With a succinct representation of networks as a bag of triangular motifs, a parsimonious statistical model, and an efficient stochastic variational inference algorithm, we are able to analyze real networks with over a million vertices and hundreds of latent roles on a single machine in a matter of hours, a setting that is out of reach for many existing methods. When compared to the state-of-the-art probabilistic approaches, our method is several orders of magnitude faster, with competitive or improved accuracy for latent space recovery and link prediction.",
    "authors": [
      "Yin, Junming",
      "Ho, Qirong",
      "Xing, Eric P."
    ]
  },
  {
    "id": "0bb4aec1710521c12ee76289d9440817",
    "title": "Multi-Prediction Deep Boltzmann Machines",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/0bb4aec1710521c12ee76289d9440817-Paper.pdf",
    "abstract": "We introduce the Multi-Prediction Deep Boltzmann Machine (MP-DBM). The MP-DBM can be seen as a single probabilistic model trained to maximize a variational approximation to the generalized pseudolikelihood, or as a family of recurrent nets that share parameters and approximately solve different inference problems. Prior methods of training DBMs either do not perform well on classification tasks or require an initial learning pass that trains the DBM greedily, one layer at a time. The MP-DBM does not require greedy layerwise pretraining, and outperforms the standard DBM at classification, classification with missing inputs, and mean field prediction tasks.",
    "authors": [
      "Goodfellow, Ian",
      "Mirza, Mehdi",
      "Courville, Aaron",
      "Bengio, Yoshua"
    ]
  },
  {
    "id": "0c0a7566915f4f24853fc4192689aa7e",
    "title": "Learning and using language via recursive pragmatic reasoning about other agents",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/0c0a7566915f4f24853fc4192689aa7e-Paper.pdf",
    "abstract": "Language users are remarkably good at making inferences about speakers' intentions in context, and children learning their native language also display substantial skill in acquiring the meanings of unknown words. These two cases are deeply related: Language users invent new terms in conversation, and language learners learn the literal meanings of words based on their pragmatic inferences about how those words are used.  While pragmatic inference and word learning have both been independently characterized in probabilistic terms, no current work unifies these two. We describe a model in which language learners assume that they jointly approximate a shared, external lexicon and reason recursively about the goals of others in using this lexicon. This model captures phenomena in word learning and pragmatic inference; it additionally leads to insights about the emergence of communicative systems in conversation and the mechanisms by which pragmatic inferences become incorporated into word meanings.",
    "authors": [
      "Smith, Nathaniel J.",
      "Goodman, Noah",
      "Frank, Michael"
    ]
  },
  {
    "id": "0deb1c54814305ca9ad266f53bc82511",
    "title": "Reinforcement Learning in Robust Markov Decision Processes",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/0deb1c54814305ca9ad266f53bc82511-Paper.pdf",
    "abstract": "An important challenge in Markov decision processes is to ensure robustness with respect to unexpected or adversarial system behavior while taking advantage of well-behaving parts of the system. We consider a problem setting where some unknown parts of the state space can have arbitrary transitions while other parts are purely stochastic. We devise an algorithm that is adaptive to potentially adversarial behavior and show that it achieves similar regret bounds as the purely stochastic case.",
    "authors": [
      "Lim, Shiau Hong",
      "Xu, Huan",
      "Mannor, Shie"
    ]
  },
  {
    "id": "0ed9422357395a0d4879191c66f4faa2",
    "title": "Regularized Spectral Clustering under the Degree-Corrected Stochastic Blockmodel",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/0ed9422357395a0d4879191c66f4faa2-Paper.pdf",
    "abstract": "Spectral clustering is a fast and popular algorithm for finding clusters in networks. Recently, Chaudhuri et al. and Amini et al. proposed variations on the algorithm that artificially inflate the node degrees for improved statistical performance.  The current paper extends the previous theoretical results to the more canonical spectral clustering algorithm in a way that removes any assumption on the minimum degree and  provides guidance on the choice of tuning parameter.  Moreover, our results show how the star shape\" in the eigenvectors--which are consistently observed in empirical networks--can be explained by the Degree-Corrected Stochastic Blockmodel and the Extended Planted Partition model, two statistical model that allow for highly heterogeneous degrees.  Throughout, the paper characterizes and justifies several of the variations of the spectral clustering algorithm in terms of these models.  \"",
    "authors": [
      "Qin, Tai",
      "Rohe, Karl"
    ]
  },
  {
    "id": "0ff39bbbf981ac0151d340c9aa40e63e",
    "title": "A Novel Two-Step Method for Cross Language Representation Learning",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/0ff39bbbf981ac0151d340c9aa40e63e-Paper.pdf",
    "abstract": "Cross language text classi\ufb01cation is an important learning task in natural language processing. A critical challenge of cross language learning lies in that words of different languages are in disjoint feature spaces. In this paper, we propose a two-step representation learning method to bridge the feature spaces of different languages by exploiting a set of parallel bilingual documents. Speci\ufb01cally, we \ufb01rst formulate a matrix completion problem to produce a complete parallel document-term matrix for all documents in two languages, and then induce a cross-lingual document representation by applying latent semantic indexing on the obtained matrix. We use a projected gradient descent algorithm to solve the formulated matrix completion problem with convergence guarantees. The proposed approach is evaluated by conducting a set of experiments with cross language sentiment classi\ufb01cation tasks on Amazon product reviews. The experimental results demonstrate that the proposed learning approach outperforms a number of comparison cross language representation learning methods, especially when the number of parallel bilingual documents is small.",
    "authors": [
      "Xiao, Min",
      "Guo, Yuhong"
    ]
  },
  {
    "id": "0ff8033cf9437c213ee13937b1c4c455",
    "title": "Graphical Models for Inference with Missing Data",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/0ff8033cf9437c213ee13937b1c4c455-Paper.pdf",
    "abstract": "We address the problem of deciding whether there exists a consistent estimator of a given relation Q, when data are missing not at random. We employ a formal representation called `Missingness Graphs' to explicitly portray the causal mechanisms responsible for missingness and to encode dependencies between these mechanisms and the variables being measured. Using this representation, we define the notion of \\textit{recoverability} which ensures that, for a given missingness-graph $G$ and a given query $Q$ an algorithm exists such that in the limit of large samples, it produces an estimate of $Q$ \\textit{as if} no data were missing. We further present conditions that the graph should satisfy in order for recoverability to hold and devise algorithms to detect the presence of these conditions.",
    "authors": [
      "Mohan, Karthika",
      "Pearl, Judea",
      "Tian, Jin"
    ]
  },
  {
    "id": "109a0ca3bc27f3e96597370d5c8cf03d",
    "title": "Convex Tensor Decomposition via Structured Schatten Norm Regularization",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/109a0ca3bc27f3e96597370d5c8cf03d-Paper.pdf",
    "abstract": "We propose a new class of structured Schatten norms for tensors that includes two recently  proposed norms (overlapped'' and \"latent'') for convex-optimization-based  tensor decomposition. Based on the properties of the structured Schatten norms, we mathematically analyze the performance of \"latent'' approach for tensor decomposition, which was empirically found to perform better than the \"overlapped'' approach in some settings. We show theoretically that this is indeed the case. In particular, when the unknown true tensor is low-rank in a specific mode, this approach performs as well as knowing the mode with the smallest rank. Along the way, we show a novel duality result for structures Schatten norms, which is also interesting in the general context of structured sparsity. We confirm through  numerical simulations that our theory can precisely predict the scaling behaviour of the mean squared error.  \"",
    "authors": [
      "Tomioka, Ryota",
      "Suzuki, Taiji"
    ]
  },
  {
    "id": "115f89503138416a242f40fb7d7f338e",
    "title": "Variational Inference for Mahalanobis Distance Metrics in Gaussian Process Regression",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/115f89503138416a242f40fb7d7f338e-Paper.pdf",
    "abstract": "We introduce a novel variational method that allows to approximately integrate out kernel hyperparameters, such as length-scales, in Gaussian process regression. This approach consists of a novel variant of the variational framework that has been recently developed for the Gaussian process latent variable model which additionally makes use of a standardised representation of the Gaussian process. We consider this technique for learning Mahalanobis distance metrics in a Gaussian process regression setting and provide experimental evaluations and comparisons with existing methods by considering  datasets with high-dimensional inputs.",
    "authors": [
      "Titsias RC AUEB, Michalis",
      "Lazaro-Gredilla, Miguel"
    ]
  },
  {
    "id": "13f320e7b5ead1024ac95c3b208610db",
    "title": "Efficient Online Inference for Bayesian Nonparametric Relational Models",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/13f320e7b5ead1024ac95c3b208610db-Paper.pdf",
    "abstract": "Stochastic block models characterize observed network relationships via latent community memberships. In large social networks, we expect entities to participate in multiple communities, and the number of communities to grow with the network size. We introduce a new model for these phenomena, the hierarchical Dirichlet process relational model, which allows nodes to have mixed membership in an unbounded set of communities. To allow scalable learning, we derive an online stochastic variational inference algorithm. Focusing on assortative models of undirected networks, we also propose an efficient structured mean field variational bound, and online methods for automatically pruning unused communities. Compared to state-of-the-art online learning methods for parametric relational models, we show significantly improved perplexity and link prediction accuracy for sparse networks with tens of thousands of nodes. We also showcase an analysis of LittleSis, a large network of who-knows-who at the heights of business and government.",
    "authors": [
      "Kim, Dae Il",
      "Gopalan, Prem K.",
      "Blei, David",
      "Sudderth, Erik"
    ]
  },
  {
    "id": "1579779b98ce9edb98dd85606f2c119d",
    "title": "Convergence of Monte Carlo Tree Search in Simultaneous Move Games",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/1579779b98ce9edb98dd85606f2c119d-Paper.pdf",
    "abstract": "In this paper, we study Monte Carlo tree search (MCTS) in zero-sum extensive-form games with perfect information and simultaneous moves. We present a general template of MCTS algorithms for these games, which can be instantiated by various selection methods. We formally prove that if a selection method is $\\epsilon$-Hannan consistent in a matrix game and satisfies additional requirements on exploration, then the MCTS algorithm eventually converges to an approximate Nash equilibrium (NE) of the extensive-form game. We empirically evaluate this claim using regret matching and Exp3 as the selection methods on randomly generated and worst case games. We confirm the formal result and show that additional MCTS variants also converge to approximate NE on the evaluated games.",
    "authors": [
      "Lisy, Viliam",
      "Kovarik, Vojta",
      "Lanctot, Marc",
      "Bosansky, Branislav"
    ]
  },
  {
    "id": "1714726c817af50457d810aae9d27a2e",
    "title": "Learning to Pass Expectation Propagation Messages",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/1714726c817af50457d810aae9d27a2e-Paper.pdf",
    "abstract": "Expectation Propagation (EP) is a popular approximate posterior inference algorithm that often  provides a fast and accurate alternative to sampling-based methods. However, while the EP framework in theory allows for complex non-Gaussian factors, there is still a significant practical barrier to using them within EP, because doing so requires the implementation of message update operators, which can be difficult and require hand-crafted approximations. In this work, we study the question of whether it is possible to automatically derive fast and accurate EP updates by learning a discriminative model e.g., a neural network or random forest) to map EP message inputs to EP message outputs.  We address the practical concerns that arise in the process, and we provide empirical analysis on several challenging and diverse factors, indicating that there is a space of factors where this approach appears promising.",
    "authors": [
      "Heess, Nicolas",
      "Tarlow, Daniel",
      "Winn, John"
    ]
  },
  {
    "id": "17c276c8e723eb46aef576537e9d56d0",
    "title": "Bayesian Inference and Online Experimental Design for Mapping Neural Microcircuits",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/17c276c8e723eb46aef576537e9d56d0-Paper.pdf",
    "abstract": "We develop an inference and optimal design procedure for recovering synaptic weights in neural microcircuits. We base our procedure on data from an experiment in which populations of putative presynaptic neurons can be stimulated while a subthreshold recording is made from a single postsynaptic neuron. We present a realistic statistical model which accounts for the main sources of variability in this experiment and allows for large amounts of information about the biological system to be incorporated if available. We then present a simpler model to facilitate online experimental design which entails the use of efficient Bayesian inference. The optimized approach results in equal quality posterior estimates of the synaptic weights in roughly half the number of experimental trials under experimentally realistic conditions, tested on synthetic data generated from the full model.",
    "authors": [
      "Shababo, Ben",
      "Paige, Brooks",
      "Pakman, Ari",
      "Paninski, Liam"
    ]
  },
  {
    "id": "184260348236f9554fe9375772ff966e",
    "title": "Action is in the Eye of the Beholder: Eye-gaze Driven Model for Spatio-Temporal Action Localization",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/184260348236f9554fe9375772ff966e-Paper.pdf",
    "abstract": "We propose a new weakly-supervised structured learning approach for recognition and spatio-temporal localization of actions in video. As part of the proposed approach we develop a generalization of the Max-Path search algorithm, which allows us to efficiently search over a structured space of multiple spatio-temporal paths, while also allowing to incorporate context information into the model. Instead of using spatial annotations, in the form of bounding boxes, to guide the latent model during training, we utilize human gaze data in the form of a weak supervisory signal. This is achieved by incorporating gaze, along with the classification, into the structured loss within the latent SVM learning framework. Experiments on a challenging benchmark dataset, UCF-Sports, show that our model is more accurate, in terms of classification, and achieves state-of-the-art results in localization. In addition, we show how our model can produce top-down saliency maps conditioned on the classification label and localized latent paths.",
    "authors": [
      "Shapovalova, Nataliya",
      "Raptis, Michalis",
      "Sigal, Leonid",
      "Mori, Greg"
    ]
  },
  {
    "id": "1896a3bf730516dd643ba67b4c447d36",
    "title": "Integrated Non-Factorized Variational Inference",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/1896a3bf730516dd643ba67b4c447d36-Paper.pdf",
    "abstract": "We present a non-factorized variational method for full posterior inference in Bayesian hierarchical models,  with the goal of capturing the posterior variable dependencies via efficient and possibly parallel computation. Our approach unifies the integrated nested Laplace approximation (INLA) under the variational framework. The proposed method is applicable in more challenging scenarios than typically assumed by INLA,  such as Bayesian Lasso,  which is characterized by the non-differentiability of the $\\ell_{1}$ norm arising from independent Laplace priors. We derive an upper bound for the Kullback-Leibler divergence,  which yields a fast closed-form solution via decoupled optimization. Our method is a reliable analytic alternative to Markov chain Monte Carlo (MCMC), and it results in a tighter evidence lower bound than that of mean-field variational Bayes (VB) method.",
    "authors": [
      "Han, Shaobo",
      "Liao, Xuejun",
      "Carin, Lawrence"
    ]
  },
  {
    "id": "18997733ec258a9fcaf239cc55d53363",
    "title": "A Gang of Bandits",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/18997733ec258a9fcaf239cc55d53363-Paper.pdf",
    "abstract": "Multi-armed bandit problems are receiving a great deal of attention because they adequately formalize the exploration-exploitation trade-offs arising in several industrially relevant applications, such as online advertisement and, more generally, recommendation systems.  In many cases, however, these applications have a strong social component, whose integration in the bandit algorithm could lead to a dramatic performance increase. For instance, we may want to serve content to a group of users by taking advantage of an underlying network of social relationships among them. In this paper, we introduce novel algorithmic approaches to the solution of such networked bandit problems. More specifically, we design and analyze a global strategy which allocates a bandit algorithm to each network node (user) and allows it to \u201cshare\u201d signals (contexts and payoffs) with the neghboring nodes. We then derive two more scalable variants of this strategy based on different ways of clustering the graph nodes. We experimentally compare the algorithm and its variants to state-of-the-art methods for contextual bandits that do not use the relational information. Our experiments, carried out on synthetic and real-world datasets, show a marked increase in prediction performance obtained by exploiting the network structure.",
    "authors": [
      "Cesa-Bianchi, Nicol\u00f2",
      "Gentile, Claudio",
      "Zappella, Giovanni"
    ]
  },
  {
    "id": "19bc916108fc6938f52cb96f7e087941",
    "title": "Multiclass Total Variation Clustering",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/19bc916108fc6938f52cb96f7e087941-Paper.pdf",
    "abstract": "Ideas from the image processing literature have recently motivated a new set of clustering algorithms that rely on the concept of total variation. While these algorithms perform well for bi-partitioning tasks, their recursive extensions yield unimpressive results for multiclass clustering tasks. This paper presents a general framework for multiclass total variation clustering that does not rely on recursion. The results greatly outperform previous total variation algorithms and compare well with state-of-the-art NMF approaches.",
    "authors": [
      "Bresson, Xavier",
      "Laurent, Thomas",
      "Uminsky, David",
      "von Brecht, James"
    ]
  },
  {
    "id": "1aa48fc4880bb0c9b8a3bf979d3b917e",
    "title": "Simultaneous Rectification and Alignment via Robust Recovery of Low-rank Tensors",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/1aa48fc4880bb0c9b8a3bf979d3b917e-Paper.pdf",
    "abstract": "In this work, we propose a general method for recovering low-rank three-order tensors, in which the data can be deformed by some unknown transformation and corrupted by arbitrary sparse errors. Since the unfolding matrices of a tensor are interdependent, we introduce auxiliary variables and relax the hard equality constraints by the augmented Lagrange multiplier method. To improve the computational efficiency, we introduce a proximal gradient step to the alternating direction minimization method. We have provided proof for the convergence of the linearized version of the problem which is the inner loop of the overall algorithm. Both simulations and experiments show that our methods are more efficient and effective than previous work. The proposed method can be easily applied to simultaneously  rectify and align multiple images or videos frames. In this context, the state-of-the-art algorithms RASL'' and \"TILT'' can be viewed as two special cases of our work, and yet each only performs part of the function of our method.\"",
    "authors": [
      "Zhang, Xiaoqin",
      "Wang, Di",
      "Zhou, Zhengyuan",
      "Ma, Yi"
    ]
  },
  {
    "id": "1abb1e1ea5f481b589da52303b091cbb",
    "title": "BIG & QUIC: Sparse Inverse Covariance Estimation for a Million Variables",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/1abb1e1ea5f481b589da52303b091cbb-Paper.pdf",
    "abstract": "The l1-regularized Gaussian maximum likelihood estimator (MLE) has been shown to have strong statistical guarantees in recovering a sparse inverse covariance matrix even under high-dimensional settings. However, it requires solving a difficult non-smooth log-determinant program with number of parameters scaling quadratically with the number of Gaussian variables. State-of-the-art methods thus do not scale to problems with more than 20,000 variables. In this paper, we develop an algorithm BigQUIC, which can solve 1 million dimensional l1-regularized Gaussian MLE problems (which would thus have 1000 billion parameters) using a single machine, with bounded memory. In order to do so, we carefully exploit the underlying structure of the problem. Our innovations include a novel block-coordinate descent method with the blocks chosen via a clustering scheme to minimize repeated computations; and allowing for inexact computation of specific components. In spite of these modifications,  we are able to theoretically analyze our procedure and show that BigQUIC can achieve super-linear or even quadratic convergence rates.",
    "authors": [
      "Hsieh, Cho-Jui",
      "Sustik, Matyas A.",
      "Dhillon, Inderjit S.",
      "Ravikumar, Pradeep K.",
      "Poldrack, Russell"
    ]
  },
  {
    "id": "1afa34a7f984eeabdbb0a7d494132ee5",
    "title": "Robust Multimodal Graph Matching: Sparse Coding Meets Graph Matching",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/1afa34a7f984eeabdbb0a7d494132ee5-Paper.pdf",
    "abstract": "Graph matching is a challenging problem with very important applications in a wide range of fields, from image and video analysis to biological and biomedical problems. We propose a robust graph matching algorithm inspired in sparsity-related techniques. We cast the problem, resembling group or collaborative sparsity formulations, as a non-smooth convex optimization problem that can be efficiently solved using augmented Lagrangian techniques. The method can deal with weighted or unweighted graphs, as well as multimodal data, where different graphs represent different types of data. The proposed approach is also naturally integrated with collaborative graph inference techniques, solving general network inference problems where the observed variables, possibly coming from different modalities, are not in correspondence. The algorithm is tested and compared with state-of-the-art graph matching techniques in both synthetic and real graphs. We also present results on multimodal graphs and applications to collaborative inference of brain connectivity from alignment-free functional magnetic resonance imaging (fMRI) data.",
    "authors": [
      "Fiori, Marcelo",
      "Sprechmann, Pablo",
      "Vogelstein, Joshua",
      "Muse, Pablo",
      "Sapiro, Guillermo"
    ]
  },
  {
    "id": "1baff70e2669e8376347efd3a874a341",
    "title": "Optimal integration of visual speed across different spatiotemporal frequency channels",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/1baff70e2669e8376347efd3a874a341-Paper.pdf",
    "abstract": "How does the human visual system compute the speed of a coherent motion stimulus that contains motion energy in different spatiotemporal frequency bands? Here we propose that perceived speed is the result of optimal integration of speed information from independent spatiotemporal frequency tuned channels. We formalize this hypothesis with a Bayesian observer model that treats the channel activity as independent cues, which are optimally combined with a prior expectation for slow speeds. We test the model against behavioral data from a 2AFC speed discrimination task with which we measured subjects' perceived speed of drifting sinusoidal gratings with different contrasts and spatial frequencies, and of various combinations of these single gratings. We find that perceived speed of the combined stimuli is independent of the relative phase of the underlying grating components, and that the perceptual biases and discrimination thresholds are always smaller for the combined stimuli, supporting the cue combination hypothesis. The proposed Bayesian model fits the data well, accounting for perceptual biases and thresholds of both simple and combined stimuli. Fits are improved if we assume that the channel responses are subject to divisive normalization, which is in line with physiological evidence.  Our results provide an important step toward a more complete model of visual motion perception that can predict perceived speeds for stimuli of arbitrary spatial structure.",
    "authors": [
      "Jogan, Matjaz",
      "Stocker, Alan A."
    ]
  },
  {
    "id": "1cecc7a77928ca8133fa24680a88d2f9",
    "title": "Translating Embeddings for Modeling Multi-relational Data",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf",
    "abstract": "We consider the problem of embedding entities and relationships of multi-relational data in low-dimensional vector spaces. Our objective is to propose a canonical model which is easy to train, contains a reduced number of parameters and can scale up to very large databases. Hence, we propose, TransE, a method which models relationships by interpreting them as translations operating on the low-dimensional embeddings of the entities. Despite its simplicity, this assumption proves to be powerful since extensive experiments show that TransE significantly outperforms state-of-the-art methods in link prediction on two knowledge bases. Besides, it can be successfully trained on a large scale data set with 1M entities, 25k relationships and more than 17M training samples.",
    "authors": [
      "Bordes, Antoine",
      "Usunier, Nicolas",
      "Garcia-Duran, Alberto",
      "Weston, Jason",
      "Yakhnenko, Oksana"
    ]
  },
  {
    "id": "1e1d184167ca7676cf665225e236a3d2",
    "title": "Synthesizing Robust Plans under Incomplete Domain Models",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/1e1d184167ca7676cf665225e236a3d2-Paper.pdf",
    "abstract": "Most current planners assume complete domain models and focus on generating correct plans. Unfortunately, domain modeling is a laborious and error-prone task, thus real world agents have to plan with incomplete domain models. While domain experts cannot guarantee completeness, often they are able to circumscribe the incompleteness of the model by providing annotations as to which parts of the domain model may be incomplete. In such cases, the goal should be to synthesize plans that are robust with respect to any known incompleteness of the domain. In this paper, we first introduce annotations expressing the knowledge of the domain incompleteness and formalize the notion of plan robustness with respect to an incomplete domain model. We then show an approach to compiling the problem of finding robust plans to the conformant probabilistic planning problem, and present experimental results with Probabilistic-FF planner.",
    "authors": [
      "Nguyen, Tuan A.",
      "Kambhampati, Subbarao",
      "Do, Minh"
    ]
  },
  {
    "id": "1f4477bad7af3616c1f933a02bfabe4e",
    "title": "Learning Gaussian Graphical Models with Observed or Latent FVSs",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/1f4477bad7af3616c1f933a02bfabe4e-Paper.pdf",
    "abstract": "Gaussian Graphical Models (GGMs) or Gauss Markov random fields are widely used in many applications, and the trade-off between the modeling capacity and the efficiency of learning and inference has been an important research problem. In this paper, we study the family of GGMs with small feedback vertex sets (FVSs), where an FVS is a set of nodes whose removal breaks all the cycles. Exact inference such as computing the marginal distributions and the partition function has complexity $O(k^{2}n)$  using message-passing algorithms, where k  is the size of the FVS, and n  is the total number of nodes. We propose efficient structure learning algorithms for two cases: 1) All nodes are observed, which is useful in modeling social or flight networks where the FVS nodes often correspond to a small number of high-degree nodes, or hubs, while the rest of the networks is modeled by a tree. Regardless of the maximum degree, without knowing the full graph structure, we can exactly compute the maximum likelihood estimate in $O(kn^2+n^2\\log n)$  if the FVS is known or in polynomial time if the FVS is unknown but has bounded size. 2) The FVS nodes are latent variables, where structure learning is equivalent to decomposing a inverse covariance matrix (exactly or approximately) into the sum of a tree-structured matrix and a low-rank matrix. By incorporating efficient inference into the learning steps, we can obtain a learning algorithm using alternating low-rank correction with complexity $O(kn^{2}+n^{2}\\log n)$  per iteration. We also perform experiments using both synthetic data as well as real data of flight delays to demonstrate the modeling capacity with FVSs of various sizes. We show that empirically the family of GGMs of size $O(\\log n)$ strikes a good balance between the modeling capacity and the efficiency.",
    "authors": [
      "Liu, Ying",
      "Willsky, Alan"
    ]
  },
  {
    "id": "1f50893f80d6830d62765ffad7721742",
    "title": "Extracting regions of interest from biological images with convolutional sparse block coding",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/1f50893f80d6830d62765ffad7721742-Paper.pdf",
    "abstract": "Biological tissue is often composed of cells with similar morphologies replicated throughout large volumes and many biological applications rely on the accurate identification of these cells and their locations from image data. Here we develop a generative model that captures the regularities present in images composed of repeating elements of a few different types. Formally, the model can be described as convolutional sparse block coding. For inference we use a variant of convolutional matching pursuit adapted to block-based representations. We extend the K-SVD learning algorithm to subspaces by retaining several principal vectors from the SVD decomposition instead of just one. Good models with little cross-talk between subspaces can be obtained by learning the blocks incrementally. We perform extensive experiments on simulated images and the inference algorithm consistently recovers a large proportion of the cells with a small number of false positives. We fit the convolutional model to noisy GCaMP6 two-photon images of spiking neurons and to Nissl-stained slices of cortical tissue and show that it recovers cell body locations without supervision. The flexibility of the block-based representation is reflected in the variability of the recovered cell shapes.",
    "authors": [
      "Pachitariu, Marius",
      "Packer, Adam M.",
      "Pettit, Noah",
      "Dalgleish, Henry",
      "Hausser, Michael",
      "Sahani, Maneesh"
    ]
  },
  {
    "id": "1ff8a7b5dc7a7d1f0ed65aaa29c04b1e",
    "title": "Training and Analysing Deep Recurrent Neural Networks",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/1ff8a7b5dc7a7d1f0ed65aaa29c04b1e-Paper.pdf",
    "abstract": "Time series often have a temporal hierarchy, with information that is spread out over multiple time scales. Common recurrent neural networks, however, do not explicitly accommodate such a hierarchy, and most research on them has been focusing on training algorithms rather than on their basic architecture. In this pa- per we study the effect of a hierarchy of recurrent neural networks on processing time series. Here, each layer is a recurrent network which receives the hidden state of the previous layer as input. This architecture allows us to perform hi- erarchical processing on difficult temporal tasks, and more naturally capture the structure of time series. We show that they reach state-of-the-art performance for recurrent networks in character-level language modelling when trained with sim- ple stochastic gradient descent. We also offer an analysis of the different emergent time scales.",
    "authors": [
      "Hermans, Michiel",
      "Schrauwen, Benjamin"
    ]
  },
  {
    "id": "2050e03ca119580f74cca14cc6e97462",
    "title": "Low-Rank Matrix and Tensor Completion via Adaptive Sampling",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/2050e03ca119580f74cca14cc6e97462-Paper.pdf",
    "abstract": "We study low rank matrix and tensor completion and propose novel algorithms that employ adaptive sampling schemes to obtain strong performance guarantees for these problems. Our algorithms exploit adaptivity to identify entries that are highly informative for identifying the column space of the matrix (tensor) and consequently, our results hold even when the row space is highly coherent, in contrast with previous analysis of matrix completion. In the absence of noise, we show that one can exactly recover a $n \\times n$ matrix of rank $r$ using $O(r^2 n \\log(r))$ observations, which is better than the best known bound under random sampling. We also show that one can recover an order $T$ tensor using $O(r^{2(T-1)}T^2 n \\log(r))$. For noisy recovery, we show that one can consistently estimate a low rank matrix corrupted with noise using $O(nr \\textrm{polylog}(n))$ observations. We complement our study with simulations that verify our theoretical guarantees and demonstrate the scalability of our algorithms.",
    "authors": [
      "Krishnamurthy, Akshay",
      "Singh, Aarti"
    ]
  },
  {
    "id": "20d135f0f28185b84a4cf7aa51f29500",
    "title": "Fast Determinantal Point Process Sampling with Application to Clustering",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/20d135f0f28185b84a4cf7aa51f29500-Paper.pdf",
    "abstract": "Determinantal Point Process (DPP) has gained much popularity for modeling sets of diverse items. The gist of DPP is that the probability of choosing a particular set of items is proportional to the determinant of a positive definite matrix that defines the similarity of those items. However, computing the determinant requires time cubic in the number of items, and is hence impractical for large sets. In this paper, we address this problem by constructing a rapidly mixing Markov chain, from which we can acquire a sample from the given DPP in sub-cubic time.  In addition, we show that this framework can be extended to sampling from cardinality-constrained DPPs. As an application, we show how our sampling algorithm can be used to provide a fast heuristic for determining the number of clusters, resulting in better clustering.",
    "authors": [
      "Kang, Byungkon"
    ]
  },
  {
    "id": "226d1f15ecd35f784d2a20c3ecf56d7f",
    "title": "Matrix factorization with binary components",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/226d1f15ecd35f784d2a20c3ecf56d7f-Paper.pdf",
    "abstract": "Motivated by an application in computational biology, we consider constrained low-rank matrix factorization problems with $\\{0,1\\}$-constraints on one of the factors. In addition to the the non-convexity shared with more general matrix factorization schemes, our problem is further complicated by a combinatorial constraint set of size $2^{m \\cdot r}$, where $m$ is the dimension of the data points and $r$ the rank of the factorization. Despite apparent intractability, we provide $-$in the line of recent work on non-negative matrix factorization by Arora et al.~(2012)$-$ an algorithm that provably recovers the underlying factorization in the exact case with operations of the order $O(m r 2^r + mnr)$ in the worst case. To obtain that result, we invoke theory centered around a fundamental result in combinatorics, the Littlewood-Offord lemma.",
    "authors": [
      "Slawski, Martin",
      "Hein, Matthias",
      "Lutsik, Pavlo"
    ]
  },
  {
    "id": "2291d2ec3b3048d1a6f86c2c4591b7e0",
    "title": "Reshaping Visual Datasets for Domain Adaptation",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/2291d2ec3b3048d1a6f86c2c4591b7e0-Paper.pdf",
    "abstract": "In visual recognition problems, the common data distribution mismatches between training and testing make domain adaptation essential. However, image data is difficult to manually divide into the discrete domains required by adaptation algorithms, and the standard practice of equating datasets with domains is a weak proxy for all the real conditions that alter the statistics in complex ways (lighting, pose, background, resolution, etc.) We propose an approach to automatically discover latent domains in image or video datasets. Our formulation imposes two key properties on domains: maximum distinctiveness and maximum learnability. By maximum distinctiveness, we require the underlying distributions of the identified domains to be different from each other; by maximum learnability, we ensure that a strong discriminative model can be learned from the domain. We devise a nonparametric representation and efficient optimization procedure for distinctiveness, which, when coupled with our learnability constraint, can successfully discover domains among both training and test data. We extensively evaluate our approach on object recognition and human activity recognition tasks.",
    "authors": [
      "Gong, Boqing",
      "Grauman, Kristen",
      "Sha, Fei"
    ]
  },
  {
    "id": "22fb0cee7e1f3bde58293de743871417",
    "title": "Perfect Associative Learning with Spike-Timing-Dependent Plasticity",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/22fb0cee7e1f3bde58293de743871417-Paper.pdf",
    "abstract": "Recent extensions of the Perceptron, as e.g. the Tempotron, suggest that this theoretical concept is highly relevant also for understanding networks of spiking neurons in the brain. It is not known, however, how the computational power of the Perceptron and of its variants might be accomplished by the plasticity mechanisms of real synapses. Here we prove that spike-timing-dependent plasticity having an anti-Hebbian form for excitatory synapses as well as a spike-timing-dependent plasticity of Hebbian shape for inhibitory synapses are sufficient for realizing the original Perceptron Learning Rule if the respective plasticity mechanisms act in concert with the hyperpolarisation of the post-synaptic neurons. We also show that with these simple yet biologically realistic dynamics Tempotrons are efficiently learned. The proposed mechanism might underly the acquisition of mappings of spatio-temporal activity patterns in one area of the brain onto other spatio-temporal spike patterns in another region and of long term memories in cortex. Our results underline that learning processes in realistic networks of spiking neurons depend crucially on the interactions of synaptic plasticity mechanisms with the dynamics of participating neurons.",
    "authors": [
      "Albers, Christian",
      "Westkott, Maren",
      "Pawelzik, Klaus"
    ]
  },
  {
    "id": "233509073ed3432027d48b1a83f5fbd2",
    "title": "Tracking Time-varying Graphical Structure",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/233509073ed3432027d48b1a83f5fbd2-Paper.pdf",
    "abstract": "Structure learning algorithms for graphical models have focused almost exclusively on stable environments in which the underlying generative process does not change; that is, they assume that the generating model is globally stationary. In real-world environments, however, such changes often occur without warning or signal. Real-world data often come from generating models that are only locally stationary. In this paper, we present LoSST, a novel, heuristic structure learning algorithm that tracks changes in graphical model structure or parameters in a dynamic, real-time manner. We show by simulation that the algorithm performs comparably to batch-mode learning when the generating graphical structure is globally stationary, and significantly better when it is only locally stationary.",
    "authors": [
      "Kummerfeld, Erich",
      "Danks, David"
    ]
  },
  {
    "id": "242c100dc94f871b6d7215b868a875f8",
    "title": "Phase Retrieval using Alternating Minimization",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/242c100dc94f871b6d7215b868a875f8-Paper.pdf",
    "abstract": "Phase retrieval problems involve solving linear equations, but with missing sign (or phase, for complex numbers) information. Over the last two decades, a popular generic empirical approach to the many variants of this problem has been one of alternating minimization; i.e. alternating between estimating the missing phase information, and the candidate solution. In this paper, we show that a simple alternating minimization algorithm geometrically converges to the solution of one such problem -- finding a vector $x$ from $y,A$, where $y = |A'x|$ and $|z|$ denotes a vector of element-wise magnitudes of $z$ -- under the assumption that $A$ is Gaussian.  Empirically, our algorithm performs similar to recently proposed convex techniques for this variant (which are based on lifting\" to a convex matrix problem) in sample complexity and robustness to noise. However, our algorithm is much more efficient and can scale to large problems. Analytically, we show geometric convergence to the solution, and sample complexity that is off by log factors from obvious lower bounds. We also establish close to optimal scaling for the case when the unknown vector is sparse. Our work represents the only known proof of alternating minimization for any variant of phase retrieval problems in the non-convex setting.\"",
    "authors": [
      "Netrapalli, Praneeth",
      "Jain, Prateek",
      "Sanghavi, Sujay"
    ]
  },
  {
    "id": "24681928425f5a9133504de568f5f6df",
    "title": "Unsupervised Structure Learning of Stochastic And-Or Grammars",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/24681928425f5a9133504de568f5f6df-Paper.pdf",
    "abstract": "Stochastic And-Or grammars compactly represent both compositionality and reconfigurability and have been used to model different types of data such as images and events. We present a unified formalization of stochastic And-Or grammars that is agnostic to the type of the data being modeled, and propose an unsupervised approach to learning the structures as well as the parameters of such grammars. Starting from a trivial initial grammar, our approach iteratively induces compositions and reconfigurations in a unified manner and optimizes the posterior probability of the grammar. In our empirical evaluation, we applied our approach to learning event grammars and image grammars and achieved comparable or better performance than previous approaches.",
    "authors": [
      "Tu, Kewei",
      "Pavlovskaia, Maria",
      "Zhu, Song-Chun"
    ]
  },
  {
    "id": "26337353b7962f533d78c762373b3318",
    "title": "Learning Multi-level Sparse Representations",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/26337353b7962f533d78c762373b3318-Paper.pdf",
    "abstract": "Bilinear approximation of a matrix is a powerful paradigm of unsupervised learning. In some applications, however, there is a natural hierarchy of concepts that ought to be reflected in the unsupervised analysis. For example, in the neurosciences image sequence considered here, there are the semantic concepts of pixel $\\rightarrow$ neuron $\\rightarrow$ assembly that should find their counterpart in the unsupervised analysis. Driven by this concrete problem, we propose a decomposition of the matrix of observations into a product of more than two sparse matrices, with the rank decreasing from lower to higher levels. In contrast to prior work, we allow for both hierarchical and heterarchical relations of lower-level to higher-level concepts. In addition, we learn the nature of these relations rather than imposing them. Finally, we describe an optimization scheme that allows to optimize the decomposition over all levels jointly, rather than in a greedy level-by-level fashion.   The proposed bilevel SHMF (sparse heterarchical matrix factorization) is the first formalism that allows to simultaneously interpret a calcium imaging sequence in terms of the constituent neurons, their membership in assemblies, and the time courses of both neurons and assemblies.  Experiments show that the proposed model fully recovers the structure from difficult synthetic data designed to imitate the experimental data. More importantly, bilevel SHMF yields plausible interpretations of real-world Calcium imaging data.",
    "authors": [
      "Diego Andilla, Ferran",
      "Hamprecht, Fred A."
    ]
  },
  {
    "id": "2812e5cf6d8f21d69c91dddeefb792a7",
    "title": "Estimation, Optimization, and Parallelism when Data is Sparse",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/2812e5cf6d8f21d69c91dddeefb792a7-Paper.pdf",
    "abstract": "We study stochastic optimization problems when the \\emph{data} is sparse,  which is in a sense dual to the current understanding of high-dimensional statistical learning and optimization. We highlight both the difficulties---in terms of increased sample complexity that sparse data necessitates---and the potential benefits, in terms of allowing parallelism and asynchrony in the design of algorithms. Concretely, we derive matching upper and lower bounds on the minimax rate for optimization and learning with sparse data, and we exhibit algorithms achieving these rates. Our algorithms are adaptive: they achieve the best possible rate for the data observed. We also show how leveraging sparsity leads to (still minimax optimal) parallel and asynchronous algorithms, providing experimental evidence complementing our theoretical results on medium to large-scale learning tasks.",
    "authors": [
      "Duchi, John",
      "Jordan, Michael I.",
      "McMahan, Brendan"
    ]
  },
  {
    "id": "28267ab848bcf807b2ed53c3a8f8fc8a",
    "title": "Predictive PAC Learning and Process Decompositions",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/28267ab848bcf807b2ed53c3a8f8fc8a-Paper.pdf",
    "abstract": "We informally call a stochastic process learnable if it admits a generalization error approaching zero in probability for any concept class with finite VC-dimension (IID processes are the simplest example).  A mixture of learnable processes need not be learnable itself, and certainly its generalization error need not decay at the same rate. In this paper, we argue that it is natural in predictive PAC to condition not on the past observations but on the mixture component of the sample path. This definition not only matches what a realistic learner might demand, but also allows us to sidestep several otherwise grave problems in learning from dependent data. In particular, we give a novel PAC generalization bound for mixtures of learnable processes with a generalization error that is not worse than that of each mixture component.  We also provide a characterization of mixtures of absolutely regular ($\\beta$-mixing) processes, of independent interest.",
    "authors": [
      "Shalizi, Cosma",
      "Kontorovich, Aryeh"
    ]
  },
  {
    "id": "285f89b802bcb2651801455c86d78f2a",
    "title": "Scalable Inference for Logistic-Normal Topic Models",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/285f89b802bcb2651801455c86d78f2a-Paper.pdf",
    "abstract": "Logistic-normal topic models can effectively discover correlation structures among latent topics. However, their inference remains a challenge because of the non-conjugacy between the logistic-normal prior and multinomial topic mixing proportions. Existing algorithms either make restricting mean-field assumptions or are not scalable to large-scale applications. This paper presents a partially collapsed Gibbs sampling algorithm that approaches the provably correct distribution by exploring the ideas of data augmentation. To improve time efficiency, we further present a parallel implementation that can deal with large-scale applications and learn the correlation structures of thousands of topics from millions of documents. Extensive empirical results demonstrate the promise.",
    "authors": [
      "Chen, Jianfei",
      "Zhu, Jun",
      "Wang, Zi",
      "Zheng, Xun",
      "Zhang, Bo"
    ]
  },
  {
    "id": "286674e3082feb7e5afb92777e48821f",
    "title": "A multi-agent control framework for co-adaptation in brain-computer interfaces",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/286674e3082feb7e5afb92777e48821f-Paper.pdf",
    "abstract": "In a closed-loop brain-computer interface (BCI), adaptive decoders are used to learn parameters suited to decoding the user's neural response. Feedback to the user provides information which permits the neural tuning to also adapt. We present an approach to model this process of co-adaptation between the encoding model of the neural signal and the decoding algorithm as a multi-agent formulation of the linear quadratic Gaussian (LQG) control problem. In simulation we characterize how decoding performance improves as the neural encoding and adaptive decoder optimize, qualitatively resembling experimentally demonstrated closed-loop improvement. We then propose a novel, modified decoder update rule which is aware of the fact that the encoder is also changing and show it can improve simulated co-adaptation dynamics. Our modeling approach offers promise for gaining insights into co-adaptation as well as improving user learning of BCI control in practical settings.",
    "authors": [
      "Merel, Josh S.",
      "Fox, Roy",
      "Jebara, Tony",
      "Paninski, Liam"
    ]
  },
  {
    "id": "28f0b864598a1291557bed248a998d4e",
    "title": "Conditional Random Fields via Univariate Exponential Families",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/28f0b864598a1291557bed248a998d4e-Paper.pdf",
    "abstract": "Conditional random fields, which model the distribution of a multivariate response conditioned on a set of covariates using undirected graphs, are widely used in a variety of multivariate prediction applications. Popular instances of this class of models such as categorical-discrete CRFs, Ising CRFs, and conditional Gaussian based CRFs, are not however best suited to the varied types of response variables in many applications, including count-valued responses. We thus introduce a \u201cnovel subclass of CRFs\u201d, derived by imposing node-wise conditional distributions of response variables conditioned on the rest of the responses and the covariates as arising from univariate exponential families.  This allows us to derive novel multivariate CRFs given any univariate exponential distribution, including the Poisson, negative binomial, and exponential distributions. Also in particular, it addresses the common CRF problem of specifying feature'' functions determining the interactions between response variables and covariates. We develop a class of tractable penalized $M$-estimators to learn these CRF distributions from data, as well as a unified sparsistency analysis for this general class of CRFs showing exact structure recovery can be achieved with high probability.\"",
    "authors": [
      "Yang, Eunho",
      "Ravikumar, Pradeep K.",
      "Allen, Genevera I.",
      "Liu, Zhandong"
    ]
  },
  {
    "id": "28fc2782ea7ef51c1104ccf7b9bea13d",
    "title": "Adaptivity to Local Smoothness and Dimension in Kernel Regression",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/28fc2782ea7ef51c1104ccf7b9bea13d-Paper.pdf",
    "abstract": "We present the first result for kernel regression where the procedure adapts locally at a point $x$ to both the unknown local dimension of the metric and the unknown H\\{o}lder-continuity of the regression function at $x$. The result holds with high probability simultaneously at all points $x$ in a metric space of unknown structure.\"",
    "authors": [
      "Kpotufe, Samory",
      "Garg, Vikas"
    ]
  },
  {
    "id": "291597a100aadd814d197af4f4bab3a7",
    "title": "Online Learning with Costly Features and Labels",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/291597a100aadd814d197af4f4bab3a7-Paper.pdf",
    "abstract": "This paper introduces the online probing\" problem: In each round, the learner is able to purchase the values of a subset of feature values. After the learner uses this information to come up with a prediction for the given round, he then has the option of paying for seeing the loss that he is evaluated against. Either way, the learner pays for the imperfections of his predictions and whatever he chooses to observe, including the cost of observing the loss function for the given round and the cost of the observed features. We consider two variations of this problem, depending on whether the learner can observe the label for free or not. We provide algorithms and upper and lower bounds on the regret for both variants. We show that a positive cost for observing the label significantly increases the regret of the problem.\"",
    "authors": [
      "Zolghadr, Navid",
      "Bartok, Gabor",
      "Greiner, Russell",
      "Gy\u00f6rgy, Andr\u00e1s",
      "Szepesvari, Csaba"
    ]
  },
  {
    "id": "2a50e9c2d6b89b95bcb416d6857f8b45",
    "title": "An Approximate, Efficient LP Solver for LP Rounding",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/2a50e9c2d6b89b95bcb416d6857f8b45-Paper.pdf",
    "abstract": "Many problems in machine learning can be solved by rounding the solution of an appropriate linear program. We propose a scheme that is based on a quadratic program relaxation which allows us to use parallel stochastic-coordinate-descent to approximately solve large linear programs efficiently. Our software is an order of magnitude faster than Cplex (a commercial linear programming solver) and yields similar solution quality. Our results include a novel perturbation analysis of a quadratic-penalty formulation of linear programming and a convergence result, which we use to derive running time and quality guarantees.",
    "authors": [
      "Sridhar, Srikrishna",
      "Wright, Stephen",
      "Re, Christopher",
      "Liu, Ji",
      "Bittorf, Victor",
      "Zhang, Ce"
    ]
  },
  {
    "id": "2a9d121cd9c3a1832bb6d2cc6bd7a8a7",
    "title": "Regression-tree Tuning in a Streaming Setting",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/2a9d121cd9c3a1832bb6d2cc6bd7a8a7-Paper.pdf",
    "abstract": "We consider the problem of maintaining the data-structures of a partition-based regression procedure in a setting where the training data arrives sequentially over time. We prove that it is possible to maintain such a structure in time $O(\\log n)$ at any time step $n$ while achieving a nearly-optimal regression rate of $\\tilde{O}(n^{-2/(2+d)})$ in terms of the unknown metric dimension $d$. Finally we prove a new regression lower-bound which is independent of a given data size, and hence is more appropriate for the streaming setting.",
    "authors": [
      "Kpotufe, Samory",
      "Orabona, Francesco"
    ]
  },
  {
    "id": "2b8a61594b1f4c4db0902a8a395ced93",
    "title": "Estimating LASSO Risk and Noise Level",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/2b8a61594b1f4c4db0902a8a395ced93-Paper.pdf",
    "abstract": "We study the fundamental problems of variance and risk estimation in high dimensional statistical modeling. In particular, we consider the problem of learning a coefficient vector $\\theta_0\\in R^p$ from noisy linear observation $y=X\\theta_0+w\\in R^n$ and the popular estimation procedure of solving an $\\ell_1$-penalized least squares objective known as the LASSO or Basis Pursuit DeNoising (BPDN). In this context, we develop new estimators for the $\\ell_2$ estimation risk $\\|\\hat{\\theta}-\\theta_0\\|_2$ and the variance of the noise. These can be used to select the regularization parameter optimally. Our approach combines Stein unbiased risk estimate (Stein'81) and recent results of (Bayati and Montanari'11-12) on the analysis of approximate message passing and risk of LASSO.  We establish high-dimensional consistency of our estimators for sequences of matrices $X$ of increasing dimensions, with independent Gaussian entries. We establish validity for a broader class of Gaussian designs, conditional on the validity of a certain conjecture from statistical physics.  Our approach is the first that provides an asymptotically consistent risk estimator. In addition, we demonstrate through simulation that our variance estimation outperforms several existing methods in the literature.",
    "authors": [
      "Bayati, Mohsen",
      "Erdogdu, Murat A.",
      "Montanari, Andrea"
    ]
  },
  {
    "id": "2bcab9d935d219641434683dd9d18a03",
    "title": "Demixing odors - fast inference in olfaction",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/2bcab9d935d219641434683dd9d18a03-Paper.pdf",
    "abstract": "The olfactory system faces a difficult inference problem: it has to determine what odors are present based on the distributed activation of its receptor neurons. Here we derive neural implementations of two approximate inference algorithms that could be used by the brain. One is a variational algorithm (which builds on the work of Beck. et al., 2012), the other is based on sampling. Importantly, we use a more realistic prior distribution over odors than has been used in the past: we use a spike and slab'' prior, for which most odors have zero concentration. After mapping the two algorithms onto neural dynamics, we find that both can infer correct odors in less than 100 ms, although it takes ~500 ms to eliminate false positives. Thus, at the behavioral level, the two algorithms make very similar predictions. However, they make different assumptions about connectivity and neural computations, and make different predictions about neural activity. Thus, they should be distinguishable experimentally.  If so, that would provide insight into the mechanisms employed by the olfactory system, and, because the two algorithms use very different coding strategies, that would also provide insight into how networks represent probabilities.\"",
    "authors": [
      "Grabska-Barwinska, Agnieszka",
      "Beck, Jeff",
      "Pouget, Alexandre",
      "Latham, Peter"
    ]
  },
  {
    "id": "2d6cc4b2d139a53512fb8cbb3086ae2e",
    "title": "Zero-Shot Learning Through Cross-Modal Transfer",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/2d6cc4b2d139a53512fb8cbb3086ae2e-Paper.pdf",
    "abstract": "This work introduces a model that can recognize objects in images even if no training data is available for the object class. The only necessary knowledge about unseen categories comes from unsupervised text corpora.  Unlike previous zero-shot learning models, which can only differentiate between unseen classes, our model can operate on a mixture of objects, simultaneously obtaining state of the art performance on classes with thousands of training images and reasonable performance on unseen classes.  This is achieved by seeing the distributions of words in texts as a semantic space for understanding what objects look like. Our deep learning model does not require any manually defined semantic or visual features for either words or images.  Images are mapped to be close to semantic word vectors corresponding to their classes, and the resulting image embeddings can be used to distinguish whether an image is of a seen or unseen class. Then, a separate recognition model can be employed for each type. We demonstrate two strategies, the first gives high accuracy on unseen classes, while the second is conservative in its prediction of novelty and keeps the seen classes' accuracy high.",
    "authors": [
      "Socher, Richard",
      "Ganjoo, Milind",
      "Manning, Christopher D.",
      "Ng, Andrew"
    ]
  },
  {
    "id": "2dace78f80bc92e6d7493423d729448e",
    "title": "Optimistic policy iteration and natural actor-critic: A unifying view and a non-optimality result",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/2dace78f80bc92e6d7493423d729448e-Paper.pdf",
    "abstract": "Approximate dynamic programming approaches to the reinforcement learning problem are often categorized into greedy value function methods and value-based policy gradient methods. As our first main result, we show that an important subset of the latter methodology is, in fact, a limiting special case of a general formulation of the former methodology; optimistic policy iteration encompasses not only most of the greedy value function methods but also natural actor-critic methods, and permits one to directly interpolate between them. The resulting continuum adjusts the strength of the Markov assumption in policy improvement and, as such, can be seen as dual in spirit to the continuum in TD($\\lambda$)-style algorithms in policy evaluation. As our second main result, we show for a substantial subset of soft-greedy value function approaches that, while having the potential to avoid policy oscillation and policy chattering, this subset can never converge toward any optimal policy, except in a certain pathological case. Consequently, in the context of approximations, the majority of greedy value function methods seem to be deemed to suffer either from the risk of oscillation/chattering or from the presence of systematic sub-optimality.",
    "authors": [
      "Wagner, Paul"
    ]
  },
  {
    "id": "2dffbc474aa176b6dc957938c15d0c8b",
    "title": "Bayesian Inference and Learning in Gaussian Process State-Space Models with Particle MCMC",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/2dffbc474aa176b6dc957938c15d0c8b-Paper.pdf",
    "abstract": "State-space models are successfully used in many areas of science, engineering and economics to model time series and dynamical systems. We present a fully Bayesian approach to inference and learning in nonlinear nonparametric state-space models. We place a Gaussian process prior over the transition dynamics, resulting in a flexible model able to capture complex dynamical phenomena. However, to enable efficient inference, we marginalize over the dynamics of the model and instead infer directly the joint smoothing distribution through the use of specially tailored Particle Markov Chain Monte Carlo samplers. Once an approximation of the smoothing distribution is computed, the state transition predictive distribution can be formulated analytically. We make use of sparse Gaussian process models to greatly reduce the computational complexity of the approach.",
    "authors": [
      "Frigola, Roger",
      "Lindsten, Fredrik",
      "Sch\u00f6n, Thomas B.",
      "Rasmussen, Carl Edward"
    ]
  },
  {
    "id": "309928d4b100a5d75adff48a9bfc1ddb",
    "title": "Stochastic Gradient Riemannian Langevin Dynamics on the Probability Simplex",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/309928d4b100a5d75adff48a9bfc1ddb-Paper.pdf",
    "abstract": "In this paper we investigate the use of Langevin Monte Carlo methods on the probability simplex and propose a new method, Stochastic gradient Riemannian Langevin dynamics, which is simple to implement and can be applied online. We apply this method to latent Dirichlet allocation in an online setting, and demonstrate that it achieves substantial performance improvements to the state of the art online variational Bayesian methods.",
    "authors": [
      "Patterson, Sam",
      "Teh, Yee Whye"
    ]
  },
  {
    "id": "31b3b31a1c2f8a370206f111127c0dbd",
    "title": "When are Overcomplete Topic Models Identifiable? Uniqueness of Tensor Tucker Decompositions with Structured Sparsity",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/31b3b31a1c2f8a370206f111127c0dbd-Paper.pdf",
    "abstract": "Overcomplete latent representations have been very popular for unsupervised feature learning in recent years. In this paper, we specify which overcomplete models can be identified given observable moments of a certain order. We consider   probabilistic admixture or topic models  in the overcomplete regime, where the number of latent topics can greatly exceed the size of the observed word vocabulary. While   general  overcomplete topic models are not   identifiable, we establish {\\em generic} identifiability under  a  constraint, referred to as  {\\em topic persistence}. Our sufficient conditions for identifiability involve a novel set of higher order'' expansion conditions on   the {\\em topic-word matrix} or the {\\em population structure}   of the   model. This set of higher-order expansion conditions allow for overcomplete models, and require  the existence of a perfect  matching from latent topics to higher order observed words. We establish that random structured topic models are identifiable w.h.p. in the overcomplete regime. Our identifiability results allow for   general (non-degenerate) distributions for modeling the topic proportions, and thus, we can handle arbitrarily correlated topics in our framework.  Our identifiability results imply uniqueness of a class of tensor decompositions with structured sparsity which is contained in the class of {\\em Tucker} decompositions, but is more general than the {\\em Candecomp/Parafac} (CP)   decomposition.\"",
    "authors": [
      "Anandkumar, Anima",
      "Hsu, Daniel J.",
      "Janzamin, Majid",
      "Kakade, Sham M."
    ]
  },
  {
    "id": "3210ddbeaa16948a702b6049b8d9a202",
    "title": "Sign Cauchy Projections and Chi-Square Kernel",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/3210ddbeaa16948a702b6049b8d9a202-Paper.pdf",
    "abstract": "The method of Cauchy random projections is popular  for computing the $l_1$ distance in high dimension. In this paper, we propose to use only the signs of the projected data and show that the  probability of collision (i.e., when the two signs differ) can be accurately approximated as a function of the chi-square ($\\chi^2$) similarity, which is a popular  measure for nonnegative data (e.g., when features are generated from histograms as common in text and vision applications). Our experiments   confirm that this method of sign Cauchy random projections is promising for large-scale  learning applications. Furthermore, we extend the idea to sign $\\alpha$-stable random projections and derive a bound of the collision probability.",
    "authors": [
      "Li, Ping",
      "Samorodnitsk, Gennady",
      "Hopcroft, John"
    ]
  },
  {
    "id": "3295c76acbf4caaed33c36b1b5fc2cb1",
    "title": "Transfer Learning in a Transductive Setting",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/3295c76acbf4caaed33c36b1b5fc2cb1-Paper.pdf",
    "abstract": "Category models for objects or activities typically rely on supervised learning requiring sufficiently large training sets. Transferring knowledge from known categories to novel classes with no or only a few labels however is far less researched even though it is a common scenario. In this work, we extend transfer learning with semi-supervised learning to exploit unlabeled instances of (novel) categories with no or only a few labeled instances. Our proposed approach Propagated Semantic Transfer combines three main ingredients. First, we transfer information from known to novel categories by incorporating external knowledge, such as linguistic or expert-specified information, e.g., by a mid-level layer of semantic attributes. Second, we exploit the manifold structure of novel classes. More specifically we adapt a graph-based learning algorithm - so far only used for semi-supervised learning - to zero-shot and few-shot learning. Third, we improve the local neighborhood in such graph structures by replacing the raw feature-based representation with a mid-level object- or attribute-based representation. We evaluate our approach on three challenging datasets in two different applications, namely on Animals with Attributes and ImageNet for image classification and on MPII Composites for activity recognition. Our approach consistently outperforms state-of-the-art transfer and semi-supervised approaches on all datasets.",
    "authors": [
      "Rohrbach, Marcus",
      "Ebert, Sandra",
      "Schiele, Bernt"
    ]
  },
  {
    "id": "32b30a250abd6331e03a2a1f16466346",
    "title": "Solving inverse problem of Markov chain with partial observations",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/32b30a250abd6331e03a2a1f16466346-Paper.pdf",
    "abstract": "The Markov chain is a convenient tool to represent the dynamics of complex systems such as traffic and social systems, where probabilistic transition takes place between internal states. A Markov chain is characterized by initial-state probabilities and a state-transition probability matrix. In the traditional setting, a major goal is to figure out properties of a Markov chain when those probabilities are known. This paper tackles an inverse version of the problem: we find those probabilities from partial observations at a limited number of states. The observations include the frequency of visiting a state and the rate of reaching a state from another. Practical examples of this task include traffic monitoring systems in cities, where we need to infer the traffic volume on every single link on a road network from a very limited number of observation points. We formulate this task as a regularized optimization problem for probability functions, which is efficiently solved using the notion of natural gradient. Using synthetic and real-world data sets including city traffic monitoring data, we demonstrate the effectiveness of our method.",
    "authors": [
      "Morimura, Tetsuro",
      "Osogami, Takayuki",
      "Ide, Tsuyoshi"
    ]
  },
  {
    "id": "33e8075e9970de0cfea955afd4644bb2",
    "title": "Wavelets on Graphs via Deep Learning",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/33e8075e9970de0cfea955afd4644bb2-Paper.pdf",
    "abstract": "An increasing number of applications require processing of signals defined on weighted graphs. While wavelets provide a flexible tool for signal processing in the classical setting of regular domains, the existing graph wavelet constructions are less flexible -- they are guided solely by the structure of the underlying graph and do not take directly into consideration the particular class of signals to be processed. This paper introduces a machine learning framework for constructing graph wavelets that can sparsely represent a given class of signals. Our construction uses the lifting scheme, and is based on the observation that the recurrent nature of the lifting scheme gives rise to a structure resembling a deep auto-encoder network. Particular properties that the resulting wavelets must satisfy determine the training objective and the structure of the involved neural networks. The training is unsupervised, and is conducted similarly to the greedy pre-training of a stack of auto-encoders. After training is completed, we obtain a linear wavelet transform that can be applied to any graph signal in time and memory linear in the size of the graph. Improved sparsity of our wavelet transform for the test signals is confirmed via experiments both on synthetic and real data.",
    "authors": [
      "Rustamov, Raif",
      "Guibas, Leonidas J."
    ]
  },
  {
    "id": "3493894fa4ea036cfc6433c3e2ee63b0",
    "title": "Stochastic Convex Optimization with Multiple  Objectives",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/3493894fa4ea036cfc6433c3e2ee63b0-Paper.pdf",
    "abstract": "In this paper, we are interested in the development of efficient algorithms  for convex optimization problems in the simultaneous presence of multiple objectives and stochasticity in the first-order information.   We  cast the stochastic multiple objective optimization problem into a constrained optimization problem by choosing one function as the objective and try to bound other objectives by appropriate thresholds.  We first examine  a two stages exploration-exploitation based algorithm which first approximates  the stochastic objectives by sampling  and  then solves a constrained stochastic optimization problem by projected gradient method. This method  attains a  suboptimal  convergence rate  even under  strong assumption on the objectives. Our second approach is an efficient primal-dual stochastic algorithm. It leverages on the theory of Lagrangian method in constrained optimization and attains  the optimal convergence rate of $[O(1/ \\sqrt{T})]$ in high probability for general Lipschitz continuous objectives.",
    "authors": [
      "Mahdavi, Mehrdad",
      "Yang, Tianbao",
      "Jin, Rong"
    ]
  },
  {
    "id": "35cf8659cfcb13224cbd47863a34fc58",
    "title": "Bayesian Hierarchical Community Discovery",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/35cf8659cfcb13224cbd47863a34fc58-Paper.pdf",
    "abstract": "We propose an efficient Bayesian nonparametric model for discovering hierarchical community structure in social networks. Our model is a tree-structured mixture of potentially exponentially many stochastic blockmodels. We describe a family of greedy agglomerative model selection algorithms whose worst case scales quadratically in the number of vertices of the network, but independent of the number of communities. Our algorithms are two orders of magnitude faster than the infinite relational model, achieving comparable or better accuracy.",
    "authors": [
      "Blundell, Charles",
      "Teh, Yee Whye"
    ]
  },
  {
    "id": "36a16a2505369e0c922b6ea7a23a56d2",
    "title": "Contrastive Learning Using Spectral Methods",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/36a16a2505369e0c922b6ea7a23a56d2-Paper.pdf",
    "abstract": "In many natural settings, the analysis goal is not to characterize a single data set in isolation, but rather to understand the difference between one set of observations and another. For example, given a background corpus of news articles together with writings of a particular author, one may want a topic model that explains word patterns and themes specific to the author. Another example comes from genomics, in which biological signals may be collected from different regions of a genome, and one wants a model that captures the differential statistics observed in these regions. This paper formalizes this notion of contrastive learning for  mixture models, and develops spectral algorithms for inferring mixture components specific to a foreground data set when contrasted with a background data set. The method builds on recent moment-based estimators and tensor decompositions for latent variable models, and has the intuitive feature of using background data statistics to appropriately modify moments estimated from foreground data. A key advantage of the method is that the background data need only be coarsely modeled, which is important when the background is too complex, noisy, or not of interest. The method is demonstrated on applications in contrastive topic modeling and genomic sequence analysis.",
    "authors": [
      "Zou, James Y.",
      "Hsu, Daniel J.",
      "Parkes, David C.",
      "Adams, Ryan P."
    ]
  },
  {
    "id": "37a749d808e46495a8da1e5352d03cae",
    "title": "Deep Fisher Networks for Large-Scale Image Classification",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/37a749d808e46495a8da1e5352d03cae-Paper.pdf",
    "abstract": "As massively parallel computations have become broadly available with modern GPUs, deep architectures trained on very large datasets have risen in popularity. Discriminatively trained convolutional neural networks, in particular, were recently shown to yield state-of-the-art performance in challenging image classification benchmarks such as ImageNet. However, elements of these architectures are similar to standard hand-crafted representations used in computer vision. In this paper, we explore the extent of this analogy, proposing a version of the state-of-the-art Fisher vector image encoding that can be stacked in multiple layers. This architecture significantly improves on standard Fisher vectors, and obtains competitive results with deep convolutional networks at a significantly smaller computational cost. Our hybrid architecture allows us to measure the performance improvement brought by a deeper image classification pipeline, while staying in the realms of conventional SIFT features and FV encodings.",
    "authors": [
      "Simonyan, Karen",
      "Vedaldi, Andrea",
      "Zisserman, Andrew"
    ]
  },
  {
    "id": "37f0e884fbad9667e38940169d0a3c95",
    "title": "Linear Convergence with Condition Number Independent Access of Full Gradients",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/37f0e884fbad9667e38940169d0a3c95-Paper.pdf",
    "abstract": "For smooth and strongly convex optimization, the optimal iteration complexity of the gradient-based algorithm is $O(\\sqrt{\\kappa}\\log 1/\\epsilon)$, where $\\kappa$ is the conditional number. In the case that the optimization problem is ill-conditioned, we need to evaluate a larger number of full gradients, which could be computationally expensive. In this paper, we propose to reduce the number of full gradient required by allowing the algorithm to access the stochastic gradients of the objective function. To this end, we present a novel algorithm named Epoch Mixed Gradient Descent (EMGD) that is able to utilize two kinds of gradients.  A distinctive step in EMGD is the mixed gradient descent, where we use an combination of the gradient and the stochastic gradient to update the intermediate solutions. By performing a fixed number of mixed gradient descents, we are able to improve the sub-optimality of the solution by a constant factor, and thus achieve a linear convergence rate. Theoretical analysis shows that EMGD is able to find an $\\epsilon$-optimal solution by computing $O(\\log 1/\\epsilon)$ full gradients and $O(\\kappa^2\\log 1/\\epsilon)$ stochastic gradients.",
    "authors": [
      "Zhang, Lijun",
      "Mahdavi, Mehrdad",
      "Jin, Rong"
    ]
  },
  {
    "id": "3871bd64012152bfb53fdf04b401193f",
    "title": "Learning with Noisy Labels",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/3871bd64012152bfb53fdf04b401193f-Paper.pdf",
    "abstract": "In this paper, we theoretically study the problem of binary classification in the presence of random classification noise --- the learner, instead of seeing the true labels, sees labels that have independently been flipped with some small probability. Moreover, random label noise is \\emph{class-conditional} --- the flip probability depends on the class. We provide two approaches to suitably modify any given surrogate loss function. First, we provide a simple unbiased estimator of any loss, and obtain performance bounds for empirical risk minimization in the presence of iid data with noisy labels. If the loss function satisfies a simple symmetry condition, we show that the method leads to an efficient algorithm for empirical minimization. Second, by leveraging a reduction of risk minimization under noisy labels to classification with weighted 0-1 loss, we suggest the use of a simple weighted surrogate loss, for which we are able to obtain strong empirical risk bounds. This approach has a very remarkable consequence --- methods used in practice such as biased SVM and weighted logistic regression are provably noise-tolerant. On a synthetic non-separable dataset, our methods achieve over 88\\% accuracy even when 40\\% of the labels are corrupted, and are competitive with respect to recently proposed methods for dealing with label noise in several benchmark datasets.",
    "authors": [
      "Natarajan, Nagarajan",
      "Dhillon, Inderjit S.",
      "Ravikumar, Pradeep K.",
      "Tewari, Ambuj"
    ]
  },
  {
    "id": "38af86134b65d0f10fe33d30dd76442e",
    "title": "Variational Policy Search via Trajectory Optimization",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/38af86134b65d0f10fe33d30dd76442e-Paper.pdf",
    "abstract": "In order to learn effective control policies for dynamical systems, policy search methods must be able to discover successful executions of the desired task. While random exploration can work well in simple domains, complex and high-dimensional tasks present a serious challenge, particularly when combined with high-dimensional policies that make parameter-space exploration infeasible. We present a method that uses trajectory optimization as a powerful exploration strategy that guides the policy search. A variational decomposition of a maximum likelihood policy objective allows us to use standard trajectory optimization algorithms such as differential dynamic programming, interleaved with standard supervised learning for the policy itself. We demonstrate that the resulting algorithm can outperform prior methods on two challenging locomotion tasks.",
    "authors": [
      "Levine, Sergey",
      "Koltun, Vladlen"
    ]
  },
  {
    "id": "38db3aed920cf82ab059bfccbd02be6a",
    "title": "Dropout Training as Adaptive Regularization",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/38db3aed920cf82ab059bfccbd02be6a-Paper.pdf",
    "abstract": "Dropout and other feature noising schemes control overfitting by artificially corrupting the training data. For generalized linear models, dropout performs a form of adaptive regularization. Using this viewpoint, we show that the dropout regularizer is first-order equivalent to an $\\LII$ regularizer applied after scaling the features by an estimate of the inverse diagonal Fisher information matrix. We also establish a connection to AdaGrad, an online learner, and find that a close relative of AdaGrad operates by repeatedly solving linear dropout-regularized problems. By casting dropout as regularization, we develop a natural semi-supervised algorithm that uses unlabeled data to create a better adaptive regularizer. We apply this idea to document classification tasks, and show that it consistently boosts the performance of dropout training, improving on state-of-the-art results on the IMDB reviews dataset.",
    "authors": [
      "Wager, Stefan",
      "Wang, Sida",
      "Liang, Percy S."
    ]
  },
  {
    "id": "39461a19e9eddfb385ea76b26521ea48",
    "title": "Prior-free and prior-dependent regret bounds for Thompson Sampling",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/39461a19e9eddfb385ea76b26521ea48-Paper.pdf",
    "abstract": "We consider the stochastic multi-armed bandit problem with a prior distribution on the reward distributions. We are interested in studying prior-free and prior-dependent regret bounds, very much in the same spirit than the usual distribution-free and distribution-dependent bounds for the non-Bayesian stochastic bandit. We first show that Thompson Sampling attains an optimal prior-free bound in the sense that for any prior distribution its Bayesian regret is bounded from above by $14 \\sqrt{n K}$. This result is unimprovable in the sense that there exists a prior distribution such that any algorithm has a Bayesian regret bounded from below by $\\frac{1}{20} \\sqrt{n K}$. We also study the case of priors for the setting of Bubeck et al. [2013] (where the optimal mean is known as well as a lower bound on the smallest gap) and we show that in this case the regret of Thompson Sampling is in fact uniformly bounded over time, thus showing that Thompson Sampling can greatly take advantage of the nice properties of these priors.",
    "authors": [
      "Bubeck, Sebastien",
      "Liu, Che-Yu"
    ]
  },
  {
    "id": "3948ead63a9f2944218de038d8934305",
    "title": "Geometric optimisation on positive definite matrices for elliptically contoured distributions",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/3948ead63a9f2944218de038d8934305-Paper.pdf",
    "abstract": "Hermitian positive definite matrices (HPD) recur throughout statistics and machine learning. In this paper we develop \\emph{geometric optimisation} for globally optimising certain nonconvex loss functions arising in the modelling of data via elliptically contoured distributions (ECDs). We exploit the remarkable structure of the convex cone of positive definite matrices which allows one to uncover hidden geodesic convexity of objective functions that are nonconvex in the ordinary Euclidean sense. Going even beyond manifold convexity we show how further metric properties of HPD matrices can be exploited to globally optimise several ECD log-likelihoods that are not even geodesic convex. We present key results that help recognise this geometric structure, as well as obtain efficient fixed-point algorithms to optimise the corresponding objective functions. To our knowledge, ours are the most general results on geometric optimisation of HPD matrices known so far. Experiments reveal the benefits of our approach---it avoids any eigenvalue computations which makes it very competitive.",
    "authors": [
      "Sra, Suvrit",
      "Hosseini, Reshad"
    ]
  },
  {
    "id": "39e4973ba3321b80f37d9b55f63ed8b8",
    "title": "Capacity of strong attractor patterns to model behavioural and cognitive prototypes",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/39e4973ba3321b80f37d9b55f63ed8b8-Paper.pdf",
    "abstract": "We solve the mean field equations for a stochastic Hopfield network with temperature (noise) in the presence of strong, i.e., multiply stored patterns, and use this solution to obtain the storage capacity of such a network. Our result provides for the first time a rigorous solution of the mean field equations for the standard Hopfield model and is in contrast to the mathematically unjustifiable replica technique that has been hitherto used for this derivation. We show that the critical temperature for stability of a strong pattern is equal to its degree or multiplicity, when sum of the cubes of degrees of all stored patterns is negligible compared to the network size. In the case of a single strong pattern in the presence of simple patterns, when the ratio of the number of all stored patterns and the network size is a positive constant, we obtain the distribution of the overlaps of the patterns with the mean field and deduce that the storage capacity for retrieving a strong pattern exceeds that for retrieving a simple pattern by a multiplicative factor equal to the square of the degree of the strong pattern. This square law property provides justification for using strong patterns to model attachment types and behavioural prototypes in psychology and psychotherapy.",
    "authors": [
      "Edalat, Abbas"
    ]
  },
  {
    "id": "3a835d3215755c435ef4fe9965a3f2a0",
    "title": "Manifold-based Similarity Adaptation for Label Propagation",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/3a835d3215755c435ef4fe9965a3f2a0-Paper.pdf",
    "abstract": "Label propagation is one of the state-of-the-art methods for semi-supervised learning, which estimates labels by propagating label information through a graph. Label propagation assumes that data points (nodes) connected in a graph should have similar labels. Consequently, the label estimation heavily depends on edge weights in a graph which represent similarity of each node pair. We propose a method for a graph to capture the manifold structure of input features using edge weights parameterized by a similarity function. In this approach, edge weights represent both similarity and local reconstruction weight simultaneously, both being reasonable for label propagation. For further justification, we provide analytical considerations including an interpretation as a cross-validation of a propagation model in the feature space, and an error analysis based on a low dimensional manifold model. Experimental results demonstrated the effectiveness of our approach both in synthetic and real datasets.",
    "authors": [
      "Karasuyama, Masayuki",
      "Mamitsuka, Hiroshi"
    ]
  },
  {
    "id": "3cec07e9ba5f5bb252d13f5f431e4bbb",
    "title": "New Subsampling Algorithms for Fast Least Squares Regression",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/3cec07e9ba5f5bb252d13f5f431e4bbb-Paper.pdf",
    "abstract": "We address the problem of fast estimation of ordinary least squares (OLS) from large amounts of data ($n \\gg p$). We propose three methods which solve the big data problem by subsampling the covariance matrix using either a single or two stage estimation. All three run in the order of size of input i.e. O($np$) and our best method, {\\it Uluru}, gives an error bound of $O(\\sqrt{p/n})$ which is independent of the amount of subsampling as long as it is above a threshold.  We provide theoretical bounds for our algorithms in the fixed design (with Randomized Hadamard preconditioning) as well as sub-Gaussian random design setting. We also compare the performance of our methods on synthetic and real-world datasets and show that if observations are i.i.d., sub-Gaussian then one can directly subsample without the expensive Randomized Hadamard preconditioning without loss of accuracy.",
    "authors": [
      "Dhillon, Paramveer",
      "Lu, Yichao",
      "Foster, Dean P.",
      "Ungar, Lyle"
    ]
  },
  {
    "id": "3dd48ab31d016ffcbf3314df2b3cb9ce",
    "title": "A message-passing algorithm for multi-agent trajectory planning",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/3dd48ab31d016ffcbf3314df2b3cb9ce-Paper.pdf",
    "abstract": "We describe a novel approach for computing collision-free \\emph{global} trajectories for $p$ agents with specified initial and final configurations, based on an improved version of the alternating direction method of multipliers (ADMM) algorithm. Compared with existing methods, our approach is naturally parallelizable and allows for incorporating different cost functionals with only minor adjustments. We apply our method to classical challenging instances and observe that its computational requirements scale well with $p$ for several cost functionals. We also show that a specialization of our algorithm can be used for {\\em local} motion planning by solving the problem of joint optimization in velocity space.",
    "authors": [
      "Bento, Jos\u00e9",
      "Derbinsky, Nate",
      "Alonso-Mora, Javier",
      "Yedidia, Jonathan S."
    ]
  },
  {
    "id": "3df1d4b96d8976ff5986393e8767f5b2",
    "title": "Solving the multi-way matching problem by permutation synchronization",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/3df1d4b96d8976ff5986393e8767f5b2-Paper.pdf",
    "abstract": "The problem of matching not just two, but m different sets of objects to each other arises in a variety of contexts, including finding the correspondence between feature points across multiple images in computer vision. At present it is usually solved by matching the sets pairwise, in series. In contrast, we propose a new method, permutation synchronization, which finds all the matchings jointly, in one shot, via a relaxation to eigenvector decomposition. The resulting algorithm is both computationally efficient, and, as we demonstrate with theoretical arguments as well as experimental results, much more stable to noise than previous methods.",
    "authors": [
      "Pachauri, Deepti",
      "Kondor, Risi",
      "Singh, Vikas"
    ]
  },
  {
    "id": "40008b9a5380fcacce3976bf7c08af5b",
    "title": "Auditing: Active Learning with Outcome-Dependent Query Costs",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/40008b9a5380fcacce3976bf7c08af5b-Paper.pdf",
    "abstract": "We propose a learning setting in which unlabeled data is free, and the cost of a label depends on its value, which is not known in advance. We study binary classification in an extreme case, where the algorithm only pays for negative labels. Our motivation are applications such as fraud detection, in which investigating an honest transaction should be avoided if possible. We term the setting auditing, and consider the auditing complexity of an algorithm: The number of negative points it labels to learn a hypothesis with low relative error. We design auditing algorithms for thresholds on the line and axis-aligned rectangles, and show that with these algorithms, the auditing complexity can be significantly lower than the active label complexity. We discuss a general approach for auditing for a general hypothesis class, and describe several interesting directions for future work.",
    "authors": [
      "Sabato, Sivan",
      "Sarwate, Anand D.",
      "Srebro, Nati"
    ]
  },
  {
    "id": "4122cb13c7a474c1976c9706ae36521d",
    "title": "Restricting exchangeable nonparametric distributions",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/4122cb13c7a474c1976c9706ae36521d-Paper.pdf",
    "abstract": "Distributions over exchangeable matrices with infinitely many columns are useful in constructing nonparametric latent variable models. However, the distribution implied by such models over the number of features exhibited by each data point may be poorly-suited for many modeling tasks. In this paper, we propose a class of exchangeable nonparametric priors obtained by restricting the domain of existing models. Such models allow us to specify the distribution over the number of features per data point, and can achieve better performance on data sets where the number of features is not well-modeled by the original distribution.",
    "authors": [
      "Williamson, Sinead A.",
      "MacEachern, Steve N.",
      "Xing, Eric P."
    ]
  },
  {
    "id": "41ae36ecb9b3eee609d05b90c14222fb",
    "title": "On the Linear Convergence of the Proximal Gradient Method for Trace Norm Regularization",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/41ae36ecb9b3eee609d05b90c14222fb-Paper.pdf",
    "abstract": "Motivated by various applications in machine learning, the problem of minimizing a convex smooth loss function with trace norm regularization has received much attention lately.  Currently, a popular method for solving such problem is the proximal gradient method (PGM), which is known to have a sublinear rate of convergence.  In this paper, we show that for a large class of loss functions, the convergence rate of the PGM is in fact linear.  Our result is established without any strong convexity assumption on the loss function.  A key ingredient in our proof is a new Lipschitzian error bound for the aforementioned trace norm-regularized problem, which may be of independent interest.",
    "authors": [
      "Hou, Ke",
      "Zhou, Zirui",
      "So, Anthony Man-Cho",
      "Luo, Zhi-Quan"
    ]
  },
  {
    "id": "41bfd20a38bb1b0bec75acf0845530a7",
    "title": "Eluder Dimension and the Sample Complexity of Optimistic Exploration",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/41bfd20a38bb1b0bec75acf0845530a7-Paper.pdf",
    "abstract": "This paper considers the sample complexity of the multi-armed bandit with dependencies among the arms. Some of the most successful algorithms for this problem use the principle of optimism in the face of uncertainty to guide exploration. The clearest example of this is the class of upper confidence bound (UCB) algorithms, but recent work has shown that a simple posterior sampling algorithm, sometimes called Thompson sampling, also shares a close theoretical connection with optimistic approaches. In this paper, we develop a regret bound that holds for both classes of algorithms. This bound applies broadly and can be specialized to many model classes. It depends on a new notion we refer to as the eluder dimension, which measures the degree of dependence among action rewards. Compared to UCB algorithm regret bounds for specific model classes, our general bound matches the best available for linear models and is stronger than the best available for generalized linear models.",
    "authors": [
      "Russo, Daniel",
      "Van Roy, Benjamin"
    ]
  },
  {
    "id": "428fca9bc1921c25c5121f9da7815cde",
    "title": "Efficient Algorithm for Privately Releasing Smooth Queries",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/428fca9bc1921c25c5121f9da7815cde-Paper.pdf",
    "abstract": "We study differentially private mechanisms for answering \\emph{smooth} queries on databases consisting of data points in $\\mathbb{R}^d$. A $K$-smooth query is specified by a function whose partial derivatives up to order $K$ are all bounded. We develop an $\\epsilon$-differentially private mechanism which for the class of $K$-smooth queries has accuracy $O (\\left(\\frac{1}{n}\\right)^{\\frac{K}{2d+K}}/\\epsilon)$. The mechanism first outputs a summary of the database. To obtain an answer of a query, the user runs a public evaluation algorithm which contains no information of the database. Outputting the summary runs in time $O(n^{1+\\frac{d}{2d+K}})$, and the evaluation algorithm for answering a query runs in time $\\tilde O (n^{\\frac{d+2+\\frac{2d}{K}}{2d+K}} )$. Our mechanism is based on $L_{\\infty}$-approximation of (transformed) smooth functions by low degree even trigonometric polynomials with small and efficiently computable coefficients.",
    "authors": [
      "Wang, Ziteng",
      "Fan, Kai",
      "Zhang, Jiaqi",
      "Wang, Liwei"
    ]
  },
  {
    "id": "43baa6762fa81bb43b39c62553b2970d",
    "title": "Buy-in-Bulk Active Learning",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/43baa6762fa81bb43b39c62553b2970d-Paper.pdf",
    "abstract": "In many practical applications of active learning, it is more cost-effective to request labels in large batches, rather than one-at-a-time. This is because the cost of labeling a large batch of examples at once is often sublinear in the number of examples in the batch. In this work, we study the label complexity of active learning algorithms that request labels in a given number of batches, as well as the tradeoff between the total number of queries and the number of rounds allowed. We additionally study the total cost sufficient for learning, for an abstract notion of the cost of requesting the labels of a given number of examples at once. In particular, we find that for sublinear cost functions, it is often desirable to request labels in large batches (i.e., buying in bulk); although this may increase the total number of labels requested, it reduces the total cost required for learning.",
    "authors": [
      "Yang, Liu",
      "Carbonell, Jaime"
    ]
  },
  {
    "id": "43feaeeecd7b2fe2ae2e26d917b6477d",
    "title": "On Poisson Graphical Models",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/43feaeeecd7b2fe2ae2e26d917b6477d-Paper.pdf",
    "abstract": "Undirected graphical models, such as Gaussian graphical models, Ising, and multinomial/categorical graphical models, are widely used in a variety of applications for modeling distributions over a large number of variables. These standard instances, however, are ill-suited to modeling count data, which are increasingly ubiquitous in big-data settings such as genomic sequencing data, user-ratings data, spatial incidence data, climate studies, and site visits. Existing classes of Poisson graphical models, which arise as the joint distributions that correspond to Poisson distributed node-conditional distributions, have a major drawback: they can only model negative conditional dependencies for reasons of normalizability given its infinite domain. In this paper, our objective is to modify the Poisson graphical model distribution so that it can capture a rich dependence structure between count-valued variables. We begin by discussing two strategies for truncating the Poisson distribution and show that only one of these leads to a valid joint distribution; even this model, however, has limitations on the types of variables and dependencies that may be modeled. To address this, we propose two novel variants of the Poisson distribution and their corresponding joint graphical model distributions.  These models provide a class of Poisson graphical models that can capture both positive and negative conditional dependencies between count-valued variables. One can learn the graph structure of our model via penalized neighborhood selection, and we demonstrate the performance of our methods by learning simulated networks as well as a network from microRNA-Sequencing data.",
    "authors": [
      "Yang, Eunho",
      "Ravikumar, Pradeep K.",
      "Allen, Genevera I.",
      "Liu, Zhandong"
    ]
  },
  {
    "id": "443cb001c138b2561a0d90720d6ce111",
    "title": "On Sampling from the Gibbs Distribution with Random Maximum A-Posteriori Perturbations",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/443cb001c138b2561a0d90720d6ce111-Paper.pdf",
    "abstract": "In this paper we describe how MAP inference can be used to sample efficiently from Gibbs distributions. Specifically, we provide  means for drawing either approximate or unbiased samples from Gibbs' distributions by introducing low dimensional perturbations and solving the corresponding MAP assignments. Our approach also leads to new ways to derive lower bounds on partition functions. We demonstrate empirically that our method excels in the typical high signal - high coupling'' regime. The setting results in ragged energy landscapes that are challenging for alternative approaches to sampling and/or lower bounds. \"",
    "authors": [
      "Hazan, Tamir",
      "Maji, Subhransu",
      "Jaakkola, Tommi"
    ]
  },
  {
    "id": "45645a27c4f1adc8a7a835976064a86d",
    "title": "Factorized Asymptotic Bayesian Inference for Latent Feature Models",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/45645a27c4f1adc8a7a835976064a86d-Paper.pdf",
    "abstract": "This paper extends factorized asymptotic Bayesian (FAB) inference for latent feature models~(LFMs). FAB inference has not been applicable to models, including LFMs, without a specific condition on the Hesqsian matrix of a complete log-likelihood, which is required to derive a factorized information criterion''~(FIC). Our asymptotic analysis of the Hessian matrix of LFMs shows that FIC of LFMs has the same form as those of mixture models.  FAB/LFMs have several desirable properties (e.g., automatic hidden states selection and parameter identifiability) and empirically perform better than state-of-the-art Indian Buffet processes in terms of model selection, prediction, and computational efficiency.\"",
    "authors": [
      "Hayashi, Kohei",
      "Fujimaki, Ryohei"
    ]
  },
  {
    "id": "456ac9b0d15a8b7f1e71073221059886",
    "title": "Minimax Theory for High-dimensional Gaussian Mixtures with Sparse Mean Separation",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/456ac9b0d15a8b7f1e71073221059886-Paper.pdf",
    "abstract": "While several papers have investigated computationally and statistically efficient methods for learning Gaussian mixtures, precise minimax bounds for their statistical performance as well as fundamental limits in high-dimensional settings are not well-understood. In this paper, we provide precise information theoretic bounds on the clustering accuracy and sample complexity of learning a mixture of two isotropic Gaussians in high dimensions under small mean separation. If there is a sparse subset of relevant dimensions that determine the mean separation, then the sample complexity only depends on the number of relevant dimensions and mean separation, and can be achieved by a simple computationally efficient procedure. Our results provide the first step of a theoretical basis for recent methods that combine feature selection and clustering.",
    "authors": [
      "Azizyan, Martin",
      "Singh, Aarti",
      "Wasserman, Larry"
    ]
  },
  {
    "id": "46922a0880a8f11f8f69cbb52b1396be",
    "title": "Efficient Optimization for Sparse Gaussian Process Regression",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/46922a0880a8f11f8f69cbb52b1396be-Paper.pdf",
    "abstract": "We propose an efficient discrete optimization algorithm for selecting a subset of training data to induce sparsity for Gaussian process regression. The algorithm estimates this inducing set and the hyperparameters using a single objective, either the marginal likelihood or a variational free energy. The space and time complexity are linear in the training set size, and the algorithm can be applied to large regression problems on discrete or continuous domains. Empirical evaluation shows state-of-art performance in the discrete case and competitive results in the continuous case.",
    "authors": [
      "Cao, Yanshuai",
      "Brubaker, Marcus A.",
      "Fleet, David J.",
      "Hertzmann, Aaron"
    ]
  },
  {
    "id": "47a658229eb2368a99f1d032c8848542",
    "title": "Robust learning of low-dimensional dynamics from large neural ensembles",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/47a658229eb2368a99f1d032c8848542-Paper.pdf",
    "abstract": "Recordings from large populations of neurons make it possible to search for hypothesized low-dimensional dynamics. Finding these dynamics requires models that take into account biophysical constraints and can be fit efficiently and robustly. Here, we present an approach to dimensionality reduction for neural data that is convex, does not make strong assumptions about dynamics, does not require averaging over many trials and is extensible to more complex statistical models that combine local and global influences. The results can be combined with spectral methods to learn dynamical systems models. The basic method can be seen as an extension of PCA to the exponential family using nuclear norm minimization. We evaluate the effectiveness of this method using an exact decomposition of the Bregman divergence that is analogous to variance explained for PCA. We show on model data that the parameters of latent linear dynamical systems can be recovered, and that even if the dynamics are not stationary we can still recover the true latent subspace. We also demonstrate an extension of nuclear norm minimization that can separate sparse local connections from global latent dynamics. Finally, we demonstrate improved prediction on real neural data from monkey motor cortex compared to fitting linear dynamical models without nuclear norm smoothing.",
    "authors": [
      "Pfau, David",
      "Pnevmatikakis, Eftychios A.",
      "Paninski, Liam"
    ]
  },
  {
    "id": "47d1e990583c9c67424d369f3414728e",
    "title": "Causal Inference on Time Series using Restricted Structural Equation Models",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/47d1e990583c9c67424d369f3414728e-Paper.pdf",
    "abstract": "Causal inference uses observational data to infer the causal structure of the data generating system. We study a class of restricted Structural Equation Models for time series that we call Time Series Models with Independent Noise (TiMINo). These models require independent residual time series, whereas traditional methods like Granger causality exploit the variance of residuals. This work contains two main contributions: (1) Theoretical: By restricting the model class (e.g. to additive noise) we provide more general identifiability results than existing ones. The results cover lagged and instantaneous effects that can be nonlinear and unfaithful, and non-instantaneous feedbacks between the time series. (2) Practical: If there are no feedback loops between time series, we propose an algorithm based on non-linear independence tests of time series. When the data are causally insufficient, or the data generating process does not satisfy the model assumptions, this algorithm may still give partial results, but mostly avoids incorrect answers. The Structural Equation Model point of view allows us to extend both the theoretical and the algorithmic part to situations in which the time series have been measured with different time delays (as may happen for fMRI data, for example). TiMINo outperforms existing methods on artificial and real data. Code is provided.",
    "authors": [
      "Peters, Jonas",
      "Janzing, Dominik",
      "Sch\u00f6lkopf, Bernhard"
    ]
  },
  {
    "id": "49182f81e6a13cf5eaa496d51fea6406",
    "title": "Better Approximation and Faster Algorithm Using the Proximal Average",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/49182f81e6a13cf5eaa496d51fea6406-Paper.pdf",
    "abstract": "It is a common practice to approximate complicated'' functions with more friendly ones. In large-scale machine learning applications, nonsmooth losses/regularizers that entail great computational challenges are usually approximated by smooth functions. We re-examine this powerful methodology and point out a nonsmooth approximation which simply pretends  the linearity of the proximal map. The new approximation is justified using a recent convex analysis tool---proximal average, and yields a novel proximal gradient algorithm that is strictly better than the one based on smoothing, without incurring any extra overhead. Numerical experiments conducted on two important applications, overlapping group lasso and graph-guided fused lasso, corroborate the theoretical claims.\"",
    "authors": [
      "Yu, Yao-Liang"
    ]
  },
  {
    "id": "49b8b4f95f02e055801da3b4f58e28b7",
    "title": "Robust Low Rank Kernel Embeddings of Multivariate Distributions",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/49b8b4f95f02e055801da3b4f58e28b7-Paper.pdf",
    "abstract": "Kernel embedding of distributions has led to many recent advances in machine learning. However, latent and low rank structures prevalent in real world distributions have rarely been taken into account in this setting. Furthermore, no prior work in kernel embedding literature has addressed the issue of robust embedding when the latent and low rank information are misspecified. In this paper, we propose a hierarchical low rank decomposition of kernels embeddings which can exploit such low rank structures in data while being robust to model misspecification. We also illustrate with empirical evidence that the estimated low rank embeddings lead to improved performance in density estimation.",
    "authors": [
      "Song, Le",
      "Dai, Bo"
    ]
  },
  {
    "id": "4a213d37242bdcad8e7300e202e7caa4",
    "title": "Learning the Local Statistics of Optical Flow",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/4a213d37242bdcad8e7300e202e7caa4-Paper.pdf",
    "abstract": "Motivated by recent progress in natural image statistics, we use newly available datasets with ground truth optical flow to learn the  local statistics of optical flow and rigorously compare the learned model to prior models assumed by computer vision optical flow algorithms.  We find that a Gaussian mixture model with 64 components provides a significantly better model for local flow statistics when compared to commonly used models. We investigate the source of the GMMs success and show it is related to an explicit representation of flow boundaries. We also learn a model that jointly models the local intensity pattern and the local optical flow. In accordance with the assumptions often made in computer vision, the model learns that flow boundaries are more likely at intensity boundaries. However, when evaluated on a large dataset, this dependency is very weak and the benefit of conditioning flow estimation on the local intensity pattern is marginal.",
    "authors": [
      "Rosenbaum, Dan",
      "Zoran, Daniel",
      "Weiss, Yair"
    ]
  },
  {
    "id": "4d2e7bd33c475784381a64e43e50922f",
    "title": "Fast Algorithms for Gaussian Noise Invariant Independent Component Analysis",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/4d2e7bd33c475784381a64e43e50922f-Paper.pdf",
    "abstract": "The performance of standard algorithms for Independent Component Analysis quickly deteriorates under the addition of Gaussian noise. This is partially due to a common first step that typically consists of whitening, i.e., applying Principal Component Analysis (PCA) and rescaling the components to have identity covariance, which is not invariant under Gaussian noise.   In our paper we develop the first practical algorithm for Independent Component Analysis that is provably invariant under Gaussian noise. The two main contributions of this work are as follows: 1. We develop and implement a more efficient version of a Gaussian noise invariant decorrelation (quasi-orthogonalization) algorithm using Hessians of the cumulant functions. 2. We propose a very simple and efficient fixed-point GI-ICA (Gradient Iteration ICA) algorithm, which is compatible with quasi-orthogonalization, as well as with the usual PCA-based whitening in the noiseless case.  The algorithm is based on a special form of gradient iteration (different from gradient descent).   We provide an analysis of our algorithm demonstrating fast convergence following from the basic properties of cumulants. We also present a number of experimental comparisons with the existing methods, showing superior results on noisy data and very competitive performance in the noiseless case.",
    "authors": [
      "Voss, James R.",
      "Rademacher, Luis",
      "Belkin, Mikhail"
    ]
  },
  {
    "id": "4da04049a062f5adfe81b67dd755cecc",
    "title": "Stochastic Majorization-Minimization Algorithms for Large-Scale Optimization",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/4da04049a062f5adfe81b67dd755cecc-Paper.pdf",
    "abstract": "Majorization-minimization algorithms consist of iteratively minimizing a majorizing surrogate of an objective function. Because of its simplicity and its wide applicability, this principle has been very popular in statistics and in signal processing. In this paper, we intend to make this principle scalable. We introduce a stochastic majorization-minimization scheme which is able to deal with large-scale or possibly infinite data sets. When applied to convex optimization problems under suitable assumptions, we show that it achieves an expected convergence rate of $O(1/\\sqrt{n})$ after~$n$ iterations, and of $O(1/n)$ for strongly convex functions. Equally important, our scheme almost surely converges to stationary points for a large class of non-convex problems. We develop several efficient algorithms based on our framework. First, we propose a new stochastic proximal gradient method, which experimentally matches state-of-the-art solvers for large-scale $\\ell_1$-logistic regression. Second, we develop an online DC programming algorithm for non-convex sparse estimation. Finally, we demonstrate the effectiveness of our technique for solving large-scale structured matrix factorization problems.",
    "authors": [
      "Mairal, Julien"
    ]
  },
  {
    "id": "4f284803bd0966cc24fa8683a34afc6e",
    "title": "Online Learning in Markov Decision Processes with Adversarially Chosen Transition Probability Distributions",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/4f284803bd0966cc24fa8683a34afc6e-Paper.pdf",
    "abstract": "We study the problem of online learning Markov Decision Processes (MDPs) when both the transition distributions and loss functions are chosen by an adversary. We present an algorithm that, under a mixing assumption, achieves $O(\\sqrt{T\\log|\\Pi|}+\\log|\\Pi|)$ regret with respect to a comparison set of policies $\\Pi$.  The regret is independent of the size of the state and action spaces. When expectations over sample paths can be computed efficiently and the comparison set $\\Pi$ has polynomial size, this algorithm is efficient.  We also consider the episodic adversarial online shortest path problem.  Here, in each episode an adversary may choose a weighted directed acyclic graph with an identified start and finish node. The goal of the learning algorithm is to choose a path that minimizes the loss while traversing from the start to finish node. At the end of each episode the loss function (given by weights on the edges) is revealed to the learning algorithm. The goal is to minimize regret with respect to a fixed policy for selecting paths. This problem is a special case of the online MDP problem. For randomly chosen graphs and adversarial losses, this problem can be efficiently solved. We show that it also can be efficiently solved for adversarial graphs and randomly chosen losses.  When both graphs and losses are adversarially chosen, we present an efficient algorithm whose regret scales linearly with the number of distinct graphs.  Finally, we show that designing efficient algorithms for the adversarial online shortest path problem (and hence for the adversarial MDP problem) is as hard as learning parity with noise, a notoriously difficult problem that has been used to design efficient cryptographic schemes.",
    "authors": [
      "Abbasi Yadkori, Yasin",
      "Bartlett, Peter L.",
      "Kanade, Varun",
      "Seldin, Yevgeny",
      "Szepesvari, Csaba"
    ]
  },
  {
    "id": "502e4a16930e414107ee22b6198c578f",
    "title": "Improved and Generalized Upper Bounds on the Complexity of Policy Iteration",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/502e4a16930e414107ee22b6198c578f-Paper.pdf",
    "abstract": "Given a Markov Decision Process (MDP) with $n$ states and $m$ actions per state, we study the number of iterations needed by Policy Iteration (PI) algorithms to converge to the optimal $\\gamma$-discounted optimal policy. We consider two variations of PI: Howard's PI that changes the actions in all states with a positive advantage, and Simplex-PI that only changes the action in the state with maximal advantage. We show that Howard's PI terminates after at most  $ O \\left( \\frac{ n m}{1-\\gamma} \\log \\left( \\frac{1}{1-\\gamma} \\right)\\right) $ iterations, improving by a factor $O(\\log n)$ a result by Hansen et al. (2013), while Simplex-PI terminates after at most $ O \\left(  \\frac{n^2 m}{1-\\gamma} \\log \\left( \\frac{1}{1-\\gamma} \\right)\\right) $ iterations, improving by a factor $O(\\log n)$ a result by Ye (2011). Under some structural assumptions of the MDP, we then consider bounds that are independent of the discount factor~$\\gamma$: given a measure of the maximal transient time $\\tau_t$ and the maximal time $\\tau_r$ to revisit states in recurrent classes under all policies, we show that Simplex-PI terminates after at most $ \\tilde O \\left( n^3 m^2 \\tau_t \\tau_r \\right) $ iterations. This generalizes a recent result for deterministic MDPs by Post & Ye (2012), in which $\\tau_t \\le n$ and $\\tau_r \\le n$. We explain why similar results seem hard to derive for Howard's PI. Finally, under the additional (restrictive) assumption that the state space is partitioned in two sets, respectively states that are transient and recurrent for all policies, we show that Simplex-PI and Howard's PI terminate after at most  $ \\tilde O(nm (\\tau_t+\\tau_r))$ iterations.",
    "authors": [
      "Scherrer, Bruno"
    ]
  },
  {
    "id": "50c3d7614917b24303ee6a220679dab3",
    "title": "Approximate Inference in Continuous Determinantal Processes",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/50c3d7614917b24303ee6a220679dab3-Paper.pdf",
    "abstract": "Determinantal point processes (DPPs) are random point processes well-suited for modeling repulsion. In machine learning, the focus of DPP-based models has been on diverse subset selection from a discrete and finite base set. This discrete setting admits an efficient algorithm for sampling based on the eigendecomposition of the defining kernel matrix. Recently, there has been growing interest in using DPPs defined on continuous spaces. While the discrete-DPP sampler extends formally to the continuous case, computationally, the steps required cannot be directly extended except in a few restricted cases. In this paper, we present efficient approximate DPP sampling schemes based on Nystrom and random Fourier feature approximations that apply to a wide range of kernel functions. We demonstrate the utility of continuous DPPs in repulsive mixture modeling applications and synthesizing human poses spanning activity spaces.",
    "authors": [
      "Affandi, Raja Hafiz",
      "Fox, Emily",
      "Taskar, Ben"
    ]
  },
  {
    "id": "51ef186e18dc00c2d31982567235c559",
    "title": "Streaming Variational Bayes",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/51ef186e18dc00c2d31982567235c559-Paper.pdf",
    "abstract": "We present SDA-Bayes, a framework for (S)treaming, (D)istributed, (A)synchronous computation of a Bayesian posterior. The framework makes streaming updates to the estimated posterior according to a user-specified approximation primitive function.  We demonstrate the usefulness of our framework, with variational Bayes (VB) as the primitive, by fitting the latent Dirichlet allocation model to two large-scale document collections.  We demonstrate the advantages of our algorithm over stochastic variational inference (SVI), both in the single-pass setting SVI was designed for and in the streaming setting, to which SVI does not apply.",
    "authors": [
      "Broderick, Tamara",
      "Boyd, Nicholas",
      "Wibisono, Andre",
      "Wilson, Ashia C.",
      "Jordan, Michael I."
    ]
  },
  {
    "id": "52292e0c763fd027c6eba6b8f494d2eb",
    "title": "One-shot learning by inverting a compositional causal process",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/52292e0c763fd027c6eba6b8f494d2eb-Paper.pdf",
    "abstract": "People can learn a new visual class from just one example, yet machine learning algorithms typically require hundreds or thousands of examples to tackle the same problems. Here we present a Hierarchical Bayesian model based on compositionality and causality that can learn a wide range of natural (although simple) visual concepts, generalizing in human-like ways from just one image. We evaluated performance on a challenging one-shot classification task, where our model achieved a human-level error rate while substantially outperforming two deep learning models. We also used a visual Turing test\" to show that our model produces human-like performance on other conceptual tasks, including generating new examples and parsing.\"",
    "authors": [
      "Lake, Brenden M.",
      "Salakhutdinov, Russ R.",
      "Tenenbaum, Josh"
    ]
  },
  {
    "id": "52720e003547c70561bf5e03b95aa99f",
    "title": "Large Scale Distributed Sparse Precision Estimation",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/52720e003547c70561bf5e03b95aa99f-Paper.pdf",
    "abstract": "We consider the problem of sparse precision matrix estimation in high dimensions using the CLIME estimator, which has several desirable theoretical properties. We present an inexact alternating direction method of multiplier (ADMM) algorithm for CLIME, and establish rates of convergence for both the objective and optimality conditions. Further, we develop a large scale distributed framework for the computations, which scales to millions of dimensions and trillions of parameters, using hundreds of cores. The proposed framework solves CLIME in column-blocks and only involves elementwise operations and parallel matrix multiplications.  We evaluate our algorithm on both shared-memory and distributed-memory architectures, which can use block cyclic distribution of data and parameters to achieve load balance and improve the efficiency in the use of memory hierarchies. Experimental results show that our algorithm is substantially more scalable than state-of-the-art methods and scales almost linearly with the number of cores.",
    "authors": [
      "Wang, Huahua",
      "Banerjee, Arindam",
      "Hsieh, Cho-Jui",
      "Ravikumar, Pradeep K.",
      "Dhillon, Inderjit S."
    ]
  },
  {
    "id": "539fd53b59e3bb12d203f45a912eeaf2",
    "title": "Online Variational Approximations to non-Exponential Family Change Point Models: With Application to Radar Tracking",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/539fd53b59e3bb12d203f45a912eeaf2-Paper.pdf",
    "abstract": "The Bayesian online change point detection (BOCPD) algorithm provides an efficient way to do exact inference when the parameters of an underlying model may suddenly change over time. BOCPD requires computation of the underlying model's posterior predictives, which can only be computed online in $O(1)$ time and memory for exponential family models. We develop variational approximations to the posterior on change point times (formulated as run lengths) for efficient inference when the underlying model is not in the exponential family, and does not have tractable posterior predictive distributions. In doing so, we develop improvements to online variational inference. We apply our methodology to a tracking problem using radar data with a signal-to-noise feature that is Rice distributed. We also develop a variational method for inferring the parameters of the (non-exponential family) Rice distribution.",
    "authors": [
      "Turner, Ryan D.",
      "Bottone, Steven",
      "Stanek, Clay J."
    ]
  },
  {
    "id": "53adaf494dc89ef7196d73636eb2451b",
    "title": "RNADE: The real-valued neural autoregressive density-estimator",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/53adaf494dc89ef7196d73636eb2451b-Paper.pdf",
    "abstract": "We introduce RNADE, a new model for joint density estimation of real-valued vectors. Our model calculates the density of a datapoint as the product of one-dimensional conditionals modeled using mixture density networks with shared parameters. RNADE learns a distributed representation of the data, while having a tractable expression for the calculation of densities. A tractable likelihood allows direct comparison with other methods and training by standard gradient-based optimizers. We compare the performance of RNADE on several datasets of heterogeneous and perceptual data, finding it outperforms mixture models in all but one case.",
    "authors": [
      "Uria, Benigno",
      "Murray, Iain",
      "Larochelle, Hugo"
    ]
  },
  {
    "id": "53c04118df112c13a8c34b38343b9c10",
    "title": "Estimating the Unseen: Improved Estimators for Entropy and other Properties",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/53c04118df112c13a8c34b38343b9c10-Paper.pdf",
    "abstract": "Recently, [Valiant and Valiant] showed that a class of distributional properties, which includes such practically relevant properties as entropy, the number of distinct elements, and distance metrics between pairs of distributions, can be estimated given a SUBLINEAR sized sample.  Specifically, given a sample consisting of independent draws from any distribution over at most n distinct elements, these properties can be estimated accurately using a sample of size O(n / log n).  We propose a novel modification of this approach and show: 1) theoretically, our estimator is optimal (to constant factors, over worst-case instances), and 2) in practice, it performs exceptionally well for a variety of estimation tasks, on a variety of natural distributions, for a wide range of parameters.  Perhaps unsurprisingly, the key step in this approach is to first use the sample to characterize the unseen\" portion of the distribution.  This goes beyond such tools as the Good-Turing frequency estimation scheme, which estimates the total probability mass of the unobserved portion of the distribution: we seek to estimate the \"shape\"of the unobserved portion of the distribution.  This approach is robust, general, and theoretically principled;  we expect that it may be fruitfully used as a component within larger machine learning and data analysis systems. \"",
    "authors": [
      "Valiant, Paul",
      "Valiant, Gregory"
    ]
  },
  {
    "id": "53c3bce66e43be4f209556518c2fcb54",
    "title": "Dynamic Clustering via Asymptotics of the Dependent Dirichlet Process Mixture",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/53c3bce66e43be4f209556518c2fcb54-Paper.pdf",
    "abstract": "This paper presents a novel algorithm, based upon the dependent Dirichlet process mixture model (DDPMM), for clustering batch-sequential data containing an unknown number of evolving clusters. The algorithm is derived via a low-variance asymptotic analysis of the Gibbs sampling algorithm for the DDPMM, and provides a hard clustering with convergence guarantees similar to those of the k-means algorithm. Empirical results from a synthetic test with moving Gaussian clusters and a test with real ADS-B aircraft trajectory data demonstrate that the algorithm requires orders of magnitude less computational time than contemporary probabilistic and hard clustering algorithms, while providing higher accuracy on the examined datasets.",
    "authors": [
      "Campbell, Trevor",
      "Liu, Miao",
      "Kulis, Brian",
      "How, Jonathan P.",
      "Carin, Lawrence"
    ]
  },
  {
    "id": "53e3a7161e428b65688f14b84d61c610",
    "title": "Parametric Task Learning",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/53e3a7161e428b65688f14b84d61c610-Paper.pdf",
    "abstract": "We introduce a novel formulation of multi-task learning (MTL) called parametric task learning (PTL) that can systematically handle infinitely many tasks parameterized by a continuous parameter. Our key finding is that, for a certain class of PTL problems, the path of optimal task-wise solutions can be represented as piecewise-linear functions of the continuous task parameter. Based on this fact, we employ a parametric programming technique to obtain the common shared representation across all the continuously parameterized tasks efficiently. We show that our PTL formulation is useful in various scenarios such as learning under non-stationarity, cost-sensitive learning, and quantile regression, and demonstrate the usefulness of the proposed method experimentally in these scenarios.",
    "authors": [
      "Takeuchi, Ichiro",
      "Hongo, Tatsuya",
      "Sugiyama, Masashi",
      "Nakajima, Shinichi"
    ]
  },
  {
    "id": "559cb990c9dffd8675f6bc2186971dc2",
    "title": "Generalized Denoising Auto-Encoders as Generative Models",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/559cb990c9dffd8675f6bc2186971dc2-Paper.pdf",
    "abstract": "Recent work has shown how denoising and contractive autoencoders implicitly capture the structure of the data generating density, in the case where the corruption noise is Gaussian, the reconstruction error is the squared error, and the data is continuous-valued. This has led to various proposals for sampling from this implicitly learned density function, using Langevin and Metropolis-Hastings MCMC. However, it remained unclear how to connect the training procedure of regularized auto-encoders to the implicit estimation of the underlying data generating distribution when the data are discrete, or using other forms of corruption process and reconstruction errors. Another issue is the mathematical justification which is only valid in the limit of small corruption noise.  We propose here a different attack on the problem, which deals with all these issues: arbitrary (but noisy enough) corruption, arbitrary reconstruction loss (seen as a log-likelihood), handling both discrete and continuous-valued variables, and removing the bias due to non-infinitesimal corruption noise (or non-infinitesimal contractive penalty).",
    "authors": [
      "Bengio, Yoshua",
      "Yao, Li",
      "Alain, Guillaume",
      "Vincent, Pascal"
    ]
  },
  {
    "id": "5807a685d1a9ab3b599035bc566ce2b9",
    "title": "Local Privacy and Minimax Bounds: Sharp Rates for Probability Estimation",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/5807a685d1a9ab3b599035bc566ce2b9-Paper.pdf",
    "abstract": "We provide a detailed study of the estimation of probability distributions---discrete and continuous---in a stringent setting in which data is kept private even from the statistician.  We give sharp minimax rates of convergence for estimation in these locally private settings, exhibiting fundamental tradeoffs between privacy and convergence rate, as well as providing tools to allow movement along the privacy-statistical efficiency continuum. One of the consequences of our results is that Warner's classical work on randomized response is an optimal way to perform survey sampling while maintaining privacy of the respondents.",
    "authors": [
      "Duchi, John",
      "Wainwright, Martin J.",
      "Jordan, Michael I."
    ]
  },
  {
    "id": "58c54802a9fb9526cd0923353a34a7ae",
    "title": "Reward Mapping for Transfer in Long-Lived Agents",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/58c54802a9fb9526cd0923353a34a7ae-Paper.pdf",
    "abstract": "We consider how to transfer knowledge from previous tasks to a current task in long-lived and bounded agents that must solve a sequence of MDPs over a finite lifetime.  A novel aspect of our transfer approach is that we reuse reward functions.   While this may seem counterintuitive, we build on the insight of recent work on the optimal rewards problem that guiding an agent's behavior with reward functions other than the task-specifying reward function can help overcome computational  bounds of  the agent.    Specifically, we use good guidance reward functions learned on previous tasks in the sequence to incrementally train a reward mapping function that maps task-specifying reward functions into good initial guidance reward functions for subsequent tasks. We demonstrate that our approach can substantially improve the agent's performance relative to other approaches, including an approach that transfers policies.",
    "authors": [
      "Guo, Xiaoxiao",
      "Singh, Satinder",
      "Lewis, Richard L."
    ]
  },
  {
    "id": "598b3e71ec378bd83e0a727608b5db01",
    "title": "Distributed Exploration in Multi-Armed Bandits",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/598b3e71ec378bd83e0a727608b5db01-Paper.pdf",
    "abstract": "We study exploration in Multi-Armed Bandits (MAB) in a setting where~$k$ players collaborate in order to identify an $\\epsilon$-optimal arm. Our motivation comes from recent employment of MAB algorithms in computationally intensive, large-scale applications. Our results demonstrate a non-trivial tradeoff between the number of arm pulls required by each of the players, and the amount of communication between them. In particular, our main result shows that by allowing the $k$ players to communicate \\emph{only once}, they are able to learn $\\sqrt{k}$ times faster than a single player. That is, distributing learning to $k$ players gives rise to a factor~$\\sqrt{k}$ parallel speed-up. We complement this result with a lower bound showing this is in general the best possible.  On the other extreme, we present an algorithm that achieves the ideal factor $k$ speed-up in learning performance, with communication only logarithmic in~$1/\\epsilon$.",
    "authors": [
      "Hillel, Eshcar",
      "Karnin, Zohar S.",
      "Koren, Tomer",
      "Lempel, Ronny",
      "Somekh, Oren"
    ]
  },
  {
    "id": "59c33016884a62116be975a9bb8257e3",
    "title": "It is all in the noise: Efficient multi-task Gaussian process inference with structured residuals",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/59c33016884a62116be975a9bb8257e3-Paper.pdf",
    "abstract": "Multi-task prediction models are widely being used to couple regressors or classification models by sharing information across related tasks. A common pitfall of these models is that they assume that the output tasks are independent conditioned on the inputs. Here, we propose a multi-task Gaussian process approach to model both the relatedness between regressors as well as the task correlations in the residuals, in order to more accurately identify true sharing between regressors. The resulting Gaussian model has a covariance term that is the sum of Kronecker products, for which efficient parameter inference and out of sample prediction are feasible. On both synthetic examples and applications to phenotype prediction in genetics, we find substantial benefits of modeling structured noise compared to established alternatives.",
    "authors": [
      "Rakitsch, Barbara",
      "Lippert, Christoph",
      "Borgwardt, Karsten",
      "Stegle, Oliver"
    ]
  },
  {
    "id": "5a4b25aaed25c2ee1b74de72dc03c14e",
    "title": "Projecting Ising Model Parameters for Fast Mixing",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/5a4b25aaed25c2ee1b74de72dc03c14e-Paper.pdf",
    "abstract": "Inference in general Ising models is difficult, due to high treewidth making tree-based algorithms intractable. Moreover, when interactions are strong, Gibbs sampling may take exponential time to converge to the stationary distribution. We present an algorithm to project Ising model parameters onto a parameter set that is guaranteed to be fast mixing, under several divergences. We find that Gibbs sampling using the projected parameters is more accurate than with the original parameters when interaction strengths are strong and when limited time is available for sampling.",
    "authors": [
      "Domke, Justin",
      "Liu, Xianghang"
    ]
  },
  {
    "id": "5b69b9cb83065d403869739ae7f0995e",
    "title": "Low-rank matrix reconstruction and clustering via approximate message passing",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/5b69b9cb83065d403869739ae7f0995e-Paper.pdf",
    "abstract": "We study the problem of reconstructing low-rank matrices from their noisy observations. We formulate the problem in the Bayesian framework, which allows us to exploit structural properties of matrices in addition to low-rankedness, such as sparsity. We propose an efficient approximate message passing algorithm, derived from the belief propagation algorithm, to perform the Bayesian inference for matrix reconstruction. We have also successfully applied the proposed algorithm to a clustering problem, by formulating the problem of clustering as a low-rank matrix reconstruction problem with an additional structural property. Numerical experiments show that the proposed algorithm outperforms Lloyd's K-means algorithm.",
    "authors": [
      "Matsushita, Ryosuke",
      "Tanaka, Toshiyuki"
    ]
  },
  {
    "id": "5c572eca050594c7bc3c36e7e8ab9550",
    "title": "Inverse Density as an Inverse Problem: the Fredholm Equation Approach",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/5c572eca050594c7bc3c36e7e8ab9550-Paper.pdf",
    "abstract": "We address the problem of estimating the ratio $\\frac{q}{p}$ where $p$ is a density function and $q$ is another density, or, more generally an arbitrary function.  Knowing or approximating this ratio is needed in various problems of inference and integration, in particular, when one needs to average a function with respect to one probability distribution, given a sample from another. It is often referred as {\\it importance sampling} in statistical inference and is  also closely related to the problem of {\\it covariate shift} in transfer learning as well as to various MCMC methods. Our approach is based on reformulating the problem of estimating the ratio as an inverse problem in terms of an integral operator corresponding to a kernel, and thus reducing it to an integral equation, known as the Fredholm problem of the first kind.   This formulation, combined with the techniques of regularization and kernel methods, leads to a principled kernel-based framework for constructing algorithms and for analyzing them theoretically.  The resulting family of algorithms (FIRE, for Fredholm Inverse Regularized Estimator) is flexible,  simple and  easy to implement. We provide detailed theoretical analysis including concentration bounds and convergence rates for the Gaussian kernel for densities defined on $\\R^d$ and smooth $d$-dimensional sub-manifolds of the Euclidean space. Model selection for unsupervised or semi-supervised inference is generally a difficult problem. Interestingly, it turns out that in the density ratio estimation setting, when samples from both distributions are available, there are simple completely unsupervised methods for choosing parameters. We  call this model selection mechanism CD-CV for Cross-Density Cross-Validation. Finally, we show encouraging experimental results including applications to classification  within the covariate shift framework.",
    "authors": [
      "Que, Qichao",
      "Belkin, Mikhail"
    ]
  },
  {
    "id": "5caf41d62364d5b41a893adc1a9dd5d4",
    "title": "Modeling Overlapping Communities with Node Popularities",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/5caf41d62364d5b41a893adc1a9dd5d4-Paper.pdf",
    "abstract": "We develop a probabilistic approach for accurate network modeling using node popularities within the framework of the mixed-membership stochastic blockmodel (MMSB). Our model integrates some of the basic properties of nodes in social networks: homophily and preferential connection to popular nodes. We develop a scalable algorithm for posterior inference, based on a novel nonconjugate variant of stochastic variational inference. We evaluate the link prediction accuracy of our algorithm on eight real-world networks with up to 60,000 nodes, and 24 benchmark networks. We demonstrate that our algorithm predicts better than the MMSB. Further, using benchmark networks we show that node popularities are essential to achieving high accuracy in the presence of skewed degree distribution and noisy links---both characteristics of real networks.",
    "authors": [
      "Gopalan, Prem K.",
      "Wang, Chong",
      "Blei, David"
    ]
  },
  {
    "id": "5dd9db5e033da9c6fb5ba83c7a7ebea9",
    "title": "Reflection methods for user-friendly submodular optimization",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf",
    "abstract": "Recently, it has become evident that submodularity naturally captures widely occurring concepts in machine learning, signal processing and computer vision. In consequence, there is need for efficient optimization procedures for submodular functions, in particular for minimization problems. While general submodular minimization is challenging, we propose a new approach that exploits existing decomposability of submodular functions. In contrast to previous approaches, our method is neither approximate, nor impractical, nor does it need any cumbersome parameter tuning. Moreover, it is easy to implement and parallelize. A key component of our approach is a formulation of the discrete submodular minimization problem as a continuous best approximation problem. It is solved through a sequence of reflections and its solution can be  automatically thresholded to obtain an optimal discrete solution. Our method solves both the continuous and discrete formulations of the problem, and therefore has applications in learning, inference, and reconstruction. In our experiments, we show the benefits of our new algorithms for two image segmentation tasks.",
    "authors": [
      "Jegelka, Stefanie",
      "Bach, Francis",
      "Sra, Suvrit"
    ]
  },
  {
    "id": "5e1b18c4c6a6d31695acbae3fd70ecc6",
    "title": "Compressive Feature Learning",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/5e1b18c4c6a6d31695acbae3fd70ecc6-Paper.pdf",
    "abstract": "This paper addresses the problem of unsupervised feature learning for text data. Our method is grounded in the principle of minimum description length and uses a dictionary-based compression scheme to extract a succinct feature set. Specifically, our method finds a set of word $k$-grams that minimizes the cost of reconstructing the text losslessly. We formulate document compression as a binary optimization task and show how to solve it approximately via a sequence of reweighted linear programs that are efficient to solve and parallelizable. As our method is unsupervised, features may be extracted once and subsequently used in a variety of tasks. We demonstrate the performance of these features over a range of scenarios including unsupervised exploratory analysis and supervised text categorization. Our compressed feature space is two orders of magnitude smaller than the full $k$-gram space and matches the text categorization accuracy achieved in the full feature space. This dimensionality reduction not only results in faster training times, but it can also help elucidate structure in unsupervised learning tasks and reduce the amount of training data necessary for supervised learning.",
    "authors": [
      "Paskov, Hristo S.",
      "West, Robert",
      "Mitchell, John C.",
      "Hastie, Trevor"
    ]
  },
  {
    "id": "5e9f92a01c986bafcabbafd145520b13",
    "title": "Sparse nonnegative deconvolution for compressive calcium imaging: algorithms and phase transitions",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/5e9f92a01c986bafcabbafd145520b13-Paper.pdf",
    "abstract": "We propose a compressed sensing (CS) calcium imaging framework for monitoring large neuronal populations, where we image randomized projections of the spatial calcium concentration at each timestep, instead of measuring the concentration at individual locations. We develop scalable nonnegative deconvolution methods for extracting the neuronal spike time series from such observations. We also address the problem of demixing the spatial locations of the neurons using rank-penalized matrix factorization methods. By exploiting the sparsity of neural spiking we demonstrate that the number of measurements needed per timestep is significantly smaller than the total number of neurons, a result that can potentially enable imaging of larger populations at considerably faster rates compared to traditional raster-scanning techniques. Unlike traditional CS setups, our problem involves a block-diagonal sensing matrix and a non-orthogonal sparse basis that spans multiple timesteps. We study the effect of these distinctive features in a noiseless setup using recent results relating conic geometry to CS. We provide tight approximations to the number of measurements needed for perfect deconvolution for certain classes of spiking processes, and show that this number displays a phase transition,\" similar to phenomena observed in more standard CS settings; however, in this case the required measurement rate depends not just on the mean sparsity level but also on other details of the underlying spiking process.\"",
    "authors": [
      "Pnevmatikakis, Eftychios A.",
      "Paninski, Liam"
    ]
  },
  {
    "id": "5ef0b4eba35ab2d6180b0bca7e46b6f9",
    "title": "Probabilistic Low-Rank Matrix Completion with Adaptive Spectral Regularization Algorithms",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/5ef0b4eba35ab2d6180b0bca7e46b6f9-Paper.pdf",
    "abstract": "We propose a novel class of algorithms for low rank matrix completion. Our approach builds on novel penalty functions on the singular values of the low rank matrix. By exploiting a mixture model representation of this penalty, we show that a suitably chosen set of latent variables enables to derive an Expectation-Maximization algorithm to obtain a Maximum A Posteriori estimate of the completed low rank matrix. The resulting algorithm is an iterative soft-thresholded algorithm which iteratively adapts the shrinkage coefficients associated to the singular values. The algorithm is simple to implement and can scale to large matrices. We provide numerical comparisons between our approach and recent alternatives showing the interest of the proposed approach for low rank matrix completion.",
    "authors": [
      "Todeschini, Adrien",
      "Caron, Fran\u00e7ois",
      "Chavent, Marie"
    ]
  },
  {
    "id": "5f2c22cb4a5380af7ca75622a6426917",
    "title": "Global Solver and Its Efficient Approximation for Variational Bayesian Low-rank Subspace Clustering",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/5f2c22cb4a5380af7ca75622a6426917-Paper.pdf",
    "abstract": "When a probabilistic model and its  prior are given, Bayesian learning offers inference with automatic parameter tuning. However, Bayesian learning is often obstructed by computational difficulty: the rigorous Bayesian learning is  intractable in many models, and its variational Bayesian (VB) approximation is prone to suffer from local minima. In this paper, we overcome this difficulty for low-rank subspace clustering (LRSC) by providing an exact global solver and its efficient approximation. LRSC extracts a low-dimensional structure of data by embedding  samples into the union of low-dimensional subspaces, and its variational Bayesian variant has shown good performance. We first prove a key property that the VB-LRSC model is highly redundant. Thanks to this property, the optimization problem of VB-LRSC can be separated into small subproblems, each of which has only a small number of unknown variables. Our exact global solver relies on another key property that the stationary condition of each subproblem is written as a set of polynomial equations, which is solvable with the homotopy method. For further computational efficiency,  we also propose an efficient approximate variant, of which the stationary condition can be written as a polynomial equation with a single variable. Experimental results show the usefulness of our approach.",
    "authors": [
      "Nakajima, Shinichi",
      "Takeda, Akiko",
      "Babacan, S. Derin",
      "Sugiyama, Masashi",
      "Takeuchi, Ichiro"
    ]
  },
  {
    "id": "6081594975a764c8e3a691fa2b3a321d",
    "title": "Reservoir Boosting : Between Online and Offline Ensemble Learning",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/6081594975a764c8e3a691fa2b3a321d-Paper.pdf",
    "abstract": "We propose to train an ensemble with the help of a reservoir in which the learning algorithm can store a limited number of samples. This novel approach lies in the area between offline and online ensemble approaches and can be seen either as a restriction of the former or an enhancement of the latter.  We identify some basic strategies that can be used to populate this reservoir and present our main contribution, dubbed Greedy Edge Expectation Maximization (GEEM), that maintains the reservoir content in the case of Boosting by viewing the samples through their projections into the weak classifier response space.  We propose an efficient algorithmic implementation which makes it tractable in practice, and demonstrate its efficiency experimentally on several compute-vision data-sets, on which it outperforms both online and offline methods in a memory constrained setting.",
    "authors": [
      "Lefakis, Leonidas",
      "Fleuret, Fran\u00e7ois"
    ]
  },
  {
    "id": "621bf66ddb7c962aa0d22ac97d69b793",
    "title": "Faster Ridge Regression via the Subsampled Randomized Hadamard Transform",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/621bf66ddb7c962aa0d22ac97d69b793-Paper.pdf",
    "abstract": "We propose a fast algorithm for ridge regression when the number of features is much larger than the number of observations ($p \\gg n$). The standard way to solve ridge regression in this setting works in the dual space and gives a running time of $O(n^2p)$. Our algorithm (SRHT-DRR) runs in time $O(np\\log(n))$ and works by preconditioning the design matrix by a Randomized Walsh-Hadamard Transform with a subsequent subsampling of features. We provide risk bounds for our SRHT-DRR algorithm in the fixed design setting and show experimental results on synthetic and real datasets.",
    "authors": [
      "Lu, Yichao",
      "Dhillon, Paramveer",
      "Foster, Dean P.",
      "Ungar, Lyle"
    ]
  },
  {
    "id": "647bba344396e7c8170902bcf2e15551",
    "title": "Convex Relaxations for Permutation Problems",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/647bba344396e7c8170902bcf2e15551-Paper.pdf",
    "abstract": "Seriation seeks to reconstruct a linear order between variables using unsorted similarity information. It has direct applications in archeology and shotgun gene sequencing for example. We prove the equivalence between the seriation and the combinatorial 2-sum problem (a quadratic minimization problem over permutations) over a class of similarity matrices. The seriation problem can be solved exactly by a spectral algorithm in the noiseless case and we produce a convex relaxation for the 2-sum problem to improve the robustness of solutions in a noisy setting.  This relaxation also allows us to impose additional structural constraints on the solution, to solve semi-supervised seriation problems. We present numerical experiments on archeological data, Markov chains and gene sequences.",
    "authors": [
      "Fogel, Fajwel",
      "Jenatton, Rodolphe",
      "Bach, Francis",
      "D'Aspremont, Alexandre"
    ]
  },
  {
    "id": "65cc2c8205a05d7379fa3a6386f710e1",
    "title": "Online Learning of Dynamic Parameters in Social Networks",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/65cc2c8205a05d7379fa3a6386f710e1-Paper.pdf",
    "abstract": "This paper addresses the problem of online learning in a dynamic setting. We consider a social network in which each individual observes a private signal about the underlying state of the world and communicates with her neighbors at each time period. Unlike many existing approaches, the underlying state is dynamic, and evolves according to a geometric random walk. We view the scenario as an optimization problem where agents aim to learn the true state while suffering the smallest possible loss. Based on the decomposition of the global loss function, we introduce two update mechanisms, each of which generates an estimate of the true state. We establish a tight bound on the rate of change of the underlying state, under which individuals can track the parameter with a bounded variance. Then, we characterize explicit expressions for the steady state mean-square deviation(MSD) of the estimates from the truth, per individual. We observe that only one of the estimators recovers the optimal MSD, which underscores the impact of the objective function decomposition on the learning quality. Finally, we provide an upper bound on the regret of the proposed methods, measured as an average of errors in estimating the parameter in a finite time.",
    "authors": [
      "Shahrampour, Shahin",
      "Rakhlin, Sasha",
      "Jadbabaie, Ali"
    ]
  },
  {
    "id": "678a1491514b7f1006d605e9161946b1",
    "title": "Discovering Hidden Variables in Noisy-Or Networks using Quartet Tests",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/678a1491514b7f1006d605e9161946b1-Paper.pdf",
    "abstract": "We give a polynomial-time algorithm for provably learning the structure and parameters of bipartite noisy-or Bayesian networks of binary variables where the top layer is completely hidden. Unsupervised learning of these models is a form of discrete factor analysis, enabling the discovery of hidden variables and their causal relationships with observed data. We obtain an efficient learning algorithm for a family of Bayesian networks that we call quartet-learnable, meaning that every latent variable has four children that do not have any other parents in common. We show that the existence of such a quartet allows us to uniquely identify each latent variable and to learn all parameters involving that latent variable. Underlying our algorithm are two new techniques for structure learning: a quartet test to determine whether a set of binary variables are singly coupled, and a conditional mutual information test that we use to learn parameters. We also show how to subtract already learned latent variables from the model to create new singly-coupled quartets, which substantially expands the class of structures that we can learn. Finally, we give a proof of the polynomial sample complexity of our learning algorithm, and experimentally compare it to variational EM.",
    "authors": [
      "Jernite, Yacine",
      "Halpern, Yonatan",
      "Sontag, David"
    ]
  },
  {
    "id": "67d16d00201083a2b118dd5128dd6f59",
    "title": "Gaussian Process Conditional Copulas with Applications to Financial Time Series",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/67d16d00201083a2b118dd5128dd6f59-Paper.pdf",
    "abstract": "The estimation of dependencies between multiple variables is a central problem in the analysis of financial time series. A common approach is to express these dependencies in terms of a copula function. Typically the copula function is assumed to be constant but this may be innacurate when there are covariates that could have a large influence on the dependence structure of the data. To account for this, a Bayesian framework for the estimation of conditional copulas is proposed. In this framework the parameters of a copula are non-linearly related to some arbitrary conditioning variables. We evaluate the ability of our method to predict time-varying dependencies on several equities and currencies and observe consistent performance gains compared to static copula models and other time-varying copula methods.",
    "authors": [
      "Hern\u00e1ndez-Lobato, Jos\u00e9 Miguel",
      "Lloyd, James R.",
      "Hern\u00e1ndez-Lobato, Daniel"
    ]
  },
  {
    "id": "67d96d458abdef21792e6d8e590244e7",
    "title": "Non-Uniform Camera Shake Removal Using a Spatially-Adaptive Sparse Penalty",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/67d96d458abdef21792e6d8e590244e7-Paper.pdf",
    "abstract": "Typical blur from camera shake often deviates from the standard uniform convolutional assumption, in part because of problematic rotations which create greater blurring away from some unknown center point.  Consequently, successful blind deconvolution for removing shake artifacts requires the estimation of a spatially-varying or non-uniform blur operator.  Using ideas from Bayesian inference and convex analysis, this paper derives a non-uniform blind deblurring algorithm with several desirable, yet previously-unexplored attributes.  The underlying objective function includes a spatially-adaptive penalty that couples the latent sharp image, non-uniform blur operator, and noise level together.  This coupling allows the penalty to automatically adjust its shape based on the estimated degree of local blur and image structure such that regions with large blur or few prominent edges are discounted.  Remaining regions with modest blur and revealing edges therefore dominate the overall estimation process without explicitly incorporating structure-selection heuristics.  The algorithm can be implemented using an optimization strategy  that is virtually parameter free and simpler than existing methods.  Detailed theoretical analysis and empirical validation on real images serve to validate the proposed method.",
    "authors": [
      "Zhang, Haichao",
      "Wipf, David"
    ]
  },
  {
    "id": "68053af2923e00204c3ca7c6a3150cf7",
    "title": "Online learning in episodic Markovian decision processes by relative entropy policy search",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/68053af2923e00204c3ca7c6a3150cf7-Paper.pdf",
    "abstract": "We study the problem of online learning in finite episodic Markov decision processes where the loss function is allowed to change between episodes. The natural performance measure in this learning problem is the regret defined as the difference between the total loss of the best stationary policy and the total loss suffered by the learner. We assume that the learner is given access to a finite action space $\\A$ and the state space $\\X$ has a layered structure with $L$ layers, so that state transitions are only possible between consecutive layers. We describe a variant of the recently proposed Relative Entropy Policy Search algorithm and show that its regret after $T$ episodes is $2\\sqrt{L\\nX\\nA T\\log(\\nX\\nA/L)}$ in the bandit setting and $2L\\sqrt{T\\log(\\nX\\nA/L)}$ in the full information setting. These guarantees largely improve previously known results under much milder assumptions and cannot be significantly improved under general assumptions.",
    "authors": [
      "Zimin, Alexander",
      "Neu, Gergely"
    ]
  },
  {
    "id": "68a83eeb494a308fe5295da69428a507",
    "title": "Bayesian inference for low rank spatiotemporal neural receptive fields",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/68a83eeb494a308fe5295da69428a507-Paper.pdf",
    "abstract": "The receptive field (RF) of a sensory neuron describes how the   neuron integrates sensory stimuli over time and space. In typical   experiments with naturalistic or flickering spatiotemporal stimuli,   RFs are very high-dimensional, due to the large number of   coefficients needed to specify an integration profile across time   and space.  Estimating these coefficients from small amounts of data   poses a variety of challenging statistical and computational   problems.  Here we address these challenges by developing Bayesian   reduced rank regression methods for RF estimation. This corresponds   to modeling the RF as a sum of several space-time separable (i.e.,   rank-1) filters, which proves accurate even for neurons with   strongly oriented space-time RFs.  This approach substantially   reduces the number of parameters needed to specify the RF, from   1K-100K down to mere 100s in the examples we consider, and confers   substantial benefits in statistical power and computational   efficiency.  In particular, we introduce a novel prior over low-rank   RFs using the restriction of a matrix normal prior to the manifold   of low-rank matrices. We then use a localized'' prior over row and   column covariances to obtain sparse, smooth, localized estimates of   the spatial and temporal RF components.  We develop two methods for   inference in the resulting hierarchical model: (1) a fully Bayesian   method using blocked-Gibbs sampling; and (2) a fast, approximate   method that employs alternating coordinate ascent of the conditional   marginal likelihood.  We develop these methods under Gaussian and   Poisson noise models, and show that low-rank estimates substantially   outperform full rank estimates in accuracy and speed using neural   data from retina and V1.\"",
    "authors": [
      "Park, Mijung",
      "Pillow, Jonathan W."
    ]
  },
  {
    "id": "692f93be8c7a41525c0baf2076aecfb4",
    "title": "Global MAP-Optimality by Shrinking the Combinatorial Search Area with Convex Relaxation",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/692f93be8c7a41525c0baf2076aecfb4-Paper.pdf",
    "abstract": "We consider energy minimization for undirected graphical models, also known as MAP-inference problem for Markov random fields. Although combinatorial methods, which return a provably optimal integral solution of the problem, made a big progress in the past decade, they are still typically unable to cope with large-scale datasets. On the other hand, large scale datasets are typically defined on sparse graphs, and convex relaxation methods, such as linear programming relaxations often provide good approximations to integral solutions.   We propose a novel method of combining combinatorial and convex programming techniques to obtain a global solution of the initial combinatorial problem. Based on the information obtained from the solution of the convex relaxation, our method confines application of the combinatorial solver to a small fraction of the initial graphical model, which allows to optimally solve big problems.   We demonstrate the power of our approach on a computer vision energy minimization benchmark.",
    "authors": [
      "Savchynskyy, Bogdan",
      "Kappes, J\u00f6rg Hendrik",
      "Swoboda, Paul",
      "Schn\u00f6rr, Christoph"
    ]
  },
  {
    "id": "69a5b5995110b36a9a347898d97a610e",
    "title": "Error-Minimizing Estimates and Universal Entry-Wise Error Bounds for Low-Rank Matrix Completion",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/69a5b5995110b36a9a347898d97a610e-Paper.pdf",
    "abstract": "We propose a general framework for reconstructing and denoising single entries of incomplete and noisy entries. We describe: effective algorithms for deciding if and entry can be reconstructed and, if so, for reconstructing and denoising it; and a priori bounds on the error of each entry, individually.  In the noiseless case our algorithm is exact. For rank-one matrices, the new algorithm is fast, admits a highly-parallel implementation, and produces an error minimizing estimate that is qualitatively close to our theoretical and the state-of-the-art Nuclear Norm and OptSpace methods.",
    "authors": [
      "Kiraly, Franz",
      "Theran, Louis"
    ]
  },
  {
    "id": "69adc1e107f7f7d035d7baf04342e1ca",
    "title": "Decision Jungles: Compact and Rich Models for Classification",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/69adc1e107f7f7d035d7baf04342e1ca-Paper.pdf",
    "abstract": "Randomized decision trees and forests have a rich history in machine learning and have seen considerable success in application, perhaps particularly so for computer vision. However, they face a fundamental limitation: given enough data, the number of nodes in decision trees will grow exponentially with depth. For certain applications, for example on mobile or embedded processors, memory is a limited resource, and so the exponential growth of trees limits their depth, and thus their potential accuracy. This paper proposes decision jungles, revisiting the idea of ensembles of rooted decision directed acyclic graphs (DAGs), and shows these to be compact and powerful discriminative models for classification. Unlike conventional decision trees that only allow one path to every node, a DAG in a decision jungle allows multiple paths from the root to each leaf. We present and compare two new node merging algorithms that jointly optimize both the features and the structure of the DAGs efficiently. During training, node splitting and node merging are driven by the minimization of exactly the same objective function, here the weighted sum of entropies at the leaves. Results on varied datasets show that, compared to decision forests and several other baselines, decision jungles require dramatically less memory while considerably improving generalization.",
    "authors": [
      "Shotton, Jamie",
      "Sharp, Toby",
      "Kohli, Pushmeet",
      "Nowozin, Sebastian",
      "Winn, John",
      "Criminisi, Antonio"
    ]
  },
  {
    "id": "6a10bbd480e4c5573d8f3af73ae0454b",
    "title": "Bayesian Estimation of Latently-grouped Parameters in Undirected Graphical Models",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/6a10bbd480e4c5573d8f3af73ae0454b-Paper.pdf",
    "abstract": "In large-scale applications of undirected graphical models, such as social networks and biological networks, similar patterns occur frequently and give rise to similar parameters. In this situation, it is beneficial to group the parameters for more efficient learning. We show that even when the grouping is unknown, we can infer these parameter groups during learning via a Bayesian approach. We impose a Dirichlet process prior on the parameters. Posterior inference usually involves calculating intractable terms, and we propose two approximation algorithms, namely a Metropolis-Hastings algorithm with auxiliary variables and a Gibbs sampling algorithm with stripped Beta approximation (GibbsSBA). Simulations show that both algorithms outperform conventional maximum likelihood estimation (MLE). GibbsSBA's performance is close to Gibbs sampling with exact likelihood calculation. Models learned with Gibbs_SBA also generalize better than the models learned by MLE on real-world Senate voting data.",
    "authors": [
      "Liu, Jie",
      "Page, David"
    ]
  },
  {
    "id": "6a5889bb0190d0211a991f47bb19a777",
    "title": "(More) Efficient Reinforcement Learning via Posterior Sampling",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/6a5889bb0190d0211a991f47bb19a777-Paper.pdf",
    "abstract": "Most provably efficient learning algorithms introduce optimism about poorly-understood states and actions to encourage exploration. We study an alternative approach for efficient exploration, posterior sampling for reinforcement learning (PSRL). This algorithm proceeds in repeated episodes of known duration. At the start of each episode, PSRL updates a prior distribution over Markov decision processes and takes one sample from this posterior. PSRL then follows the policy that is optimal for this sample during the episode. The algorithm is conceptually simple, computationally efficient and allows an agent to encode prior knowledge in a natural way. We establish an $\\tilde{O}(\\tau S \\sqrt{AT} )$ bound on the expected regret, where $T$ is time, $\\tau$ is the episode length and $S$ and $A$ are the cardinalities of the state and action spaces. This bound is one of the first for an algorithm not based on optimism and close to the state of the art for any reinforcement learning algorithm. We show through simulation that PSRL significantly outperforms existing algorithms with similar regret bounds.",
    "authors": [
      "Osband, Ian",
      "Russo, Daniel",
      "Van Roy, Benjamin"
    ]
  },
  {
    "id": "6c14da109e294d1e8155be8aa4b1ce8e",
    "title": "Forgetful Bayes and myopic planning: Human learning and decision-making in a bandit setting",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/6c14da109e294d1e8155be8aa4b1ce8e-Paper.pdf",
    "abstract": "How humans achieve long-term goals in an uncertain environment, via repeated trials and noisy observations, is an important problem in cognitive science. We investigate this behavior in the context of a multi-armed bandit task. We compare human behavior to a variety of models that vary in their representational and computational complexity. Our result shows that subjects' choices, on a trial-to-trial basis, are best captured by a forgetful\" Bayesian iterative learning model in combination with a partially myopic decision policy known as Knowledge Gradient. This model accounts for subjects' trial-by-trial choice better than a number of other previously proposed models, including optimal Bayesian learning and risk minimization, epsilon-greedy and win-stay-lose-shift. It has the added benefit of being closest in performance to the optimal Bayesian model than all the other heuristic models that have the same computational complexity (all are significantly less complex than the optimal model). These results constitute an advancement in the theoretical understanding of how humans negotiate the tension between exploration and exploitation in a noisy, imperfectly known environment.\"",
    "authors": [
      "Zhang, Shunan",
      "Yu, Angela J."
    ]
  },
  {
    "id": "6d0f846348a856321729a2f36734d1a7",
    "title": "Stochastic Optimization of PCA with Capped MSG",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/6d0f846348a856321729a2f36734d1a7-Paper.pdf",
    "abstract": "We study PCA as a stochastic optimization problem and propose a novel stochastic approximation algorithm which we refer to as Matrix Stochastic Gradient'' (MSG), as well as a practical variant, Capped MSG. We study the method both theoretically and empirically. \"",
    "authors": [
      "Arora, Raman",
      "Cotter, Andy",
      "Srebro, Nati"
    ]
  },
  {
    "id": "6d70cb65d15211726dcce4c0e971e21c",
    "title": "Embed and Project: Discrete Sampling with Universal Hashing",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/6d70cb65d15211726dcce4c0e971e21c-Paper.pdf",
    "abstract": "We consider the problem of sampling from a probability distribution defined over a high-dimensional discrete set, specified for instance by a graphical model. We propose a sampling algorithm, called PAWS, based on embedding the set into a higher-dimensional space which is then randomly projected using universal hash functions to a lower-dimensional subspace and explored using combinatorial search methods. Our scheme can leverage fast combinatorial optimization tools as a blackbox and, unlike MCMC methods, samples produced are guaranteed to be within an (arbitrarily small) constant factor of the true probability distribution. We demonstrate that by using state-of-the-art combinatorial search tools, PAWS can efficiently sample from Ising grids with strong interactions and from software verification instances, while MCMC and variational methods fail in both cases.",
    "authors": [
      "Ermon, Stefano",
      "Gomes, Carla P.",
      "Sabharwal, Ashish",
      "Selman, Bart"
    ]
  },
  {
    "id": "6da9003b743b65f4c0ccd295cc484e57",
    "title": "Optimal Neural Population Codes for High-dimensional Stimulus Variables",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/6da9003b743b65f4c0ccd295cc484e57-Paper.pdf",
    "abstract": "How does neural population process sensory information? Optimal coding theories assume that neural tuning curves are adapted to the prior distribution of the stimulus variable. Most of the previous work has discussed optimal solutions for only one-dimensional stimulus variables. Here, we expand some of these ideas and present new solutions that define optimal tuning curves for high-dimensional stimulus variables. We consider solutions for a minimal case where the number of neurons in the population is equal to the number of stimulus dimensions (diffeomorphic). In the case of two-dimensional stimulus variables, we analytically derive optimal solutions for different optimal criteria such as minimal L2 reconstruction error or maximal mutual information. For higher dimensional case, the learning rule to improve the population code is provided.",
    "authors": [
      "Wang, Zhuo",
      "Stocker, Alan A.",
      "Lee, Daniel D."
    ]
  },
  {
    "id": "6e0721b2c6977135b916ef286bcb49ec",
    "title": "Near-Optimal Entrywise Sampling for Data Matrices",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/6e0721b2c6977135b916ef286bcb49ec-Paper.pdf",
    "abstract": "We consider the problem of independently sampling $s$ non-zero entries of a matrix $A$ in order to produce a sparse sketch of it, $B$, that minimizes $\\|A-B\\|_2$.  For large $m \\times n$ matrices, such that $n \\gg m$ (for example, representing $n$ observations over $m$ attributes) we give  distributions exhibiting four important properties. First, they have closed forms for the probability of sampling each item which are computable from minimal information regarding $A$.  Second, they allow sketching of matrices whose non-zeros are presented to the algorithm in arbitrary order as a stream, with $O(1)$ computation per non-zero.  Third, the resulting sketch matrices are not only sparse, but their non-zero entries are highly compressible.  Lastly, and most importantly, under mild assumptions, our distributions are provably competitive with the optimal offline distribution.  Note that the probabilities in the optimal offline distribution may be complex functions of all the entries in the matrix.  Therefore, regardless of computational complexity, the optimal distribution might be impossible to compute in the streaming model.",
    "authors": [
      "Achlioptas, Dimitris",
      "Karnin, Zohar S.",
      "Liberty, Edo"
    ]
  },
  {
    "id": "6e2713a6efee97bacb63e52c54f0ada0",
    "title": "A Comparative Framework for Preconditioned Lasso Algorithms",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/6e2713a6efee97bacb63e52c54f0ada0-Paper.pdf",
    "abstract": "The Lasso is a cornerstone of modern multivariate data analysis, yet its performance suffers in the common situation in which covariates are correlated. This limitation has led to a growing number of \\emph{Preconditioned Lasso} algorithms that pre-multiply $X$ and $y$ by matrices $P_X$, $P_y$ prior to running the standard Lasso. A direct comparison of these and similar Lasso-style algorithms to the original Lasso is difficult because the performance of all of these methods depends critically on an auxiliary penalty parameter $\\lambda$. In this paper we propose an agnostic, theoretical framework for comparing Preconditioned Lasso algorithms to the Lasso without having to choose $\\lambda$. We apply our framework to three Preconditioned Lasso instances and highlight when they will outperform the Lasso. Additionally, our theory offers insights into the fragilities of these algorithms to which we provide partial solutions.",
    "authors": [
      "Wauthier, Fabian L.",
      "Jojic, Nebojsa",
      "Jordan, Michael I."
    ]
  },
  {
    "id": "6e7d2da6d3953058db75714ac400b584",
    "title": "Universal models for binary spike patterns using centered Dirichlet processes",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/6e7d2da6d3953058db75714ac400b584-Paper.pdf",
    "abstract": "Probabilistic models for binary spike patterns provide a powerful   tool for understanding the statistical dependencies in large-scale   neural recordings.  Maximum entropy (or maxent'') models, which   seek to explain dependencies in terms of low-order interactions   between neurons, have enjoyed remarkable success in modeling such   patterns, particularly for small groups of neurons. However, these   models are computationally intractable for large populations, and   low-order maxent models have been shown to be inadequate for some   datasets.  To overcome these limitations, we propose a family of   \"universal'' models for binary spike patterns, where universality   refers to the ability to model arbitrary distributions over all   $2^m$ binary patterns.  We construct universal models using a   Dirichlet process centered on a well-behaved parametric base   measure, which naturally combines the flexibility of a histogram and   the parsimony of a parametric model.  We derive computationally   efficient inference methods using Bernoulli and cascade-logistic   base measures, which scale tractably to large populations. We also   establish a condition for equivalence between the cascade-logistic   and the 2nd-order maxent or \"Ising'' model, making cascade-logistic   a reasonable choice for base measure in a universal model. We illustrate the performance of these models using neural data.\"",
    "authors": [
      "Park, Il Memming",
      "Archer, Evan W.",
      "Latimer, Kenneth",
      "Pillow, Jonathan W."
    ]
  },
  {
    "id": "6f3ef77ac0e3619e98159e9b6febf557",
    "title": "What Are the Invariant Occlusive Components of Image Patches? A Probabilistic Generative Approach",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/6f3ef77ac0e3619e98159e9b6febf557-Paper.pdf",
    "abstract": "We study optimal image encoding based on a generative approach with non-linear feature combinations and explicit position encoding. By far most approaches to unsupervised learning learning of visual features, such as sparse coding or ICA, account for translations by representing the same features at different positions. Some earlier models used a separate encoding of features and their positions to facilitate invariant data encoding and recognition. All probabilistic generative models with explicit position encoding have so far assumed a linear superposition of components to encode image patches. Here, we for the first time apply a model with non-linear feature superposition and explicit position encoding. By avoiding linear superpositions, the studied model represents a closer match to component occlusions which are ubiquitous in natural images. In order to account for occlusions, the non-linear model encodes patches qualitatively very different from linear models by using component representations separated into mask and feature parameters. We first investigated encodings learned by the model using artificial data with mutually occluding components. We find that the model extracts the components, and that it can correctly identify the occlusive components with the hidden variables of the model. On natural image patches, the model learns component masks and features for typical image components. By using reverse correlation, we estimate the receptive fields associated with the model's hidden units. We find many Gabor-like or globular receptive fields as well as fields sensitive to more complex structures. Our results show that probabilistic models that capture occlusions and invariances can be trained efficiently on image patches, and that the resulting encoding represents an alternative model for the neural encoding of images in the primary visual cortex.",
    "authors": [
      "Dai, Zhenwen",
      "Exarchakis, Georgios",
      "L\u00fccke, J\u00f6rg"
    ]
  },
  {
    "id": "705f2172834666788607efbfca35afb3",
    "title": "Correlations strike back (again): the case of associative memory retrieval",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/705f2172834666788607efbfca35afb3-Paper.pdf",
    "abstract": "It has long been recognised that statistical dependencies in neuronal activity need to be taken into account when decoding stimuli encoded in a neural population. Less studied, though equally pernicious, is the need to take account of dependencies between synaptic weights when decoding patterns previously encoded in an auto-associative memory. We show that activity-dependent learning generically produces such correlations, and failing to take them into account in the dynamics of memory retrieval leads to catastrophically poor recall. We derive optimal network dynamics for recall in the face of synaptic correlations caused by a range of synaptic plasticity rules. These dynamics involve well-studied circuit motifs, such as forms of feedback inhibition and experimentally observed dendritic nonlinearities. We therefore show how addressing the problem of synaptic correlations leads to a novel functional account of key biophysical features of the neural substrate.",
    "authors": [
      "Savin, Cristina",
      "Dayan, Peter",
      "Lengyel, Mate"
    ]
  },
  {
    "id": "71f6278d140af599e06ad9bf1ba03cb0",
    "title": "Understanding Dropout",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/71f6278d140af599e06ad9bf1ba03cb0-Paper.pdf",
    "abstract": "Dropout is a relatively new algorithm for training neural networks which relies on stochastically dropping out'' neurons during training in order to avoid the co-adaptation of feature detectors. We introduce a general formalism for studying dropout on either units or connections, with arbitrary probability values, and use it to analyze the averaging and regularizing properties of dropout in both linear and non-linear networks. For deep neural networks, the averaging properties of dropout are characterized by three recursive equations, including the approximation of expectations by normalized weighted geometric means. We provide estimates and bounds for these approximations and corroborate the results with simulations. We also show in simple cases how dropout performs stochastic gradient descent on a regularized error function.\"",
    "authors": [
      "Baldi, Pierre",
      "Sadowski, Peter J."
    ]
  },
  {
    "id": "7380ad8a673226ae47fce7bff88e9c33",
    "title": "Supervised Sparse Analysis and Synthesis Operators",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/7380ad8a673226ae47fce7bff88e9c33-Paper.pdf",
    "abstract": "In this paper, we propose a new and computationally efficient framework for learning sparse models. We formulate a unified approach that contains as particular cases models promoting sparse synthesis and analysis type of priors, and mixtures thereof. The supervised training of the proposed model is formulated as a bilevel optimization problem, in which the operators are optimized to achieve the best possible performance on a specific task, e.g., reconstruction or classification. By restricting the operators to be shift invariant, our approach can be thought as a way of learning analysis+synthesis sparsity-promoting convolutional operators. Leveraging recent ideas on fast trainable regressors designed to approximate exact sparse codes, we propose a way of constructing feed-forward neural networks capable of approximating the learned models at a fraction of the computational cost of exact solvers. In the shift-invariant case, this leads to a principled way of constructing task-specific convolutional networks. We illustrate the proposed models on several experiments in music analysis and image processing applications.",
    "authors": [
      "Sprechmann, Pablo",
      "Litman, Roee",
      "Ben Yakar, Tal",
      "Bronstein, Alexander M.",
      "Sapiro, Guillermo"
    ]
  },
  {
    "id": "74071a673307ca7459bcf75fbd024e09",
    "title": "The Pareto Regret Frontier",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/74071a673307ca7459bcf75fbd024e09-Paper.pdf",
    "abstract": "Performance guarantees for online learning algorithms typically take the form of regret bounds, which express that the cumulative loss overhead compared to the best expert in hindsight is small. In the common case of large but structured expert sets we typically wish to keep the regret especially small compared to simple experts, at the cost of modest additional overhead  compared to more complex others. We study which such regret trade-offs can be achieved, and how.  We analyse regret w.r.t. each individual expert as a multi-objective criterion in the simple but fundamental case of absolute loss. We characterise the achievable and Pareto optimal trade-offs, and the corresponding optimal strategies for each sample size both exactly for each finite horizon and asymptotically.",
    "authors": [
      "Koolen, Wouter M."
    ]
  },
  {
    "id": "7504adad8bb96320eb3afdd4df6e1f60",
    "title": "Approximate Dynamic Programming Finally Performs Well in the Game of Tetris",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/7504adad8bb96320eb3afdd4df6e1f60-Paper.pdf",
    "abstract": "Tetris is a popular video game that has been widely used as a benchmark for various optimization techniques including approximate dynamic programming (ADP) algorithms. A close look at the literature of this game shows that while ADP algorithms, that have been (almost) entirely based on approximating the value function (value function based), have performed poorly in Tetris, the methods that search directly in the space of policies by learning the policy parameters using an optimization black box, such as the cross entropy (CE) method, have achieved the best reported results. This makes us conjecture that Tetris is a game in which good policies are easier to represent, and thus, learn than their corresponding value functions. So, in order to obtain a good performance with ADP, we should use ADP algorithms that search in a policy space, instead of the more traditional ones that search in a value function space. In this paper, we put our conjecture to test by applying such an ADP algorithm, called classification-based modified policy iteration (CBMPI), to the game of Tetris. Our extensive experimental results show that for the first time an ADP algorithm, namely CBMPI, obtains the best results reported in the literature for Tetris in both small $10\\times 10$ and large $10\\times 20$ boards. Although the CBMPI's results are similar to those achieved by the CE method in the large board, CBMPI uses considerably fewer (almost 1/10) samples (call to the generative model of the game) than CE.",
    "authors": [
      "Gabillon, Victor",
      "Ghavamzadeh, Mohammad",
      "Scherrer, Bruno"
    ]
  },
  {
    "id": "75fc093c0ee742f6dddaa13fff98f104",
    "title": "Learning Feature Selection Dependencies in Multi-task Learning",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/75fc093c0ee742f6dddaa13fff98f104-Paper.pdf",
    "abstract": "A probabilistic model based on the horseshoe prior is proposed for learning dependencies in the process of identifying relevant features for prediction. Exact inference is intractable in this model. However, expectation propagation offers an approximate alternative. Because the process of estimating feature selection dependencies may suffer from over-fitting in the model proposed, additional data from a multi-task learning scenario are considered for induction. The same model can be used in this setting with few modifications. Furthermore, the assumptions made are less restrictive than in other multi-task methods: The different tasks must share feature selection dependencies, but can have different relevant features and model coefficients. Experiments with real and synthetic data show that this model performs better than other multi-task alternatives from the literature. The experiments also show that the model is able to induce suitable feature selection dependencies for the problems considered, only from the training data.",
    "authors": [
      "Hern\u00e1ndez-Lobato, Daniel",
      "Hern\u00e1ndez-Lobato, Jos\u00e9 Miguel"
    ]
  },
  {
    "id": "7634ea65a4e6d9041cfd3f7de18e334a",
    "title": "Dimension-Free Exponentiated Gradient",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/7634ea65a4e6d9041cfd3f7de18e334a-Paper.pdf",
    "abstract": "We present a new online learning algorithm that extends the exponentiated gradient to infinite dimensional spaces. Our analysis shows that the algorithm is implicitly able to estimate the $L_2$ norm of the unknown competitor, $U$, achieving a regret bound of the order of $O(U \\log (U T+1))\\sqrt{T})$, instead of the standard $O((U^2 +1) \\sqrt{T})$, achievable without knowing $U$. For this analysis, we introduce novel tools for algorithms with time-varying regularizers, through the use of local smoothness. Through a lower bound, we also show that the algorithm is optimal up to $\\sqrt{\\log T}$ term for linear and Lipschitz losses.",
    "authors": [
      "Orabona, Francesco"
    ]
  },
  {
    "id": "76cf99d3614e23eabab16fb27e944bf9",
    "title": "Memory Limited, Streaming PCA",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/76cf99d3614e23eabab16fb27e944bf9-Paper.pdf",
    "abstract": "We consider streaming, one-pass principal component analysis (PCA), in the high-dimensional regime, with limited memory. Here, $p$-dimensional samples are presented sequentially, and the goal is to produce the $k$-dimensional subspace that best approximates these points. Standard algorithms require $O(p^2)$ memory; meanwhile no algorithm can do better than $O(kp)$ memory, since this is what the output itself requires. Memory (or storage) complexity is most meaningful when understood in the context of computational and sample complexity. Sample complexity for high-dimensional PCA is typically studied in the setting of the {\\em spiked covariance model}, where $p$-dimensional points are generated from a population covariance equal to the identity (white noise) plus a low-dimensional perturbation (the spike) which is the signal to be recovered. It is now well-understood that the spike can be recovered when the number of samples, $n$, scales proportionally with the dimension, $p$. Yet, all algorithms that provably achieve this, have memory complexity $O(p^2)$. Meanwhile, algorithms with memory-complexity $O(kp)$ do not have provable bounds on sample complexity comparable to $p$. We present an algorithm that achieves both: it uses $O(kp)$ memory (meaning storage of any kind) and is able to compute the $k$-dimensional spike with $O(p \\log p)$ sample-complexity -- the first algorithm of its kind. While our theoretical analysis focuses on the spiked covariance model, our simulations show that our algorithm is successful on much more general models for the data.",
    "authors": [
      "Mitliagkas, Ioannis",
      "Caramanis, Constantine",
      "Jain, Prateek"
    ]
  },
  {
    "id": "7810ccd41bf26faaa2c4e1f20db70a71",
    "title": "\u03a3-Optimality for Active Learning on Gaussian Random Fields",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/7810ccd41bf26faaa2c4e1f20db70a71-Paper.pdf",
    "abstract": "A common classifier for unlabeled nodes on undirected graphs uses label propagation from the labeled nodes, equivalent to the harmonic predictor on Gaussian random fields (GRFs). For active learning on GRFs, the commonly used V-optimality criterion queries nodes that reduce the L2 (regression) loss. V-optimality satisfies a submodularity property showing that greedy reduction produces a (1 \u2212 1/e) globally optimal solution. However, L2 loss may not characterise the true nature of 0/1 loss in classification problems and thus may not be the best choice for active learning. We consider a new criterion we call \u03a3-optimality, which queries the node that minimizes the sum of the elements in the predictive covariance. \u03a3-optimality directly optimizes the risk of the surveying problem, which is to determine the proportion of nodes belonging to one class. In this paper we extend submodularity guarantees from V-optimality to \u03a3-optimality using properties specific to GRFs. We further show that GRFs satisfy the suppressor-free condition in addition to the conditional independence inherited from Markov random fields. We test \u03a3-optimality on real-world graphs with both synthetic and real data and show that it outperforms V-optimality and other related methods on classification.",
    "authors": [
      "Ma, Yifei",
      "Garnett, Roman",
      "Schneider, Jeff"
    ]
  },
  {
    "id": "7895fc13088ee37f511913bac71fa66f",
    "title": "Recurrent linear models of simultaneously-recorded neural   populations",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/7895fc13088ee37f511913bac71fa66f-Paper.pdf",
    "abstract": "Population neural recordings with long-range temporal structure are often best understood in terms of a shared underlying low-dimensional dynamical process. Advances in recording technology provide access to an ever larger fraction of the population, but the standard computational approaches available to identify the collective dynamics scale poorly with the size of the dataset. Here we describe a new, scalable approach to discovering the low-dimensional dynamics that underlie simultaneously recorded spike trains from a neural population. Our method is based on recurrent linear models (RLMs), and relates closely to timeseries models based on recurrent neural networks. We formulate RLMs for neural data by generalising the Kalman-filter-based likelihood calculation for latent linear dynamical systems (LDS) models to incorporate a generalised-linear observation process. We show that RLMs describe motor-cortical population data better than either directly-coupled generalised-linear models or latent linear dynamical system models with generalised-linear observations. We also introduce the cascaded linear model (CLM) to capture low-dimensional instantaneous correlations in neural populations. The CLM describes the cortical recordings better than either Ising or Gaussian models and, like the RLM, can be fit exactly and quickly. The CLM can also be seen as a generalization of a low-rank Gaussian model, in this case factor analysis. The computational tractability of the RLM and CLM allow both to scale to very high-dimensional neural data.",
    "authors": [
      "Pachitariu, Marius",
      "Petreska, Biljana",
      "Sahani, Maneesh"
    ]
  },
  {
    "id": "7940ab47468396569a906f75ff3f20ef",
    "title": "On the Complexity and Approximation of Binary Evidence in Lifted Inference",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/7940ab47468396569a906f75ff3f20ef-Paper.pdf",
    "abstract": "Lifted inference algorithms exploit symmetries in probabilistic models to speed up inference. They show impressive performance when calculating unconditional probabilities in relational models, but often resort to non-lifted inference when computing conditional probabilities.  The reason is that conditioning on evidence breaks many of the model's symmetries, which preempts standard lifting techniques.  Recent theoretical results show, for example, that conditioning on evidence which corresponds to binary relations is #P-hard, suggesting that no lifting is to be expected in the worst case. In this paper, we balance this grim result by identifying the Boolean rank of the evidence as a key parameter for characterizing the complexity of conditioning in lifted inference.  In particular, we show that conditioning on binary evidence with bounded Boolean rank is efficient. This opens up the possibility of approximating evidence by a low-rank Boolean matrix factorization, which we investigate both theoretically and empirically.",
    "authors": [
      "Van den Broeck, Guy",
      "Darwiche, Adnan"
    ]
  },
  {
    "id": "7a53928fa4dd31e82c6ef826f341daec",
    "title": "Pass-efficient unsupervised feature selection",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/7a53928fa4dd31e82c6ef826f341daec-Paper.pdf",
    "abstract": "The goal of unsupervised feature selection is to identify a small number of important features that can represent the data. We propose a new algorithm, a modification of the classical pivoted QR algorithm of Businger and Golub, that requires a small number of passes over the data. The improvements are based on two ideas: keeping track of multiple features in each pass, and skipping calculations that can be shown not to affect the final selection. Our algorithm selects the exact same features as the classical pivoted QR algorithm, and has the same favorable numerical stability. We describe experiments on real-world datasets which sometimes show improvements of {\\em several orders of magnitude} over the classical algorithm. These results appear to be competitive with  recently proposed randomized algorithms in terms of pass efficiency and run time. On the other hand, the randomized algorithms may produce better features, at the cost of small probability of failure.",
    "authors": [
      "Maung, Crystal",
      "Schweitzer, Haim"
    ]
  },
  {
    "id": "7b5b23f4aadf9513306bcd59afb6e4c9",
    "title": "Adaptive dropout for training deep neural networks",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/7b5b23f4aadf9513306bcd59afb6e4c9-Paper.pdf",
    "abstract": "Recently, it was shown that by dropping out hidden activities with a probability of 0.5, deep neural networks can perform very well. We describe a model in which a binary belief network is overlaid on a neural network and is used to decrease the information content of its hidden units by selectively setting activities to zero. This ''dropout network can be trained jointly with the neural network by approximately computing local expectations of binary dropout variables, computing derivatives using back-propagation, and using stochastic gradient descent. Interestingly, experiments show that the learnt dropout network parameters recapitulate the neural network parameters, suggesting that a good dropout network regularizes activities according to magnitude. When evaluated on the MNIST and NORB datasets, we found our method can be used to achieve lower classification error rates than other feather learning methods, including standard dropout, denoising auto-encoders, and restricted Boltzmann machines. For example, our model achieves 5.8% error on the NORB test set, which is better than state-of-the-art results obtained using convolutional architectures. \"",
    "authors": [
      "Ba, Jimmy",
      "Frey, Brendan"
    ]
  },
  {
    "id": "7bb060764a818184ebb1cc0d43d382aa",
    "title": "On the Representational Efficiency of Restricted Boltzmann Machines",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/7bb060764a818184ebb1cc0d43d382aa-Paper.pdf",
    "abstract": "This paper examines the question: What kinds of distributions can be efficiently represented by Restricted Boltzmann Machines (RBMs)?   We characterize the RBM's unnormalized log-likelihood function as a type of neural network (called an RBM network), and through a series of simulation results relate these networks to types that are better understood.  We show the surprising result that RBM networks can efficiently compute any function that depends on the number of 1's in the input, such as parity.  We also provide the first known example of a particular type of distribution which provably cannot be efficiently represented by an RBM (or equivalently, cannot be efficiently computed by an RBM network), assuming a realistic exponential upper bound on the size of the weights.  By formally demonstrating that a relatively simple distribution cannot be represented efficiently by an RBM our results provide a new rigorous justification for the use of potentially more expressive generative models, such as deeper ones.",
    "authors": [
      "Martens, James",
      "Chattopadhya, Arkadev",
      "Pitassi, Toni",
      "Zemel, Richard"
    ]
  },
  {
    "id": "7bcdf75ad237b8e02e301f4091fb6bc8",
    "title": "Robust Spatial Filtering with Beta Divergence",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/7bcdf75ad237b8e02e301f4091fb6bc8-Paper.pdf",
    "abstract": "The efficiency of Brain-Computer Interfaces (BCI) largely depends upon a reliable extraction of informative features from the high-dimensional EEG signal. A crucial step in this protocol is the computation of spatial filters. The Common Spatial Patterns (CSP) algorithm computes filters that maximize the difference in band power between two conditions, thus it is tailored to extract the relevant information in motor imagery experiments. However, CSP is highly sensitive to artifacts in the EEG data, i.e. few outliers may alter the estimate drastically and decrease classification performance. Inspired by concepts from the field of information geometry we propose a novel approach for robustifying CSP. More precisely, we formulate CSP as a divergence maximization problem and utilize the property of a particular type of divergence, namely beta divergence, for robustifying the estimation of spatial filters in the presence of artifacts in the data. We demonstrate the usefulness of our method on toy data and on EEG recordings from 80 subjects.",
    "authors": [
      "Samek, Wojciech",
      "Blythe, Duncan",
      "M\u00fcller, Klaus-Robert",
      "Kawanabe, Motoaki"
    ]
  },
  {
    "id": "7cce53cf90577442771720a370c3c723",
    "title": "DeViSE: A Deep Visual-Semantic Embedding Model",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/7cce53cf90577442771720a370c3c723-Paper.pdf",
    "abstract": "Modern visual recognition systems are often limited in their ability to scale to large numbers of object categories. This limitation is in part due to the increasing difficulty of acquiring sufficient training data in the form of labeled images as the number of object categories grows. One remedy is to leverage data from other sources -- such as text data -- both to train visual models and to constrain their predictions. In this paper we present a new deep visual-semantic embedding model trained to identify visual objects using both labeled image data as well as semantic information gleaned from unannotated text. We demonstrate that this model matches state-of-the-art performance on the 1000-class ImageNet object recognition challenge while making more semantically reasonable errors, and also show that the semantic information can be exploited to make predictions about tens of thousands of image labels not observed during training. Semantic knowledge improves such zero-shot predictions by up to 65%, achieving hit rates of up to 10% across thousands of novel labels never seen by the visual model.",
    "authors": [
      "Frome, Andrea",
      "Corrado, Greg S.",
      "Shlens, Jon",
      "Bengio, Samy",
      "Dean, Jeff",
      "Ranzato, Marc'Aurelio",
      "Mikolov, Tomas"
    ]
  },
  {
    "id": "7d771e0e8f3633ab54856925ecdefc5d",
    "title": "Symbolic Opportunistic Policy Iteration for Factored-Action MDPs",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/7d771e0e8f3633ab54856925ecdefc5d-Paper.pdf",
    "abstract": "We address the scalability of symbolic planning under uncertainty with factored states and actions. Prior work has focused almost exclusively on factored states but not factored actions, and on value iteration (VI) compared to policy iteration (PI). Our \ufb01rst contribution is a novel method for symbolic policy backups via the application of constraints, which is used to yield a new ef\ufb01cient symbolic imple- mentation of modi\ufb01ed PI (MPI) for factored action spaces. While this approach improves scalability in some cases, naive handling of policy constraints comes with its own scalability issues. This leads to our second and main contribution, symbolic Opportunistic Policy Iteration (OPI), which is a novel convergent al- gorithm lying between VI and MPI. The core idea is a symbolic procedure that applies policy constraints only when they reduce the space and time complexity of the update, and otherwise performs full Bellman backups, thus automatically adjusting the backup per state. We also give a memory bounded version of this algorithm allowing a space-time tradeoff. Empirical results show signi\ufb01cantly improved scalability over the state-of-the-art.",
    "authors": [
      "Raghavan, Aswin",
      "Khardon, Roni",
      "Fern, Alan",
      "Tadepalli, Prasad"
    ]
  },
  {
    "id": "7f100b7b36092fb9b06dfb4fac360931",
    "title": "Least Informative Dimensions",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/7f100b7b36092fb9b06dfb4fac360931-Paper.pdf",
    "abstract": "We present a novel non-parametric method for finding a subspace of stimulus features that contains all information about the response of a system. Our method generalizes similar approaches to this problem such as spike triggered average, spike triggered covariance, or maximally informative dimensions. Instead of maximizing the mutual information between features and responses directly, we use integral probability metrics in kernel Hilbert spaces to minimize the information between uninformative features and the combination of informative features and responses. Since estimators of these metrics access the data via kernels, are easy to compute, and exhibit good theoretical convergence properties, our method can easily be generalized to populations of neurons or spike patterns. By using a particular expansion of the mutual information, we can show that the informative features must contain all information if we can make the uninformative features independent of the rest.",
    "authors": [
      "Sinz, Fabian",
      "Stockl, Anna",
      "Grewe, Jan",
      "Benda, Jan"
    ]
  },
  {
    "id": "7f24d240521d99071c93af3917215ef7",
    "title": "A memory frontier for complex synapses",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/7f24d240521d99071c93af3917215ef7-Paper.pdf",
    "abstract": "An incredible gulf separates theoretical models of synapses, often described solely by a single scalar value denoting the size of a postsynaptic potential, from the immense complexity of molecular signaling pathways underlying real synapses. To understand the functional contribution of such molecular complexity to learning and memory, it is essential to expand our theoretical conception of a synapse from a single scalar to an entire dynamical system with many internal molecular functional states. Moreover, theoretical considerations alone demand such an expansion; network models with scalar synapses assuming finite numbers of distinguishable synaptic strengths have strikingly limited memory capacity. This raises the fundamental question, how does synaptic complexity give rise to memory? To address this, we develop new mathematical theorems elucidating the relationship between the structural organization and memory properties of complex synapses that are themselves molecular networks. Moreover, in proving such theorems, we uncover a framework, based on first passage time theory, to impose an order on the internal states of complex synaptic models, thereby simplifying the relationship between synaptic structure and function.",
    "authors": [
      "Lahiri, Subhaneil",
      "Ganguli, Surya"
    ]
  },
  {
    "id": "7f39f8317fbdb1988ef4c628eba02591",
    "title": "Data-driven Distributionally Robust Polynomial Optimization",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/7f39f8317fbdb1988ef4c628eba02591-Paper.pdf",
    "abstract": "We consider robust optimization for polynomial optimization problems where the uncertainty set is a set of candidate probability density functions. This set is a ball around a density function estimated from data samples, i.e., it is data-driven and random.  Polynomial optimization problems are inherently hard due to nonconvex objectives and constraints.  However, we show that by employing polynomial and histogram density estimates, we can introduce robustness with respect to distributional uncertainty sets without making the problem harder.  We show that the solution to the distributionally robust problem is the limit of a sequence of tractable semidefinite programming relaxations.  We also give finite-sample consistency guarantees for the data-driven uncertainty  sets.  Finally, we apply our model and solution method in a water network problem.",
    "authors": [
      "Mevissen, Martin",
      "Ragnoli, Emanuele",
      "Yu, Jia Yuan"
    ]
  },
  {
    "id": "7f53f8c6c730af6aeb52e66eb74d8507",
    "title": "Learning Stochastic Inverses",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/7f53f8c6c730af6aeb52e66eb74d8507-Paper.pdf",
    "abstract": "We describe a class of algorithms for amortized inference in Bayesian networks. In this setting, we invest computation upfront to support rapid online inference for a wide range of queries. Our approach is based on learning an inverse factorization of a model's joint distribution: a factorization that turns observations into root nodes.  Our algorithms accumulate information to estimate the local conditional distributions that constitute such a factorization. These stochastic inverses can be used to invert each of the computation steps leading to an observation, sampling backwards in order to quickly find a likely explanation. We show that estimated inverses converge asymptotically in number of (prior or posterior) training samples.  To make use of inverses before convergence, we describe the Inverse MCMC algorithm, which uses stochastic inverses to make block proposals for a Metropolis-Hastings sampler.  We explore the efficiency of this sampler for a variety of parameter regimes and Bayes nets.",
    "authors": [
      "Stuhlm\u00fcller, Andreas",
      "Taylor, Jacob",
      "Goodman, Noah"
    ]
  },
  {
    "id": "7f5d04d189dfb634e6a85bb9d9adf21e",
    "title": "Stochastic Ratio Matching of RBMs for Sparse High-Dimensional Inputs",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/7f5d04d189dfb634e6a85bb9d9adf21e-Paper.pdf",
    "abstract": "Sparse high-dimensional data vectors are common in many application domains where a very large number of rarely non-zero features can be devised. Unfortunately, this creates a computational bottleneck for unsupervised feature learning algorithms such as those based on auto-encoders and RBMs, because they involve a reconstruction step where the whole input vector is predicted from the current feature values. An algorithm was recently developed to successfully handle the case of auto-encoders, based on an importance sampling  scheme stochastically selecting which input elements to actually reconstruct during training for each particular example.  To generalize this idea to RBMs, we propose a stochastic ratio-matching algorithm that inherits all the computational advantages and unbiasedness of the importance sampling scheme. We show that stochastic ratio matching is a good estimator, allowing the approach to beat the state-of-the-art on two bag-of-word text classification benchmarks (20 Newsgroups and RCV1), while keeping computational cost linear in the number of non-zeros.",
    "authors": [
      "Dauphin, Yann",
      "Bengio, Yoshua"
    ]
  },
  {
    "id": "7f975a56c761db6506eca0b37ce6ec87",
    "title": "Distributed $k$-means and $k$-median Clustering on General Topologies",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/7f975a56c761db6506eca0b37ce6ec87-Paper.pdf",
    "abstract": "This paper provides new algorithms for distributed clustering for two popular center-based objectives, $k$-median and $k$-means. These algorithms have provable guarantees and improve communication complexity over existing approaches. Following a classic approach in clustering by \\cite{har2004coresets}, we reduce the problem of finding a clustering with low cost to the problem of finding a `coreset' of small size. We provide a distributed method for constructing a global coreset which improves over the previous methods by reducing the communication complexity, and which works over general communication topologies. We provide experimental evidence for this approach on both synthetic and real data sets.",
    "authors": [
      "Balcan, Maria-Florina F.",
      "Ehrlich, Steven",
      "Liang, Yingyu"
    ]
  },
  {
    "id": "7fe1f8abaad094e0b5cb1b01d712f708",
    "title": "Non-strongly-convex smooth stochastic approximation with convergence rate O(1/n)",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/7fe1f8abaad094e0b5cb1b01d712f708-Paper.pdf",
    "abstract": "We consider the stochastic approximation problem where a convex function has to be minimized, given only the knowledge of unbiased estimates of its gradients at certain points, a framework which includes machine learning methods based on the minimization of the empirical risk. We focus on problems without strong convexity, for which all previously known algorithms achieve a convergence rate for function values of $O(1/\\sqrt{n})$. We consider and analyze two algorithms that achieve a rate of $O(1/n)$ for classical   supervised learning problems. For least-squares regression, we show that   averaged stochastic gradient descent with constant step-size achieves the desired rate.  For logistic regression, this is achieved by a simple novel stochastic gradient algorithm that (a) constructs successive local quadratic approximations of the loss functions, while (b) preserving the same running time complexity as stochastic gradient descent. For these algorithms, we provide a non-asymptotic analysis of the generalization error (in expectation, and also in high probability for least-squares), and run extensive experiments showing that they often outperform existing approaches.",
    "authors": [
      "Bach, Francis",
      "Moulines, Eric"
    ]
  },
  {
    "id": "7fec306d1e665bc9c748b5d2b99a6e97",
    "title": "Predicting Parameters in Deep Learning",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/7fec306d1e665bc9c748b5d2b99a6e97-Paper.pdf",
    "abstract": "We demonstrate that there is significant redundancy in the parameterization of   several deep learning models.  Given only a few weight values for each feature   it is possible to accurately predict the remaining values.  Moreover, we show   that not only can the parameter values be predicted, but many of them need not   be learned at all.  We train several different architectures by learning only   a small number of weights and predicting the rest.  In the best case we are   able to predict more than 95% of the weights of a network without any drop in   accuracy.",
    "authors": [
      "Denil, Misha",
      "Shakibi, Babak",
      "Dinh, Laurent",
      "Ranzato, Marc'Aurelio",
      "de Freitas, Nando"
    ]
  },
  {
    "id": "801c14f07f9724229175b8ef8b4585a8",
    "title": "Estimation Bias in Multi-Armed Bandit Algorithms for Search Advertising",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/801c14f07f9724229175b8ef8b4585a8-Paper.pdf",
    "abstract": "In search advertising, the search engine needs to select the most profitable advertisements to display, which can be formulated as an instance of online learning with partial feedback, also known as the stochastic multi-armed bandit (MAB) problem. In this paper, we show that the naive application of MAB algorithms to search advertising for advertisement selection will produce sample selection bias that harms the search engine by decreasing expected revenue and \u201cestimation of the largest mean\u201d (ELM) bias that harms the advertisers by increasing game-theoretic player-regret. We then propose simple bias-correction methods with benefits to both the search engine and the advertisers.",
    "authors": [
      "Xu, Min",
      "Qin, Tao",
      "Liu, Tie-Yan"
    ]
  },
  {
    "id": "8065d07da4a77621450aa84fee5656d9",
    "title": "Learning Efficient Random Maximum A-Posteriori Predictors with Non-Decomposable Loss Functions",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/8065d07da4a77621450aa84fee5656d9-Paper.pdf",
    "abstract": "In this work we develop efficient methods for learning random MAP predictors for structured label problems. In particular, we construct posterior distributions over perturbations that can be adjusted via stochastic gradient methods. We show that every smooth posterior distribution would suffice to define a smooth PAC-Bayesian risk bound suitable for gradient methods. In addition, we relate the posterior distributions to computational properties of the MAP predictors. We suggest multiplicative posteriors to learn super-modular potential functions that accompany specialized MAP predictors such as graph-cuts. We also describe label-augmented posterior models that can use efficient MAP approximations, such as those arising from linear program relaxations.",
    "authors": [
      "Hazan, Tamir",
      "Maji, Subhransu",
      "Keshet, Joseph",
      "Jaakkola, Tommi"
    ]
  },
  {
    "id": "819f46e52c25763a55cc642422644317",
    "title": "q-OCSVM: A q-Quantile Estimator for High-Dimensional Distributions",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/819f46e52c25763a55cc642422644317-Paper.pdf",
    "abstract": "In this paper we introduce a novel method that can efficiently estimate a family of hierarchical dense sets in high-dimensional distributions. Our method can be regarded as a natural extension of the one-class SVM (OCSVM) algorithm that finds multiple parallel separating hyperplanes in a reproducing kernel Hilbert space. We call our method q-OCSVM, as it can be used to estimate $q$ quantiles of a high-dimensional distribution. For this purpose, we introduce a new global convex optimization program that finds all estimated sets at once and show that it can be solved efficiently. We prove the correctness of our method and present empirical results that demonstrate its superiority over existing methods.",
    "authors": [
      "Glazer, Assaf",
      "Lindenbaum, Michael",
      "Markovitch, Shaul"
    ]
  },
  {
    "id": "81e5f81db77c596492e6f1a5a792ed53",
    "title": "Fantope Projection and Selection: A near-optimal convex relaxation of sparse PCA",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/81e5f81db77c596492e6f1a5a792ed53-Paper.pdf",
    "abstract": "We propose a novel convex relaxation of sparse principal subspace estimation based on the convex hull of rank-$d$ projection matrices (the Fantope). The convex problem can be solved efficiently using alternating direction method of multipliers (ADMM). We establish a near-optimal convergence rate, in terms of the sparsity, ambient dimension, and sample size, for estimation of the principal subspace of a general covariance matrix without assuming the spiked covariance model. In the special case of $d=1$, our result implies the near- optimality of DSPCA even when the solution is not rank 1. We also provide a general theoretical framework for analyzing the statistical  properties of the method for arbitrary input matrices that extends the  applicability and provable guarantees to a wide array of settings.  We  demonstrate this with an application to Kendall's tau correlation matrices  and transelliptical component analysis.",
    "authors": [
      "Vu, Vincent Q.",
      "Cho, Juhee",
      "Lei, Jing",
      "Rohe, Karl"
    ]
  },
  {
    "id": "82965d4ed8150294d4330ace00821d77",
    "title": "Fast Template Evaluation with Vector Quantization",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/82965d4ed8150294d4330ace00821d77-Paper.pdf",
    "abstract": "Applying linear templates is an integral part of many object detection systems and accounts for a significant portion of computation time. We describe a method that achieves a substantial end-to-end speedup over the best current methods, without loss of accuracy. Our method is a combination of approximating scores by vector quantizing feature windows and a number of speedup techniques including cascade. Our procedure allows speed and accuracy to be traded off in two ways: by choosing the number of Vector Quantization levels, and by choosing to rescore windows or not. Our method can be directly plugged into any recognition system that relies on linear templates. We demonstrate our method to speed up the original Exemplar SVM detector [1] by an order of magnitude and Deformable Part models [2] by two orders of magnitude with no loss of accuracy.",
    "authors": [
      "Sadeghi, Mohammad Amin",
      "Forsyth, David"
    ]
  },
  {
    "id": "82aa4b0af34c2313a562076992e50aa3",
    "title": "Sparse Additive Text Models with Low Rank Background",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/82aa4b0af34c2313a562076992e50aa3-Paper.pdf",
    "abstract": "The sparse additive model for text modeling involves the sum-of-exp computing, with consuming costs for large scales. Moreover, the assumption of equal background across all classes/topics may be too strong. This paper extends to propose sparse additive model with low rank background (SAM-LRB), and simple yet efficient estimation. Particularly, by employing a double majorization bound, we approximate the log-likelihood into a quadratic lower-bound with the sum-of-exp terms absent. The constraints of low rank and sparsity are then simply embodied by nuclear norm and $\\ell_1$-norm regularizers. Interestingly, we find that the optimization task in this manner can be transformed into the same form as that in Robust PCA. Consequently, parameters of supervised SAM-LRB can be efficiently learned using an existing algorithm for Robust PCA based on accelerated proximal gradient. Besides the supervised case, we extend SAM-LRB to also favor unsupervised and multifaceted scenarios. Experiments on real world data demonstrate the effectiveness and efficiency of SAM-LRB, showing state-of-the-art performances.",
    "authors": [
      "Shi, Lei"
    ]
  },
  {
    "id": "839ab46820b524afda05122893c2fe8e",
    "title": "Correlated random features for fast semi-supervised learning",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/839ab46820b524afda05122893c2fe8e-Paper.pdf",
    "abstract": "This paper presents Correlated Nystrom Views (XNV), a fast semi-supervised algorithm for regression and classification. The algorithm draws on two main ideas. First, it generates two views consisting of computationally inexpensive random features. Second, multiview regression, using Canonical Correlation Analysis (CCA) on unlabeled data, biases the regression towards useful features. It has been shown that CCA regression can substantially reduce variance with a minimal increase in bias if the views contains accurate estimators. Recent theoretical and empirical work shows that regression with random features closely approximates kernel regression, implying that the accuracy requirement holds for random views. We show that XNV consistently outperforms a state-of-the-art algorithm for semi-supervised learning: substantially improving predictive performance and reducing the variability of performance on a wide variety of real-world datasets, whilst also reducing runtime by orders of magnitude.",
    "authors": [
      "McWilliams, Brian",
      "Balduzzi, David",
      "Buhmann, Joachim M."
    ]
  },
  {
    "id": "83adc9225e4deb67d7ce42d58fe5157c",
    "title": "Variational Planning for Graph-based MDPs",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/83adc9225e4deb67d7ce42d58fe5157c-Paper.pdf",
    "abstract": "Markov Decision Processes (MDPs) are extremely useful for modeling and solving sequential decision making problems. Graph-based MDPs provide a compact representation for MDPs with large numbers of random variables. However, the complexity of exactly solving a graph-based MDP usually grows exponentially in the number of variables, which limits their application. We present a new variational framework to describe and solve the planning problem of MDPs, and derive both exact and approximate planning algorithms. In particular, by exploiting the graph structure of graph-based MDPs, we propose a factored variational value iteration algorithm in which the value function is first approximated by the multiplication of local-scope value functions, then solved by minimizing a Kullback-Leibler (KL) divergence. The KL divergence is optimized using the belief propagation algorithm, with complexity exponential in only the cluster size of the graph. Experimental comparison on different models shows that our algorithm outperforms existing approximation algorithms at finding good policies.",
    "authors": [
      "Cheng, Qiang",
      "Liu, Qiang",
      "Chen, Feng",
      "Ihler, Alexander T."
    ]
  },
  {
    "id": "83cdcec08fbf90370fcf53bdd56604ff",
    "title": "Adaptive Anonymity via $b$-Matching",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/83cdcec08fbf90370fcf53bdd56604ff-Paper.pdf",
    "abstract": "The adaptive anonymity problem is formalized where each individual shares their data along with an integer value to indicate their personal level of desired privacy. This problem leads to a generalization of $k$-anonymity to the $b$-matching setting. Novel algorithms and theory are provided to implement this type of anonymity. The relaxation achieves better utility, admits theoretical privacy guarantees that are as strong, and, most importantly, accommodates a variable level of anonymity for each individual. Empirical results confirm improved utility on benchmark and social data-sets.",
    "authors": [
      "Choromanski, Krzysztof M.",
      "Jebara, Tony",
      "Tang, Kui"
    ]
  },
  {
    "id": "84117275be999ff55a987b9381e01f96",
    "title": "Statistical Active Learning Algorithms",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/84117275be999ff55a987b9381e01f96-Paper.pdf",
    "abstract": "We describe a framework for designing efficient active learning algorithms that are tolerant to random classification noise. The framework is based on active learning algorithms that are statistical in the sense that they rely on estimates of expectations of functions of filtered random examples. It builds on the powerful statistical query framework of Kearns (1993).  We show that any efficient active statistical learning algorithm can be automatically converted to an efficient active learning algorithm which is tolerant to random classification noise as well as other forms of uncorrelated\" noise. The complexity of the resulting algorithms has information-theoretically optimal quadratic dependence on $1/(1-2\\eta)$, where $\\eta$ is the noise rate.  We demonstrate the power of our framework by showing that commonly studied concept classes including thresholds, rectangles, and linear separators can be efficiently actively learned in our framework. These results combined with our generic conversion lead to the first known computationally-efficient algorithms for actively learning some of these concept classes in the presence of random classification noise that provide exponential improvement in the dependence on the error $\\epsilon$ over their passive counterparts. In addition, we show that our algorithms can be automatically converted to efficient active differentially-private algorithms. This leads to the first differentially-private active learning algorithms with exponential label savings over the passive case.\"",
    "authors": [
      "Balcan, Maria-Florina F.",
      "Feldman, Vitaly"
    ]
  },
  {
    "id": "84438b7aae55a0638073ef798e50b4ef",
    "title": "The Power of Asymmetry in Binary Hashing",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/84438b7aae55a0638073ef798e50b4ef-Paper.pdf",
    "abstract": "When approximating binary similarity using the hamming distance between short binary hashes, we shown that even if the similarity is symmetric, we can have shorter and more accurate hashes by using two distinct code maps. I.e.~by approximating the similarity between $x$ and $x'$ as the hamming distance between $f(x)$ and $g(x')$, for two distinct binary codes $f,g$, rather than as the hamming distance between $f(x)$ and $f(x')$.",
    "authors": [
      "Neyshabur, Behnam",
      "Srebro, Nati",
      "Salakhutdinov, Russ R.",
      "Makarychev, Yury",
      "Yadollahpour, Payman"
    ]
  },
  {
    "id": "846c260d715e5b854ffad5f70a516c88",
    "title": "Bayesian Mixture Modelling and Inference based Thompson Sampling in Monte-Carlo Tree Search",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/846c260d715e5b854ffad5f70a516c88-Paper.pdf",
    "abstract": "Monte-Carlo tree search is drawing great interest in the domain of planning under uncertainty, particularly when little or no domain knowledge is available. One of the central problems is the trade-off between exploration and exploitation. In this paper we present a novel Bayesian mixture modelling and inference based Thompson sampling approach to addressing this dilemma. The proposed Dirichlet-NormalGamma MCTS (DNG-MCTS) algorithm represents the uncertainty of the accumulated reward for actions in the MCTS search tree as a mixture of Normal distributions and inferences on it in Bayesian settings by choosing conjugate priors in the form of combinations of Dirichlet and NormalGamma distributions. Thompson sampling is used to select the best action at each decision node. Experimental results show that our proposed algorithm has achieved the state-of-the-art comparing with popular UCT algorithm in the context of online planning for general Markov decision processes.",
    "authors": [
      "Bai, Aijun",
      "Wu, Feng",
      "Chen, Xiaoping"
    ]
  },
  {
    "id": "84d2004bf28a2095230e8e14993d398d",
    "title": "Distributed Submodular Maximization: Identifying Representative Elements in Massive Data",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/84d2004bf28a2095230e8e14993d398d-Paper.pdf",
    "abstract": "Many large-scale machine learning problems (such as clustering, non-parametric learning, kernel machines, etc.) require selecting, out of a massive data set, a manageable, representative subset. Such problems can often be reduced to maximizing a submodular set function subject to cardinality constraints. Classical approaches require centralized access to the full data set; but for truly large-scale problems, rendering the data centrally is often impractical.  In this paper, we consider the problem of submodular function maximization in a distributed fashion. We develop a simple, two-stage protocol GreeDI, that is easily implemented using MapReduce style computations. We theoretically analyze our approach, and show, that under certain natural conditions, performance close to the (impractical) centralized approach can be achieved. In our extensive experiments, we demonstrate the effectiveness of our approach on several applications, including sparse Gaussian process inference on tens of millions of examples using Hadoop.",
    "authors": [
      "Mirzasoleiman, Baharan",
      "Karbasi, Amin",
      "Sarkar, Rik",
      "Krause, Andreas"
    ]
  },
  {
    "id": "856fc81623da2150ba2210ba1b51d241",
    "title": "Analyzing the Harmonic Structure in Graph-Based Learning",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/856fc81623da2150ba2210ba1b51d241-Paper.pdf",
    "abstract": "We show that either explicitly or implicitly, various well-known graph-based models exhibit a common significant \\emph{harmonic} structure in its target function -- the value of a vertex is approximately the weighted average of the values of its adjacent neighbors. Understanding of such structure and analysis of the loss defined over such structure help reveal important properties of the target function over a graph. In this paper, we show that the variation of the target function across a cut can be upper and lower bounded by the ratio of its harmonic loss and the cut cost. We use this to develop an analytical tool and analyze 5 popular models in graph-based learning: absorbing random walks, partially absorbing random walks, hitting times, pseudo-inverse of graph Laplacian, and eigenvectors of the Laplacian matrices. Our analysis well explains several open questions of these models reported in the literature. Furthermore, it provides theoretical justifications and guidelines for their practical use. Simulations on synthetic and real datasets support our analysis.",
    "authors": [
      "Wu, Xiao-Ming",
      "Li, Zhenguo",
      "Chang, Shih-Fu"
    ]
  },
  {
    "id": "860320be12a1c050cd7731794e231bd3",
    "title": "Near-optimal Anomaly Detection in Graphs using Lovasz Extended Scan Statistic",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/860320be12a1c050cd7731794e231bd3-Paper.pdf",
    "abstract": "The detection of anomalous activity in graphs is a statistical problem that arises in many applications, such as network surveillance, disease outbreak detection, and activity monitoring in social networks. Beyond its wide applicability, graph structured anomaly detection serves as a case study in the difficulty of balancing computational complexity with statistical power. In this work, we develop from first principles the generalized likelihood ratio test for determining if there is a well connected region of activation over the vertices in the graph in Gaussian noise. Because this test is computationally infeasible, we provide a relaxation, called the Lov\\'asz extended scan statistic (LESS) that uses submodularity to approximate the intractable generalized likelihood ratio. We demonstrate a connection between LESS and maximum a-posteriori inference in Markov random fields, which provides us with a poly-time algorithm for LESS. Using electrical network theory, we are able to control type 1 error for LESS and prove conditions under which LESS is risk consistent. Finally, we consider specific graph models, the torus, $k$-nearest neighbor graphs, and $\\epsilon$-random graphs. We show that on these graphs our results provide near-optimal performance by matching our results to known lower bounds.",
    "authors": [
      "Sharpnack, James L.",
      "Krishnamurthy, Akshay",
      "Singh, Aarti"
    ]
  },
  {
    "id": "86e8f7ab32cfd12577bc2619bc635690",
    "title": "Bellman Error Based Feature Generation using Random Projections on Sparse Spaces",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf",
    "abstract": "This paper addresses the problem of automatic generation of features for value function approximation in reinforcement learning.  Bellman Error Basis Functions (BEBFs) have been shown to improve the error of policy evaluation with function approximation, with a convergence rate similar to that of value iteration. We propose a simple, fast and robust algorithm based on random projections, which generates BEBFs for sparse feature spaces. We provide a finite sample analysis of the proposed method, and prove that projections logarithmic in the dimension of the original space guarantee a contraction in the error.  Empirical results demonstrate the strength of this method in domains in which choosing a good state representation is challenging.",
    "authors": [
      "Milani Fard, Mahdi",
      "Grinberg, Yuri",
      "Farahmand, Amir-massoud",
      "Pineau, Joelle",
      "Precup, Doina"
    ]
  },
  {
    "id": "872488f88d1b2db54d55bc8bba2fad1b",
    "title": "Similarity Component Analysis",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/872488f88d1b2db54d55bc8bba2fad1b-Paper.pdf",
    "abstract": "Measuring similarity is crucial to many learning tasks. It is also a richer and broader notion than what most metric learning algorithms can model. For example, similarity can arise from the process of aggregating the decisions of multiple latent components, where each latent component compares data in its own way by focusing on a different subset of features. In this paper, we propose Similarity Component Analysis (SCA), a probabilistic graphical model that discovers those latent components from data. In SCA, a latent component generates a local similarity value, computed with its own metric, independently of other components. The final similarity measure is then obtained by combining the local similarity values with a (noisy-)OR gate. We derive an EM-based algorithm for fitting the model parameters with similarity-annotated data from pairwise comparisons. We validate the SCA model on synthetic datasets where SCA discovers the ground-truth about the latent components. We also apply SCA to a multiway classification task and a link prediction task. For both tasks, SCA attains significantly better prediction accuracies than competing methods. Moreover, we show how SCA can be instrumental in exploratory analysis of data, where we gain insights about the data by examining patterns hidden in its latent components' local similarity values.",
    "authors": [
      "Changpinyo, Soravit",
      "Liu, Kuan",
      "Sha, Fei"
    ]
  },
  {
    "id": "892c91e0a653ba19df81a90f89d99bcd",
    "title": "Matrix Completion From any Given Set of Observations",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/892c91e0a653ba19df81a90f89d99bcd-Paper.pdf",
    "abstract": "In the matrix completion problem the aim is to recover an unknown real matrix from a subset of its entries.  This problem comes up in many application areas, and has received a great deal of attention in the context of the netflix prize.  A central approach to this problem is to output a matrix of lowest  possible complexity (e.g. rank or trace norm) that agrees with the partially  specified matrix.  The performance of this approach under the assumption that the revealed entries are sampled randomly   has received considerable attention. In practice, often the set of revealed entries is not chosen at random and these results do not apply. We are therefore left with no guarantees on the performance of the algorithm we are using.  We present a means to obtain performance guarantees with respect to any set of initial observations.  The first  step remains the same: find a matrix of lowest possible complexity that agrees with the partially specified matrix.   We give a new way to interpret the output of this algorithm by next finding a probability distribution over  the non-revealed entries with respect to which a bound on the generalization error can be proven.  The more  complex the set of revealed entries according to a certain measure, the better the bound on the generalization  error.",
    "authors": [
      "Lee, Troy",
      "Shraibman, Adi"
    ]
  },
  {
    "id": "8a0e1141fd37fa5b98d5bb769ba1a7cc",
    "title": "A Deep Architecture for Matching Short Texts",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/8a0e1141fd37fa5b98d5bb769ba1a7cc-Paper.pdf",
    "abstract": "Many machine learning problems can be interpreted as learning for matching two types of objects (e.g., images and captions, users and products, queries and documents). The matching level of two objects is usually measured as the inner product in a certain feature space, while the modeling effort focuses on mapping of objects from the original space to the feature space. This schema, although proven successful on a range of matching tasks, is insufficient for capturing the rich structure in the matching process of more complicated objects.  In this paper, we propose a new deep architecture to more effectively model the complicated matching relations between two objects from heterogeneous domains. More specifically, we apply this model to matching tasks in natural language, e.g., finding sensible responses for a tweet, or  relevant answers to a given question. This new architecture naturally combines the localness and hierarchy intrinsic to the natural language problems, and therefore greatly improves upon the state-of-the-art models.",
    "authors": [
      "Lu, Zhengdong",
      "Li, Hang"
    ]
  },
  {
    "id": "8a1e808b55fde9455cb3d8857ed88389",
    "title": "Sensor Selection in High-Dimensional Gaussian Trees with Nuisances",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/8a1e808b55fde9455cb3d8857ed88389-Paper.pdf",
    "abstract": "We consider the sensor selection problem on multivariate Gaussian distributions where only a \\emph{subset} of latent variables is of inferential interest.  For pairs of vertices connected by a unique path in the graph, we show that there exist decompositions of nonlocal mutual information into local information measures that can be computed efficiently from the output of message passing algorithms.  We integrate these decompositions into a computationally efficient greedy selector where the computational expense of quantification can be distributed across nodes in the network.  Experimental results demonstrate the comparative efficiency of our algorithms for sensor selection in high-dimensional distributions. We additionally derive an online-computable performance bound based on augmentations of the relevant latent variable set that, when such a valid augmentation exists, is applicable for \\emph{any} distribution with nuisances.",
    "authors": [
      "Levine, Daniel S.",
      "How, Jonathan P."
    ]
  },
  {
    "id": "8a3363abe792db2d8761d6403605aeb7",
    "title": "The Total Variation on Hypergraphs - Learning on Hypergraphs Revisited",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/8a3363abe792db2d8761d6403605aeb7-Paper.pdf",
    "abstract": "Hypergraphs allow to encode higher-order relationships in data and are thus a very flexible modeling tool. Current learning methods are either based on approximations of the hypergraphs via graphs or  on tensor methods which are only applicable under special conditions. In this paper we present a new learning framework on hypergraphs which fully uses the hypergraph structure. The key element  is a family of regularization functionals based on  the total variation on hypergraphs.",
    "authors": [
      "Hein, Matthias",
      "Setzer, Simon",
      "Jost, Leonardo",
      "Rangapuram, Syama Sundar"
    ]
  },
  {
    "id": "8b16ebc056e613024c057be590b542eb",
    "title": "Lasso Screening Rules via Dual Polytope Projection",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/8b16ebc056e613024c057be590b542eb-Paper.pdf",
    "abstract": "Lasso is a widely used regression technique to find sparse representations. When the dimension of the feature space and the number of samples are extremely large, solving the Lasso problem remains challenging. To improve the efficiency of solving large-scale Lasso problems, El Ghaoui and his colleagues have proposed the SAFE rules which are able to quickly identify the inactive predictors, i.e., predictors that have $0$ components in the solution vector. Then, the inactive predictors or features can be removed from the optimization problem to reduce its scale. By transforming the standard Lasso to its dual form, it can be shown that the inactive predictors include the set of inactive constraints on the optimal dual solution. In this paper, we propose an efficient and effective screening rule via Dual Polytope Projections (DPP), which is mainly based on the uniqueness and nonexpansiveness  of the optimal dual solution due to the fact that the feasible set in the dual space is a convex and closed polytope. Moreover, we show that our screening rule can be extended to identify inactive groups in group Lasso. To the best of our knowledge, there is currently no exact\" screening rule for group Lasso. We have evaluated our screening rule using many real data sets. Results show that our rule is more effective to identify inactive predictors than existing state-of-the-art screening rules for Lasso.\"",
    "authors": [
      "Wang, Jie",
      "Zhou, Jiayu",
      "Wonka, Peter",
      "Ye, Jieping"
    ]
  },
  {
    "id": "8b5040a8a5baf3e0e67386c2e3a9b903",
    "title": "Multiscale Dictionary Learning for Estimating Conditional Distributions",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/8b5040a8a5baf3e0e67386c2e3a9b903-Paper.pdf",
    "abstract": "Nonparametric estimation of the conditional distribution of a response given high-dimensional features is a challenging problem.  It is important to allow not only the mean but also the variance and shape of the response density to change flexibly with features, which are massive-dimensional.  We propose a multiscale dictionary learning model, which expresses the conditional response density as a convex combination of dictionary densities, with the densities used and their weights dependent on the path through a tree decomposition of the feature space.  A fast graph partitioning algorithm is applied to obtain the tree decomposition, with Bayesian methods then used to adaptively prune and average over different sub-trees in a soft probabilistic manner.  The algorithm scales efficiently to approximately one million features.  State of the art predictive performance is demonstrated for toy examples and two neuroscience applications including up to a million features.",
    "authors": [
      "Petralia, Francesca",
      "Vogelstein, Joshua T.",
      "Dunson, David B."
    ]
  },
  {
    "id": "8bf1211fd4b7b94528899de0a43b9fb3",
    "title": "Dirty Statistical Models",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/8bf1211fd4b7b94528899de0a43b9fb3-Paper.pdf",
    "abstract": "We provide a unified framework for the high-dimensional analysis of \u201csuperposition-structured\u201d or \u201cdirty\u201d statistical models: where the model parameters are a \u201csuperposition\u201d of structurally constrained parameters. We allow for any number and types of structures, and any statistical model. We consider the general class of $M$-estimators that minimize the sum of any loss function, and an instance of what we call a \u201chybrid\u201d regularization, that is the infimal convolution of weighted regularization functions, one for each structural component. We provide corollaries showcasing our unified framework for varied statistical models such as linear regression, multiple regression and principal component analysis, over varied superposition structures.",
    "authors": [
      "Yang, Eunho",
      "Ravikumar, Pradeep K."
    ]
  },
  {
    "id": "8c19f571e251e61cb8dd3612f26d5ecf",
    "title": "Online Learning of Nonparametric Mixture Models via Sequential Variational Approximation",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/8c19f571e251e61cb8dd3612f26d5ecf-Paper.pdf",
    "abstract": "Reliance on computationally expensive algorithms for inference has been limiting the use of Bayesian nonparametric models in large scale applications. To tackle this problem, we propose a Bayesian learning algorithm for DP mixture models. Instead of following the conventional paradigm -- random initialization plus iterative update, we take an progressive approach. Starting with a given prior, our method recursively transforms it into an approximate posterior through sequential variational approximation. In this process, new components will be incorporated on the fly when needed. The algorithm can reliably estimate a DP mixture model in one pass, making it particularly suited for applications with massive data. Experiments on both synthetic data and real datasets demonstrate remarkable improvement on efficiency -- orders of magnitude speed-up compared to the state-of-the-art.",
    "authors": [
      "Lin, Dahua"
    ]
  },
  {
    "id": "8c235f89a8143a28a1d6067e959dd858",
    "title": "Exact and Stable Recovery of Pairwise Interaction Tensors",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/8c235f89a8143a28a1d6067e959dd858-Paper.pdf",
    "abstract": "Tensor completion from incomplete observations is a problem of significant practical interest.  However, it is unlikely that there exists an efficient algorithm with provable guarantee to recover a general tensor from a limited number of observations. In this paper, we study the recovery algorithm for pairwise interaction tensors, which has recently gained considerable attention for modeling multiple attribute data due to its simplicity and effectiveness. Specifically, in the absence of noise, we show that one can exactly recover a pairwise interaction tensor by solving a constrained convex program which minimizes the weighted sum of nuclear norms of matrices from $O(nr\\log^2(n))$ observations.  For the noisy cases, we also prove error bounds for a constrained convex program for recovering the tensors. Our experiments on the synthetic dataset demonstrate that the recovery performance of our algorithm agrees well with the theory. In addition, we apply our algorithm on a temporal collaborative filtering task and obtain state-of-the-art results.",
    "authors": [
      "Chen, Shouyuan",
      "Lyu, Michael R.",
      "King, Irwin",
      "Xu, Zenglin"
    ]
  },
  {
    "id": "8ce6790cc6a94e65f17f908f462fae85",
    "title": "A* Lasso for Learning a Sparse Bayesian Network Structure for Continuous Variables",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/8ce6790cc6a94e65f17f908f462fae85-Paper.pdf",
    "abstract": "We address the problem of learning a sparse Bayesian network  structure for continuous variables in a high-dimensional space. The constraint that the estimated Bayesian network structure  must be a directed acyclic graph (DAG) makes the problem challenging because of the huge search space of network structures. Most previous methods were based on a two-stage approach that prunes the search space in the first stage and then searches for a network structure that satisfies the DAG constraint in the second stage. Although this approach is effective in a low-dimensional setting, it is difficult to ensure that the correct network structure is not pruned in the first stage in a high-dimensional setting.  In this paper, we propose a single-stage method, called A* lasso, that recovers the optimal  sparse Bayesian network structure by solving a single optimization problem with A* search algorithm that uses lasso in its scoring system. Our approach substantially improves the computational efficiency of the well-known exact methods  based on dynamic programming. We also present a heuristic scheme that further improves the efficiency of A* lasso without significantly compromising the quality of solutions and   demonstrate this on benchmark Bayesian networks and real data.",
    "authors": [
      "Xiang, Jing",
      "Kim, Seyoung"
    ]
  },
  {
    "id": "8d34201a5b85900908db6cae92723617",
    "title": "High-Dimensional Gaussian Process Bandits",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/8d34201a5b85900908db6cae92723617-Paper.pdf",
    "abstract": "Many applications in machine learning require optimizing unknown functions defined over a high-dimensional space from noisy samples that are expensive to obtain. We address this notoriously hard challenge, under the assumptions that the function varies only along some low-dimensional subspace and is smooth (i.e., it has a low norm in a Reproducible Kernel Hilbert Space). In particular, we present the SI-BO algorithm, which leverages recent low-rank matrix recovery techniques to learn the underlying subspace of the unknown function and applies Gaussian Process Upper Confidence sampling for optimization of the function. We carefully calibrate the exploration\u2013exploitation tradeoff by allocating sampling budget to subspace estimation and function optimization, and obtain the first subexponential cumulative regret bounds and convergence rates for Bayesian optimization in high-dimensions under noisy observations. Numerical results demonstrate the effectiveness of our approach in difficult scenarios.",
    "authors": [
      "Djolonga, Josip",
      "Krause, Andreas",
      "Cevher, Volkan"
    ]
  },
  {
    "id": "8d6dc35e506fc23349dd10ee68dabb64",
    "title": "Generalizing Analytic Shrinkage for Arbitrary Covariance Structures",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/8d6dc35e506fc23349dd10ee68dabb64-Paper.pdf",
    "abstract": "Analytic shrinkage is a statistical technique that offers a fast alternative to cross-validation for the regularization of covariance matrices and has appealing consistency properties. We show that the proof of consistency implies bounds on the growth rates of eigenvalues and their dispersion, which are often violated in data. We prove consistency under assumptions which do not restrict the covariance structure and therefore better match real world data. In addition, we propose an extension of analytic shrinkage --orthogonal complement shrinkage-- which adapts to the covariance structure. Finally we demonstrate the superior performance of our novel approach on data from the domains of finance, spoken letter and optical character recognition, and neuroscience.",
    "authors": [
      "Bartz, Daniel",
      "M\u00fcller, Klaus-Robert"
    ]
  },
  {
    "id": "8dd48d6a2e2cad213179a3992c0be53c",
    "title": "Higher Order Priors for Joint Intrinsic Image, Objects, and Attributes Estimation",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/8dd48d6a2e2cad213179a3992c0be53c-Paper.pdf",
    "abstract": "Many methods have been proposed to recover the intrinsic scene properties such as shape, reflectance and illumination from a single image. However, most of these models have been applied on laboratory datasets. In this work we explore the synergy effects between intrinsic scene properties recovered from an image, and the objects and attributes present in the scene. We cast the problem in a joint energy minimization framework; thus our model is able to encode the strong correlations between intrinsic properties (reflectance, shape, illumination), objects (table, tv-monitor), and materials (wooden, plastic) in a given scene. We tested our approach on the NYU and Pascal datasets, and observe both qualitative and quantitative improvements in the overall accuracy.",
    "authors": [
      "Vineet, Vibhav",
      "Rother, Carsten",
      "Torr, Philip"
    ]
  },
  {
    "id": "8f121ce07d74717e0b1f21d122e04521",
    "title": "Online Robust PCA via Stochastic Optimization",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/8f121ce07d74717e0b1f21d122e04521-Paper.pdf",
    "abstract": "Robust PCA methods are typically based on batch optimization and have to load all the samples into memory. This prevents them from efficiently processing big data. In this paper, we develop  an Online Robust Principal Component Analysis (OR-PCA) that processes one sample per time instance and hence its memory cost is independent of the data size,  significantly enhancing the computation and storage efficiency. The proposed method is based on stochastic optimization of an equivalent reformulation of the batch RPCA method. Indeed, we show that OR-PCA provides a sequence of subspace estimations converging to the optimum of its batch counterpart and hence is provably robust  to sparse corruption. Moreover, OR-PCA can naturally be applied for tracking dynamic subspace. Comprehensive simulations on subspace recovering and tracking demonstrate the robustness and efficiency advantages of the OR-PCA over online PCA and batch RPCA methods.",
    "authors": [
      "Feng, Jiashi",
      "Xu, Huan",
      "Yan, Shuicheng"
    ]
  },
  {
    "id": "8f1d43620bc6bb580df6e80b0dc05c48",
    "title": "Compete to Compute",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/8f1d43620bc6bb580df6e80b0dc05c48-Paper.pdf",
    "abstract": "Local competition among neighboring neurons is common in biological neural networks (NNs). We apply the concept to gradient-based, backprop-trained artificial multilayer NNs. NNs with competing linear units tend to outperform those with non-competing nonlinear units, and avoid catastrophic forgetting when training sets change over time.",
    "authors": [
      "Srivastava, Rupesh K.",
      "Masci, Jonathan",
      "Kazerounian, Sohrob",
      "Gomez, Faustino",
      "Schmidhuber, J\u00fcrgen"
    ]
  },
  {
    "id": "8f468c873a32bb0619eaeb2050ba45d1",
    "title": "Heterogeneous-Neighborhood-based Multi-Task Local Learning Algorithms",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/8f468c873a32bb0619eaeb2050ba45d1-Paper.pdf",
    "abstract": "All the existing multi-task local learning methods are defined on homogeneous neighborhood which consists of all data points from only one task. In this paper, different from existing methods, we propose local learning methods for multi-task classification and regression problems based on heterogeneous neighborhood which is defined on data points from all tasks. Specifically, we extend the k-nearest-neighbor classifier by formulating the decision function for each data point as a weighted voting among the neighbors from all tasks where the weights are task-specific. By defining a regularizer to enforce the task-specific weight matrix to approach a symmetric one, a regularized objective function is proposed and an efficient coordinate descent method is developed to solve it. For regression problems, we extend the kernel regression to multi-task setting in a similar way to the classification case. Experiments on some toy data and real-world datasets demonstrate the effectiveness of our proposed methods.",
    "authors": [
      "Zhang, Yu"
    ]
  },
  {
    "id": "8fb21ee7a2207526da55a679f0332de2",
    "title": "Scalable Influence Estimation in Continuous-Time Diffusion Networks",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/8fb21ee7a2207526da55a679f0332de2-Paper.pdf",
    "abstract": "If a piece of information is released from a media site, can it spread, in 1 month, to a million web pages? This influence estimation problem is very challenging since both the time-sensitive nature of the problem and the issue of scalability need to be addressed simultaneously. In this paper, we propose a randomized algorithm for influence estimation in continuous-time diffusion networks. Our algorithm can estimate the influence of every node in a network with $|\\Vcal|$ nodes and $|\\Ecal|$ edges to an accuracy of $\\epsilon$ using  $n=O(1/\\epsilon^2)$ randomizations and up to logarithmic factors $O(n|\\Ecal|+n|\\Vcal|)$ computations. When used as a subroutine in a greedy influence maximization algorithm, our proposed method is guaranteed to find a set of nodes with an influence of at least $(1 - 1/e)\\operatorname{OPT} - 2\\epsilon$, where $\\operatorname{OPT}$ is the optimal value. Experiments on both synthetic and real-world data show that the proposed method can easily scale up to networks of millions of nodes while significantly improves over previous state-of-the-arts in terms of the accuracy of the estimated influence and the quality of the selected nodes in maximizing the influence.",
    "authors": [
      "Du, Nan",
      "Song, Le",
      "Gomez Rodriguez, Manuel",
      "Zha, Hongyuan"
    ]
  },
  {
    "id": "903ce9225fca3e988c2af215d4e544d3",
    "title": "More data speeds up training time in learning halfspaces over sparse vectors",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/903ce9225fca3e988c2af215d4e544d3-Paper.pdf",
    "abstract": "The increased availability of data in recent years led several authors to ask whether it is possible to use data as  a {\\em computational} resource. That is, if more data is available, beyond the sample complexity limit, is it possible to use the extra examples to speed up the computation time required to perform the learning task?  We give the first positive answer to this question for a {\\em natural supervised learning problem} --- we consider agnostic PAC learning of halfspaces over $3$-sparse vectors in $\\{-1,1,0\\}^n$. This class is inefficiently learnable using $O\\left(n/\\epsilon^2\\right)$ examples. Our main contribution is a novel, non-cryptographic, methodology for establishing computational-statistical gaps, which allows us to show that, under a widely believed assumption that refuting random $\\mathrm{3CNF}$ formulas is hard, efficiently learning this class using $O\\left(n/\\epsilon^2\\right)$ examples is impossible. We further show that under stronger hardness assumptions, even $O\\left(n^{1.499}/\\epsilon^2\\right)$ examples do not suffice.  On the other hand, we show a new algorithm that learns this class efficiently using $\\tilde{\\Omega}\\left(n^2/\\epsilon^2\\right)$ examples. This formally establishes the tradeoff between sample and computational complexity for a natural supervised learning problem.",
    "authors": [
      "Daniely, Amit",
      "Linial, Nati",
      "Shalev-Shwartz, Shai"
    ]
  },
  {
    "id": "9232fe81225bcaef853ae32870a2b0fe",
    "title": "Top-Down Regularization of Deep Belief Networks",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/9232fe81225bcaef853ae32870a2b0fe-Paper.pdf",
    "abstract": "Designing a principled and effective algorithm for learning deep architectures is a challenging problem. The current approach involves two training phases: a fully unsupervised learning followed by a strongly discriminative optimization. We suggest a deep learning strategy that bridges the gap between the two phases, resulting in a three-phase learning procedure. We propose to implement the scheme using a method to regularize deep belief networks with top-down information. The network is constructed from building blocks of restricted Boltzmann machines learned by combining bottom-up and top-down sampled signals. A global optimization procedure that merges samples from a forward bottom-up pass and a top-down pass is used. Experiments on the MNIST dataset show improvements over the existing algorithms for deep belief networks. Object recognition results on the Caltech-101 dataset also yield competitive results.",
    "authors": [
      "Goh, Hanlin",
      "Thome, Nicolas",
      "Cord, Matthieu",
      "Lim, Joo-Hwee"
    ]
  },
  {
    "id": "92cc227532d17e56e07902b254dfad10",
    "title": "Polar Operators for Structured Sparse Estimation",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/92cc227532d17e56e07902b254dfad10-Paper.pdf",
    "abstract": "Structured sparse estimation has become an important technique in many areas of data analysis. Unfortunately, these estimators normally create computational difficulties that entail sophisticated algorithms. Our first contribution is to uncover a rich class of structured sparse regularizers whose polar operator can be evaluated efficiently. With such an operator, a simple conditional gradient method can then be developed that, when combined with smoothing and local optimization, significantly reduces training time vs. the state of the art. We also demonstrate a new reduction of polar to proximal maps that enables more efficient latent fused lasso.",
    "authors": [
      "Zhang, Xinhua",
      "Yu, Yao-Liang",
      "Schuurmans, Dale"
    ]
  },
  {
    "id": "93d65641ff3f1586614cf2c1ad240b6c",
    "title": "Learning with Invariance via Linear Functionals on Reproducing Kernel Hilbert Space",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/93d65641ff3f1586614cf2c1ad240b6c-Paper.pdf",
    "abstract": "Incorporating invariance information is important for many learning problems. To exploit invariances, most existing methods resort to approximations that either lead to expensive optimization problems such as semi-definite programming, or rely on separation oracles to retain tractability.  Some methods further limit the space of functions and settle for non-convex models.  In this paper, we propose a framework for learning in reproducing kernel Hilbert spaces (RKHS) using local invariances that explicitly characterize the behavior of the target function around data instances.  These invariances are \\emph{compactly} encoded as linear functionals whose value are penalized by some loss function.  Based on a representer theorem that we establish, our formulation can be efficiently optimized via a convex program. For the representer theorem to hold, the linear functionals are required to be bounded in the RKHS, and we show that this is true for a variety of commonly used RKHS and invariances. Experiments on learning with unlabeled data and transform invariances show that the proposed method yields better or similar results compared with the state of the art.",
    "authors": [
      "Zhang, Xinhua",
      "Lee, Wee Sun",
      "Teh, Yee Whye"
    ]
  },
  {
    "id": "944bdd9636749a0801c39b6e449dbedc",
    "title": "Real-Time Inference for a Gamma Process Model of Neural Spiking",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/944bdd9636749a0801c39b6e449dbedc-Paper.pdf",
    "abstract": "With simultaneous measurements from ever increasing populations of neurons, there is a growing need for sophisticated tools to recover signals from individual neurons. In electrophysiology experiments, this classically proceeds in a two-step process: (i) threshold the waveforms to detect putative spikes and (ii) cluster the waveforms into single units (neurons). We extend previous Bayesian nonparamet- ric models of neural spiking to jointly detect and cluster neurons using a Gamma process model. Importantly, we develop an online approximate inference scheme enabling real-time analysis, with performance exceeding the previous state-of-the- art. Via exploratory data analysis\u2014using data with partial ground truth as well as two novel data sets\u2014we find several features of our model collectively contribute to our improved performance including: (i) accounting for colored noise, (ii) de- tecting overlapping spikes, (iii) tracking waveform dynamics, and (iv) using mul- tiple channels. We hope to enable novel experiments simultaneously measuring many thousands of neurons and possibly adapting stimuli dynamically to probe ever deeper into the mysteries of the brain.",
    "authors": [
      "Carlson, David E.",
      "Rao, Vinayak",
      "Vogelstein, Joshua T.",
      "Carin, Lawrence"
    ]
  },
  {
    "id": "9461cce28ebe3e76fb4b931c35a169b0",
    "title": "Direct 0-1 Loss Minimization and Margin Maximization with Boosting",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/9461cce28ebe3e76fb4b931c35a169b0-Paper.pdf",
    "abstract": "We propose a boosting method, DirectBoost, a greedy coordinate descent algorithm that builds an ensemble classifier of weak classifiers through directly minimizing  empirical classification error over labeled training examples; once the training classification error is reduced to a local coordinatewise minimum, DirectBoost runs a greedy coordinate ascent algorithm that continuously adds weak classifiers to maximize any targeted arbitrarily defined margins until reaching a local coordinatewise maximum of the margins in a certain sense. Experimental results on a collection of machine-learning benchmark datasets show that DirectBoost gives consistently better results than AdaBoost, LogitBoost, LPBoost with column generation and BrownBoost, and is noise tolerant when it maximizes an n'th order bottom sample margin.",
    "authors": [
      "Zhai, Shaodan",
      "Xia, Tian",
      "Tan, Ming",
      "Wang, Shaojun"
    ]
  },
  {
    "id": "94c7bb58efc3b337800875b5d382a072",
    "title": "Marginals-to-Models Reducibility",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/94c7bb58efc3b337800875b5d382a072-Paper.pdf",
    "abstract": "We consider a number of classical and new computational problems regarding marginal distributions, and inference in models specifying a full joint distribution. We prove general and efficient reductions between a number of these problems, which demonstrate that algorithmic progress in inference automatically yields progress for \u201cpure data\u201d problems. Our main technique involves formulating the problems as linear programs, and proving that the dual separation oracle for the Ellipsoid Method is provided by the target problem. This technique may be of independent interest in probabilistic inference.",
    "authors": [
      "Roughgarden, Tim",
      "Kearns, Michael"
    ]
  },
  {
    "id": "9683cc5f89562ea48e72bb321d9f03fb",
    "title": "Sketching Structured Matrices for Faster Nonlinear Regression",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/9683cc5f89562ea48e72bb321d9f03fb-Paper.pdf",
    "abstract": "Motivated by the desire to extend fast randomized techniques to nonlinear $l_p$ regression, we consider a class of structured regression problems. These problems involve Vandermonde matrices which arise naturally in various statistical modeling settings, including classical polynomial fitting problems and recently developed randomized techniques for scalable kernel methods. We show that this structure can be exploited to further accelerate the solution of the regression problem, achieving running times that are faster than input sparsity''. We present empirical results confirming both the practical value of our modeling framework, as well as speedup benefits of randomized regression.\"",
    "authors": [
      "Avron, Haim",
      "Sindhwani, Vikas",
      "Woodruff, David"
    ]
  },
  {
    "id": "9766527f2b5d3e95d4a733fcfb77bd7e",
    "title": "Variance Reduction for Stochastic Gradient Optimization",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/9766527f2b5d3e95d4a733fcfb77bd7e-Paper.pdf",
    "abstract": "Stochastic gradient optimization is a class of widely used algorithms for training machine learning models. To optimize an objective, it uses the noisy gradient computed from the random data samples instead of the true gradient computed from the entire dataset. However, when the variance of the noisy gradient is large, the algorithm might spend much time bouncing around, leading to slower convergence and worse performance. In this paper, we develop a general approach of using control variate for variance reduction in stochastic gradient. Data statistics such as low-order moments (pre-computed or estimated online) is used to form the control variate. We demonstrate how to construct the control variate for two practical problems using stochastic gradient optimization. One is convex---the MAP estimation for logistic regression, and the other is non-convex---stochastic variational inference for latent Dirichlet allocation. On both problems, our approach shows faster convergence and better performance than the classical approach.",
    "authors": [
      "Wang, Chong",
      "Chen, Xi",
      "Smola, Alexander J.",
      "Xing, Eric P."
    ]
  },
  {
    "id": "98dce83da57b0395e163467c9dae521b",
    "title": "On Decomposing the Proximal Map",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/98dce83da57b0395e163467c9dae521b-Paper.pdf",
    "abstract": "The proximal map is the key step in gradient-type algorithms, which have become prevalent in large-scale high-dimensional problems. For simple functions this proximal map is available in closed-form while for more complicated functions it can become highly nontrivial. Motivated by the need of combining regularizers to simultaneously induce different types of structures, this paper initiates a systematic investigation of when the proximal map of a sum of functions decomposes into the composition of the proximal maps of the individual summands. We not only unify a few known results scattered in the literature but also discover several new decompositions obtained almost effortlessly from our theory.",
    "authors": [
      "Yu, Yao-Liang"
    ]
  },
  {
    "id": "98f13708210194c475687be6106a3b84",
    "title": "Documents as multiple overlapping windows into grids of counts",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/98f13708210194c475687be6106a3b84-Paper.pdf",
    "abstract": "In text analysis documents are represented as disorganized bags of words, models of count features are typically based on mixing a small number of topics \\cite{lda,sam}. Recently, it has been observed that for many text corpora documents evolve into one another in a smooth way, with some features dropping and new ones being introduced. The counting grid \\cite{cgUai} models this spatial metaphor literally: it is multidimensional grid of word distributions learned in such a way that a document's own distribution of features can be modeled as the sum of the histograms found in a window into the grid. The major drawback of this method is that it is essentially a mixture and all the content much be generated by a single contiguous area on the grid. This may be problematic especially for lower dimensional grids. In this paper, we overcome to this issue with the \\emph{Componential Counting Grid} which brings the componential nature of topic models to the basic counting grid. We also introduce a generative kernel based on the document's grid usage and a visualization strategy useful for understanding large text corpora. We evaluate our approach on document classification and multimodal retrieval obtaining state of the art results on standard benchmarks.",
    "authors": [
      "Perina, Alessandro",
      "Jojic, Nebojsa",
      "Bicego, Manuele",
      "Truski, Andrzej"
    ]
  },
  {
    "id": "9908279ebbf1f9b250ba689db6a0222b",
    "title": "Robust Sparse Principal Component Regression under the High Dimensional Elliptical Model",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/9908279ebbf1f9b250ba689db6a0222b-Paper.pdf",
    "abstract": "In this paper we focus on the principal component regression and its application to high dimension non-Gaussian data. The major contributions are in two folds. First, in low dimensions and under a double asymptotic framework where both the dimension $d$ and sample size $n$ can increase, by borrowing the strength from recent development in minimax optimal principal component estimation, we first time sharply characterize the potential advantage of classical principal component regression over least square estimation under the Gaussian model. Secondly, we propose and analyze a new robust sparse principal component regression on high dimensional elliptically distributed data. The elliptical distribution is a semiparametric generalization of the Gaussian, including many well known distributions such as multivariate Gaussian, rank-deficient Gaussian, $t$, Cauchy, and logistic. It allows the random vector to be heavy tailed and have tail dependence. These extra flexibilities make it very suitable for modeling finance and biomedical imaging data. Under the elliptical model, we prove that our method can estimate the regression coefficients in the optimal parametric rate and therefore is a good alternative to the Gaussian based methods. Experiments on synthetic and real world data are conducted to illustrate the empirical usefulness of the proposed method.",
    "authors": [
      "Han, Fang",
      "Liu, Han"
    ]
  },
  {
    "id": "995665640dc319973d3173a74a03860c",
    "title": "Optimizing Instructional Policies",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/995665640dc319973d3173a74a03860c-Paper.pdf",
    "abstract": "Psychologists are interested in developing instructional policies that boost student learning. An instructional policy specifies the manner and content of instruction. For example, in the domain of concept learning, a policy might specify the nature of exemplars chosen over a training sequence.  Traditional psychological studies compare several hand-selected policies, e.g., contrasting a policy that selects only difficult-to-classify exemplars with a policy that gradually progresses over the training sequence from easy exemplars to more difficult (known as {\\em fading}). We propose an alternative to the traditional methodology in which we define a  parameterized space of policies and search this space to identify the optimum policy. For example, in concept learning, policies might be described by a fading function that specifies exemplar difficulty over time.  We propose an experimental technique for searching policy spaces using Gaussian process surrogate-based optimization and a generative model of student performance. Instead of evaluating a few experimental conditions each with many human subjects, as the traditional methodology does, our technique evaluates many experimental conditions each with a few subjects.  Even though individual subjects provide only a noisy estimate of the population mean, the optimization method allows us to determine the shape of the policy space and identify the global optimum, and is as efficient in its subject budget as a traditional A-B comparison.  We evaluate the method via two behavioral studies, and suggest that the method has broad applicability to optimization problems involving humans in domains beyond the educational arena.",
    "authors": [
      "Lindsey, Robert V.",
      "Mozer, Michael C.",
      "Huggins, William J.",
      "Pashler, Harold"
    ]
  },
  {
    "id": "995e1fda4a2b5f55ef0df50868bf2a8f",
    "title": "Adaptive Market Making via Online Learning",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/995e1fda4a2b5f55ef0df50868bf2a8f-Paper.pdf",
    "abstract": "We consider the design of strategies for \\emph{market making} in a market like a stock, commodity, or currency exchange. In order to obtain profit guarantees for a market maker one typically requires very particular stochastic assumptions on the sequence of price fluctuations of the asset in question. We propose a class of spread-based market making strategies whose performance can be controlled even under worst-case (adversarial) settings. We prove structural properties of these strategies which allows us to design a master algorithm which obtains low regret relative to the best such strategy in hindsight. We run a set of experiments showing favorable performance on real-world price data.",
    "authors": [
      "Abernethy, Jacob",
      "Kale, Satyen"
    ]
  },
  {
    "id": "996a7fa078cc36c46d02f9af3bef918b",
    "title": "Learning Prices for Repeated Auctions with Strategic Buyers",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/996a7fa078cc36c46d02f9af3bef918b-Paper.pdf",
    "abstract": "Inspired by real-time ad exchanges for online display advertising, we consider the problem of inferring a buyer's value distribution for a good when the buyer is repeatedly interacting with a seller through a posted-price mechanism.  We model the buyer as a strategic agent, whose goal is to maximize her long-term surplus, and we are interested in mechanisms that maximize the seller's long-term revenue. We present seller algorithms that are no-regret when the buyer discounts her future surplus --- i.e. the buyer prefers showing advertisements to users sooner rather than later. We also give a lower bound on regret that increases as the buyer's discounting weakens and shows, in particular, that any seller algorithm will suffer linear regret if there is no discounting.",
    "authors": [
      "Amin, Kareem",
      "Rostamizadeh, Afshin",
      "Syed, Umar"
    ]
  },
  {
    "id": "9996535e07258a7bbfd8b132435c5962",
    "title": "Multilinear Dynamical Systems for Tensor Time Series",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/9996535e07258a7bbfd8b132435c5962-Paper.pdf",
    "abstract": "Many scientific data occur as sequences of multidimensional arrays called tensors.  How can hidden, evolving trends in such data be extracted while preserving the tensor structure?  The model that is traditionally used is the linear dynamical system (LDS), which treats the observation at each time slice as a vector.  In this paper, we propose the multilinear dynamical system (MLDS) for modeling tensor time series and an expectation-maximization (EM) algorithm to estimate the parameters.  The MLDS models each time slice of the tensor time series as the multilinear projection of a corresponding member of a sequence of latent, low-dimensional tensors.  Compared to the LDS with an equal number of parameters, the MLDS achieves higher prediction accuracy and marginal likelihood for both simulated and real datasets.",
    "authors": [
      "Rogers, Mark",
      "Li, Lei",
      "Russell, Stuart J."
    ]
  },
  {
    "id": "99bcfcd754a98ce89cb86f73acc04645",
    "title": "Computing the Stationary Distribution Locally",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/99bcfcd754a98ce89cb86f73acc04645-Paper.pdf",
    "abstract": "Computing the stationary distribution of a large finite or countably infinite state space Markov Chain (MC) has become central in many problems such as statistical inference and network analysis. Standard methods involve large matrix multiplications as in power iteration, or simulations of long random walks to sample states from the stationary distribution, as in Markov Chain Monte Carlo (MCMC). However these methods are computationally costly; either they involve operations at every state or they scale (in computation time) at least linearly in the size of the state space. In this paper, we provide a novel algorithm that answers whether a chosen state in a MC has stationary probability larger than some $\\Delta \\in (0,1)$. If so, it estimates the stationary probability. Our algorithm uses information from a local neighborhood of the state on the graph induced by the MC, which has constant size relative to the state space. We provide correctness and convergence guarantees that depend on the algorithm parameters and mixing properties of the MC. Simulation results show MCs for which this method gives tight estimates.",
    "authors": [
      "Lee, Christina E.",
      "Ozdaglar, Asuman",
      "Shah, Devavrat"
    ]
  },
  {
    "id": "9a1158154dfa42caddbd0694a4e9bdc8",
    "title": "Latent Maximum Margin Clustering",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/9a1158154dfa42caddbd0694a4e9bdc8-Paper.pdf",
    "abstract": "We present a maximum margin framework that clusters data using latent variables. Using latent representations enables our framework to model unobserved information embedded in the data. We implement our idea by large margin learning, and develop an alternating descent algorithm to effectively solve the resultant non-convex optimization problem. We instantiate our latent maximum margin clustering framework with tag-based video clustering tasks, where each video is represented by a latent tag model describing the presence or absence of video tags. Experimental results obtained on three standard datasets show that the proposed method outperforms non-latent maximum margin clustering as well as conventional clustering approaches.",
    "authors": [
      "Zhou, Guang-Tong",
      "Lan, Tian",
      "Vahdat, Arash",
      "Mori, Greg"
    ]
  },
  {
    "id": "9a1756fd0c741126d7bbd4b692ccbd91",
    "title": "Hierarchical Modular Optimization of Convolutional Networks Achieves Representations Similar to Macaque IT and Human Ventral Stream",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/9a1756fd0c741126d7bbd4b692ccbd91-Paper.pdf",
    "abstract": "Humans recognize visually-presented objects rapidly and accurately. To understand this ability, we seek to construct models of the ventral stream, the series of cortical areas thought to subserve object recognition. One tool to assess the quality of a model of the ventral stream is the Representation Dissimilarity Matrix (RDM), which uses a set of visual stimuli and measures the distances produced in either the brain (i.e. fMRI voxel responses, neural firing rates) or in models (features). Previous work has shown that all known models of the ventral stream fail to capture the RDM pattern observed in either IT cortex, the highest ventral area, or in the human ventral stream. In this work, we construct models of the ventral stream using a novel optimization procedure for category-level object recognition problems, and produce RDMs resembling both macaque IT and human ventral stream. The model, while novel in the optimization procedure, further develops a long-standing functional hypothesis that the ventral visual stream is a hierarchically arranged series of processing stages optimized for visual object recognition.",
    "authors": [
      "Yamins, Daniel L.",
      "Hong, Ha",
      "Cadieu, Charles",
      "DiCarlo, James J."
    ]
  },
  {
    "id": "9a96876e2f8f3dc4f3cf45f02c61c0c1",
    "title": "Online PCA for Contaminated Data",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/9a96876e2f8f3dc4f3cf45f02c61c0c1-Paper.pdf",
    "abstract": "We consider the online Principal Component Analysis (PCA) for contaminated samples (containing outliers) which are revealed sequentially to the Principal Components (PCs) estimator. Due to their sensitiveness to outliers, previous online PCA algorithms fail in this case and their results can be arbitrarily bad. Here we propose the online robust PCA algorithm, which is able to improve the PCs estimation upon an initial one steadily, even when faced with a constant fraction of outliers. We show that the final result of the proposed online RPCA has an acceptable degradation from the optimum. Actually, under mild conditions, online RPCA achieves the maximal robustness with a $50\\%$ breakdown point. Moreover, online RPCA is shown to be efficient for both storage and computation, since it need not re-explore the previous samples as in traditional robust PCA algorithms. This endows online RPCA with scalability for large scale data.",
    "authors": [
      "Feng, Jiashi",
      "Xu, Huan",
      "Mannor, Shie",
      "Yan, Shuicheng"
    ]
  },
  {
    "id": "9aa42b31882ec039965f3c4923ce901b",
    "title": "Distributed Representations of Words and Phrases and their Compositionality",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf",
    "abstract": "The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships.  In this paper we present several improvements that make the Skip-gram model more expressive and enable it to learn higher quality vectors more rapidly.  We show that by subsampling frequent words we obtain significant speedup,  and also learn higher quality representations as measured by our tasks. We also introduce Negative Sampling, a simplified variant of Noise Contrastive Estimation (NCE) that learns more accurate vectors for frequent words compared to the hierarchical softmax.   An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases.  For example, the meanings of Canada'' and \"Air'' cannot be easily combined to obtain \"Air Canada''.  Motivated by this example, we present a simple and efficient method for finding phrases, and show that their vector representations can be accurately learned by the Skip-gram model. \"",
    "authors": [
      "Mikolov, Tomas",
      "Sutskever, Ilya",
      "Chen, Kai",
      "Corrado, Greg S.",
      "Dean, Jeff"
    ]
  },
  {
    "id": "9ab0d88431732957a618d4a469a0d4c3",
    "title": "Learning Multiple Models via Regularized Weighting",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/9ab0d88431732957a618d4a469a0d4c3-Paper.pdf",
    "abstract": "We consider the general problem of Multiple Model Learning (MML) from data, from the statistical and algorithmic perspectives; this problem includes clustering, multiple regression and subspace clustering as special cases. A common approach to solving new MML problems is to generalize Lloyd's algorithm  for clustering (or Expectation-Maximization for soft clustering). However this approach is unfortunately sensitive to outliers and large noise: a single exceptional point may take over one of the models.   We propose a different general formulation that seeks for each model a distribution over data points; the weights are regularized to be sufficiently spread out. This enhances robustness by making assumptions on class balance. We further provide generalization bounds and explain how the new iterations may be computed efficiently. We demonstrate the robustness benefits of our approach with some experimental results and prove for the important  case of clustering that our approach has a non-trivial breakdown point, i.e., is guaranteed to be robust to a fixed percentage of adversarial unbounded outliers.",
    "authors": [
      "Vainsencher, Daniel",
      "Mannor, Shie",
      "Xu, Huan"
    ]
  },
  {
    "id": "9ac403da7947a183884c18a67d3aa8de",
    "title": "Discriminative Transfer Learning with Tree-based Priors",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/9ac403da7947a183884c18a67d3aa8de-Paper.pdf",
    "abstract": "This paper proposes a way of improving classification performance for classes which have very few training examples. The key idea is to discover classes which are similar and transfer knowledge among them. Our method organizes the classes into a tree hierarchy. The tree structure can be used to impose a generative prior over classification parameters. We show that these priors can be combined with discriminative models such as deep neural networks. Our method benefits from the power of discriminative training of deep neural networks, at the same time using tree-based generative priors over classification parameters. We also propose an algorithm for learning the underlying tree structure. This gives the model some flexibility to tune the tree so that the tree is pertinent to task being solved. We show that the model can transfer knowledge across related classes using fixed semantic trees. Moreover, it can learn new meaningful trees usually leading to improved performance. Our method achieves state-of-the-art classification results on the CIFAR-100 image data set and the MIR Flickr multimodal data set.",
    "authors": [
      "Srivastava, Nitish",
      "Salakhutdinov, Russ R."
    ]
  },
  {
    "id": "9c01802ddb981e6bcfbec0f0516b8e35",
    "title": "Machine Teaching for Bayesian Learners in the Exponential Family",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/9c01802ddb981e6bcfbec0f0516b8e35-Paper.pdf",
    "abstract": "What if there is a teacher who knows the learning goal and wants to design good training data for a machine learner?  We propose an optimal teaching framework aimed at learners who employ Bayesian models.  Our framework is expressed as an optimization problem over teaching examples that balance the future loss of the learner and the effort of the teacher.  This optimization problem is in general hard.  In the case where the learner employs conjugate exponential family models, we present an approximate algorithm for finding the optimal teaching set.  Our algorithm optimizes the aggregate sufficient statistics, then unpacks them into actual teaching examples.  We give several examples to illustrate our framework.",
    "authors": [
      "Zhu, Jerry"
    ]
  },
  {
    "id": "9cf81d8026a9018052c429cc4e56739b",
    "title": "Online Learning with Switching Costs and Other Adaptive Adversaries",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/9cf81d8026a9018052c429cc4e56739b-Paper.pdf",
    "abstract": "We study the power of different types of adaptive (nonoblivious) adversaries in the setting of prediction with expert advice, under both full-information and bandit feedback. We measure the player's performance using a new notion of regret, also known as policy regret, which better captures the adversary's adaptiveness to the player's behavior. In a setting where losses are allowed to drift, we characterize ---in a nearly complete manner--- the power of adaptive adversaries with bounded memories and switching costs. In particular, we show that with switching costs, the attainable rate with bandit feedback is $T^{2/3}$. Interestingly, this rate is significantly worse than the $\\sqrt{T}$ rate attainable with switching costs in the full-information case. Via a novel reduction from experts to bandits, we also show that a bounded memory adversary can force $T^{2/3}$ regret even in the full information case, proving that switching costs are easier to control than bounded memory adversaries. Our lower bounds rely on a new stochastic adversary strategy that generates loss processes with strong dependencies.",
    "authors": [
      "Cesa-Bianchi, Nicol\u00f2",
      "Dekel, Ofer",
      "Shamir, Ohad"
    ]
  },
  {
    "id": "9e3cfc48eccf81a0d57663e129aef3cb",
    "title": "From Bandits to Experts: A Tale of Domination and Independence",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/9e3cfc48eccf81a0d57663e129aef3cb-Paper.pdf",
    "abstract": "We consider the partial observability model for multi-armed bandits, introduced by Mannor and Shamir (2011). Our main result is a characterization of regret in the directed observability model in terms of the dominating and independence numbers of the observability graph. We also show that in the undirected case, the learner can achieve optimal regret without even accessing the observability graph before selecting an action. Both results are shown using variants of the Exp3 algorithm operating on the observability graph in a time-efficient manner.",
    "authors": [
      "Alon, Noga",
      "Cesa-Bianchi, Nicol\u00f2",
      "Gentile, Claudio",
      "Mansour, Yishay"
    ]
  },
  {
    "id": "a01a0380ca3c61428c26a231f0e49a09",
    "title": "Which Space Partitioning Tree to Use for Search?",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/a01a0380ca3c61428c26a231f0e49a09-Paper.pdf",
    "abstract": "We consider the task of nearest-neighbor search with the class of binary-space-partitioning trees, which includes kd-trees, principal axis trees and random projection trees, and try to rigorously answer the question which tree to use for nearest-neighbor search?'' To this end, we present the theoretical results which imply that trees with better vector quantization performance have better search performance guarantees. We also explore another factor affecting the search performance -- margins of the partitions in these trees. We demonstrate, both theoretically and empirically, that large margin partitions can improve the search performance of a space-partitioning tree. \"",
    "authors": [
      "Ram, Parikshit",
      "Gray, Alexander"
    ]
  },
  {
    "id": "a0e2a2c563d57df27213ede1ac4ac780",
    "title": "Small-Variance Asymptotics for Hidden Markov Models",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/a0e2a2c563d57df27213ede1ac4ac780-Paper.pdf",
    "abstract": "Small-variance asymptotics provide an emerging technique for obtaining scalable combinatorial algorithms from rich probabilistic models. We present a small-variance asymptotic analysis of the Hidden Markov Model and its infinite-state Bayesian nonparametric extension. Starting with the standard HMM, we first derive a \u201chard\u201d inference algorithm analogous to k-means that arises when particular variances in the model tend to zero. This analysis is then extended to the Bayesian nonparametric case, yielding a simple, scalable, and flexible algorithm for discrete-state sequence data with a non-fixed number of states. We also derive the corresponding combinatorial objective functions arising from our analysis, which involve a k-means-like term along with penalties based on state transitions and the number of states. A key property of such algorithms is that \u2014 particularly in the nonparametric setting \u2014 standard probabilistic inference algorithms lack scalability and are heavily dependent on good initialization. A number of results on synthetic and real data sets demonstrate the advantages of the proposed framework.",
    "authors": [
      "Roychowdhury, Anirban",
      "Jiang, Ke",
      "Kulis, Brian"
    ]
  },
  {
    "id": "a1519de5b5d44b31a01de013b9b51a80",
    "title": "Sparse Overlapping Sets Lasso for Multitask Learning and its Application to fMRI Analysis",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/a1519de5b5d44b31a01de013b9b51a80-Paper.pdf",
    "abstract": "Multitask learning can be effective when features useful in one task are also useful for other tasks, and the group lasso is a standard method for selecting a common subset of features. In this paper, we are interested in a less restrictive form of multitask learning, wherein (1) the available features can be organized into subsets according to a notion of similarity and (2) features useful in one task are similar, but not necessarily identical, to the features best suited for other tasks. The main contribution of this paper is a new procedure called {\\em Sparse Overlapping Sets (SOS) lasso}, a convex optimization that automatically selects similar features for related learning tasks.  Error bounds are derived for SOSlasso and its consistency is established for squared error loss. In particular,  SOSlasso is motivated by multi-subject fMRI studies in which functional activity is classified using brain voxels as features. Experiments with real and synthetic data demonstrate the advantages of SOSlasso compared to the lasso and group lasso.",
    "authors": [
      "Rao, Nikhil",
      "Cox, Christopher",
      "Nowak, Rob",
      "Rogers, Timothy T."
    ]
  },
  {
    "id": "a1d50185e7426cbb0acad1e6ca74b9aa",
    "title": "Submodular Optimization with Submodular Cover and Submodular Knapsack Constraints",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/a1d50185e7426cbb0acad1e6ca74b9aa-Paper.pdf",
    "abstract": "We investigate two new optimization problems \u2014 minimizing a submodular function subject to a submodular lower bound constraint (submodular cover) and maximizing a submodular function subject to a submodular upper bound constraint (submodular knapsack). We are motivated by a number of real-world applications in machine learning including sensor placement and data subset selection, which require maximizing a certain submodular function (like coverage or diversity) while simultaneously minimizing another (like cooperative cost). These problems are often posed as minimizing the difference between submodular functions [9, 23] which is in the worst case inapproximable. We show, however, that by phrasing these problems as constrained optimization, which is more natural for many applications, we achieve a number of bounded approximation guarantees. We also show that both these problems are closely related and, an approximation algorithm solving one can be used to obtain an approximation guarantee for the other. We provide hardness results for both problems thus showing that our approximation factors are tight up to log-factors. Finally, we empirically demonstrate the performance and good scalability properties of our algorithms.",
    "authors": [
      "Iyer, Rishabh K.",
      "Bilmes, Jeff A."
    ]
  },
  {
    "id": "a223c6b3710f85df22e9377d6c4f7553",
    "title": "Model Selection for High-Dimensional Regression under the Generalized Irrepresentability Condition",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/a223c6b3710f85df22e9377d6c4f7553-Paper.pdf",
    "abstract": "In the high-dimensional regression model a response variable is linearly related to $p$ covariates,  but the sample size $n$ is smaller than $p$. We assume that only a small subset of covariates is `active'  (i.e., the corresponding coefficients are non-zero), and consider the model-selection problem of identifying  the active covariates.  A popular approach is to estimate the regression coefficients through the Lasso ($\\ell_1$-regularized least squares).  This is known to correctly identify the active set only if the irrelevant covariates are roughly orthogonal to the relevant ones, as quantified through the so called `irrepresentability' condition. In this paper we study the `Gauss-Lasso' selector, a simple two-stage method that first solves the Lasso, and then performs ordinary least squares restricted to the Lasso active set.   We formulate `generalized irrepresentability condition' (GIC), an assumption that is substantially weaker than  irrepresentability. We prove that, under GIC, the Gauss-Lasso correctly recovers the active set.",
    "authors": [
      "Javanmard, Adel",
      "Montanari, Andrea"
    ]
  },
  {
    "id": "a2557a7b2e94197ff767970b67041697",
    "title": "Scalable kernels for graphs with continuous attributes",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/a2557a7b2e94197ff767970b67041697-Paper.pdf",
    "abstract": "While graphs with continuous node attributes arise in many applications, state-of-the-art graph kernels for comparing continuous-attributed graphs suffer from a high runtime complexity; for instance, the popular shortest path kernel scales as $\\mathcal{O}(n^4)$, where $n$ is the number of nodes. In this paper, we present a class of path kernels with computational complexity $\\mathcal{O}(n^2 (m + \\delta^2))$, where $\\delta$ is the graph diameter and $m$ the number of edges. Due to the sparsity and small diameter of real-world graphs, these kernels scale comfortably to large graphs. In our experiments, the presented kernels outperform state-of-the-art kernels in terms of speed and accuracy on classification benchmark datasets.",
    "authors": [
      "Feragen, Aasa",
      "Kasenburg, Niklas",
      "Petersen, Jens",
      "de Bruijne, Marleen",
      "Borgwardt, Karsten"
    ]
  },
  {
    "id": "a3f390d88e4c41f2747bfa2f1b5f87db",
    "title": "Bayesian optimization explains human active search",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/a3f390d88e4c41f2747bfa2f1b5f87db-Paper.pdf",
    "abstract": "Many real-world problems have complicated objective functions. To optimize such functions, humans utilize sophisticated sequential decision-making strategies. Many optimization algorithms have also been developed for this same purpose, but how do they compare to humans in terms of both performance and behavior? We try to unravel the general underlying algorithm people may be using while searching for the maximum of an invisible 1D function. Subjects click on a blank screen and are shown the ordinate of the function at each clicked abscissa location. Their task is to find the function\u2019s maximum in as few clicks as possible. Subjects win if they get close enough to the maximum location. Analysis over 23 non-maths undergraduates, optimizing 25 functions from different families, shows that humans outperform 24 well-known optimization algorithms. Bayesian Optimization based on Gaussian Processes, which exploit all the x values tried and all the f(x) values obtained so far to pick the next x, predicts human performance and searched locations better. In 6 follow-up controlled experiments over 76 subjects, covering interpolation, extrapolation, and optimization tasks, we further confirm that Gaussian Processes provide a general and unified theoretical account to explain passive and active function learning and search in humans.",
    "authors": [
      "Borji, Ali",
      "Itti, Laurent"
    ]
  },
  {
    "id": "a49e9411d64ff53eccfdd09ad10a15b3",
    "title": "B-test: A Non-parametric, Low Variance Kernel Two-sample Test",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/a49e9411d64ff53eccfdd09ad10a15b3-Paper.pdf",
    "abstract": "We propose a family of maximum mean discrepancy (MMD) kernel two-sample tests that have low sample complexity and are consistent. The test has a hyperparameter that allows one to control the tradeoff between sample complexity and computational time. Our family of tests, which we denote as B-tests, is both computationally and statistically efficient, combining favorable properties of previously proposed MMD two-sample tests.  It does so by better leveraging samples to produce low variance estimates in the finite sample case, while avoiding a quadratic number of kernel evaluations and complex null-hypothesis approximation as would be required by tests relying on one sample U-statistics. The B-test uses a smaller than quadratic number of kernel evaluations and avoids completely the computational burden of complex null-hypothesis approximation while maintaining consistency and probabilistically conservative thresholds on Type I error. Finally, recent results of combining multiple kernels transfer seamlessly to our hypothesis test, allowing a further increase in discriminative power and decrease in sample complexity.",
    "authors": [
      "Zaremba, Wojciech",
      "Gretton, Arthur",
      "Blaschko, Matthew"
    ]
  },
  {
    "id": "a50abba8132a77191791390c3eb19fe7",
    "title": "Moment-based Uniform Deviation Bounds for $k$-means and Friends",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/a50abba8132a77191791390c3eb19fe7-Paper.pdf",
    "abstract": "Suppose $k$ centers are fit to $m$ points by heuristically minimizing the $k$-means cost; what is the corresponding fit over the source distribution?  This question is resolved here for distributions with $p\\geq 4$ bounded moments; in particular, the difference between the sample cost and distribution cost decays with $m$ and $p$ as $m^{\\min\\{-1/4, -1/2+2/p\\}}$.  The essential technical contribution is a mechanism to uniformly control deviations in the face of unbounded parameter sets, cost functions, and source distributions.  To further demonstrate this mechanism, a soft clustering variant of $k$-means cost is also considered, namely the log likelihood of a Gaussian mixture, subject to the constraint that all covariance matrices have bounded spectrum.  Lastly, a rate with refined constants is provided for $k$-means instances possessing some cluster structure.",
    "authors": [
      "Telgarsky, Matus J.",
      "Dasgupta, Sanjoy"
    ]
  },
  {
    "id": "a5cdd4aa0048b187f7182f1b9ce7a6a7",
    "title": "Convex Calibrated Surrogates for Low-Rank Loss Matrices with Applications to Subset Ranking Losses",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/a5cdd4aa0048b187f7182f1b9ce7a6a7-Paper.pdf",
    "abstract": "The design of convex, calibrated surrogate losses, whose minimization entails consistency with respect to a desired target loss, is an important concept to have emerged in the theory of machine learning in recent years. We give an explicit construction of a convex least-squares type surrogate loss that can be designed to be calibrated for any multiclass learning problem for which the target loss matrix has a low-rank structure; the surrogate loss operates on a surrogate target space of dimension at most the rank of the target loss. We use this result to design convex calibrated surrogates for a variety of subset ranking problems, with target losses including the precision@q, expected rank utility, mean average precision, and pairwise disagreement.",
    "authors": [
      "Ramaswamy, Harish G.",
      "Agarwal, Shivani",
      "Tewari, Ambuj"
    ]
  },
  {
    "id": "a7d8ae4569120b5bec12e7b6e9648b86",
    "title": "Auxiliary-variable Exact Hamiltonian Monte Carlo Samplers for Binary Distributions",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/a7d8ae4569120b5bec12e7b6e9648b86-Paper.pdf",
    "abstract": "We present a new approach to sample from generic binary distributions, based on an exact  Hamiltonian Monte Carlo algorithm applied to a piecewise continuous augmentation  of the binary distribution of interest. An extension of this idea to distributions over mixtures of binary and continuous variables allows us to sample from posteriors of   linear and probit regression models with spike-and-slab priors and truncated parameters. We illustrate the advantages of these algorithms in several examples in which they outperform the Metropolis or Gibbs samplers.",
    "authors": [
      "Pakman, Ari",
      "Paninski, Liam"
    ]
  },
  {
    "id": "a8240cb8235e9c493a0c30607586166c",
    "title": "Spectral methods for neural characterization using generalized quadratic models",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/a8240cb8235e9c493a0c30607586166c-Paper.pdf",
    "abstract": "We describe a set of fast, tractable methods for characterizing neural responses to high-dimensional sensory stimuli using a model we refer to as the generalized quadratic model (GQM). The GQM consists of a low-rank quadratic form followed by a point nonlinearity and exponential-family noise. The quadratic form characterizes the neuron's stimulus selectivity in terms of a set linear receptive fields followed by a quadratic combination rule, and the invertible nonlinearity maps this output to the desired response range. Special cases of the GQM include the 2nd-order Volterra model (Marmarelis and Marmarelis 1978, Koh and Powers 1985) and the elliptical Linear-Nonlinear-Poisson model (Park and Pillow 2011). Here we show that for canonical form\" GQMs, spectral decomposition of the first two response-weighted moments yields approximate maximum-likelihood estimators via a quantity called the expected log-likelihood. The resulting theory generalizes moment-based estimators such as the spike-triggered covariance, and, in the Gaussian noise case, provides closed-form estimators under a large class of non-Gaussian stimulus distributions. We show that these estimators are fast and provide highly accurate estimates with far lower computational cost than full maximum likelihood. Moreover, the GQM provides a natural framework for combining multi-dimensional stimulus sensitivity and spike-history dependencies within a single model. We show applications to both analog and spiking data using intracellular recordings of V1 membrane potential and extracellular recordings of retinal spike trains.\"",
    "authors": [
      "Park, Il Memming",
      "Archer, Evan W.",
      "Priebe, Nicholas",
      "Pillow, Jonathan W."
    ]
  },
  {
    "id": "a8849b052492b5106526b2331e526138",
    "title": "A Latent Source Model for Nonparametric Time Series Classification",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/a8849b052492b5106526b2331e526138-Paper.pdf",
    "abstract": "For classifying time series, a nearest-neighbor approach is widely used in practice with performance often competitive with or better than more elaborate methods such as neural networks, decision trees, and support vector machines. We develop theoretical justification for the effectiveness of nearest-neighbor-like classification of time series. Our guiding hypothesis is that in many applications, such as forecasting which topics will become trends on Twitter, there aren't actually that many prototypical time series to begin with, relative to the number of time series we have access to, e.g., topics become trends on Twitter only in a few distinct manners whereas we can collect massive amounts of Twitter data. To operationalize this hypothesis, we propose a latent source model for time series, which naturally leads to a weighted majority voting\" classification rule that can be approximated by a nearest-neighbor classifier. We establish nonasymptotic performance guarantees of both weighted majority voting and nearest-neighbor classification under our model accounting for how much of the time series we observe and the model complexity. Experimental results on synthetic data show weighted majority voting achieving the same misclassification rate as nearest-neighbor classification while observing less of the time series. We then use weighted majority to forecast which news topics on Twitter become trends, where we are able to detect such \"trending topics\" in advance of Twitter 79% of the time, with a mean early advantage of 1 hour and 26 minutes, a true positive rate of 95%, and a false positive rate of 4%.\"",
    "authors": [
      "Chen, George H.",
      "Nikolov, Stanislav",
      "Shah, Devavrat"
    ]
  },
  {
    "id": "a97da629b098b75c294dffdc3e463904",
    "title": "PAC-Bayes-Empirical-Bernstein Inequality",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/a97da629b098b75c294dffdc3e463904-Paper.pdf",
    "abstract": "We present PAC-Bayes-Empirical-Bernstein inequality. The inequality is based on combination of PAC-Bayesian bounding technique with Empirical Bernstein bound. It allows to take advantage of small empirical variance and is especially useful in regression. We show that when the empirical variance is significantly smaller than the empirical loss PAC-Bayes-Empirical-Bernstein inequality is significantly tighter than PAC-Bayes-kl inequality of Seeger (2002) and otherwise it is comparable. PAC-Bayes-Empirical-Bernstein inequality is an interesting example of application of PAC-Bayesian bounding technique to self-bounding functions. We provide empirical comparison of PAC-Bayes-Empirical-Bernstein inequality with PAC-Bayes-kl inequality on a synthetic example and several UCI datasets.",
    "authors": [
      "Tolstikhin, Ilya O.",
      "Seldin, Yevgeny"
    ]
  },
  {
    "id": "a9be4c2a4041cadbf9d61ae16dd1389e",
    "title": "Convex Two-Layer Modeling",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/a9be4c2a4041cadbf9d61ae16dd1389e-Paper.pdf",
    "abstract": "Latent variable prediction models, such as multi-layer networks, impose auxiliary latent variables between inputs and outputs to allow automatic inference of implicit features useful for prediction.  Unfortunately, such models are difficult to train because inference over latent variables must be performed concurrently with parameter optimization---creating a highly non-convex problem.  Instead of proposing another local training method, we develop a convex relaxation of hidden-layer conditional models that admits global training.  Our approach extends current convex modeling approaches to handle two nested nonlinearities separated by a non-trivial adaptive latent layer.  The resulting methods are able to acquire two-layer models that cannot be represented by any single-layer model over the same features, while improving training quality over local heuristics.",
    "authors": [
      "Aslan, \u00d6zlem",
      "Cheng, Hao",
      "Zhang, Xinhua",
      "Schuurmans, Dale"
    ]
  },
  {
    "id": "aab3238922bcc25a6f606eb525ffdc56",
    "title": "The Randomized Dependence Coefficient",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/aab3238922bcc25a6f606eb525ffdc56-Paper.pdf",
    "abstract": "We introduce the Randomized Dependence Coefficient (RDC), a measure of non-linear dependence between random variables of arbitrary dimension based on the Hirschfeld-Gebelein-R\u00e9nyi Maximum Correlation Coefficient. RDC is defined in terms of correlation of random non-linear copula projections; it is invariant with respect to marginal distribution transformations, has low computational cost and is easy to implement: just five lines of R code, included at the end of the paper.",
    "authors": [
      "Lopez-Paz, David",
      "Hennig, Philipp",
      "Sch\u00f6lkopf, Bernhard"
    ]
  },
  {
    "id": "aace49c7d80767cffec0e513ae886df0",
    "title": "Sparse Inverse Covariance Estimation with Calibration",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/aace49c7d80767cffec0e513ae886df0-Paper.pdf",
    "abstract": "We propose a semiparametric procedure for estimating high dimensional sparse inverse covariance matrix. Our method, named ALICE, is applicable to the elliptical family. Computationally, we develop an efficient dual inexact iterative projection (${\\rm D_2}$P) algorithm based on the alternating direction method of multipliers (ADMM). Theoretically, we prove that the ALICE estimator achieves the parametric rate of convergence in both parameter estimation and model selection. Moreover, ALICE calibrates regularizations when estimating each column of the inverse covariance matrix. So it not only is asymptotically tuning free, but also achieves an improved finite sample performance. We present numerical simulations to support our theory, and a real data example to illustrate the effectiveness of the proposed estimator.",
    "authors": [
      "Zhao, Tuo",
      "Liu, Han"
    ]
  },
  {
    "id": "aba3b6fd5d186d28e06ff97135cade7f",
    "title": "Thompson Sampling for 1-Dimensional Exponential Family Bandits",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/aba3b6fd5d186d28e06ff97135cade7f-Paper.pdf",
    "abstract": "Thompson Sampling has been demonstrated in many complex bandit models, however the theoretical guarantees available for the parametric multi-armed bandit are still limited to the Bernoulli case. Here we extend them by proving asymptotic optimality of the algorithm using the Jeffreys prior for $1$-dimensional exponential family bandits. Our proof builds on previous work, but also makes extensive use of closed forms for Kullback-Leibler divergence and Fisher information (and thus Jeffreys prior) available in an exponential family. This allow us to give a finite time exponential concentration inequality for posterior distributions on exponential families that may be of interest in its own right. Moreover our analysis covers some distributions for which no optimistic algorithm has yet been proposed, including heavy-tailed exponential families.",
    "authors": [
      "Korda, Nathaniel",
      "Kaufmann, Emilie",
      "Munos, Remi"
    ]
  },
  {
    "id": "ac1dd209cbcc5e5d1c6e28598e8cbbe8",
    "title": "Accelerating Stochastic Gradient Descent using Predictive Variance Reduction",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/ac1dd209cbcc5e5d1c6e28598e8cbbe8-Paper.pdf",
    "abstract": "Stochastic gradient descent is popular for large scale optimization but has slow convergence asymptotically due to the inherent variance. To remedy this problem, we introduce an explicit variance reduction method for stochastic gradient descent which we call stochastic variance reduced gradient (SVRG). For smooth and strongly convex functions, we  prove that this method enjoys the same fast convergence rate as those of stochastic dual coordinate ascent (SDCA) and Stochastic Average Gradient (SAG).  However, our analysis is significantly simpler and more intuitive. Moreover, unlike SDCA or SAG, our method does not require the storage of gradients, and thus is more easily applicable to complex problems such as some structured prediction problems and neural network learning.",
    "authors": [
      "Johnson, Rie",
      "Zhang, Tong"
    ]
  },
  {
    "id": "ac796a52db3f16bbdb6557d3d89d1c5a",
    "title": "Multisensory Encoding, Decoding, and Identification",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/ac796a52db3f16bbdb6557d3d89d1c5a-Paper.pdf",
    "abstract": "We investigate a spiking neuron model of multisensory integration. Multiple stimuli from different sensory modalities are encoded by a single neural circuit comprised of a multisensory bank of receptive fields in cascade with a population of biophysical spike generators. We demonstrate that stimuli of different dimensions can be faithfully multiplexed and encoded in the spike domain and derive tractable algorithms for decoding each stimulus from the common pool of spikes. We also show that the identification of multisensory processing in a single neuron is dual to the recovery of stimuli encoded with a population of multisensory neurons, and prove that only a projection of the circuit onto input stimuli can be identified. We provide an example of multisensory integration using natural audio and video and discuss the performance of the proposed decoding and identification algorithms.",
    "authors": [
      "Lazar, Aurel A.",
      "Slutskiy, Yevgeniy"
    ]
  },
  {
    "id": "ad3019b856147c17e82a5bead782d2a8",
    "title": "Learning invariant representations and applications to face verification",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/ad3019b856147c17e82a5bead782d2a8-Paper.pdf",
    "abstract": "One approach to computer object recognition and modeling the brain's ventral stream involves unsupervised learning of representations that are invariant to common transformations. However, applications of these ideas have usually been limited to 2D affine transformations, e.g., translation and scaling, since they are easiest to solve via convolution. In accord with a recent theory of transformation-invariance, we propose a model that, while capturing other common convolutional networks as special cases, can also be used with arbitrary identity-preserving transformations. The model's wiring can be learned from videos of transforming objects---or any other grouping of images into sets by their depicted object. Through a series of successively more complex empirical tests, we study the invariance/discriminability properties of this model with respect to different transformations. First, we empirically confirm theoretical predictions for the case of 2D affine transformations. Next, we apply the model to non-affine transformations: as expected, it performs well on face verification tasks requiring invariance to the relatively smooth transformations of 3D rotation-in-depth and changes in illumination direction. Surprisingly, it can also tolerate clutter transformations'' which map an image of a face on one background to an image of the same face on a different background. Motivated by these empirical findings, we tested the same model on face verification benchmark tasks from the computer vision literature: Labeled Faces in the Wild, PubFig and a new dataset we gathered---achieving strong performance in these highly unconstrained cases as well.\"",
    "authors": [
      "Liao, Qianli",
      "Leibo, Joel Z.",
      "Poggio, Tomaso"
    ]
  },
  {
    "id": "ae0eb3eed39d2bcef4622b2499a05fe6",
    "title": "Optimistic Concurrency Control for Distributed Unsupervised Learning",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/ae0eb3eed39d2bcef4622b2499a05fe6-Paper.pdf",
    "abstract": "Research on distributed machine learning algorithms has focused primarily on one of two extremes---algorithms that obey strict concurrency constraints or algorithms that obey few or no such constraints.  We consider an intermediate alternative in which algorithms optimistically assume that conflicts are unlikely and if conflicts do arise a conflict-resolution protocol is invoked. We view this optimistic concurrency control'' paradigm as particularly appropriate for large-scale machine learning algorithms, particularly in the unsupervised setting.  We demonstrate our approach in three problem areas: clustering, feature learning and online facility location.  We evaluate  our methods via large-scale experiments in a cluster computing environment.  \"",
    "authors": [
      "Pan, Xinghao",
      "Gonzalez, Joseph E.",
      "Jegelka, Stefanie",
      "Broderick, Tamara",
      "Jordan, Michael I."
    ]
  },
  {
    "id": "ae5e3ce40e0404a45ecacaaf05e5f735",
    "title": "Statistical analysis of coupled time series with Kernel Cross-Spectral Density operators.",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/ae5e3ce40e0404a45ecacaaf05e5f735-Paper.pdf",
    "abstract": "Many applications require the analysis of complex interactions between time series. These interactions can be non-linear and involve vector valued as well as complex data structures such as graphs or strings. Here we provide a general framework for the statistical analysis of these interactions when random variables are sampled from stationary time-series of arbitrary objects. To achieve this goal we analyze the properties of the kernel cross-spectral density operator induced by positive definite kernels on arbitrary input domains. This framework enables us to develop an independence test between time series as well as a similarity measure to compare different types of coupling. The performance of our test is compared to the HSIC test using i.i.d. assumptions, showing improvement in terms of detection errors as well as the suitability of this approach for testing dependency in complex dynamical systems. Finally, we use this approach to characterize complex interactions in electrophysiological neural time series.",
    "authors": [
      "Besserve, Michel",
      "Logothetis, Nikos K.",
      "Sch\u00f6lkopf, Bernhard"
    ]
  },
  {
    "id": "af21d0c97db2e27e13572cbf59eb343d",
    "title": "Sinkhorn Distances: Lightspeed Computation of Optimal Transport",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/af21d0c97db2e27e13572cbf59eb343d-Paper.pdf",
    "abstract": "Optimal transportation distances are a fundamental family of parameterized distances for histograms in the probability simplex. Despite their appealing theoretical properties, excellent performance and intuitive formulation, their computation involves the resolution of a linear program whose cost is prohibitive whenever the histograms' dimension exceeds a few hundreds. We propose in this work a new family of optimal transportation distances that look at transportation problems from a maximum-entropy perspective. We smooth the classical optimal transportation problem with an entropic regularization term, and show that the resulting optimum is also a distance which can be computed through Sinkhorn's matrix scaling algorithm at a speed that is several orders of magnitude faster than that of transportation solvers. We also report improved performance on the MNIST benchmark problem over competing distances.",
    "authors": [
      "Cuturi, Marco"
    ]
  },
  {
    "id": "afd4836712c5e77550897e25711e1d96",
    "title": "Nonparametric Multi-group Membership Model for Dynamic Networks",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/afd4836712c5e77550897e25711e1d96-Paper.pdf",
    "abstract": "Relational data\u2014like graphs, networks, and matrices\u2014is often dynamic, where the relational structure evolves over time. A fundamental problem in the analysis of time-varying network data is to extract a summary of the common structure and the dynamics of underlying relations between entities. Here we build on the intuition that changes in the network structure are driven by the dynamics at the level of groups of nodes. We propose a nonparametric multi-group membership model for dynamic networks. Our model contains three main components. We model the birth and death of groups with respect to the dynamics of the network structure via a distance dependent Indian Buffet Process. We capture the evolution of individual node group memberships via a Factorial Hidden Markov model. And, we explain the dynamics of the network structure by explicitly modeling the connectivity structure. We demonstrate our model\u2019s capability of identifying the dynamics of latent groups in a number of different types of network data. Experimental results show our model achieves higher predictive performance on the future network forecasting and missing link prediction.",
    "authors": [
      "Kim, Myunghwan",
      "Leskovec, Jure"
    ]
  },
  {
    "id": "b056eb1587586b71e2da9acfe4fbd19e",
    "title": "EDML for Learning Parameters in Directed and Undirected Graphical Models",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/b056eb1587586b71e2da9acfe4fbd19e-Paper.pdf",
    "abstract": "EDML is a recently proposed algorithm for learning parameters in Bayesian networks.  It was originally derived in terms of approximate inference on a meta-network, which underlies the Bayesian approach to parameter estimation.  While this initial derivation helped discover EDML in the first place and provided a concrete context for identifying some of its properties (e.g., in contrast to EM), the formal setting was somewhat tedious in the number of concepts it drew on.  In this paper, we propose a greatly simplified perspective on EDML, which casts it as a general approach to continuous optimization. The new perspective has several advantages. First, it makes immediate some results that were non-trivial to prove initially. Second, it facilitates the design of EDML algorithms for new graphical models, leading to a new algorithm for learning parameters in Markov networks.  We derive this algorithm in this paper, and show, empirically, that it can sometimes learn better estimates from complete data, several times faster than commonly used optimization methods, such as conjugate gradient and L-BFGS.",
    "authors": [
      "Refaat, Khaled S.",
      "Choi, Arthur",
      "Darwiche, Adnan"
    ]
  },
  {
    "id": "b20bb95ab626d93fd976af958fbc61ba",
    "title": "Flexible sampling of discrete data correlations without the marginal distributions",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/b20bb95ab626d93fd976af958fbc61ba-Paper.pdf",
    "abstract": "Learning the joint dependence of discrete variables is a fundamental problem in machine learning, with many applications including prediction, clustering and dimensionality reduction. More recently, the framework of copula modeling has gained popularity due to its modular parametrization of joint distributions. Among other properties, copulas provide a recipe for combining flexible models for univariate marginal distributions with parametric families suitable for potentially high dimensional dependence structures. More radically, the extended rank likelihood approach of Hoff (2007) bypasses learning marginal models completely when such information is ancillary to the learning task at hand as in, e.g., standard dimensionality reduction problems or copula parameter estimation. The main idea is to represent data by their observable rank statistics, ignoring any other information from the marginals. Inference is typically done in a Bayesian framework with Gaussian copulas, and it is complicated by the fact this implies sampling within a space where the number of constraints increase quadratically with the number of data points. The result is slow mixing when using off-the-shelf Gibbs sampling. We present an efficient algorithm based on recent advances on constrained Hamiltonian Markov chain Monte Carlo that is simple to implement and does not require paying for a quadratic cost in sample size.",
    "authors": [
      "Kalaitzis, Alfredo",
      "Silva, Ricardo"
    ]
  },
  {
    "id": "b2f627fff19fda463cb386442eac2b3d",
    "title": "Designed Measurements for Vector Count Data",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/b2f627fff19fda463cb386442eac2b3d-Paper.pdf",
    "abstract": "We consider design of linear projection measurements for a vector Poisson signal model. The projections are performed on the vector Poisson rate, $X\\in\\mathbb{R}_+^n$, and the observed data are a vector of counts, $Y\\in\\mathbb{Z}_+^m$. The projection matrix is designed by maximizing mutual information between $Y$ and $X$, $I(Y;X)$. When there is a latent class label $C\\in\\{1,\\dots,L\\}$ associated with $X$, we consider the mutual information with respect to $Y$ and $C$, $I(Y;C)$. New analytic expressions for the gradient of $I(Y;X)$ and $I(Y;C)$ are presented, with gradient performed with respect to the measurement matrix. Connections are made to the more widely studied Gaussian measurement model. Example results are presented for compressive topic modeling of a document corpora (word counting), and hyperspectral compressive sensing for chemical classification (photon counting).",
    "authors": [
      "Wang, Liming",
      "Carlson, David E.",
      "Rodrigues, Miguel",
      "Wilcox, David",
      "Calderbank, Robert",
      "Carin, Lawrence"
    ]
  },
  {
    "id": "b337e84de8752b27eda3a12363109e80",
    "title": "Reasoning With Neural Tensor Networks for Knowledge Base Completion",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/b337e84de8752b27eda3a12363109e80-Paper.pdf",
    "abstract": "A common problem in knowledge representation and related fields is reasoning over a large joint knowledge graph, represented as triples of a relation between two entities. The goal of this paper is to develop a more powerful neural network model suitable for inference over these relationships. Previous models suffer from weak interaction between entities or simple linear projection of the vector space. We address these problems by introducing a neural tensor network (NTN) model which allow the entities and relations to interact multiplicatively. Additionally, we observe that such knowledge base models can be further improved by representing each entity as the average of vectors for the words in the entity name, giving an additional dimension of similarity by which entities can share statistical strength. We assess the model by considering the problem of predicting additional true relations between entities given a partial knowledge base. Our model outperforms previous models and can classify unseen relationships in WordNet and FreeBase with an accuracy of 86.2% and 90.0%, respectively.",
    "authors": [
      "Socher, Richard",
      "Chen, Danqi",
      "Manning, Christopher D.",
      "Ng, Andrew"
    ]
  },
  {
    "id": "b3ba8f1bee1238a2f37603d90b58898d",
    "title": "Deep content-based music recommendation",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/b3ba8f1bee1238a2f37603d90b58898d-Paper.pdf",
    "abstract": "Automatic music recommendation has become an increasingly relevant problem in recent years, since a lot of music is now sold and consumed digitally. Most recommender systems rely on collaborative filtering. However, this approach suffers from the cold start problem: it fails when no usage data is available, so it is not effective for recommending new and unpopular songs. In this paper, we propose to use a latent factor model for recommendation, and predict the latent factors from music audio when they cannot be obtained from usage data. We compare a traditional approach using a bag-of-words representation of the audio signals with deep convolutional neural networks, and evaluate the predictions quantitatively and qualitatively on the Million Song Dataset. We show that using predicted latent factors produces sensible recommendations, despite the fact that there is a large semantic gap between the characteristics of a song that affect user preference and the corresponding audio signal. We also show that recent advances in deep learning translate very well to the music recommendation setting, with deep convolutional neural networks significantly outperforming the traditional approach.",
    "authors": [
      "van den Oord, Aaron",
      "Dieleman, Sander",
      "Schrauwen, Benjamin"
    ]
  },
  {
    "id": "b4d168b48157c623fbd095b4a565b5bb",
    "title": "What do row and column marginals reveal about your dataset?",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/b4d168b48157c623fbd095b4a565b5bb-Paper.pdf",
    "abstract": "Numerous datasets ranging from group memberships within social networks to purchase histories on e-commerce sites are represented by binary matrices. While this data is often either proprietary or sensitive, aggregated data, notably row and column marginals, is often viewed as much less sensitive, and may be furnished for analysis. Here, we investigate how these data can be exploited to make inferences about the underlying matrix H. Instead of assuming a generative model for H, we view the input marginals as constraints on the dataspace of possible realizations of H and compute the probability density function of particular entries H(i,j) of interest. We do this, for all the cells of H simultaneously, without generating realizations but rather via implicitly sampling the datasets that satisfy the input marginals. The end result is an efficient algorithm with running time equal to the time required by standard sampling techniques to generate a single dataset from the same dataspace. Our experimental evaluation demonstrates the efficiency and the efficacy of our framework in multiple settings.",
    "authors": [
      "Golshan, Behzad",
      "Byers, John",
      "Terzi, Evimaria"
    ]
  },
  {
    "id": "b51a15f382ac914391a58850ab343b00",
    "title": "Analyzing Hogwild Parallel Gaussian Gibbs Sampling",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/b51a15f382ac914391a58850ab343b00-Paper.pdf",
    "abstract": "Sampling inference methods are computationally difficult to scale for many models in part because global dependencies can reduce opportunities for parallel computation.  Without strict conditional independence structure among variables, standard Gibbs sampling theory requires sample updates to be performed sequentially, even if dependence between most variables is not strong. Empirical work has shown that some models can be sampled effectively by going Hogwild'' and simply running Gibbs updates in parallel with only periodic global communication, but the successes and limitations of such a strategy are not well understood.  As a step towards such an understanding, we study the Hogwild Gibbs sampling strategy in the context of Gaussian distributions. We develop a framework which provides convergence conditions and error bounds along with simple proofs and connections to methods in numerical linear algebra.  In particular, we show that if the Gaussian precision matrix is generalized diagonally dominant, then any Hogwild Gibbs sampler, with any update schedule or allocation of variables to processors, yields a stable sampling process with the correct sample mean. \"",
    "authors": [
      "Johnson, Matthew J.",
      "Saunderson, James",
      "Willsky, Alan"
    ]
  },
  {
    "id": "b6f0479ae87d244975439c6124592772",
    "title": "Latent Structured Active Learning",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/b6f0479ae87d244975439c6124592772-Paper.pdf",
    "abstract": "In this paper we present active learning algorithms in the context of structured prediction problems. To reduce the amount of labeling necessary to learn good models, our algorithms only label subsets of the output. To this end, we query examples using entropies of local marginals, which are a good surrogate for uncertainty. We demonstrate the effectiveness of our approach in the task of 3D layout prediction from single images, and show that good models are learned when labeling only a handful of random variables. In particular, the same performance as using the full training set can be obtained while only labeling ~10\\% of the random variables.",
    "authors": [
      "Luo, Wenjie",
      "Schwing, Alex",
      "Urtasun, Raquel"
    ]
  },
  {
    "id": "b73dfe25b4b8714c029b37a6ad3006fa",
    "title": "Confidence Intervals and Hypothesis Testing for High-Dimensional Statistical Models",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/b73dfe25b4b8714c029b37a6ad3006fa-Paper.pdf",
    "abstract": "Fitting high-dimensional statistical models often requires the use of non-linear parameter estimation procedures. As a consequence, it is generally impossible to obtain an exact characterization of the probability distribution of the parameter estimates. This in turn implies that it is extremely challenging to quantify the uncertainty' associated with a certain parameter estimate. Concretely, no commonly accepted procedure exists for computing classical measures of uncertainty and statistical significance as confidence intervals or p-values.  We consider here a broad class of regression problems, and propose  an efficient algorithm  for constructing confidence intervals and p-values.  The resulting confidence intervals have nearly optimal size. When testing for the null hypothesis that a certain parameter is vanishing, our method has nearly optimal power.  Our approach is based on constructing ade-biased' version of regularized M-estimators. The new construction  improves over recent work in the field in that it does not assume a special structure on the design matrix. Furthermore, proofs are remarkably simple. We test our method on a diabetes prediction problem.",
    "authors": [
      "Javanmard, Adel",
      "Montanari, Andrea"
    ]
  },
  {
    "id": "b7b16ecf8ca53723593894116071700c",
    "title": "Stochastic blockmodel approximation of a graphon: Theory and consistent estimation",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/b7b16ecf8ca53723593894116071700c-Paper.pdf",
    "abstract": "Given a convergent sequence of graphs, there exists a limit object called the graphon from which random graphs are generated. This nonparametric perspective of random graphs opens the door to study graphs beyond the traditional parametric models, but at the same time also poses the challenging question of how to estimate the graphon underlying observed graphs. In this paper, we propose a computationally efficient algorithm to estimate a graphon from a set of observed graphs generated from it. We show that, by approximating the graphon with stochastic block models, the graphon can be consistently estimated, that is, the estimation error vanishes as the size of the graph approaches infinity.",
    "authors": [
      "Airoldi, Edo M.",
      "Costa, Thiago B.",
      "Chan, Stanley H."
    ]
  },
  {
    "id": "b7bb35b9c6ca2aee2df08cf09d7016c2",
    "title": "More Effective Distributed ML via a Stale Synchronous Parallel Parameter Server",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/b7bb35b9c6ca2aee2df08cf09d7016c2-Paper.pdf",
    "abstract": "We propose a parameter server system for distributed ML, which follows a Stale Synchronous Parallel (SSP) model of computation that maximizes the time computational workers spend doing useful work on ML algorithms, while still providing correctness guarantees. The parameter server provides an easy-to-use shared interface for read/write access to an ML model's values (parameters and variables), and the SSP model allows distributed workers to read older, stale versions of these values from a local cache, instead of waiting to get them from a central storage. This significantly increases the proportion of time workers spend computing, as opposed to waiting. Furthermore, the SSP model ensures ML algorithm correctness by limiting the maximum age of the stale values. We provide a proof of correctness under SSP, as well as empirical results demonstrating that the SSP model achieves faster algorithm convergence on several different ML problems, compared to fully-synchronous and asynchronous schemes.",
    "authors": [
      "Ho, Qirong",
      "Cipar, James",
      "Cui, Henggang",
      "Lee, Seunghak",
      "Kim, Jin Kyu",
      "Gibbons, Phillip B.",
      "Gibson, Garth A.",
      "Ganger, Greg",
      "Xing, Eric P."
    ]
  },
  {
    "id": "bac9162b47c56fc8a4d2a519803d51b3",
    "title": "On Algorithms for Sparse Multi-factor NMF",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/bac9162b47c56fc8a4d2a519803d51b3-Paper.pdf",
    "abstract": "Nonnegative matrix factorization (NMF) is a popular data analysis method, the objective of which is to decompose a matrix with all nonnegative components into the product of two other nonnegative matrices.  In this work, we describe a new simple and efficient algorithm for multi-factor nonnegative matrix factorization problem ({mfNMF}), which generalizes the original NMF problem to more than two factors.  Furthermore, we extend the mfNMF algorithm to incorporate a regularizer based on Dirichlet distribution over normalized columns to encourage sparsity in the obtained factors.  Our sparse NMF algorithm affords a closed form and an intuitive interpretation, and is more efficient in comparison with previous works that use fix point iterations. We demonstrate the effectiveness and efficiency of our algorithms on both synthetic and real data sets.",
    "authors": [
      "Lyu, Siwei",
      "Wang, Xin"
    ]
  },
  {
    "id": "bad5f33780c42f2588878a9d07405083",
    "title": "Efficient Exploration and Value Function Generalization in Deterministic Systems",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/bad5f33780c42f2588878a9d07405083-Paper.pdf",
    "abstract": "We consider the problem of reinforcement learning over episodes of a finite-horizon deterministic system  and as a solution propose optimistic constraint propagation (OCP), an algorithm designed to synthesize efficient exploration and value function generalization. We establish that when the true value function lies within the given hypothesis class, OCP selects optimal actions over all but at most K episodes, where K is the eluder dimension of the given hypothesis class. We establish further efficiency and asymptotic performance guarantees that apply even if the true value function does not lie in the given hypothesis space, for  the special case where the hypothesis space is the span of pre-specified indicator functions over disjoint sets.",
    "authors": [
      "Wen, Zheng",
      "Van Roy, Benjamin"
    ]
  },
  {
    "id": "bca82e41ee7b0833588399b1fcd177c7",
    "title": "Parallel Sampling of DP Mixture Models using Sub-Cluster Splits",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/bca82e41ee7b0833588399b1fcd177c7-Paper.pdf",
    "abstract": "We present a novel MCMC sampler for Dirichlet process mixture models that can be used for conjugate or non-conjugate prior distributions. The proposed sampler can be massively parallelized to achieve significant computational gains. A non-ergodic restricted Gibbs iteration is mixed with split/merge proposals to produce a valid sampler. Each regular cluster is augmented with two sub-clusters to construct likely split moves. Unlike many previous parallel samplers, the proposed sampler accurately enforces the correct stationary distribution of the Markov chain without the need for approximate models. Empirical results illustrate that the new sampler exhibits better convergence properties than current methods.",
    "authors": [
      "Chang, Jason",
      "Fisher III, John W."
    ]
  },
  {
    "id": "bcc0d400288793e8bdcd7c19a8ac0c2b",
    "title": "Context-sensitive active sensing in humans",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/bcc0d400288793e8bdcd7c19a8ac0c2b-Paper.pdf",
    "abstract": "Humans and animals readily utilize active sensing, or the use of self-motion, to focus sensory and cognitive resources on the behaviorally most relevant stimuli and events in the environment. Understanding the computational basis of natural active sensing is important both for advancing brain sciences and for developing more powerful artificial systems. Recently, a goal-directed, context-sensitive, Bayesian control strategy for active sensing, termed C-DAC (Context-Dependent Active Controller), was proposed (Ahmad & Yu, 2013). In contrast to previously proposed algorithms for human active vision, which tend to optimize abstract statistical objectives and therefore cannot adapt to changing behavioral context or task goals, C-DAC directly minimizes behavioral costs and thus, automatically adapts itself to different task conditions. However, C-DAC is limited as a model of human active sensing, given its computational/representational requirements, especially for more complex, real-world situations. Here, we propose a myopic approximation to C-DAC, which also takes behavioral costs into account, but achieves a significant reduction in complexity by looking only one step ahead. We also present data from a human active visual search experiment, and compare the performance of the various models against human behavior. We find that C-DAC and its myopic variant both achieve better fit to human data than Infomax (Butko & Movellan, 2010), which maximizes expected cumulative future information gain. In summary, this work provides novel experimental results that differentiate theoretical models for human active sensing, as well as a novel active sensing algorithm that retains the context-sensitivity of the optimal controller while achieving significant computational savings.",
    "authors": [
      "Ahmad, Sheeraz",
      "Huang, He",
      "Yu, Angela J."
    ]
  },
  {
    "id": "bdb106a0560c4e46ccc488ef010af787",
    "title": "On the Sample Complexity of Subspace Learning",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/bdb106a0560c4e46ccc488ef010af787-Paper.pdf",
    "abstract": "A large number of algorithms in machine learning, from principal component analysis (PCA), and its non-linear (kernel) extensions, to more recent spectral embedding and support estimation methods, rely on estimating a linear subspace from samples.  In this paper we introduce  a general formulation  of this problem and derive novel learning error estimates. Our results rely on  natural  assumptions on the spectral properties of the covariance operator associated to  the data distribution, and hold for a wide class of metrics between subspaces. As special cases, we discuss sharp error estimates  for the reconstruction properties of PCA and spectral support estimation. Key to our analysis is an operator theoretic approach that has broad applicability to spectral learning methods.",
    "authors": [
      "Rudi, Alessandro",
      "Canas, Guillermo D.",
      "Rosasco, Lorenzo"
    ]
  },
  {
    "id": "c042f4db68f23406c6cecf84a7ebb0fe",
    "title": "Non-Linear Domain Adaptation with Boosting",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/c042f4db68f23406c6cecf84a7ebb0fe-Paper.pdf",
    "abstract": "A common assumption in machine vision is that the training and test samples are drawn from the same distribution. However, there are many problems when this assumption is grossly violated, as in bio-medical applications where different acquisitions can generate drastic variations in the appearance of the data due to changing experimental conditions. This problem is accentuated with 3D data, for which annotation is very time-consuming, limiting the amount of data that can be labeled in new acquisitions for training. In this paper we present a multi-task learning algorithm for domain adaptation based on boosting. Unlike previous approaches that learn task-specific decision boundaries, our method learns a single decision boundary in a shared feature space, common to all tasks. We use the boosting-trick to learn a non-linear mapping of the observations in each task, with no need for specific a-priori knowledge of its global analytical form. This yields a more parameter-free domain adaptation approach that successfully leverages learning on new tasks where labeled data is scarce. We evaluate our approach on two challenging bio-medical datasets and achieve a significant improvement over the state-of-the-art.",
    "authors": [
      "Becker, Carlos J.",
      "Christoudias, Christos M.",
      "Fua, Pascal"
    ]
  },
  {
    "id": "c058f544c737782deacefa532d9add4c",
    "title": "Learning Trajectory Preferences for  Manipulators via Iterative Improvement",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/c058f544c737782deacefa532d9add4c-Paper.pdf",
    "abstract": "We consider the problem of learning good trajectories for manipulation tasks. This is challenging because the criterion defining a good trajectory varies with users, tasks and environments.  In this paper, we propose a co-active online  learning framework for teaching robots the preferences of its users for object  manipulation tasks. The key novelty of our approach lies in the type of feedback expected from the user: the human user does not need to demonstrate optimal trajectories as training data, but merely needs to iteratively provide trajectories that slightly improve over the trajectory currently proposed by the system. We argue that this  co-active preference feedback can be more easily elicited from the user than demonstrations of optimal trajectories, which are often challenging and non-intuitive to provide on high degrees of freedom manipulators. Nevertheless, theoretical regret bounds of our algorithm match the asymptotic rates of optimal trajectory algorithms. We also formulate a score function to capture the contextual information and demonstrate the generalizability of our algorithm on a variety of household tasks, for whom, the preferences were not only influenced by the object being manipulated but also by the surrounding environment.",
    "authors": [
      "Jain, Ashesh",
      "Wojcik, Brian",
      "Joachims, Thorsten",
      "Saxena, Ashutosh"
    ]
  },
  {
    "id": "c06d06da9666a219db15cf575aff2824",
    "title": "Learning Chordal Markov Networks by Constraint Satisfaction",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/c06d06da9666a219db15cf575aff2824-Paper.pdf",
    "abstract": "We investigate the problem of learning the structure of a Markov network from data. It is shown that the structure of such networks can be described in terms of constraints which enables the use of existing solver technology with optimization capabilities to compute optimal networks starting from initial scores computed from the data. To achieve efficient encodings, we develop a novel characterization of Markov network structure using a balancing condition on the separators between cliques forming the network. The resulting translations into propositional satisfiability and its extensions such as maximum satisfiability, satisfiability modulo theories, and answer set programming, enable us to prove the optimality of networks which have been previously found by stochastic search.",
    "authors": [
      "Corander, Jukka",
      "Janhunen, Tomi",
      "Rintanen, Jussi",
      "Nyman, Henrik",
      "Pensar, Johan"
    ]
  },
  {
    "id": "c1e39d912d21c91dce811d6da9929ae8",
    "title": "Curvature and Optimal Algorithms for Learning and Minimizing Submodular Functions",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/c1e39d912d21c91dce811d6da9929ae8-Paper.pdf",
    "abstract": "We investigate three related and important problems connected to machine learning, namely approximating a submodular function everywhere, learning a submodular function (in a PAC like setting [26]), and constrained minimization of submodular functions. In all three problems, we provide improved bounds which depend on the \u201ccurvature\u201d of a submodular function and improve on the previously known best results for these problems [9, 3, 7, 25] when the function is not too curved \u2013 a property which is true of many real-world submodular functions. In the former two problems, we obtain these bounds through a generic black-box transformation (which can potentially work for any algorithm), while in the case of submodular minimization, we propose a framework of algorithms which depend on choosing an appropriate surrogate for the submodular function. In all these cases, we provide almost matching lower bounds. While improved curvature-dependent bounds were shown for monotone submodular maximization [4, 27], the existence of similar improved bounds for the aforementioned problems has been open. We resolve this question in this paper by showing that the same notion of curvature provides these improved results. Empirical experiments add further support to our claims.",
    "authors": [
      "Iyer, Rishabh K.",
      "Jegelka, Stefanie",
      "Bilmes, Jeff A."
    ]
  },
  {
    "id": "c26820b8a4c1b3c2aa868d6d57e14a79",
    "title": "A New Convex Relaxation for Tensor Completion",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/c26820b8a4c1b3c2aa868d6d57e14a79-Paper.pdf",
    "abstract": "We study the problem of learning a tensor from a set of linear measurements. A prominent methodology for this problem is based on the extension of trace norm regularization, which has been used extensively for learning low rank matrices, to the tensor setting.  In this paper, we highlight some limitations of this approach and propose an alternative convex relaxation on the Euclidean unit ball.  We then describe a technique to solve the associated regularization problem, which builds upon the alternating direction method of multipliers. Experiments on one synthetic dataset and two real datasets indicate that the proposed method improves significantly over tensor trace norm regularization in terms of estimation error, while remaining computationally tractable.",
    "authors": [
      "Romera-Paredes, Bernardino",
      "Pontil, Massimiliano"
    ]
  },
  {
    "id": "c2aee86157b4a40b78132f1e71a9e6f1",
    "title": "DESPOT: Online POMDP Planning with Regularization",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/c2aee86157b4a40b78132f1e71a9e6f1-Paper.pdf",
    "abstract": "POMDPs provide a principled framework for planning under uncertainty, but are computationally intractable, due to the \u201ccurse of dimensionality\u201d and the \u201ccurse of history\u201d. This paper presents an online lookahead search algorithm that alleviates these difficulties by limiting the search to a set of sampled scenarios. The execution of all policies on the sampled scenarios is summarized using a Determinized Sparse Partially Observable Tree (DESPOT), which is a sparsely sampled belief tree. Our algorithm, named Regularized DESPOT (R-DESPOT), searches the DESPOT for a policy that optimally balances the size of the policy and the accuracy on its value estimate obtained through sampling. We give an output-sensitive performance bound for all policies derived from the DESPOT, and show that R-DESPOT works well if a small optimal policy exists. We also give an anytime approximation to R-DESPOT. Experiments show strong results, compared with two of the fastest online POMDP algorithms.",
    "authors": [
      "Somani, Adhiraj",
      "Ye, Nan",
      "Hsu, David",
      "Lee, Wee Sun"
    ]
  },
  {
    "id": "c3c59e5f8b3e9753913f4d435b53c308",
    "title": "Speeding up Permutation Testing in Neuroimaging",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/c3c59e5f8b3e9753913f4d435b53c308-Paper.pdf",
    "abstract": "Multiple hypothesis testing is a significant problem in nearly all neuroimaging studies. In order to correct for this phenomena, we require a reliable estimate of the Family-Wise Error Rate (FWER). The well known Bonferroni correction method, while being simple to implement, is quite conservative, and can substantially under-power a study because it ignores dependencies between test statistics. Permutation testing, on the other hand, is an exact, non parametric method of estimating the FWER for a given \u03b1 threshold, but for acceptably low thresholds the computational burden can be prohibitive. In this paper, we observe that permutation testing in fact amounts to populating the columns of a very large matrix P. By analyzing the spectrum of this matrix, under certain conditions, we see that P has a low-rank plus a low-variance residual decomposition which makes it suitable for highly sub\u2013sampled \u2014 on the order of 0.5% \u2014 matrix completion methods. Thus, we propose a novel permutation testing methodology which offers a large speedup, without sacrificing the fidelity of the estimated FWER. Our valuations on four different neuroimaging datasets show that a computational speedup factor of roughly 50\u00d7 can be achieved while recovering the FWER distribution up to very high accuracy. Further, we show that the estimated \u03b1 threshold is also recovered faithfully, and is stable.",
    "authors": [
      "Hinrichs, Chris",
      "Ithapu, Vamsi K.",
      "Sun, Qinyuan",
      "Johnson, Sterling C.",
      "Singh, Vikas"
    ]
  },
  {
    "id": "c3e878e27f52e2a57ace4d9a76fd9acf",
    "title": "Neural representation of action sequences: how far can a simple snippet-matching model take us?",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/c3e878e27f52e2a57ace4d9a76fd9acf-Paper.pdf",
    "abstract": "The macaque Superior Temporal Sulcus (STS) is a brain area that receives and integrates inputs from both the ventral and dorsal visual processing streams (thought to specialize in form and motion processing respectively). For the processing of articulated actions, prior work has shown that even a small population of STS neurons contains sufficient information for the decoding of actor invariant to action, action invariant to actor, as well as the specific conjunction of actor and action. This paper addresses two questions. First, what are the invariance properties of individual neural representations (rather than the population representation) in STS? Second, what are the neural encoding mechanisms that can produce such individual neural representations from streams of pixel images? We find that a baseline model, one that simply computes a linear weighted sum of ventral and dorsal responses to short action \u201csnippets\u201d, produces surprisingly good fits to the neural data. Interestingly, even using inputs from a single stream, both actor-invariance and action-invariance can be produced simply by having different linear weights.",
    "authors": [
      "Tan, Cheston",
      "Singer, Jedediah M.",
      "Serre, Thomas",
      "Sheinberg, David",
      "Poggio, Tomaso"
    ]
  },
  {
    "id": "c45147dee729311ef5b5c3003946c48f",
    "title": "Modeling Clutter Perception using Parametric Proto-object Partitioning",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/c45147dee729311ef5b5c3003946c48f-Paper.pdf",
    "abstract": "Visual clutter, the perception of an image as being crowded and disordered, affects aspects of our lives ranging from object detection to aesthetics, yet relatively little effort has been made to model this important and ubiquitous percept. Our approach models clutter as the number of proto-objects segmented from an image, with proto-objects defined as groupings of superpixels that are similar in intensity, color, and gradient orientation features. We introduce a novel parametric method of merging superpixels by modeling mixture of Weibull distributions on similarity distance statistics, then taking the normalized number of proto-objects following partitioning as our estimate of clutter perception. We validated this model using a new $\\text{90}-$image dataset of realistic scenes rank ordered by human raters for clutter, and showed that our method not only predicted clutter extremely well (Spearman's $\\rho = 0.81$, $p < 0.05$), but also outperformed all existing clutter perception models and even a behavioral object segmentation ground truth. We conclude that the number of proto-objects in an image affects clutter perception more than the number of objects or features.",
    "authors": [
      "Yu, Chen-Ping",
      "Hua, Wen-Yu",
      "Samaras, Dimitris",
      "Zelinsky, Greg"
    ]
  },
  {
    "id": "c4851e8e264415c4094e4e85b0baa7cc",
    "title": "Relevance Topic Model for Unstructured Social Group Activity Recognition",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/c4851e8e264415c4094e4e85b0baa7cc-Paper.pdf",
    "abstract": "Unstructured social group activity recognition in web videos is a challenging task due to 1) the semantic gap between class labels and low-level visual features and 2) the lack of labeled training data. To tackle this problem, we propose a relevance topic model\" for jointly learning meaningful mid-level representations upon bag-of-words (BoW) video representations and a classifier with sparse weights. In our approach, sparse Bayesian learning is incorporated into an undirected topic model (i.e., Replicated Softmax) to discover topics which are relevant to video classes and suitable for prediction. Rectified linear units are utilized to increase the expressive power of topics so as to explain better video data containing complex contents and make variational inference tractable for the proposed model. An efficient variational EM algorithm is presented for model parameter estimation and inference. Experimental results on the Unstructured Social Activity Attribute dataset show that our model achieves state of the art performance and outperforms other supervised topic model in terms of classification accuracy, particularly in the case of a very small number of labeled training videos.\"",
    "authors": [
      "Zhao, Fang",
      "Huang, Yongzhen",
      "Wang, Liang",
      "Tan, Tieniu"
    ]
  },
  {
    "id": "c7e1249ffc03eb9ded908c236bd1996d",
    "title": "Generalized Random Utility Models with Multiple Types",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/c7e1249ffc03eb9ded908c236bd1996d-Paper.pdf",
    "abstract": "We propose a model for demand estimation in multi-agent, differentiated product settings and present an estimation algorithm that uses reversible jump MCMC techniques to classify agents' types. Our model extends the popular setup in Berry, Levinsohn and Pakes (1995) to allow for the data-driven classification of agents' types using agent-level data. We focus on applications involving data on agents' ranking over alternatives, and present theoretical conditions that establish the identifiability of the model and uni-modality of the likelihood/posterior. Results on both real and simulated data provide support for the scalability of our approach.",
    "authors": [
      "Azari Soufiani, Hossein",
      "Diao, Hansheng",
      "Lai, Zhenyu",
      "Parkes, David C."
    ]
  },
  {
    "id": "c850371fda6892fbfd1c5a5b457e5777",
    "title": "(Nearly) Optimal Algorithms for Private Online Learning in Full-information and Bandit Settings",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/c850371fda6892fbfd1c5a5b457e5777-Paper.pdf",
    "abstract": "We provide a general technique for making online learning algorithms differentially private, in both the full information and bandit settings. Our technique applies to algorithms that aim to minimize a \\emph{convex} loss function which is a sum of smaller convex loss terms, one for each data point. We modify the popular \\emph{mirror descent} approach, or rather a variant called \\emph{follow the approximate leader}.    The technique leads to the first nonprivate algorithms for private online learning in the bandit setting. In the full information setting, our algorithms improve over the regret bounds of previous work.  In many cases, our algorithms (in both settings) matching the dependence on the input length, $T$, of the \\emph{optimal nonprivate} regret bounds up to logarithmic factors in $T$. Our algorithms require logarithmic space and update time.",
    "authors": [
      "Guha Thakurta, Abhradeep",
      "Smith, Adam"
    ]
  },
  {
    "id": "c913303f392ffc643f7240b180602652",
    "title": "The Fast Convergence of Incremental PCA",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/c913303f392ffc643f7240b180602652-Paper.pdf",
    "abstract": "We prove the first finite-sample convergence rates for any incremental PCA algorithm using sub-quadratic time and memory per iteration. The algorithm analyzed is Oja's learning rule, an efficient and well-known scheme for estimating the top principal component. Our analysis of this non-convex problem yields expected and high-probability convergence rates of $\\tilde{O}(1/n)$ through a novel technique. We relate our guarantees to existing rates for stochastic gradient descent on strongly convex functions, and extend those results. We also include experiments which demonstrate convergence behaviors predicted by our analysis.",
    "authors": [
      "Balsubramani, Akshay",
      "Dasgupta, Sanjoy",
      "Freund, Yoav"
    ]
  },
  {
    "id": "c9e1074f5b3f9fc8ea15d152add07294",
    "title": "Point Based Value Iteration with Optimal Belief Compression for Dec-POMDPs",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/c9e1074f5b3f9fc8ea15d152add07294-Paper.pdf",
    "abstract": "This paper presents four major results towards solving decentralized partially observable Markov decision problems (DecPOMDPs) culminating in an algorithm that outperforms all existing algorithms on all but one standard infinite-horizon benchmark problems. (1) We give an integer program that solves collaborative Bayesian games (CBGs). The program is notable because its linear relaxation is very often integral. (2) We show that a DecPOMDP with bounded belief can be converted to a POMDP (albeit with actions exponential in the number of beliefs). These actions correspond to strategies of a CBG. (3) We present a method to transform any DecPOMDP into a DecPOMDP with bounded beliefs (the number of beliefs is a free parameter) using optimal (not lossless) belief compression. (4) We show that the combination of these results opens the door for new classes of DecPOMDP algorithms based on previous POMDP algorithms. We choose one such algorithm, point-based valued iteration, and modify it to produce the first tractable value iteration method for DecPOMDPs which outperforms existing algorithms.",
    "authors": [
      "MacDermed, Liam C.",
      "Isbell, Charles L."
    ]
  },
  {
    "id": "ca46c1b9512a7a8315fa3c5a946e8265",
    "title": "Summary Statistics for Partitionings and Feature Allocations",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/ca46c1b9512a7a8315fa3c5a946e8265-Paper.pdf",
    "abstract": "Infinite mixture models are commonly used for clustering. One can sample from the posterior of mixture assignments by Monte Carlo methods or find its maximum a posteriori solution by optimization. However, in some problems the posterior is diffuse and it is hard to interpret the sampled partitionings. In this paper, we introduce novel statistics based on block sizes for representing sample sets of partitionings and feature allocations. We develop an element-based definition of entropy to quantify segmentation among their elements. Then we propose a simple algorithm called entropy agglomeration (EA) to summarize and visualize this information. Experiments on various infinite mixture posteriors as well as a feature allocation dataset demonstrate that the proposed statistics are useful in practice.",
    "authors": [
      "Fidaner, Isik B.",
      "Cemgil, Taylan"
    ]
  },
  {
    "id": "cb70ab375662576bd1ac5aaf16b3fca4",
    "title": "Learning Hidden Markov Models from Non-sequence Data via Tensor Decomposition",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/cb70ab375662576bd1ac5aaf16b3fca4-Paper.pdf",
    "abstract": "Learning dynamic models from observed data has been a central issue in many scientific  studies or engineering tasks. The usual setting is that data are collected sequentially  from trajectories of some dynamical system operation.   In quite a few modern scientific modeling tasks, however, it turns out that  reliable sequential data are rather difficult to gather, whereas out-of-order snapshots are much easier to obtain. Examples include the modeling of galaxies, chronic diseases such Alzheimer's, or certain biological processes.  Existing methods for learning dynamic model from non-sequence data are mostly based on Expectation-Maximization, which involves non-convex optimization and is thus hard to analyze.  Inspired by recent advances in spectral learning methods, we propose to study this problem  from a different perspective: moment matching and spectral decomposition. Under that framework,  we identify reasonable assumptions on the generative process of non-sequence data,  and propose learning algorithms based on the tensor decomposition method  \\cite{anandkumar2012tensor} to \\textit{provably} recover first-order Markov models and hidden Markov models. To the best of our knowledge, this is the first formal guarantee on learning from non-sequence data. Preliminary simulation results confirm our theoretical findings.",
    "authors": [
      "Huang, Tzu-Kuo",
      "Schneider, Jeff"
    ]
  },
  {
    "id": "cbb6a3b884f4f88b3a8e3d44c636cbd8",
    "title": "On Flat versus Hierarchical Classification in Large-Scale Taxonomies",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/cbb6a3b884f4f88b3a8e3d44c636cbd8-Paper.pdf",
    "abstract": "We study in this paper flat and hierarchical classification strategies in the context of large-scale taxonomies. To this end, we first propose a multiclass, hierarchical data dependent bound on the generalization error of classifiers deployed in large-scale taxonomies. This bound provides an explanation to several empirical results reported in the literature, related to the performance of flat and hierarchical classifiers. We then introduce another type of bounds targeting the approximation error of a family of classifiers, and derive from it features used in a meta-classifier to decide which nodes to prune (or flatten) in a large-scale taxonomy. We finally illustrate the theoretical developments through several experiments conducted on two widely used taxonomies.",
    "authors": [
      "Babbar, Rohit",
      "Partalas, Ioannis",
      "Gaussier, Eric",
      "Amini, Massih R."
    ]
  },
  {
    "id": "cc1aa436277138f61cda703991069eaf",
    "title": "Scoring Workers in Crowdsourcing: How Many Control Questions are Enough?",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/cc1aa436277138f61cda703991069eaf-Paper.pdf",
    "abstract": "We study the problem of estimating continuous quantities, such as prices, probabilities, and point spreads, using a crowdsourcing approach.  A challenging aspect of combining the crowd's answers is that workers' reliabilities and biases are usually unknown and highly diverse.  Control items with known answers can be used to evaluate workers' performance, and hence improve the combined results on the target items with unknown answers.  This raises the problem of how many control items to use when the total number of items each workers can answer is limited: more control items evaluates the workers better, but leaves fewer resources for the target items that are of direct interest, and vice versa. We give theoretical results for this problem under different scenarios, and provide a simple rule of thumb for crowdsourcing practitioners.  As a byproduct, we also provide theoretical analysis of the accuracy of different consensus methods.",
    "authors": [
      "Liu, Qiang",
      "Ihler, Alexander T.",
      "Steyvers, Mark"
    ]
  },
  {
    "id": "cd758e8f59dfdf06a852adad277986ca",
    "title": "Cluster Trees on Manifolds",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/cd758e8f59dfdf06a852adad277986ca-Paper.pdf",
    "abstract": "We investigate the problem of estimating the cluster tree for a density $f$ supported on or near a smooth $d$-dimensional manifold $M$ isometrically embedded in $\\mathbb{R}^D$. We study a $k$-nearest neighbor based algorithm recently proposed by Chaudhuri and Dasgupta. Under mild assumptions on $f$ and $M$, we obtain rates of convergence that depend on $d$ only but not on the ambient dimension $D$. We also provide a sample complexity lower bound for a natural class of clustering algorithms that use $D$-dimensional neighborhoods.",
    "authors": [
      "Balakrishnan, Sivaraman",
      "Narayanan, Srivatsan",
      "Rinaldo, Alessandro",
      "Singh, Aarti",
      "Wasserman, Larry"
    ]
  },
  {
    "id": "d10ec7c16cbe9de8fbb1c42787c3ec26",
    "title": "Bayesian inference as iterated random functions with  applications to sequential inference in graphical models",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/d10ec7c16cbe9de8fbb1c42787c3ec26-Paper.pdf",
    "abstract": "We propose a general formalism of iterated random functions with semigroup property, under which exact and approximate Bayesian posterior updates can be viewed as specific instances. A convergence theory for iterated random functions is presented. As an application of the general theory we analyze convergence behaviors of exact and approximate message-passing algorithms that arise in a sequential change point detection problem formulated via a latent variable directed graphical model. The sequential inference algorithm and its supporting theory are  illustrated by simulated examples.",
    "authors": [
      "Amini, Arash",
      "Nguyen, XuanLong"
    ]
  },
  {
    "id": "d1f255a373a3cef72e03aa9d980c7eca",
    "title": "Recurrent networks of coupled Winner-Take-All oscillators for solving constraint satisfaction problems",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/d1f255a373a3cef72e03aa9d980c7eca-Paper.pdf",
    "abstract": "We present a recurrent neuronal network, modeled as a continuous-time dynamical system, that can solve constraint satisfaction problems. Discrete variables are represented by coupled Winner-Take-All (WTA) networks, and their values are encoded in localized patterns of oscillations that are learned by the recurrent weights in these networks. Constraints over the variables are encoded in the network connectivity. Although there are no sources of noise, the network can escape from local optima in its search for solutions that satisfy all constraints by modifying the effective network connectivity through oscillations. If there is no solution that satisfies all constraints, the network state changes in a pseudo-random manner and its trajectory approximates a sampling procedure that selects a variable assignment with a probability that increases with the fraction of constraints satisfied by this assignment. External evidence, or input to the network, can force variables to specific values. When new inputs are applied, the network re-evaluates the entire set of variables in its search for the states that satisfy the maximum number of constraints, while being consistent with the external input. Our results demonstrate that the proposed network architecture can perform a deterministic search for the optimal solution to problems with non-convex cost functions. The network is inspired by canonical microcircuit models of the cortex and suggests possible dynamical mechanisms to solve constraint satisfaction problems that can be present in biological networks, or implemented in neuromorphic electronic circuits.",
    "authors": [
      "Mostafa, Hesham",
      "Mueller, Lorenz. K.",
      "Indiveri, Giacomo"
    ]
  },
  {
    "id": "d296c101daa88a51f6ca8cfc1ac79b50",
    "title": "Rapid Distance-Based Outlier Detection via Sampling",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/d296c101daa88a51f6ca8cfc1ac79b50-Paper.pdf",
    "abstract": "Distance-based approaches to outlier detection are popular in data mining, as they do not require to model the underlying probability distribution, which is particularly challenging for high-dimensional data. We present an empirical comparison of various approaches to distance-based outlier detection across a large number of datasets. We report the surprising observation that a simple, sampling-based scheme outperforms state-of-the-art techniques in terms of both efficiency and effectiveness. To better understand this phenomenon, we provide a theoretical analysis why the sampling-based approach outperforms alternative methods based on k-nearest neighbor search.",
    "authors": [
      "Sugiyama, Mahito",
      "Borgwardt, Karsten"
    ]
  },
  {
    "id": "d2ed45a52bc0edfa11c2064e9edee8bf",
    "title": "Visual Concept Learning: Combining Machine Vision and Bayesian Generalization on Concept Hierarchies",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/d2ed45a52bc0edfa11c2064e9edee8bf-Paper.pdf",
    "abstract": "Learning a visual concept from a small number of positive examples is a significant challenge for machine learning algorithms. Current methods typically fail to find the appropriate level of generalization in a concept hierarchy for a given set of visual examples. Recent work in cognitive science on Bayesian models of generalization addresses this challenge, but prior results assumed that objects were perfectly  recognized. We present an algorithm for learning visual concepts directly from images, using  probabilistic predictions generated by visual classifiers as the input to a Bayesian generalization model. As no existing challenge data tests this paradigm, we collect and make available a new, large-scale dataset for visual concept learning using the ImageNet hierarchy as the source of possible concepts, with human annotators to provide ground truth labels as to whether a new image is an instance of each concept using a paradigm similar to that used in experiments studying word learning in children.  We compare the performance of our system to several baseline algorithms, and show a significant advantage results from combining visual classifiers with the ability to identify an appropriate level of abstraction using Bayesian generalization.",
    "authors": [
      "Jia, Yangqing",
      "Abbott, Joshua T.",
      "Austerweil, Joseph L.",
      "Griffiths, Tom",
      "Darrell, Trevor"
    ]
  },
  {
    "id": "d490d7b4576290fa60eb31b5fc917ad1",
    "title": "Memoized Online Variational Inference for Dirichlet Process Mixture Models",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/d490d7b4576290fa60eb31b5fc917ad1-Paper.pdf",
    "abstract": "Variational inference algorithms provide the most effective framework for large-scale training  of Bayesian nonparametric models. Stochastic online approaches are promising, but are sensitive to the chosen learning rate and often converge to poor local optima.  We present a new algorithm, memoized online variational inference, which scales to very large (yet finite) datasets while avoiding the complexities of stochastic gradient.  Our algorithm maintains finite-dimensional sufficient statistics from batches of the full dataset, requiring some additional memory but still scaling to millions of examples.  Exploiting nested families of variational bounds for infinite nonparametric models, we develop principled birth and merge moves allowing non-local optimization.  Births adaptively add components to the model to escape local optima, while merges remove redundancy and improve speed.  Using Dirichlet process mixture models for image clustering and denoising, we demonstrate major improvements in robustness and accuracy.",
    "authors": [
      "Hughes, Michael C.",
      "Sudderth, Erik"
    ]
  },
  {
    "id": "d554f7bb7be44a7267068a7df88ddd20",
    "title": "Locally Adaptive Bayesian Multivariate Time Series",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/d554f7bb7be44a7267068a7df88ddd20-Paper.pdf",
    "abstract": "In modeling multivariate time series, it is important to allow time-varying smoothness in the mean and covariance process. In particular, there may be certain time intervals exhibiting rapid changes and others in which changes are slow. If such locally adaptive smoothness is not accounted for, one can obtain misleading inferences and predictions, with over-smoothing across erratic time intervals and under-smoothing across times exhibiting slow variation. This can lead to miscalibration of predictive intervals, which can be substantially too narrow or wide depending on the time. We propose a continuous multivariate stochastic process for time series having locally varying smoothness in both the mean and covariance matrix. This process is constructed utilizing latent dictionary functions in time, which are given nested Gaussian process priors and linearly related to the observed data through a sparse mapping. Using a differential equation representation, we bypass usual computational bottlenecks in obtaining MCMC and online algorithms for approximate Bayesian inference. The performance is assessed in simulations and illustrated in a financial application.",
    "authors": [
      "Durante, Daniele",
      "Scarpa, Bruno",
      "Dunson, David B."
    ]
  },
  {
    "id": "d64a340bcb633f536d56e51874281454",
    "title": "When in Doubt, SWAP: High-Dimensional Sparse Recovery from Correlated Measurements",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/d64a340bcb633f536d56e51874281454-Paper.pdf",
    "abstract": "We consider the problem of accurately estimating a high-dimensional sparse vector using a small number of linear measurements that are contaminated by noise.  It is well known that standard computationally tractable sparse recovery algorithms, such as the Lasso, OMP, and their various extensions, perform poorly when the measurement matrix contains highly correlated columns.  We develop a simple greedy algorithm, called SWAP, that iteratively swaps variables until a desired loss function cannot be decreased any further.  SWAP is surprisingly effective in handling measurement matrices with high correlations.  We prove that SWAP can be easily used as a wrapper around standard sparse recovery algorithms for improved performance.  We theoretically quantify the statistical guarantees of SWAP and complement our analysis with numerical results on synthetic and real data.",
    "authors": [
      "Vats, Divyanshu",
      "Baraniuk, Richard"
    ]
  },
  {
    "id": "d6ef5f7fa914c19931a55bb262ec879c",
    "title": "Information-theoretic lower bounds for distributed statistical estimation with communication constraints",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/d6ef5f7fa914c19931a55bb262ec879c-Paper.pdf",
    "abstract": "We establish minimax risk lower bounds for distributed statistical estimation given a budget $B$ of the total number of bits that may be communicated. Such lower bounds in turn reveal the minimum amount of communication required by any procedure to achieve the classical optimal rate for statistical estimation. We study two classes of protocols in which machines send messages either independently or interactively. The lower bounds are established for a variety of problems, from estimating the mean of a population to estimating parameters in linear regression or binary classification.",
    "authors": [
      "Zhang, Yuchen",
      "Duchi, John",
      "Jordan, Michael I.",
      "Wainwright, Martin J."
    ]
  },
  {
    "id": "d81f9c1be2e08964bf9f24b15f0e4900",
    "title": "Learning Stochastic Feedforward Neural Networks",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/d81f9c1be2e08964bf9f24b15f0e4900-Paper.pdf",
    "abstract": "Multilayer perceptrons (MLPs) or neural networks are popular models used for nonlinear regression and classification tasks. As regressors, MLPs model the conditional distribution of the predictor variables Y given the input variables X. However, this predictive distribution is assumed to be unimodal (e.g. Gaussian). For tasks such as structured prediction problems, the conditional distribution should be multimodal, forming one-to-many mappings. By using stochastic hidden variables rather than deterministic ones, Sigmoid Belief Nets (SBNs) can induce a rich multimodal distribution in the output space. However, previously proposed learning algorithms for SBNs are very slow and do not work well for real-valued data. In this paper, we propose a stochastic feedforward network with hidden layers having \\emph{both deterministic and stochastic} variables. A new Generalized EM training procedure using importance sampling allows us to efficiently learn complicated conditional distributions. We demonstrate the superiority of our model to conditional Restricted Boltzmann Machines and Mixture Density Networks on synthetic datasets and on modeling facial expressions. Moreover, we show that latent features of our model improves classification and provide additional qualitative results on color images.",
    "authors": [
      "Tang, Charlie",
      "Salakhutdinov, Russ R."
    ]
  },
  {
    "id": "d86ea612dec96096c5e0fcc8dd42ab6d",
    "title": "Robust Transfer Principal Component Analysis with Rank Constraints",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/d86ea612dec96096c5e0fcc8dd42ab6d-Paper.pdf",
    "abstract": "Principal component analysis (PCA), a well-established technique for data analysis and processing, provides a convenient form of dimensionality reduction that is effective for cleaning small Gaussian noises presented in the data. However, the applicability of standard principal component analysis in real scenarios is limited by its sensitivity to large errors. In this paper, we tackle the challenge problem of recovering data corrupted with errors of high magnitude by developing a novel robust transfer principal component analysis method. Our method is based on the assumption that useful information for the recovery of a corrupted data matrix can be gained from an uncorrupted related data matrix. Speci\ufb01cally, we formulate the data recovery problem as a joint robust principal component analysis problem on the two data matrices, with shared common principal components across matrices and individual principal components speci\ufb01c to each data matrix. The formulated optimization problem is a minimization problem over a convex objective function but with non-convex rank constraints. We develop an ef\ufb01cient proximal projected gradient descent algorithm to solve the proposed optimization problem with convergence guarantees. Our empirical results over image denoising tasks show the proposed method can effectively recover images with random large errors, and signi\ufb01cantly outperform both standard PCA and robust PCA.",
    "authors": [
      "Guo, Yuhong"
    ]
  },
  {
    "id": "d93ed5b6db83be78efb0d05ae420158e",
    "title": "A Determinantal Point Process Latent Variable Model for Inhibition in Neural Spiking Data",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/d93ed5b6db83be78efb0d05ae420158e-Paper.pdf",
    "abstract": "Point processes are popular models of neural spiking behavior as they provide a statistical distribution over temporal sequences of spikes and help to reveal the complexities underlying a series of recorded action potentials.  However, the most common neural point process models, the Poisson process and the gamma renewal process, do not capture interactions and correlations that are critical to modeling populations of neurons.  We develop a novel model based on a determinantal point process over latent embeddings of neurons that effectively captures and helps visualize complex inhibitory and competitive interaction.  We show that this model is a natural extension of the popular generalized linear model to sets of interacting neurons.   The model is extended to incorporate gain control or divisive normalization, and the modulation of neural spiking based on periodic phenomena.  Applied to neural spike recordings from the rat hippocampus, we see that the model captures inhibitory relationships, a dichotomy of classes of neurons, and a periodic modulation by the theta rhythm known to be present in the data.",
    "authors": [
      "Snoek, Jasper",
      "Zemel, Richard",
      "Adams, Ryan P."
    ]
  },
  {
    "id": "d9d4f495e875a2e075a1a4a6e1b9770f",
    "title": "Reciprocally Coupled Local Estimators Implement Bayesian Information Integration Distributively",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/d9d4f495e875a2e075a1a4a6e1b9770f-Paper.pdf",
    "abstract": "Psychophysical experiments have demonstrated that the brain integrates information from multiple sensory cues in a near Bayesian optimal manner. The present study proposes a novel mechanism to achieve this. We consider two reciprocally connected networks, mimicking the integration of heading direction information between the dorsal medial superior temporal (MSTd) and the ventral intraparietal (VIP) areas. Each network serves as a local estimator and receives an independent cue, either the visual or the vestibular, as direct input for the external stimulus. We find that positive reciprocal interactions can improve the decoding accuracy of each individual network as if it implements Bayesian inference from two cues. Our model successfully explains the experimental finding that both MSTd and VIP achieve Bayesian multisensory integration, though each of them only receives a single cue as direct external input. Our result suggests that the brain may implement optimal information integration distributively at each local estimator through the reciprocal connections between cortical regions.",
    "authors": [
      "Zhang, Wen-Hao",
      "Wu, Si"
    ]
  },
  {
    "id": "d9fc5b73a8d78fad3d6dffe419384e70",
    "title": "Structured Learning via Logistic Regression",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/d9fc5b73a8d78fad3d6dffe419384e70-Paper.pdf",
    "abstract": "A successful approach to structured learning is to write the learning objective as a joint function of linear parameters and inference messages, and iterate between updates to each. This paper observes that if the inference problem is \u201csmoothed\u201d through the addition of entropy terms, for fixed messages, the learning objective reduces to a traditional (non-structured) logistic regression problem with respect to parameters. In these logistic regression problems, each training example has a bias term determined by the current set of messages. Based on this insight, the structured energy function can be extended from linear factors to any function class where an \u201coracle\u201d exists to minimize a logistic loss.",
    "authors": [
      "Domke, Justin"
    ]
  },
  {
    "id": "db2b4182156b2f1f817860ac9f409ad7",
    "title": "Learning word embeddings efficiently with noise-contrastive estimation",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/db2b4182156b2f1f817860ac9f409ad7-Paper.pdf",
    "abstract": "Continuous-valued word embeddings learned by neural language models have recently been shown to capture semantic and syntactic information about words very well, setting performance records on several word similarity tasks.  The best results are obtained by learning high-dimensional embeddings from very large quantities of data, which makes scalability of the training method a critical factor.  We propose a simple and scalable new approach to learning word embeddings based on training log-bilinear models with noise-contrastive estimation.  Our approach is simpler, faster, and produces better results than the current state-of-the art method of Mikolov et al. (2013a). We achieve results comparable to the best ones reported, which were obtained on a cluster, using four times less data and more than an order of magnitude less computing time. We also investigate several model types and find that the embeddings learned by the simpler models perform at least as well as those learned by the more complex ones.",
    "authors": [
      "Mnih, Andriy",
      "Kavukcuoglu, Koray"
    ]
  },
  {
    "id": "dc4c44f624d600aa568390f1f1104aa0",
    "title": "Generalized Method-of-Moments for Rank Aggregation",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/dc4c44f624d600aa568390f1f1104aa0-Paper.pdf",
    "abstract": "In this paper we propose a class of efficient Generalized Method-of-Moments(GMM) algorithms for computing parameters of the Plackett-Luce model, where the data consists of full rankings over alternatives. Our technique is based on breaking the full rankings into pairwise comparisons, and then computing parameters that satisfy a set of generalized moment conditions. We identify conditions for the output of GMM to be unique, and identify a general class of consistent and inconsistent breakings. We then show by theory and experiments that our algorithms run significantly faster than the classical Minorize-Maximization (MM) algorithm, while achieving competitive statistical efficiency.",
    "authors": [
      "Azari Soufiani, Hossein",
      "Chen, William",
      "Parkes, David C.",
      "Xia, Lirong"
    ]
  },
  {
    "id": "dc58e3a306451c9d670adcd37004f48f",
    "title": "Reconciling \"priors\" & \"priors\" without prejudice?",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/dc58e3a306451c9d670adcd37004f48f-Paper.pdf",
    "abstract": "There are two major routes to address linear inverse problems. Whereas regularization-based approaches build estimators as solutions of penalized regression optimization problems, Bayesian estimators rely on the posterior distribution of the unknown, given some assumed family of priors. While these may seem radically different approaches, recent results have shown that, in the context of additive white Gaussian denoising, the Bayesian conditional mean estimator is always the solution of a penalized regression problem. The contribution of this paper is twofold. First, we extend the additive white Gaussian denoising results to general linear inverse problems with colored Gaussian noise. Second, we characterize conditions under which the penalty function associated to the conditional mean estimator can satisfy certain popular properties such as convexity, separability, and smoothness. This sheds light on some tradeoff between computational efficiency and estimation accuracy in sparse regularization, and draws some connections between Bayesian estimation and proximal optimization.",
    "authors": [
      "Gribonval, Remi",
      "Machart, Pierre"
    ]
  },
  {
    "id": "dc6a6489640ca02b0d42dabeb8e46bb7",
    "title": "Learning a Deep Compact Image Representation for Visual Tracking",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/dc6a6489640ca02b0d42dabeb8e46bb7-Paper.pdf",
    "abstract": "In this paper, we study the challenging problem of tracking the trajectory of a moving object in a video with possibly very complex background.  In contrast to most existing trackers which only learn the appearance of the tracked object online, we take a different approach, inspired by recent advances in deep learning architectures, by putting more emphasis on the (unsupervised) feature learning problem.  Specifically, by using auxiliary natural images, we train a stacked denoising autoencoder offline to learn generic image features that are more robust against variations.  This is then followed by knowledge transfer from offline training to the online tracking process.  Online tracking involves a classification neural network which is constructed from the encoder part of the trained autoencoder as a feature extractor and an additional classification layer.  Both the feature extractor and the classifier can be further tuned to adapt to appearance changes of the moving object.  Comparison with the state-of-the-art trackers on some challenging benchmark video sequences shows that our deep learning tracker is very efficient as well as more accurate.",
    "authors": [
      "Wang, Naiyan",
      "Yeung, Dit-Yan"
    ]
  },
  {
    "id": "dc912a253d1e9ba40e2c597ed2376640",
    "title": "Trading Computation for Communication: Distributed Stochastic Dual Coordinate Ascent",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/dc912a253d1e9ba40e2c597ed2376640-Paper.pdf",
    "abstract": "We present and study a distributed optimization algorithm by employing  a stochastic dual coordinate ascent method. Stochastic dual coordinate ascent methods enjoy strong theoretical guarantees and often have better performances than stochastic gradient descent methods in optimizing regularized loss minimization problems. It still lacks of efforts in studying them in a distributed framework. We make a progress along the line by presenting a distributed stochastic dual coordinate ascent algorithm in a star network, with an analysis of the tradeoff between  computation and  communication. We verify  our analysis by experiments on real data sets. Moreover, we compare the proposed algorithm with distributed stochastic gradient descent methods and distributed alternating direction methods of multipliers for optimizing SVMs in the same distributed framework, and observe competitive performances.",
    "authors": [
      "Yang, Tianbao"
    ]
  },
  {
    "id": "dd77279f7d325eec933f05b1672f6a1f",
    "title": "Projected Natural Actor-Critic",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/dd77279f7d325eec933f05b1672f6a1f-Paper.pdf",
    "abstract": "Natural actor-critics are a popular class of policy search algorithms for finding locally optimal policies for Markov decision processes. In this paper we address a drawback of natural actor-critics that limits their real-world applicability - their lack of safety guarantees. We present a principled algorithm for performing natural gradient descent over a constrained domain. In the context of reinforcement learning, this allows for natural actor-critic algorithms that are guaranteed to remain within a known safe region of policy space. While deriving our class of constrained natural actor-critic algorithms, which we call Projected Natural Actor-Critics (PNACs), we also elucidate the relationship between natural gradient descent and mirror descent.",
    "authors": [
      "Thomas, Philip S.",
      "Dabney, William C.",
      "Giguere, Stephen",
      "Mahadevan, Sridhar"
    ]
  },
  {
    "id": "e00406144c1e7e35240afed70f34166a",
    "title": "Minimax Optimal Algorithms for Unconstrained Linear Optimization",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/e00406144c1e7e35240afed70f34166a-Paper.pdf",
    "abstract": "We design and analyze minimax-optimal algorithms for online linear   optimization games where the player's choice is unconstrained.  The   player strives to minimize regret, the difference between his loss   and the loss of a post-hoc benchmark strategy.  The standard   benchmark is the loss of the best strategy chosen from a bounded   comparator set, whereas we consider a broad range of benchmark   functions. We consider the problem as a sequential multi-stage   zero-sum game, and we give a thorough analysis of the minimax   behavior of the game, providing characterizations for the value of   the game, as well as both the player's and the adversary's optimal   strategy.  We show how these objects can be computed efficiently   under certain circumstances, and by selecting an appropriate   benchmark, we construct a novel hedging strategy for an   unconstrained betting game.",
    "authors": [
      "McMahan, Brendan",
      "Abernethy, Jacob"
    ]
  },
  {
    "id": "e034fb6b66aacc1d48f445ddfb08da98",
    "title": "Policy Shaping: Integrating Human Feedback with Reinforcement Learning",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/e034fb6b66aacc1d48f445ddfb08da98-Paper.pdf",
    "abstract": "A long term goal of Interactive Reinforcement Learning is to incorporate non-expert human feedback to solve complex tasks. State-of-the-art methods have approached this problem by mapping human information to reward and value signals to indicate preferences and then iterating over them to compute the necessary control policy. In this paper we argue for an alternate, more effective characterization of human feedback: Policy Shaping. We introduce Advise, a Bayesian approach that attempts to maximize the information gained from human feedback by utilizing it as direct labels on the policy. We compare Advise to state-of-the-art approaches and highlight scenarios where it outperforms them and importantly is robust to infrequent and inconsistent human feedback.",
    "authors": [
      "Griffith, Shane",
      "Subramanian, Kaushik",
      "Scholz, Jonathan",
      "Isbell, Charles L.",
      "Thomaz, Andrea L."
    ]
  },
  {
    "id": "e1e32e235eee1f970470a3a6658dfdd5",
    "title": "Regret based Robust Solutions for Uncertain Markov Decision Processes",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/e1e32e235eee1f970470a3a6658dfdd5-Paper.pdf",
    "abstract": "In this paper, we seek robust policies for uncertain Markov Decision Processes (MDPs). Most robust optimization approaches for these problems have focussed on the computation of {\\em maximin} policies which maximize the value corresponding to the worst realization of the uncertainty. Recent work has proposed {\\em minimax} regret as a suitable alternative to the {\\em maximin} objective for robust optimization.  However, existing algorithms for handling {\\em minimax} regret are restricted to models with uncertainty over rewards only.  We provide algorithms that employ sampling to improve across multiple dimensions: (a) Handle uncertainties over both transition and reward models; (b) Dependence of model uncertainties across state, action pairs and decision epochs; (c) Scalability and quality bounds. Finally, to demonstrate the empirical effectiveness of our sampling approaches, we provide comparisons against benchmark algorithms on two domains from literature. We also provide a Sample Average Approximation (SAA) analysis to compute a posteriori error bounds.",
    "authors": [
      "Ahmed, Asrar",
      "Varakantham, Pradeep",
      "Adulyasak, Yossiri",
      "Jaillet, Patrick"
    ]
  },
  {
    "id": "e3796ae838835da0b6f6ea37bcf8bcb7",
    "title": "Understanding variable importances in forests of randomized trees",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/e3796ae838835da0b6f6ea37bcf8bcb7-Paper.pdf",
    "abstract": "Despite growing interest and practical use in various scientific areas, variable importances derived from tree-based ensemble methods are not well understood from a theoretical point of view. In this work we characterize the Mean Decrease Impurity (MDI) variable importances as measured by an ensemble of totally randomized trees in asymptotic sample and ensemble size conditions. We derive a three-level decomposition of the  information jointly provided by all input variables about the output in terms of i) the MDI importance of each input variable, ii) the degree of interaction of a given input variable with the other input variables, iii) the different interaction terms of a given degree. We then show that this MDI importance of a variable is equal to zero if and only if the variable is irrelevant and that the MDI importance of a relevant variable is invariant with respect to the removal or the addition of irrelevant variables. We illustrate these properties on a simple example and discuss how they may change in the case of  non-totally randomized trees such as Random Forests and Extra-Trees.",
    "authors": [
      "Louppe, Gilles",
      "Wehenkel, Louis",
      "Sutera, Antonio",
      "Geurts, Pierre"
    ]
  },
  {
    "id": "e48e13207341b6bffb7fb1622282247b",
    "title": "Linear decision rule as aspiration for simple decision heuristics",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/e48e13207341b6bffb7fb1622282247b-Paper.pdf",
    "abstract": "Many attempts to understand the success of simple decision heuristics have examined heuristics as an approximation to a linear decision rule. This research has identified three environmental structures that aid heuristics: dominance, cumulative dominance, and noncompensatoriness. Here, we further develop these ideas and examine their empirical relevance in 51 natural environments. We find that all three structures are prevalent, making it possible for some simple rules to reach the accuracy levels of the linear decision rule using less information.",
    "authors": [
      "\u015eim\u015fek, \u00d6zg\u00fcr"
    ]
  },
  {
    "id": "e49b8b4053df9505e1f48c3a701c0682",
    "title": "Adaptive Multi-Column Deep Neural Networks with Application to Robust Image Denoising",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/e49b8b4053df9505e1f48c3a701c0682-Paper.pdf",
    "abstract": "Stacked sparse denoising auto-encoders (SSDAs) have recently been shown to be successful at removing noise from corrupted images. However, like most denoising techniques, the SSDA is not robust to variation in noise types beyond what it has seen during training. We present the multi-column stacked sparse denoising autoencoder, a novel technique of combining multiple SSDAs into a multi-column SSDA (MC-SSDA) by combining the outputs of each SSDA. We eliminate the need to determine the type of noise, let alone its statistics, at test time. We show that good denoising performance can be achieved with a single system on a variety of different noise types, including ones not seen in the training set. Additionally, we experimentally demonstrate the efficacy of MC-SSDA denoising by achieving MNIST digit error rates on denoised images at close to that of the uncorrupted images.",
    "authors": [
      "Agostinelli, Forest",
      "Anderson, Michael R.",
      "Lee, Honglak"
    ]
  },
  {
    "id": "e53a0a2978c28872a4505bdb51db06dc",
    "title": "Probabilistic Movement Primitives",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/e53a0a2978c28872a4505bdb51db06dc-Paper.pdf",
    "abstract": "Movement Primitives (MP) are a well-established approach for representing modular and re-usable robot movement generators. Many state-of-the-art robot learning successes are based MPs, due to their compact representation of the inherently continuous and high dimensional robot movements. A major goal in robot learning is to combine multiple MPs as building blocks in a modular control architecture to solve complex tasks. To this effect, a MP representation has to allow for blending between motions, adapting to altered task variables, and co-activating multiple MPs in parallel. We present a probabilistic formulation of the MP concept that maintains a distribution over trajectories. Our probabilistic approach allows for the derivation of new operations which are essential for implementing all aforementioned properties in one framework. In order to use such a trajectory distribution for robot movement control, we analytically derive a stochastic feedback controller which reproduces the given trajectory distribution. We evaluate and compare our approach to existing methods on several simulated as well as real robot scenarios.",
    "authors": [
      "Paraschos, Alexandros",
      "Daniel, Christian",
      "Peters, Jan R.",
      "Neumann, Gerhard"
    ]
  },
  {
    "id": "e58cc5ca94270acaceed13bc82dfedf7",
    "title": "Speedup Matrix Completion with Side Information: Application to Multi-Label Learning",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/e58cc5ca94270acaceed13bc82dfedf7-Paper.pdf",
    "abstract": "In standard matrix completion theory, it is required to have at least $O(n\\ln^2 n)$ observed entries to perfectly recover a low-rank matrix $M$ of size $n\\times n$, leading to a large number of observations when $n$ is large. In many real tasks, side information in addition to the observed entries is often available. In this work, we develop a novel theory of matrix completion that explicitly explore the side information to reduce the requirement on the number of observed entries. We show that, under appropriate conditions, with the assistance of side information matrices, the number of observed entries needed for a perfect recovery of matrix $M$ can be dramatically reduced to $O(\\ln n)$. We demonstrate the effectiveness of the proposed approach for matrix completion in transductive incomplete multi-label learning.",
    "authors": [
      "Xu, Miao",
      "Jin, Rong",
      "Zhou, Zhi-Hua"
    ]
  },
  {
    "id": "e5e63da79fcd2bebbd7cb8bf1c1d0274",
    "title": "Message Passing Inference with Chemical Reaction Networks",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/e5e63da79fcd2bebbd7cb8bf1c1d0274-Paper.pdf",
    "abstract": "Recent work on molecular programming has explored new possibilities for computational abstractions with biomolecules, including logic gates, neural networks, and linear systems.  In the future such abstractions might enable nanoscale devices that can sense and control the world at a molecular scale.  Just as in macroscale robotics, it is critical that such devices can learn about their environment and reason under uncertainty. At this small scale, systems are typically modeled as chemical reaction networks. In this work, we develop a procedure that can take arbitrary probabilistic graphical models, represented as factor graphs over discrete random variables, and compile them into chemical reaction networks that implement inference.  In particular, we show that marginalization based on sum-product message passing can be implemented in terms of reactions between chemical species whose concentrations represent probabilities.  We show algebraically that the steady state concentration of these species correspond to the marginal distributions of the random variables in the graph and validate the results in simulations.  As with standard sum-product inference, this procedure yields exact results for tree-structured graphs, and approximate solutions for loopy graphs.",
    "authors": [
      "Napp, Nils E.",
      "Adams, Ryan P."
    ]
  },
  {
    "id": "e5f6ad6ce374177eef023bf5d0c018b6",
    "title": "Binary to Bushy: Bayesian Hierarchical Clustering with the Beta Coalescent",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/e5f6ad6ce374177eef023bf5d0c018b6-Paper.pdf",
    "abstract": "Discovering hierarchical regularities in data is a key problem in interacting   with large datasets, modeling cognition, and encoding knowledge. A previous   Bayesian solution---Kingman's coalescent---provides a convenient probabilistic   model for data represented as a binary tree. Unfortunately, this is   inappropriate for data better described by bushier trees. We generalize an   existing belief propagation framework of Kingman's coalescent to the   beta coalescent, which models a wider range of tree structures.   Because of the complex combinatorial search over possible structures, we   develop new sampling schemes using sequential Monte Carlo and Dirichlet   process mixture models, which render inference efficient and tractable.     We present results on both synthetic and real data that show the beta coalescent      outperforms Kingman's coalescent on real datasets and is qualitatively better at    capturing data in bushy hierarchies.",
    "authors": [
      "Hu, Yuening",
      "Ying, Jordan L.",
      "Daume III, Hal",
      "Ying, Z. Irene"
    ]
  },
  {
    "id": "e6d8545daa42d5ced125a4bf747b3688",
    "title": "A Stability-based Validation Procedure for Differentially Private Machine Learning",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/e6d8545daa42d5ced125a4bf747b3688-Paper.pdf",
    "abstract": "Differential privacy is a cryptographically motivated definition of privacy which has gained considerable attention in the algorithms, machine-learning and data-mining communities. While there has been an explosion of work on differentially private machine learning algorithms, a major barrier to achieving end-to-end differential privacy in practical machine learning applications is the lack of an effective procedure for differentially private parameter tuning, or, determining the parameter value, such as a bin size in a histogram, or a regularization parameter, that is suitable for a particular application.   In this paper, we introduce a generic validation procedure for differentially private machine learning algorithms that apply when a certain stability condition holds on the training algorithm and the validation performance metric. The training data size and the privacy budget used for training in our procedure is independent of the number of parameter values searched over. We apply our generic procedure to two fundamental tasks in statistics and machine-learning -- training a regularized linear classifier and building a histogram density estimator that result in end-to-end differentially private solutions for these problems.",
    "authors": [
      "Chaudhuri, Kamalika",
      "Vinterbo, Staal A."
    ]
  },
  {
    "id": "e836d813fd184325132fca8edcdfb40e",
    "title": "Unsupervised Spectral Learning of Finite State Transducers",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/e836d813fd184325132fca8edcdfb40e-Paper.pdf",
    "abstract": "Finite-State Transducers (FST) are a standard tool for modeling paired input-output sequences and are used in numerous applications, ranging from computational biology to natural language processing. Recently Balle et al. presented a spectral algorithm for learning FST from samples of aligned input-output sequences.  In this paper we address the more realistic, yet challenging setting where the alignments are unknown to the learning algorithm. We frame FST learning as finding a low rank Hankel matrix satisfying constraints derived from observable statistics. Under this formulation, we provide identifiability results for FST distributions. Then, following previous work on rank minimization, we propose a regularized convex relaxation of this objective which is based on minimizing a nuclear norm penalty subject to linear constraints and can be solved efficiently.",
    "authors": [
      "Bailly, Raphael",
      "Carreras, Xavier",
      "Quattoni, Ariadna"
    ]
  },
  {
    "id": "e96ed478dab8595a7dbda4cbcbee168f",
    "title": "One-shot learning and big data with n=2",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/e96ed478dab8595a7dbda4cbcbee168f-Paper.pdf",
    "abstract": "We model a one-shot learning\" situation, where very few (scalar) observations $y_1,...,y_n$ are available. Associated with each observation $y_i$ is a very high-dimensional vector $x_i$, which provides context for $y_i$ and enables us to predict subsequent observations, given their own context. One of the salient features of our analysis is that the problems studied here are easier when the dimension of $x_i$ is large; in other words, prediction becomes easier when more context is provided. The proposed methodology is a variant of principal component regression (PCR). Our rigorous analysis sheds new light on PCR. For instance, we show that classical PCR estimators may be inconsistent in the specified setting, unless they are multiplied by a scalar $c > 1$; that is, unless the classical estimator is expanded. This expansion phenomenon appears to be somewhat novel and contrasts with shrinkage methods ($c < 1$), which are far more common in big data analyses. \"",
    "authors": [
      "Dicker, Lee H.",
      "Foster, Dean P."
    ]
  },
  {
    "id": "e97ee2054defb209c35fe4dc94599061",
    "title": "Mapping paradigm ontologies to and from the brain",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/e97ee2054defb209c35fe4dc94599061-Paper.pdf",
    "abstract": "Imaging neuroscience links brain activation maps to behavior and cognition via correlational studies. Due to the nature of the individual experiments, based on eliciting neural response from a small number of stimuli, this link is incomplete, and unidirectional from the causal point of view. To come to conclusions on the function implied  by the activation of brain regions, it is necessary to combine a wide exploration of the various brain functions and some inversion of the statistical inference. Here we introduce a methodology for accumulating knowledge towards a bidirectional link between observed brain activity and the corresponding function. We rely on a large corpus of imaging studies and a predictive engine. Technically, the challenges are to find commonality between the studies without denaturing the richness of the corpus. The key elements that we contribute are labeling the tasks performed with a cognitive ontology, and modeling the long tail of rare paradigms in the corpus. To our knowledge, our approach is the first demonstration of predicting the cognitive content of completely new brain images. To that end, we propose a method that predicts the experimental paradigms across different studies.",
    "authors": [
      "Schwartz, Yannick",
      "Thirion, Bertrand",
      "Varoquaux, Gael"
    ]
  },
  {
    "id": "eae27d77ca20db309e056e3d2dcd7d69",
    "title": "Density estimation from unweighted k-nearest neighbor graphs: a roadmap",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/eae27d77ca20db309e056e3d2dcd7d69-Paper.pdf",
    "abstract": "Consider an unweighted k-nearest neighbor graph   on n points that have been sampled i.i.d. from some unknown density p on R^d. We prove how one can estimate the density p just from the unweighted adjacency matrix of the graph, without knowing the points themselves or their distance or similarity scores. The key insights are that local differences in link numbers can be used to estimate some local function of p, and that integrating this function along shortest paths leads to an estimate of the underlying density.",
    "authors": [
      "Von Luxburg, Ulrike",
      "Alamgir, Morteza"
    ]
  },
  {
    "id": "eb163727917cbba1eea208541a643e74",
    "title": "Actor-Critic Algorithms for Risk-Sensitive MDPs",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/eb163727917cbba1eea208541a643e74-Paper.pdf",
    "abstract": "In many sequential decision-making problems we may want to manage risk by minimizing some measure of variability in rewards in addition to maximizing a standard criterion. Variance related risk measures are among the most common risk-sensitive criteria in finance and operations research. However, optimizing many such criteria is known to be a hard problem. In this paper, we consider both discounted and average reward Markov decision processes. For each formulation, we first define a measure of variability for a policy, which in turn gives us a set of risk-sensitive criteria to optimize. For each of these criteria, we derive a formula for computing its gradient. We then devise actor-critic algorithms for estimating the gradient and updating the policy parameters in the ascent direction. We establish the convergence of our algorithms to locally risk-sensitive optimal policies. Finally, we demonstrate the usefulness of our algorithms in a traffic signal control application.",
    "authors": [
      "L.A., Prashanth",
      "Ghavamzadeh, Mohammad"
    ]
  },
  {
    "id": "eb6fdc36b281b7d5eabf33396c2683a2",
    "title": "Probabilistic Principal Geodesic Analysis",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/eb6fdc36b281b7d5eabf33396c2683a2-Paper.pdf",
    "abstract": "Principal geodesic analysis (PGA) is a generalization of principal component analysis (PCA) for dimensionality reduction of data on a Riemannian manifold. Currently PGA is defined as a geometric fit to the data, rather than as a probabilistic model. Inspired by probabilistic PCA, we present a latent variable model for PGA that provides a probabilistic framework for factor analysis on manifolds. To compute maximum likelihood estimates of the parameters in our model, we develop a Monte Carlo Expectation Maximization algorithm, where the expectation is approximated by Hamiltonian Monte Carlo sampling of the latent variables. We demonstrate the ability of our method to recover the ground truth parameters in simulated sphere data, as well as its effectiveness in analyzing shape variability of a corpus callosum data set from human brain images.",
    "authors": [
      "Zhang, Miaomiao",
      "Fletcher, Tom"
    ]
  },
  {
    "id": "eb86d510361fc23b59f18c1bc9802cc6",
    "title": "k-Prototype Learning for 3D Rigid Structures",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/eb86d510361fc23b59f18c1bc9802cc6-Paper.pdf",
    "abstract": "In this paper, we study the following new variant of prototype learning, called {\\em $k$-prototype learning problem for 3D rigid structures}: Given a set of 3D rigid structures, find a set of $k$ rigid structures so that each of them is a prototype for a cluster of the given rigid structures and the total cost (or dissimilarity) is minimized. Prototype learning is a core problem in machine learning and has a wide range of applications in many areas. Existing results on this problem have mainly focused on the graph domain. In this paper, we present the first algorithm for learning multiple prototypes from 3D rigid structures. Our result is based on a number of new insights to rigid structures alignment, clustering, and prototype reconstruction, and is practically efficient with quality guarantee. We validate our approach using two type of data sets, random data and biological data of chromosome territories. Experiments suggest that our approach can effectively learn prototypes in both types of data.",
    "authors": [
      "Ding, Hu",
      "Berezney, Ronald",
      "Xu, Jinhui"
    ]
  },
  {
    "id": "ebd9629fc3ae5e9f6611e2ee05a31cef",
    "title": "Learning Adaptive Value of Information for Structured Prediction",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/ebd9629fc3ae5e9f6611e2ee05a31cef-Paper.pdf",
    "abstract": "Discriminative methods for learning structured models have enabled wide-spread use of very rich feature representations.  However, the computational cost of feature extraction is prohibitive for large-scale or time-sensitive applications, often dominating the cost of inference in the models.  Significant efforts have been devoted to sparsity-based model selection to decrease this cost.  Such feature selection methods control computation statically and miss the opportunity to fine-tune feature extraction to each input at run-time.  We address the key challenge of learning to control fine-grained feature extraction adaptively, exploiting non-homogeneity of the data. We propose an architecture that uses a rich feedback loop between extraction and prediction. The run-time control policy is learned using efficient value-function approximation, which adaptively determines the value of information of features at the level of individual variables for each input.  We demonstrate significant speedups over state-of-the-art methods on two challenging datasets. For articulated pose estimation in video, we achieve a more accurate state-of-the-art model that is simultaneously 4$\\times$ faster while using only a small fraction of possible features, with similar results on an OCR task.",
    "authors": [
      "Weiss, David J.",
      "Taskar, Ben"
    ]
  },
  {
    "id": "eddb904a6db773755d2857aacadb1cb0",
    "title": "Spike train entropy-rate estimation using hierarchical Dirichlet process priors",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/eddb904a6db773755d2857aacadb1cb0-Paper.pdf",
    "abstract": "Entropy rate quantifies the amount of disorder in a stochastic process.  For spiking neurons, the entropy rate places an upper bound on the rate at which the spike train can convey stimulus information, and a large literature has focused on the problem of estimating entropy rate from spike train data.  Here we present Bayes Least Squares and Empirical Bayesian entropy rate estimators for binary spike trains using Hierarchical Dirichlet Process (HDP) priors.  Our estimator leverages the fact that the entropy rate of an ergodic Markov Chain with known transition probabilities can be calculated analytically, and many stochastic processes that are non-Markovian can still be well approximated by Markov processes of sufficient depth.  Choosing an appropriate depth of Markov model presents challenges due to possibly long time dependencies and short data sequences: a deeper model can better account for long time-dependencies, but is more difficult to infer from limited data. Our approach mitigates this difficulty by using a hierarchical prior to share statistical power across Markov chains of different depths.   We present both a fully Bayesian and empirical Bayes entropy rate estimator based on this model, and demonstrate their performance on simulated and real neural spike train data.",
    "authors": [
      "Knudson, Karin C.",
      "Pillow, Jonathan W."
    ]
  },
  {
    "id": "ef0d3930a7b6c95bd2b32ed45989c61f",
    "title": "Regularized M-estimators with nonconvexity: Statistical and algorithmic theory for local optima",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/ef0d3930a7b6c95bd2b32ed45989c61f-Paper.pdf",
    "abstract": "We establish theoretical results concerning all local optima of various regularized M-estimators, where both loss and penalty functions are allowed to be nonconvex. Our results show that as long as the loss function satisfies restricted strong convexity and the penalty function satisfies suitable regularity conditions, any local optimum of the composite objective function lies within statistical precision of the true parameter vector. Our theory covers a broad class of nonconvex objective functions, including corrected versions of the Lasso for errors-in-variables linear models; regression in generalized linear models using nonconvex regularizers such as SCAD and MCP; and graph and inverse covariance matrix estimation. On the optimization side, we show that a simple adaptation of composite gradient descent may be used to compute a global optimum up to the statistical precision epsilon in log(1/epsilon) iterations, which is the fastest possible rate of any first-order method. We provide a variety of simulations to illustrate the sharpness of our theoretical predictions.",
    "authors": [
      "Loh, Po-Ling",
      "Wainwright, Martin J."
    ]
  },
  {
    "id": "ef575e8837d065a1683c022d2077d342",
    "title": "Robust Data-Driven Dynamic Programming",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/ef575e8837d065a1683c022d2077d342-Paper.pdf",
    "abstract": "In stochastic optimal control the distribution of the exogenous noise is typically unknown and must be inferred from limited data before dynamic programming (DP)-based solution schemes can be applied. If the conditional expectations in the DP recursions are estimated via kernel regression, however, the historical sample paths enter the solution procedure directly as they determine the evaluation points of the cost-to-go functions. The resulting data-driven DP scheme is asymptotically consistent and admits efficient computational solution when combined with parametric value function approximations. If training data is sparse, however, the estimated cost-to-go functions display a high variability and an optimistic bias, while the corresponding control policies perform poorly in out-of-sample tests. To mitigate these small sample effects, we propose a robust data-driven DP scheme, which replaces the expectations in the DP recursions with worst-case expectations over a set of distributions close to the best estimate. We show that the arising min-max problems in the DP recursions reduce to tractable conic programs. We also demonstrate that this robust algorithm dominates state-of-the-art benchmark algorithms in out-of-sample tests across several application domains.",
    "authors": [
      "Hanasusanto, Grani Adiwena",
      "Kuhn, Daniel"
    ]
  },
  {
    "id": "f033ab37c30201f73f142449d037028d",
    "title": "Provable Subspace Clustering: When LRR meets SSC",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/f033ab37c30201f73f142449d037028d-Paper.pdf",
    "abstract": "Sparse Subspace Clustering (SSC) and Low-Rank Representation (LRR) are both considered as the state-of-the-art methods for {\\em subspace clustering}. The two methods are fundamentally similar in that both are convex optimizations exploiting the intuition of Self-Expressiveness''. The main difference is that SSC minimizes the vector $\\ell_1$ norm of the representation matrix to induce sparsity while LRR minimizes nuclear norm (aka trace norm) to promote a low-rank structure. Because the representation matrix is often simultaneously sparse and low-rank,  we propose a new algorithm, termed Low-Rank Sparse Subspace Clustering (LRSSC), by combining SSC and LRR, and develops theoretical guarantees of when the algorithm succeeds. The results reveal interesting insights into the strength and weakness of SSC and LRR and demonstrate how LRSSC can take the advantages of both methods in preserving the \"Self-Expressiveness Property'' and \"Graph Connectivity'' at the same time.\"",
    "authors": [
      "Wang, Yu-Xiang",
      "Xu, Huan",
      "Leng, Chenlei"
    ]
  },
  {
    "id": "f0dd4a99fba6075a9494772b58f95280",
    "title": "Optimization, Learning, and Games with Predictable Sequences",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/f0dd4a99fba6075a9494772b58f95280-Paper.pdf",
    "abstract": "We provide several applications of Optimistic Mirror Descent, an online learning algorithm based on the idea of predictable sequences. First, we recover the Mirror-Prox algorithm, prove an extension to Holder-smooth functions, and apply the results to saddle-point type problems. Second, we prove that a version of Optimistic Mirror Descent (which has a close relation to the Exponential Weights algorithm) can be used by two strongly-uncoupled players in a finite zero-sum matrix game to converge to the minimax equilibrium at the rate of O(log T / T). This addresses a question of Daskalakis et al, 2011. Further, we consider a partial information version of the problem. We then apply the results to approximate convex programming  and show a simple algorithm for the approximate Max-Flow problem.",
    "authors": [
      "Rakhlin, Sasha",
      "Sridharan, Karthik"
    ]
  },
  {
    "id": "f1b6f2857fb6d44dd73c7041e0aa0f19",
    "title": "Beyond Pairwise: Provably Fast Algorithms for Approximate $k$-Way  Similarity Search",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/f1b6f2857fb6d44dd73c7041e0aa0f19-Paper.pdf",
    "abstract": "We go beyond the notion of pairwise similarity and look into  search problems with $k$-way similarity functions. In this paper, we focus on problems related to  \\emph{3-way Jaccard} similarity: $\\mathcal{R}^{3way}= \\frac{|S_1 \\cap S_2 \\cap S_3|}{|S_1 \\cup S_2 \\cup S_3|}$, $S_1, S_2, S_3 \\in \\mathcal{C}$, where $\\mathcal{C}$ is a size $n$ collection of sets (or binary vectors).  We show that approximate $\\mathcal{R}^{3way}$ similarity search problems admit  fast algorithms with  provable guarantees, analogous to the pairwise case. Our analysis and speedup guarantees naturally extend to $k$-way resemblance. In the process, we extend traditional framework of \\emph{locality sensitive hashing (LSH)} to handle higher order similarities, which could be of independent theoretical interest. The applicability of $\\mathcal{R}^{3way}$ search is shown on the Google sets\" application. In addition, we demonstrate the advantage of $\\mathcal{R}^{3way}$ resemblance over the pairwise case in improving retrieval quality.\"",
    "authors": [
      "Shrivastava, Anshumali",
      "Li, Ping"
    ]
  },
  {
    "id": "f2201f5191c4e92cc5af043eebfd0946",
    "title": "Firing rate predictions in optimal balanced networks",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/f2201f5191c4e92cc5af043eebfd0946-Paper.pdf",
    "abstract": "How are firing rates in a spiking network related to neural input, connectivity and network function? This is an important problem because firing rates are one of the most important measures of network activity, in both the study of neural computation and neural network dynamics. However, it is a difficult problem, because the spiking mechanism of individual neurons is highly non-linear, and these individual neurons interact strongly through connectivity. We develop a new technique for calculating firing rates in optimal balanced networks. These are particularly interesting networks because they provide an optimal spike-based signal representation while producing cortex-like spiking activity through a dynamic balance of excitation and inhibition. We can calculate firing rates by treating balanced network dynamics as an algorithm for optimizing signal representation. We identify this algorithm and then calculate firing rates by finding the solution to the algorithm. Our firing rate calculation relates network firing rates directly to network input, connectivity and function. This allows us to explain the function and underlying mechanism of tuning curves in a variety of systems.",
    "authors": [
      "Barrett, David G.",
      "Den\u00e8ve, Sophie",
      "Machens, Christian K."
    ]
  },
  {
    "id": "f33ba15effa5c10e873bf3842afb46a6",
    "title": "Multi-Task Bayesian Optimization",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/f33ba15effa5c10e873bf3842afb46a6-Paper.pdf",
    "abstract": "Bayesian optimization has recently been proposed as a framework for automatically tuning the hyperparameters of machine learning models and has been shown to yield state-of-the-art performance with impressive ease and efficiency. In this paper, we explore whether it is possible to transfer the knowledge gained from previous optimizations to new tasks in order to find optimal hyperparameter settings more efficiently. Our approach is based on extending multi-task Gaussian processes to the framework of Bayesian optimization. We show that this method significantly speeds up the optimization process when compared to the standard single-task approach. We further propose a straightforward extension of our algorithm in order to jointly minimize the average error across multiple tasks and demonstrate how this can be used to greatly speed up $k$-fold cross-validation. Lastly, our most significant contribution is an adaptation of a recently proposed acquisition function, entropy search, to the cost-sensitive and multi-task settings. We demonstrate the utility of this new acquisition function by utilizing a small dataset in order to explore hyperparameter settings for a large dataset. Our algorithm dynamically chooses which dataset to query in order to yield the most information per unit cost.",
    "authors": [
      "Swersky, Kevin",
      "Snoek, Jasper",
      "Adams, Ryan P."
    ]
  },
  {
    "id": "f340f1b1f65b6df5b5e3f94d95b11daf",
    "title": "Using multiple samples to learn mixture models",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/f340f1b1f65b6df5b5e3f94d95b11daf-Paper.pdf",
    "abstract": "In the mixture models problem it is assumed that there are $K$ distributions $\\theta_{1},\\ldots,\\theta_{K}$ and one gets to observe a sample from a mixture of these distributions with unknown coefficients. The goal is to associate instances with their generating distributions, or to identify the parameters of the hidden distributions. In this work we make the assumption that we have access to several samples drawn from the same $K$ underlying distributions, but with different mixing weights. As with topic modeling, having multiple samples is often a reasonable assumption.  Instead of pooling the data into one sample, we prove that it is possible to use the differences between the samples to better recover the underlying structure. We present algorithms that recover the underlying structure under milder assumptions than the current state of art when either the dimensionality or the separation is high.  The methods, when applied to topic modeling, allow generalization to words not present in the training data.",
    "authors": [
      "Lee, Jason D.",
      "Gilad-Bachrach, Ran",
      "Caruana, Rich"
    ]
  },
  {
    "id": "f387624df552cea2f369918c5e1e12bc",
    "title": "First-order Decomposition Trees",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/f387624df552cea2f369918c5e1e12bc-Paper.pdf",
    "abstract": "Lifting attempts to speedup probabilistic inference by exploiting symmetries in the model. Exact lifted inference methods, like their propositional counterparts, work by recursively decomposing the model and the problem. In the propositional case, there exist formal structures, such as decomposition trees (dtrees), that represent such a decomposition and allow us to determine the complexity of inference a priori. However, there is currently no equivalent structure nor analogous complexity results for lifted inference. In this paper, we introduce FO-dtrees, which upgrade propositional dtrees to the first-order level. We show how these trees can characterize a lifted inference solution for a probabilistic logical model (in terms of a sequence of lifted operations), and make a theoretical analysis of the complexity of lifted inference in terms of the novel notion of lifted width for the tree.",
    "authors": [
      "Taghipour, Nima",
      "Davis, Jesse",
      "Blockeel, Hendrik"
    ]
  },
  {
    "id": "f4552671f8909587cf485ea990207f3b",
    "title": "Noise-Enhanced Associative Memories",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/f4552671f8909587cf485ea990207f3b-Paper.pdf",
    "abstract": "Recent advances in associative memory design through structured pattern sets and graph-based inference algorithms have allowed reliable learning and recall of an exponential number of patterns. Although these designs correct external errors in recall, they assume neurons that compute noiselessly, in contrast to the highly variable neurons in hippocampus and olfactory cortex.  Here we consider associative memories with noisy internal computations and analytically characterize performance. As long as the internal noise level is below a specified threshold, the error probability in the recall phase can be made exceedingly small. More surprisingly, we show that internal noise actually improves the performance of the recall phase. Computational experiments lend additional support to our theoretical analysis. This work suggests a functional benefit to noisy neurons in biological neuronal networks.",
    "authors": [
      "Karbasi, Amin",
      "Salavati, Amir Hesam",
      "Shokrollahi, Amin",
      "Varshney, Lav R."
    ]
  },
  {
    "id": "f4573fc71c731d5c362f0d7860945b88",
    "title": "Adaptive Submodular Maximization in Bandit Setting",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/f4573fc71c731d5c362f0d7860945b88-Paper.pdf",
    "abstract": "Maximization of submodular functions has wide applications in machine learning and artificial intelligence. Adaptive submodular maximization has been traditionally studied under the assumption that the model of the world, the expected gain of choosing an item given previously selected items and their states, is known. In this paper, we study the scenario where the expected gain is initially unknown and it is learned by interacting repeatedly with the optimized function. We propose an efficient algorithm for solving our problem and prove that its expected cumulative regret increases logarithmically with time. Our regret bound captures the inherent property of submodular maximization, earlier mistakes are more costly than later ones. We refer to our approach as Optimistic Adaptive Submodular Maximization (OASM) because it trades off exploration and exploitation based on the optimism in the face of uncertainty principle. We evaluate our method on a preference elicitation problem and show that non-trivial K-step policies can be learned from just a few hundred interactions with the problem.",
    "authors": [
      "Gabillon, Victor",
      "Kveton, Branislav",
      "Wen, Zheng",
      "Eriksson, Brian",
      "Muthukrishnan, S."
    ]
  },
  {
    "id": "f4be00279ee2e0a53eafdaa94a151e2c",
    "title": "Approximate inference in latent Gaussian-Markov models from continuous time observations",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/f4be00279ee2e0a53eafdaa94a151e2c-Paper.pdf",
    "abstract": "We propose an approximate inference algorithm for continuous time Gaussian-Markov process models with both discrete and continuous time likelihoods. We show that the continuous time limit of the expectation propagation algorithm exists and results in a hybrid fixed point iteration consisting of (1) expectation propagation updates for the discrete time terms and (2) variational updates for the continuous time term. We introduce corrections methods that improve on the marginals of the approximation. This approach extends the classical Kalman-Bucy smoothing procedure to non-Gaussian observations, enabling continuous-time inference in a variety of models, including spiking neuronal models (state-space models with point process observations) and box likelihood models. Experimental results on real and simulated data demonstrate high distributional accuracy and significant computational savings compared to discrete-time approaches in a neural application.",
    "authors": [
      "Cseke, Botond",
      "Opper, Manfred",
      "Sanguinetti, Guido"
    ]
  },
  {
    "id": "f5deaeeae1538fb6c45901d524ee2f98",
    "title": "Lexical and Hierarchical Topic Regression",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/f5deaeeae1538fb6c45901d524ee2f98-Paper.pdf",
    "abstract": "Inspired by a two-level theory that unifies agenda setting and ideological  framing, we propose supervised hierarchical latent Dirichlet allocation  (SHLDA) which jointly captures documents' multi-level topic structure and  their polar response variables. Our model extends the nested Chinese restaurant  process to discover a tree-structured topic hierarchy and uses both per-topic  hierarchical and per-word lexical regression parameters to model the response  variables. Experiments in a political domain and on sentiment analysis tasks  show that SHLDA improves predictive accuracy while adding a new dimension of  insight into how topics under discussion are framed.",
    "authors": [
      "Nguyen, Viet-An",
      "Ying, Jordan L.",
      "Resnik, Philip"
    ]
  },
  {
    "id": "f64eac11f2cd8f0efa196f8ad173178e",
    "title": "Adaptive Step-Size for Policy Gradient Methods",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/f64eac11f2cd8f0efa196f8ad173178e-Paper.pdf",
    "abstract": "In the last decade, policy gradient methods have significantly grown in popularity in the reinforcement--learning field. In particular, they have been largely employed in motor control and robotic applications, thanks to their ability to cope with continuous state and action domains and partial observable problems. Policy gradient researches have been mainly focused on the identification of effective gradient directions and the proposal of efficient estimation algorithms. Nonetheless, the performance of policy gradient methods is determined not only by the gradient direction, since convergence properties are strongly influenced by the choice of the step size: small values imply slow convergence rate, while large values may lead to oscillations or even divergence of the policy parameters. Step--size value is usually chosen by hand tuning and still little attention has been paid to its automatic selection. In this paper, we propose to determine the learning rate by maximizing a lower bound to the expected performance gain. Focusing on Gaussian policies, we derive a lower bound that is second--order polynomial of the step size, and we show how a simplified version of such lower bound can be maximized when the gradient is estimated from trajectory samples. The properties of the proposed approach are empirically evaluated in a linear--quadratic regulator problem.",
    "authors": [
      "Pirotta, Matteo",
      "Restelli, Marcello",
      "Bascetta, Luca"
    ]
  },
  {
    "id": "f73b76ce8949fe29bf2a537cfa420e8f",
    "title": "Mixed Optimization for Smooth Functions",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/f73b76ce8949fe29bf2a537cfa420e8f-Paper.pdf",
    "abstract": "It is well known that the optimal convergence rate for stochastic optimization of smooth functions is $[O(1/\\sqrt{T})]$, which is same as stochastic optimization of Lipschitz continuous convex functions. This is in contrast to optimizing smooth functions using full gradients, which yields a convergence rate of $[O(1/T^2)]$. In this work, we consider a new setup for optimizing smooth functions, termed as {\\bf Mixed Optimization}, which allows to access  both a stochastic oracle  and a full gradient oracle. Our goal is to significantly improve the convergence rate of stochastic optimization of smooth functions by having an additional small number of accesses to the full gradient oracle. We show that, with an $[O(\\ln T)]$ calls to the full gradient oracle  and an $O(T)$ calls to the stochastic oracle, the proposed mixed optimization algorithm is able to achieve an optimization error of $[O(1/T)]$.",
    "authors": [
      "Mahdavi, Mehrdad",
      "Zhang, Lijun",
      "Jin, Rong"
    ]
  },
  {
    "id": "f7cade80b7cc92b991cf4d2806d6bd78",
    "title": "Deep Neural Networks for Object Detection",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/f7cade80b7cc92b991cf4d2806d6bd78-Paper.pdf",
    "abstract": "Deep Neural Networks (DNNs) have recently shown outstanding performance on the task of whole image classification. In this paper we go one step further and address the problem of object detection -- not only classifying but also precisely localizing objects of various classes using DNNs. We present a simple and yet powerful formulation of object detection as a regression to object masks. We define a multi-scale inference procedure which is able to produce a high-resolution object detection at a low cost by a few network applications. The approach achieves state-of-the-art performance on Pascal 2007 VOC.",
    "authors": [
      "Szegedy, Christian",
      "Toshev, Alexander",
      "Erhan, Dumitru"
    ]
  },
  {
    "id": "f7e6c85504ce6e82442c770f7c8606f0",
    "title": "A simple example of Dirichlet process mixture inconsistency for the number of components",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/f7e6c85504ce6e82442c770f7c8606f0-Paper.pdf",
    "abstract": "For data assumed to come from a finite mixture with an unknown number of components, it has become common to use Dirichlet process mixtures (DPMs) not only for density estimation, but also for inferences about the number of components. The typical approach is to use the posterior distribution on the number of components occurring so far --- that is, the posterior on the number of clusters in the observed data. However, it turns out that this posterior is not consistent --- it does not converge to the true number of components. In this note, we give an elementary demonstration of this inconsistency in what is perhaps the simplest possible setting: a DPM with normal components of unit variance, applied to data from a mixture\" with one standard normal component. Further, we find that this example exhibits severe inconsistency: instead of going to 1, the posterior probability that there is one cluster goes to 0.\"",
    "authors": [
      "Miller, Jeffrey W.",
      "Harrison, Matthew T."
    ]
  },
  {
    "id": "f91e24dfe80012e2a7984afa4480a6d6",
    "title": "Learning Kernels Using Local Rademacher Complexity",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/f91e24dfe80012e2a7984afa4480a6d6-Paper.pdf",
    "abstract": "We use the notion of local Rademacher complexity to design new algorithms for learning kernels. Our algorithms thereby benefit from the sharper learning bounds based on that notion which, under certain general conditions, guarantee a faster convergence rate.  We devise two new learning kernel algorithms: one based on a convex optimization problem for which we give an efficient solution using existing learning kernel techniques, and another one that can be formulated as a DC-programming problem for which we describe a solution in detail. We also report the results of experiments with both algorithms in both binary and multi-class classification tasks.",
    "authors": [
      "Cortes, Corinna",
      "Kloft, Marius",
      "Mohri, Mehryar"
    ]
  },
  {
    "id": "f9a40a4780f5e1306c46f1c8daecee3b",
    "title": "Bayesian entropy estimation for binary spike train data using parametric prior knowledge",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/f9a40a4780f5e1306c46f1c8daecee3b-Paper.pdf",
    "abstract": "Shannon's entropy is a basic quantity in information theory, and a  fundamental building block for the analysis of neural codes.   Estimating the entropy of a discrete distribution from samples is an important and difficult problem that has received considerable   attention in statistics and theoretical neuroscience.  However,  neural responses have characteristic statistical structure that   generic entropy estimators fail to exploit.  For example, existing  Bayesian entropy estimators make the naive assumption that all spike   words are equally likely a priori, which makes for an  inefficient allocation of prior probability mass in cases where   spikes are sparse.  Here we develop Bayesian estimators for the  entropy of binary spike trains using priors designed to flexibly   exploit the statistical structure of simultaneously-recorded spike  responses.  We define two prior distributions over spike words using   mixtures of Dirichlet distributions centered on simple parametric  models.  The parametric model captures high-level statistical   features of the data, such as the average spike count in a spike  word, which allows the posterior over entropy to concentrate more   rapidly than with standard estimators (e.g., in cases where the  probability of spiking differs strongly from 0.5). Conversely, the   Dirichlet distributions assign prior mass to distributions far from  the parametric model, ensuring consistent estimates for arbitrary   distributions.  We devise a compact representation of the data and  prior that allow for computationally efficient implementations of   Bayesian least squares and empirical Bayes entropy estimators with  large numbers of neurons.  We apply these estimators to simulated   and real neural data and show that they substantially outperform  traditional methods.",
    "authors": [
      "Archer, Evan W.",
      "Park, Il Memming",
      "Pillow, Jonathan W."
    ]
  },
  {
    "id": "f9b902fc3289af4dd08de5d1de54f68f",
    "title": "Mid-level Visual Element Discovery as Discriminative Mode Seeking",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/f9b902fc3289af4dd08de5d1de54f68f-Paper.pdf",
    "abstract": "Recent work on mid-level visual representations aims to capture information at the level of complexity higher than typical visual words\", but lower than full-blown semantic objects. Several approaches have been proposed to discover mid-level visual elements, that are both 1) representative, i.e. frequently occurring within a visual dataset, and 2) visually discriminative. However, the current approaches are rather ad hoc and difficult to analyze and evaluate. In this work, we pose visual element discovery as discriminative mode seeking, drawing connections to the the well-known and well-studied mean-shift algorithm. Given a weakly-labeled image collection, our method discovers visually-coherent patch clusters that are maximally discriminative with respect to the labels. One advantage of our formulation is that it requires only a single pass through the data. We also propose the Purity-Coverage plot as a principled way of experimentally analyzing and evaluating different visual discovery approaches, and compare our method against prior work on the Paris Street View dataset. We also evaluate our method on the task of scene classification, demonstrating state-of-the-art performance on the MIT Scene-67 dataset.\"",
    "authors": [
      "Doersch, Carl",
      "Gupta, Abhinav",
      "Efros, Alexei A."
    ]
  },
  {
    "id": "fa14d4fe2f19414de3ebd9f63d5c0169",
    "title": "Approximate Bayesian Image Interpretation using Generative Probabilistic Graphics Programs",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/fa14d4fe2f19414de3ebd9f63d5c0169-Paper.pdf",
    "abstract": "The idea of computer vision as the Bayesian inverse problem to computer graphics has a long history and an appealing elegance, but it has proved difficult to directly implement. Instead, most vision tasks are approached via complex bottom-up processing pipelines. Here we show that it is possible to write short, simple probabilistic graphics programs that define flexible generative models and to automatically invert them to interpret real-world images. Generative probabilistic graphics programs consist of a stochastic scene generator, a renderer based on graphics software, a stochastic likelihood model linking the renderer's output and the data, and latent variables that adjust the fidelity of the renderer and the tolerance of the likelihood model. Representations and algorithms from computer graphics, originally designed to produce high-quality images, are instead used as the deterministic backbone for highly approximate and stochastic generative models. This formulation combines probabilistic programming, computer graphics, and approximate Bayesian computation, and depends only on general-purpose, automatic inference techniques. We describe two applications: reading sequences of degraded and adversarially obscured alphanumeric characters, and inferring 3D road models from vehicle-mounted camera images. Each of the probabilistic graphics programs we present relies on under 20 lines of probabilistic code, and supports accurate, approximately Bayesian inferences about ambiguous real-world images.",
    "authors": [
      "Mansinghka, Vikash K.",
      "Kulkarni, Tejas D.",
      "Perov, Yura N.",
      "Tenenbaum, Josh"
    ]
  },
  {
    "id": "fb60d411a5c5b72b2e7d3527cfc84fd0",
    "title": "Annealing between distributions by averaging moments",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/fb60d411a5c5b72b2e7d3527cfc84fd0-Paper.pdf",
    "abstract": "Many powerful Monte Carlo techniques for estimating partition functions, such as annealed importance sampling (AIS), are based on sampling from a sequence of intermediate distributions which interpolate between a tractable initial distribution and an intractable target distribution. The near-universal practice is to use geometric averages of the initial and target distributions, but alternative paths can perform substantially better. We present a novel sequence of intermediate distributions for exponential families: averaging the moments of the initial and target distributions. We derive an asymptotically optimal piecewise linear schedule for the moments path and show that it performs at least as well as geometric averages with a linear schedule.  Moment averaging performs well empirically at estimating partition functions of restricted Boltzmann machines (RBMs), which form the building blocks of many deep learning models, including Deep Belief Networks and Deep Boltzmann Machines.",
    "authors": [
      "Grosse, Roger B.",
      "Maddison, Chris J.",
      "Salakhutdinov, Russ R."
    ]
  },
  {
    "id": "fb7b9ffa5462084c5f4e7e85a093e6d7",
    "title": "Blind Calibration in Compressed Sensing using Message Passing Algorithms",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/fb7b9ffa5462084c5f4e7e85a093e6d7-Paper.pdf",
    "abstract": "Compressed sensing (CS) is a concept that allows to acquire compressible signals with a small number of measurements. As such, it is very attractive for hardware implementations. Therefore, correct calibration of the hardware is a central issue. In this paper we study the so-called blind calibration, i.e. when the training signals that are available to perform the calibration are sparse but unknown. We extend the approximate message passing (AMP) algorithm used in CS to the case of blind calibration. In the calibration-AMP, both the gains on the sensors and the elements of the signals are treated as unknowns. Our algorithm is also applicable to settings in which the sensors distort the measurements in other ways than multiplication by a gain, unlike previously suggested blind calibration algorithms based on convex relaxations. We study numerically the phase diagram of the blind calibration problem, and show that even in cases where convex relaxation is possible, our algorithm requires a smaller number of measurements and/or signals in order to perform well.",
    "authors": [
      "Schulke, Christophe",
      "Caltagirone, Francesco",
      "Krzakala, Florent",
      "Zdeborov\u00e1, Lenka"
    ]
  },
  {
    "id": "fb89705ae6d743bf1e848c206e16a1d7",
    "title": "Active Learning for Probabilistic Hypotheses Using the Maximum Gibbs Error Criterion",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/fb89705ae6d743bf1e848c206e16a1d7-Paper.pdf",
    "abstract": "We introduce a new objective function for pool-based Bayesian active learning with probabilistic hypotheses. This objective function, called the policy Gibbs error, is the expected error rate of a random classifier drawn from the prior distribution on the examples adaptively selected by the active learning policy. Exact maximization of the policy Gibbs error is hard, so we propose a greedy strategy that maximizes the Gibbs error at each iteration, where the Gibbs error on an instance is the expected error of a random classifier selected from the posterior label distribution on that instance. We apply this maximum Gibbs error criterion to three active learning scenarios: non-adaptive, adaptive, and batch active learning. In each scenario, we prove that the criterion achieves near-maximal policy Gibbs error when constrained to a fixed budget. For practical implementations, we provide approximations to the maximum Gibbs error criterion for Bayesian conditional random fields and transductive Naive Bayes. Our experimental results on a named entity recognition task and a text classification task show that the maximum Gibbs error criterion is an effective active learning criterion for noisy models.",
    "authors": [
      "Cuong, Nguyen Viet",
      "Lee, Wee Sun",
      "Ye, Nan",
      "Chai, Kian Ming A.",
      "Chieu, Hai Leong"
    ]
  },
  {
    "id": "fc2c7c47b918d0c2d792a719dfb602ef",
    "title": "Two-Target Algorithms  for Infinite-Armed   Bandits with Bernoulli Rewards",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/fc2c7c47b918d0c2d792a719dfb602ef-Paper.pdf",
    "abstract": "We consider an infinite-armed bandit problem with Bernoulli rewards. The mean rewards are independent,   uniformly distributed over $[0,1]$. Rewards 0 and  1 are referred to as a success and a failure, respectively. We propose a novel algorithm where the decision to exploit  any arm is based on two successive  targets, namely, the total number of successes until the first failure and the first $m$ failures, respectively, where $m$ is a fixed parameter. This two-target algorithm  achieves a long-term average regret in $\\sqrt{2n}$ for  a large parameter $m$ and a known time horizon $n$. This regret is optimal and  strictly less than the regret  achieved by the best known algorithms, which is in $2\\sqrt{n}$. The results are extended to any mean-reward distribution whose support contains 1 and to unknown  time horizons. Numerical experiments show the performance of the algorithm for finite time horizons.",
    "authors": [
      "Bonald, Thomas",
      "Proutiere, Alexandre"
    ]
  },
  {
    "id": "fc8001f834f6a5f0561080d134d53d29",
    "title": "Learning to Prune in Metric and Non-Metric Spaces",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/fc8001f834f6a5f0561080d134d53d29-Paper.pdf",
    "abstract": "Our focus is on approximate nearest neighbor retrieval in metric and non-metric spaces. We employ a VP-tree and explore two simple yet effective learning-to prune approaches: density estimation through sampling and \u201cstretching\u201d of the triangle inequality. Both methods are evaluated using data sets with metric (Euclidean) and non-metric (KL-divergence and Itakura-Saito) distance functions. Conditions on spaces where the VP-tree is applicable are discussed. The VP-tree with a learned pruner is compared against the recently proposed state-of-the-art approaches: the bbtree, the multi-probe locality sensitive hashing (LSH), and permutation methods. Our method was competitive against state-of-the-art methods and, in most cases, was more efficient for the same rank approximation quality.",
    "authors": [
      "Boytsov, Leonid",
      "Naidan, Bilegsaikhan"
    ]
  },
  {
    "id": "fd5c905bcd8c3348ad1b35d7231ee2b1",
    "title": "Learning from Limited Demonstrations",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/fd5c905bcd8c3348ad1b35d7231ee2b1-Paper.pdf",
    "abstract": "We propose an approach to learning from demonstration (LfD) which leverages expert data, even if the expert examples are very few or inaccurate.  We achieve this by integrating LfD in an approximate policy iteration algorithm.  The key idea of our approach is that expert examples are used to generate linear constraints on the optimization, in a similar fashion to large-margin classification.  We prove an upper bound on the true Bellman error of the approximation computed by the algorithm at each iteration.  We show empirically that the algorithm outperforms both pure policy iteration, as well as DAgger (a state-of-art LfD algorithm) and supervised learning in a variety of scenarios, including when very few and/or imperfect demonstrations are available.  Our experiments include simulations as well as a real robotic navigation task.",
    "authors": [
      "Kim, Beomjoon",
      "Farahmand, Amir-massoud",
      "Pineau, Joelle",
      "Precup, Doina"
    ]
  },
  {
    "id": "fe709c654eac84d5239d1a12a4f71877",
    "title": "Aggregating Optimistic Planning Trees for Solving Markov Decision Processes",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/fe709c654eac84d5239d1a12a4f71877-Paper.pdf",
    "abstract": "This paper addresses the problem of online planning in Markov Decision Processes using only a generative model. We propose a new algorithm which is based on the construction of a forest of single successor state planning trees. For every explored state-action, such a tree contains exactly one successor state, drawn from the generative model. The trees are built using a planning algorithm which follows the optimism in the face of uncertainty principle, in assuming the most favorable outcome in the absence of further information. In the decision making step of the algorithm, the individual trees are combined. We discuss the approach, prove that our proposed algorithm is consistent, and empirically show that it performs better than a related algorithm which additionally assumes the knowledge of all transition distributions.",
    "authors": [
      "Kedenburg, Gunnar",
      "Fonteneau, Raphael",
      "Munos, Remi"
    ]
  },
  {
    "id": "fec8d47d412bcbeece3d9128ae855a7a",
    "title": "Action from Still Image Dataset and Inverse Optimal Control to Learn Task Specific Visual Scanpaths",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/fec8d47d412bcbeece3d9128ae855a7a-Paper.pdf",
    "abstract": "Human eye movements provide a rich source of information into the human visual processing. The complex interplay between the task and the visual stimulus is believed to determine human eye movements, yet it is not fully understood. This has precluded the development of reliable dynamic eye movement prediction systems. Our work makes three contributions towards addressing this problem. First, we complement one of the largest and most challenging static computer vision datasets, VOC 2012 Actions, with human eye movement annotations collected under the task constraints of action and context recognition. Our dataset is unique among eyetracking datasets for still images in terms of its large scale (over 1 million fixations, 9157 images), task control and action from a single image emphasis. Second, we introduce models to automatically discover areas of interest (AOI) and introduce novel dynamic consistency metrics, based on them. Our method can automatically determine the number and spatial support of the AOIs, in addition to their locations. Based on such encodings, we show that, on unconstrained read-world stimuli, task instructions have significant influence on visual behavior. Finally, we leverage our large scale dataset in conjunction with powerful machine learning techniques and computer vision features, to introduce novel dynamic eye movement prediction methods which learn task-sensitive reward functions from eye movement data and efficiently integrate these rewards to plan future saccades based on inverse optimal control. We show that the propose methodology achieves state of the art scanpath modeling results.",
    "authors": [
      "Mathe, Stefan",
      "Sminchisescu, Cristian"
    ]
  },
  {
    "id": "ffeed84c7cb1ae7bf4ec4bd78275bb98",
    "title": "How to Hedge an Option Against an Adversary: Black-Scholes Pricing is Minimax Optimal",
    "year": "2013",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2013/file/ffeed84c7cb1ae7bf4ec4bd78275bb98-Paper.pdf",
    "abstract": "We consider a popular problem in finance, option pricing, through the lens of an online learning game between Nature and an Investor.  In the Black-Scholes option pricing model from 1973, the Investor can continuously hedge the risk of an option by trading the underlying asset, assuming that the asset's price fluctuates according to Geometric Brownian Motion (GBM). We consider a worst-case model, in which Nature chooses a sequence of price fluctuations under a cumulative quadratic volatility constraint, and the Investor can make a sequence of hedging decisions. Our main result is to show that the value of our proposed game, which is the regret'' of hedging strategy, converges to the Black-Scholes option price. We use significantly weaker assumptions than previous work---for instance, we allow large jumps in the asset price---and show that the Black-Scholes hedging strategy is near-optimal for the Investor even in this non-stochastic framework.\"",
    "authors": [
      "Abernethy, Jacob",
      "Bartlett, Peter L.",
      "Frongillo, Rafael",
      "Wibisono, Andre"
    ]
  },
  {
    "id": "00ec53c4682d36f5c4359f4ae7bd7ba1",
    "title": "Two-Stream Convolutional Networks for Action Recognition in Videos",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/00ec53c4682d36f5c4359f4ae7bd7ba1-Paper.pdf",
    "abstract": "We investigate architectures of discriminatively trained deep Convolutional Networks (ConvNets) for action recognition in video. The challenge is to capture the complementary information on appearance from still frames and motion between frames. We also aim to generalise the best performing hand-crafted features within a data-driven learning framework. Our contribution is three-fold. First, we propose a two-stream ConvNet architecture which incorporates spatial and temporal networks. Second, we demonstrate that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data. Finally, we show that multi-task learning, applied to two different action classification datasets, can be used to increase the amount of training data and improve the performance on both. Our architecture is trained and evaluated on the standard video actions benchmarks of UCF-101 and HMDB-51, where it is competitive with the state of the art. It also exceeds by a large margin previous attempts to use deep nets for video classification.",
    "authors": [
      "Simonyan, Karen",
      "Zisserman, Andrew"
    ]
  },
  {
    "id": "01f78be6f7cad02658508fe4616098a9",
    "title": "Exploiting easy data in online optimization",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/01f78be6f7cad02658508fe4616098a9-Paper.pdf",
    "abstract": "We consider the problem of online optimization, where a learner chooses a decision from a given decision set and suffers some loss associated with the decision and the state of the environment. The learner's objective is to minimize its cumulative regret against the best fixed decision in hindsight. Over the past few decades numerous variants have been considered, with many algorithms designed to achieve sub-linear regret in the worst case. However, this level of robustness comes at a cost. Proposed algorithms are often over-conservative, failing to adapt to the actual complexity of the loss sequence which is often far from the worst case. In this paper we introduce a general algorithm that, provided with a safe learning algorithm and an opportunistic benchmark, can effectively combine good worst-case guarantees with much improved performance on easy data. We derive general theoretical bounds on the regret of the proposed algorithm and discuss its implementation in a wide range of applications, notably in the problem of learning with shifting experts (a recent COLT open problem). Finally, we provide numerical simulations in the setting of prediction with expert advice with comparisons to the state of the art.",
    "authors": [
      "Sani, Amir",
      "Neu, Gergely",
      "Lazaric, Alessandro"
    ]
  },
  {
    "id": "021bbc7ee20b71134d53e20206bd6feb",
    "title": "Optimal prior-dependent neural population codes under shared input noise",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/021bbc7ee20b71134d53e20206bd6feb-Paper.pdf",
    "abstract": "The brain uses population codes to form distributed, noise-tolerant representations of sensory and motor variables. Recent work has examined the theoretical optimality of such codes in order to gain insight into the principles governing population codes found in the brain. However, the majority of the population coding literature considers either conditionally independent neurons or neurons with noise governed by a stimulus-independent covariance matrix. Here we analyze population coding under a simple alternative model in which latent input noise\" corrupts the stimulus before it is encoded by the population. This provides a convenient and tractable description for irreducible uncertainty that cannot be overcome by adding neurons, and induces stimulus-dependent correlations that mimic certain aspects of the correlations observed in real populations. We examine prior-dependent, Bayesian optimal coding in such populations using exact analyses of cases in which the posterior is approximately Gaussian. These analyses extend previous results on independent Poisson population codes and yield an analytic expression for squared loss and a tight upper bound for mutual information. We show that, for homogeneous populations that tile the input domain, optimal tuning curve width depends on the prior, the loss function, the resource constraint, and the amount of input noise. This framework provides a practical testbed for examining issues of optimality, noise, correlation, and coding fidelity in realistic neural populations.\"",
    "authors": [
      "Grabska-Barwinska, Agnieszka",
      "Pillow, Jonathan W."
    ]
  },
  {
    "id": "02522a2b2726fb0a03bb19f2d8d9524d",
    "title": "Quantized Kernel Learning for Feature Matching",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/02522a2b2726fb0a03bb19f2d8d9524d-Paper.pdf",
    "abstract": "Matching local visual features is a crucial problem in computer vision and its accuracy greatly depends on the choice of similarity measure. As it is generally very difficult to design by hand a similarity or a kernel perfectly adapted to the data of interest, learning it automatically with as few assumptions as possible is preferable. However, available techniques for kernel learning suffer from several limitations, such as restrictive parametrization or scalability. In this paper, we introduce a simple and flexible family of non-linear kernels which we refer to as Quantized Kernels (QK). QKs are arbitrary kernels in the index space of a data quantizer, i.e., piecewise constant similarities in the original feature space. Quantization allows to compress features and keep the learning tractable. As a result, we obtain state-of-the-art matching performance on a standard benchmark dataset with just a few bits to represent each feature dimension. QKs also have explicit non-linear, low-dimensional feature mappings that grant access to Euclidean geometry for uncompressed features.",
    "authors": [
      "Qin, Danfeng",
      "Chen, Xuanli",
      "Guillaumin, Matthieu",
      "Gool, Luc V."
    ]
  },
  {
    "id": "03f544613917945245041ea1581df0c2",
    "title": "QUIC & DIRTY: A Quadratic Approximation Approach for Dirty Statistical Models",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/03f544613917945245041ea1581df0c2-Paper.pdf",
    "abstract": "In this paper, we develop a family of algorithms for optimizing superposition-structured\u201d or \u201cdirty\u201d statistical estimators for high-dimensional problems involving the minimization of the sum of a smooth loss function with a hybrid regularization. Most of the current approaches are first-order methods, including proximal gradient or Alternating Direction Method of Multipliers (ADMM). We propose a new family of second-order methods where we approximate the loss function using quadratic approximation. The superposition structured regularizer then leads to a subproblem that can be efficiently solved by alternating minimization. We propose a general active subspace selection approach to speed up the solver by utilizing the low-dimensional structure given by the regularizers, and provide convergence guarantees for our algorithm. Empirically, we show that our approach is more than 10 times faster than state-of-the-art first-order approaches for the latent variable graphical model selection problems and multi-task learning problems when there is more than one regularizer. For these problems, our approach appears to be the first algorithm that can extend active subspace ideas to multiple regularizers.\"",
    "authors": [
      "Hsieh, Cho-Jui",
      "Dhillon, Inderjit S.",
      "Ravikumar, Pradeep K.",
      "Becker, Stephen",
      "Olsen, Peder A."
    ]
  },
  {
    "id": "069059b7ef840f0c74a814ec9237b6ec",
    "title": "On a Theory of Nonparametric Pairwise Similarity for Clustering: Connecting Clustering to Classification",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/069059b7ef840f0c74a814ec9237b6ec-Paper.pdf",
    "abstract": "Pairwise clustering methods partition the data space into clusters by the pairwise similarity between data points. The success of pairwise clustering largely depends on the pairwise similarity function defined over the data points, where kernel similarity is broadly used. In this paper, we present a novel pairwise clustering framework by bridging the gap between clustering and multi-class classification. This pairwise clustering framework learns an unsupervised nonparametric classifier from each data partition, and search for the optimal partition of the data by minimizing the generalization error of the learned classifiers associated with the data partitions. We consider two nonparametric classifiers in this framework, i.e. the nearest neighbor classifier and the plug-in classifier. Modeling the underlying data distribution by nonparametric kernel density estimation, the generalization error bounds for both unsupervised nonparametric classifiers are the sum of nonparametric pairwise similarity terms between the data points for the purpose of clustering. Under uniform distribution, the nonparametric similarity terms induced by both unsupervised classifiers exhibit a well known form of kernel similarity. We also prove that the generalization error bound for the unsupervised plug-in classifier is asymptotically equal to the weighted volume of cluster boundary for Low Density Separation, a widely used criteria for semi-supervised learning and clustering. Based on the derived nonparametric pairwise similarity using the plug-in classifier, we propose a new nonparametric exemplar-based clustering method with enhanced discriminative capability, whose superiority is evidenced by the experimental results.",
    "authors": [
      "Yang, Yingzhen",
      "Liang, Feng",
      "Yan, Shuicheng",
      "Wang, Zhangyang",
      "Huang, Thomas S."
    ]
  },
  {
    "id": "069d3bb002acd8d7dd095917f9efe4cb",
    "title": "Predictive Entropy Search for Efficient Global Optimization of Black-box Functions",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/069d3bb002acd8d7dd095917f9efe4cb-Paper.pdf",
    "abstract": "We propose a novel information-theoretic approach for Bayesian optimization called Predictive Entropy Search (PES). At each iteration, PES selects the next evaluation point that maximizes the expected information gained with respect to the global maximum. PES codifies this intractable acquisition function in terms of the expected reduction in the differential entropy of the predictive distribution. This reformulation allows PES to obtain approximations that are both more accurate and efficient than other alternatives such as Entropy Search (ES). Furthermore, PES can easily perform a fully Bayesian treatment of the model hyperparameters while ES cannot. We evaluate PES in both synthetic and real-world applications, including optimization problems in machine learning, finance, biotechnology, and robotics. We show that the increased accuracy of PES leads to significant gains in optimization performance.",
    "authors": [
      "Hern\u00e1ndez-Lobato, Jos\u00e9 Miguel",
      "Hoffman, Matthew W.",
      "Ghahramani, Zoubin"
    ]
  },
  {
    "id": "07563a3fe3bbe7e3ba84431ad9d055af",
    "title": "Discriminative Unsupervised Feature Learning with Convolutional Neural Networks",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/07563a3fe3bbe7e3ba84431ad9d055af-Paper.pdf",
    "abstract": "Current methods for training convolutional neural networks depend on large amounts of labeled samples for supervised training. In this paper we present an approach for training a convolutional neural network using only unlabeled data. We train the network to discriminate between a set of surrogate classes. Each surrogate class is formed by applying a variety of transformations to a randomly sampled 'seed' image patch. We find that this simple feature learning algorithm is surprisingly successful when applied to visual object recognition. The feature representation learned by our algorithm achieves classification results matching or outperforming the current state-of-the-art for unsupervised learning on several popular datasets (STL-10, CIFAR-10, Caltech-101).",
    "authors": [
      "Dosovitskiy, Alexey",
      "Springenberg, Jost Tobias",
      "Riedmiller, Martin",
      "Brox, Thomas"
    ]
  },
  {
    "id": "076a0c97d09cf1a0ec3e19c7f2529f2b",
    "title": "Expectation Backpropagation: Parameter-Free Training of Multilayer Neural Networks with Continuous or Discrete Weights",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/076a0c97d09cf1a0ec3e19c7f2529f2b-Paper.pdf",
    "abstract": "Multilayer Neural Networks (MNNs) are commonly trained using gradient descent-based methods, such as BackPropagation (BP). Inference in probabilistic graphical models is often done using variational Bayes methods, such as Expectation Propagation (EP). We show how an EP based approach can also be used to train deterministic MNNs. Specifically, we approximate the posterior of the weights given the data using a \u201cmean-field\u201d factorized distribution, in an online setting. Using online EP and the central limit theorem we find an analytical approximation to the Bayes update of this posterior, as well as the resulting Bayes estimates of the weights and outputs. Despite a different origin, the resulting algorithm, Expectation BackPropagation (EBP), is very similar to BP in form and efficiency. However, it has several additional advantages: (1) Training is parameter-free, given initial conditions (prior) and the MNN architecture. This is useful for large-scale problems, where parameter tuning is a major challenge. (2) The weights can be restricted to have discrete values. This is especially useful for implementing trained MNNs in precision limited hardware chips, thus improving their speed and energy efficiency by several orders of magnitude. We test the EBP algorithm numerically in eight binary text classification tasks. In all tasks, EBP outperforms: (1) standard BP with the optimal constant learning rate (2) previously reported state of the art. Interestingly, EBP-trained MNNs with binary weights usually perform better than MNNs with continuous (real) weights - if we average the MNN output using the inferred posterior.",
    "authors": [
      "Soudry, Daniel",
      "Hubara, Itay",
      "Meir, Ron"
    ]
  },
  {
    "id": "0771fc6f0f4b1d7d1bb73bbbe14e0e31",
    "title": "Compressive Sensing of Signals from a GMM with Sparse Precision Matrices",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/0771fc6f0f4b1d7d1bb73bbbe14e0e31-Paper.pdf",
    "abstract": "This paper is concerned with compressive sensing of signals drawn from a Gaussian mixture model (GMM) with sparse precision matrices. Previous work has shown: (i) a signal drawn from a given GMM can be perfectly reconstructed from r noise-free measurements if the (dominant) rank of each covariance matrix is less than r; (ii) a sparse Gaussian graphical model can be efficiently estimated from fully-observed training signals using graphical lasso. This paper addresses a problem more challenging than both (i) and (ii), by assuming that the GMM is unknown and each signal is only partially observed through incomplete linear measurements. Under these challenging assumptions, we develop a hierarchical Bayesian method to simultaneously estimate the GMM and recover the signals using solely the incomplete measurements and a Bayesian shrinkage prior that promotes sparsity of the Gaussian precision matrices. In addition, we provide theoretical performance bounds to relate the reconstruction error to the number of signals for which measurements are available, the sparsity level of precision matrices, and the \u201cincompleteness\u201d of measurements. The proposed method is demonstrated extensively on compressive sensing of imagery and video, and the results with simulated and hardware-acquired real measurements show significant performance improvement over state-of-the-art methods.",
    "authors": [
      "Yang, Jianbo",
      "Liao, Xuejun",
      "Chen, Minhua",
      "Carin, Lawrence"
    ]
  },
  {
    "id": "07a96b1f61097ccb54be14d6a47439b0",
    "title": "Recovery of Coherent Data via Low-Rank Dictionary Pursuit",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/07a96b1f61097ccb54be14d6a47439b0-Paper.pdf",
    "abstract": "The recently established RPCA method provides a convenient way to restore low-rank matrices from grossly corrupted observations. While elegant in theory and powerful in reality, RPCA is not an ultimate solution to the low-rank matrix recovery problem. Indeed, its performance may not be perfect even when data are strictly low-rank. This is because RPCA ignores clustering structures of the data which are ubiquitous in applications. As the number of cluster grows, the coherence of data keeps increasing, and accordingly, the recovery performance of RPCA degrades. We show that the challenges raised by coherent data (i.e., data with high coherence) could be alleviated by Low-Rank Representation (LRR)~\\cite{tpami2013lrr}, provided that the dictionary in LRR is configured appropriately. More precisely, we mathematically prove that if the dictionary itself is low-rank then LRR is immune to the coherence parameter which increases with the underlying cluster number. This provides an elementary principle for dealing with coherent data and naturally leads to a practical algorithm for obtaining proper dictionaries in unsupervised environments. Experiments on randomly generated matrices and real motion sequences verify our claims. See the full paper at arXiv:1404.4032.",
    "authors": [
      "Liu, Guangcan",
      "Li, Ping"
    ]
  },
  {
    "id": "08419be897405321542838d77f855226",
    "title": "Learning to Discover Efficient Mathematical Identities",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/08419be897405321542838d77f855226-Paper.pdf",
    "abstract": "In this paper we explore how machine learning techniques can be applied to the discovery of efficient mathematical identities. We introduce an attribute grammar framework for representing symbolic expressions. Given a grammar of math operators, we build trees that combine them in different ways, looking for compositions that are analytically equivalent to a target expression but of lower computational complexity. However, as the space of trees grows exponentially with the complexity of the target expression, brute force search is impractical for all but the simplest of expressions. Consequently, we introduce two novel learning approaches that are able to learn from simpler expressions to guide the tree search. The first of these is a simple n-gram model, the other being a recursive neural-network. We show how these approaches enable us to derive complex identities, beyond reach of brute-force search, or human derivation.",
    "authors": [
      "Zaremba, Wojciech",
      "Kurach, Karol",
      "Fergus, Rob"
    ]
  },
  {
    "id": "0966289037ad9846c5e994be2a91bafa",
    "title": "Global Sensitivity Analysis for MAP Inference in Graphical Models",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/0966289037ad9846c5e994be2a91bafa-Paper.pdf",
    "abstract": "We study the sensitivity of a MAP configuration of a discrete probabilistic graphical model with respect to perturbations of its parameters. These perturbations are global, in the sense that simultaneous perturbations of all the parameters (or any chosen subset of them) are allowed. Our main contribution is an exact algorithm that can check whether the MAP configuration is robust with respect to given perturbations. Its complexity is essentially the same as that of obtaining the MAP configuration itself, so it can be promptly used with minimal effort. We use our algorithm to identify the largest global perturbation that does not induce a change in the MAP configuration, and we successfully apply this robustness measure in two practical scenarios: the prediction of facial action units with posed images and the classification of multiple real public data sets. A strong correlation between the proposed robustness measure and accuracy is verified in both scenarios.",
    "authors": [
      "De Bock, Jasper",
      "de Campos, Cassio P.",
      "Antonucci, Alessandro"
    ]
  },
  {
    "id": "09c6c3783b4a70054da74f2538ed47c6",
    "title": "Recurrent Models of Visual Attention",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/09c6c3783b4a70054da74f2538ed47c6-Paper.pdf",
    "abstract": "Applying convolutional neural networks to large images is computationally expensive because the amount of computation scales linearly with the number of image pixels. We present a novel recurrent neural network model that is capable of extracting information from an image or video by adaptively selecting a sequence of regions or locations and only processing the selected regions at high resolution. Like convolutional neural networks, the proposed model has a degree of translation invariance built-in, but the amount of computation it performs can be controlled independently of the input image size. While the model is non-differentiable, it can be trained using reinforcement learning methods to learn task-specific policies. We evaluate our model on several image classification tasks, where it significantly outperforms a convolutional neural network baseline on cluttered images, and on a dynamic visual control problem, where it learns to track a simple object without an explicit training signal for doing so.",
    "authors": [
      "Mnih, Volodymyr",
      "Heess, Nicolas",
      "Graves, Alex",
      "kavukcuoglu, koray"
    ]
  },
  {
    "id": "09fb05dd477d4ae6479985ca56c5a12d",
    "title": "LSDA: Large Scale Detection through Adaptation",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/09fb05dd477d4ae6479985ca56c5a12d-Paper.pdf",
    "abstract": "A major challenge in scaling object detection is the difficulty of obtaining labeled images for large numbers of categories. Recently, deep convolutional neural networks (CNNs) have emerged as clear winners on object classification benchmarks, in part due to training with 1.2M+ labeled classification images. Unfortunately, only a small fraction of those labels are available for the detection task. It is much cheaper and easier to collect large quantities of image-level labels from search engines than it is to collect detection data and label it with precise bounding boxes. In this paper, we propose Large Scale Detection through Adaptation (LSDA), an algorithm which learns the difference between the two tasks and transfers this knowledge to classifiers for categories without bounding box annotated data, turning them into detectors. Our method has the potential to enable detection for the tens of thousands of categories that lack bounding box annotations, yet have plenty of classification data. Evaluation on the ImageNet LSVRC-2013 detection challenge demonstrates the efficacy of our approach. This algorithm enables us to produce a >7.6K detector by using available classification data from leaf nodes in the ImageNet tree. We additionally demonstrate how to modify our architecture to produce a fast detector (running at 2fps for the 7.6K detector). Models and software are available at",
    "authors": [
      "Hoffman, Judy",
      "Guadarrama, Sergio",
      "Tzeng, Eric S.",
      "Hu, Ronghang",
      "Donahue, Jeff",
      "Girshick, Ross",
      "Darrell, Trevor",
      "Saenko, Kate"
    ]
  },
  {
    "id": "0a0a0c8aaa00ade50f74a3f0ca981ed7",
    "title": "Analog Memories in a Balanced Rate-Based Network of E-I Neurons",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/0a0a0c8aaa00ade50f74a3f0ca981ed7-Paper.pdf",
    "abstract": "The persistent and graded activity often observed in cortical circuits is sometimes seen as a signature of autoassociative retrieval of memories stored earlier in synaptic efficacies. However, despite decades of theoretical work on the subject, the mechanisms that support the storage and retrieval of memories remain unclear. Previous proposals concerning the dynamics of memory networks have fallen short of incorporating some key physiological constraints in a unified way. Specifically, some models violate Dale's law (i.e. allow neurons to be both excitatory and inhibitory), while some others restrict the representation of memories to a binary format, or induce recall states in which some neurons fire at rates close to saturation. We propose a novel control-theoretic framework to build functioning attractor networks that satisfy a set of relevant physiological constraints. We directly optimize networks of excitatory and inhibitory neurons to force sets of arbitrary analog patterns to become stable fixed points of the dynamics. The resulting networks operate in the balanced regime, are robust to corruptions of the memory cue as well as to ongoing noise, and incidentally explain the reduction of trial-to-trial variability following stimulus onset that is ubiquitously observed in sensory and motor cortices. Our results constitute a step forward in our understanding of the neural substrate of memory.",
    "authors": [
      "Festa, Dylan",
      "Hennequin, Guillaume",
      "Lengyel, Mate"
    ]
  },
  {
    "id": "0a113ef6b61820daa5611c870ed8d5ee",
    "title": "Efficient Partial Monitoring with Prior Information",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/0a113ef6b61820daa5611c870ed8d5ee-Paper.pdf",
    "abstract": "Partial monitoring is a general model for online learning with limited feedback: a learner chooses actions in a sequential manner while an opponent chooses outcomes. In every round, the learner suffers some loss and receives some feedback based on the action and the outcome. The goal of the learner is to minimize her cumulative loss. Applications range from dynamic pricing to label-efficient prediction to dueling bandits. In this paper, we assume that we are given some prior information about the distribution based on which the opponent generates the outcomes. We propose BPM, a family of new efficient algorithms whose core is to track the outcome distribution with an ellipsoid centered around the estimated distribution. We show that our algorithm provably enjoys near-optimal regret rate for locally observable partial-monitoring problems against stochastic opponents. As demonstrated with experiments on synthetic as well as real-world data, the algorithm outperforms previous approaches, even for very uninformed priors, with an order of magnitude smaller regret and lower running time.",
    "authors": [
      "Vanchinathan, Hastagiri P.",
      "Bart\u00f3k, G\u00e1bor",
      "Krause, Andreas"
    ]
  },
  {
    "id": "0deb1c54814305ca9ad266f53bc82511",
    "title": "Near-optimal Reinforcement Learning in Factored MDPs",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/0deb1c54814305ca9ad266f53bc82511-Paper.pdf",
    "abstract": "Any reinforcement learning algorithm that applies to all Markov decision processes (MDPs) will suffer $\\Omega(\\sqrt{SAT})$ regret on some MDP, where $T$ is the elapsed time and $S$ and $A$ are the cardinalities of the state and action spaces. This implies $T = \\Omega(SA)$ time to guarantee a near-optimal policy. In many settings of practical interest, due to the curse of dimensionality, $S$ and $A$ can be so enormous that this learning time is unacceptable. We establish that, if the system is known to be a \\emph{factored} MDP, it is possible to achieve regret that scales polynomially in the number of \\emph{parameters} encoding the factored MDP, which may be exponentially smaller than $S$ or $A$. We provide two algorithms that satisfy near-optimal regret bounds in this context: posterior sampling reinforcement learning (PSRL) and an upper confidence bound algorithm (UCRL-Factored).",
    "authors": [
      "Osband, Ian",
      "Van Roy, Benjamin"
    ]
  },
  {
    "id": "0e01938fc48a2cfb5f2217fbfb00722d",
    "title": "Robust Kernel Density Estimation by Scaling and Projection in Hilbert Space",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/0e01938fc48a2cfb5f2217fbfb00722d-Paper.pdf",
    "abstract": "While robust parameter estimation has been well studied in parametric density estimation, there has been little investigation into robust density estimation in the nonparametric setting. We present a robust version of the popular kernel density estimator (KDE). As with other estimators, a robust version of the KDE is useful since sample contamination is a common issue with datasets. What ``robustness'' means for a nonparametric density estimate is not straightforward and is a topic we explore in this paper. To construct a robust KDE we scale the traditional KDE and project it to its nearest weighted KDE in the $L^2$ norm. Because the squared $L^2$ norm penalizes point-wise errors superlinearly this causes the weighted KDE to allocate more weight to high density regions. We demonstrate the robustness of the SPKDE with numerical experiments and a consistency result which shows that asymptotically the SPKDE recovers the uncontaminated density under sufficient conditions on the contamination.",
    "authors": [
      "Vandermeulen, Robert A.",
      "Scott, Clayton"
    ]
  },
  {
    "id": "0e65972dce68dad4d52d063967f0a705",
    "title": "Extracting Certainty from Uncertainty: Transductive Pairwise Classification from Pairwise Similarities",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/0e65972dce68dad4d52d063967f0a705-Paper.pdf",
    "abstract": "In this work, we study the problem of transductive pairwise classification from pairwise similarities~\\footnote{The pairwise similarities are usually derived from some side information instead of the underlying class labels.}. The goal of transductive pairwise classification from pairwise similarities is to infer the pairwise class relationships, to which we refer as pairwise labels, between all examples given a subset of class relationships for a small set of examples, to which we refer as labeled examples. We propose a very simple yet effective algorithm that consists of two simple steps: the first step is to complete the sub-matrix corresponding to the labeled examples and the second step is to reconstruct the label matrix from the completed sub-matrix and the provided similarity matrix. Our analysis exhibits that under several mild preconditions we can recover the label matrix with a small error, if the top eigen-space that corresponds to the largest eigenvalues of the similarity matrix covers well the column space of label matrix and is subject to a low coherence, and the number of observed pairwise labels is sufficiently enough. We demonstrate the effectiveness of the proposed algorithm by several experiments.",
    "authors": [
      "Yang, Tianbao",
      "Jin, Rong"
    ]
  },
  {
    "id": "0eec27c419d0fe24e53c90338cdc8bc6",
    "title": "Diverse Sequential Subset Selection for Supervised Video Summarization",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/0eec27c419d0fe24e53c90338cdc8bc6-Paper.pdf",
    "abstract": "Video summarization is a challenging problem with great application potential. Whereas prior approaches, largely unsupervised in nature, focus on sampling useful frames and assembling them as summaries, we consider video summarization as a supervised subset selection problem. Our idea is to teach the system to learn from human-created summaries how to select informative and diverse subsets, so as to best meet evaluation metrics derived from human-perceived quality. To this end, we propose the sequential determinantal point process (seqDPP), a probabilistic model for diverse sequential subset selection. Our novel seqDPP heeds the inherent sequential structures in video data, thus overcoming the deficiency of the standard DPP, which treats video frames as randomly permutable items. Meanwhile, seqDPP retains the power of modeling diverse subsets, essential for summarization. Our extensive results of summarizing videos from 3 datasets demonstrate the superior performance of our method, compared to not only existing unsupervised methods but also naive applications of the standard DPP model.",
    "authors": [
      "Gong, Boqing",
      "Chao, Wei-Lun",
      "Grauman, Kristen",
      "Sha, Fei"
    ]
  },
  {
    "id": "0ff8033cf9437c213ee13937b1c4c455",
    "title": "Simultaneous Model Selection and Optimization through Parameter-free Stochastic Learning",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/0ff8033cf9437c213ee13937b1c4c455-Paper.pdf",
    "abstract": "Stochastic gradient descent algorithms for training linear and kernel predictors are gaining more and more importance, thanks to their scalability. While various methods have been proposed to speed up their convergence, the model selection phase is often ignored. In fact, in theoretical works most of the time assumptions are made, for example, on the prior knowledge of the norm of the optimal solution, while in the practical world validation methods remain the only viable approach. In this paper, we propose a new kernel-based stochastic gradient descent algorithm that performs model selection while training, with no parameters to tune, nor any form of cross-validation. The algorithm builds on recent advancement in online learning theory for unconstrained settings, to estimate over time the right regularization in a data-dependent way. Optimal rates of convergence are proved under standard smoothness assumptions on the target function as well as preliminary empirical results.",
    "authors": [
      "Orabona, Francesco"
    ]
  },
  {
    "id": "109d2dd3608f669ca17920c511c2a41e",
    "title": "On the Number of Linear Regions of Deep Neural Networks",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/109d2dd3608f669ca17920c511c2a41e-Paper.pdf",
    "abstract": "We study the complexity of functions computable by deep feedforward neural networks with piecewise linear activations in terms of the symmetries and the number of linear regions that they have. Deep networks are able to sequentially map portions of each layer's input-space to the same output. In this way, deep models compute functions that react equally to complicated patterns of different inputs. The compositional structure of these functions enables them to re-use pieces of computation exponentially often in terms of the network's depth. This paper investigates the complexity of such compositional maps and contributes new theoretical results regarding the advantage of depth for neural networks with piecewise linear activation functions. In particular, our analysis is not specific to a single family of models, and as an example, we employ it for rectifier and maxout networks. We improve complexity bounds from pre-existing work and investigate the behavior of units in higher layers.",
    "authors": [
      "Montufar, Guido F.",
      "Pascanu, Razvan",
      "Cho, Kyunghyun",
      "Bengio, Yoshua"
    ]
  },
  {
    "id": "1141938ba2c2b13f5505d7c424ebae5f",
    "title": "Model-based Reinforcement Learning and the Eluder Dimension",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/1141938ba2c2b13f5505d7c424ebae5f-Paper.pdf",
    "abstract": "We consider the problem of learning to optimize an unknown Markov decision process (MDP). We show that, if the MDP can be parameterized within some known function class, we can obtain regret bounds that scale with the dimensionality, rather than cardinality, of the system. We characterize this dependence explicitly as $\\tilde{O}(\\sqrt{d_K d_E T})$ where $T$ is time elapsed, $d_K$ is the Kolmogorov dimension and $d_E$ is the \\emph{eluder dimension}. These represent the first unified regret bounds for model-based reinforcement learning and provide state of the art guarantees in several important settings. Moreover, we present a simple and computationally efficient algorithm \\emph{posterior sampling for reinforcement learning} (PSRL) that satisfies these bounds.",
    "authors": [
      "Osband, Ian",
      "Van Roy, Benjamin"
    ]
  },
  {
    "id": "13168e6a2e6c84b4b7de9390c0ef5ec5",
    "title": "A Wild Bootstrap for Degenerate Kernel Tests",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/13168e6a2e6c84b4b7de9390c0ef5ec5-Paper.pdf",
    "abstract": "A wild bootstrap method for nonparametric hypothesis tests based on kernel distribution embeddings is proposed. This bootstrap method is used to construct provably consistent tests that apply to random processes, for which the naive permutation-based bootstrap fails. It applies to a large group of kernel tests based on V-statistics, which are degenerate under the null hypothesis, and non-degenerate elsewhere. To illustrate this approach, we construct a two-sample test, an instantaneous independence test and a multiple lag independence test for time series. In experiments, the wild bootstrap gives strong performance on synthetic examples, on audio data, and in performance benchmarking for the Gibbs sampler. The code is available at https://github.com/kacperChwialkowski/wildBootstrap.",
    "authors": [
      "Chwialkowski, Kacper P.",
      "Sejdinovic, Dino",
      "Gretton, Arthur"
    ]
  },
  {
    "id": "1373b284bc381890049e92d324f56de0",
    "title": "Extracting Latent Structure From Multiple Interacting Neural Populations",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/1373b284bc381890049e92d324f56de0-Paper.pdf",
    "abstract": "Developments in neural recording technology are rapidly enabling the recording of populations of neurons in multiple brain areas simultaneously, as well as the identification of the types of neurons being recorded (e.g., excitatory vs. inhibitory). There is a growing need for statistical methods to study the interaction among multiple, labeled populations of neurons. Rather than attempting to identify direct interactions between neurons (where the number of interactions grows with the number of neurons squared), we propose to extract a smaller number of latent variables from each population and study how the latent variables interact. Specifically, we propose extensions to probabilistic canonical correlation analysis (pCCA) to capture the temporal structure of the latent variables, as well as to distinguish within-population dynamics from across-population interactions (termed Group Latent Auto-Regressive Analysis, gLARA). We then applied these methods to populations of neurons recorded simultaneously in visual areas V1 and V2, and found that gLARA provides a better description of the recordings than pCCA. This work provides a foundation for studying how multiple populations of neurons interact and how this interaction supports brain function.",
    "authors": [
      "Semedo, Joao",
      "Zandvakili, Amin",
      "Kohn, Adam",
      "Machens, Christian K.",
      "Yu, Byron M."
    ]
  },
  {
    "id": "139f0874f2ded2e41b0393c4ac5644f7",
    "title": "Variational Gaussian Process State-Space Models",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/139f0874f2ded2e41b0393c4ac5644f7-Paper.pdf",
    "abstract": "State-space models have been successfully used for more than fifty years in different areas of science and engineering. We present a procedure for efficient variational Bayesian learning of nonlinear state-space models based on sparse Gaussian processes. The result of learning is a tractable posterior over nonlinear dynamical systems. In comparison to conventional parametric models, we offer the possibility to straightforwardly trade off model capacity and computational cost whilst avoiding overfitting. Our main algorithm uses a hybrid inference approach combining variational Bayes and sequential Monte Carlo. We also present stochastic variational inference and online learning approaches for fast learning with long time series.",
    "authors": [
      "Frigola, Roger",
      "Chen, Yutian",
      "Rasmussen, Carl Edward"
    ]
  },
  {
    "id": "140f6969d5213fd0ece03148e62e461e",
    "title": "Multi-View Perceptron: a Deep Model for Learning Face Identity and View Representations",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/140f6969d5213fd0ece03148e62e461e-Paper.pdf",
    "abstract": "Various factors, such as identities, views (poses), and illuminations, are coupled in face images. Disentangling the identity and view representations is a major challenge in face recognition. Existing face recognition systems either use handcrafted features or learn features discriminatively to improve recognition accuracy. This is different from the behavior of human brain. Intriguingly, even without accessing 3D data, human not only can recognize face identity, but can also imagine face images of a person under different viewpoints given a single 2D image, making face perception in the brain robust to view changes. In this sense, human brain has learned and encoded 3D face models from 2D images. To take into account this instinct, this paper proposes a novel deep neural net, named multi-view perceptron (MVP), which can untangle the identity and view features, and infer a full spectrum of multi-view images in the meanwhile, given a single 2D face image. The identity features of MVP achieve superior performance on the MultiPIE dataset. MVP is also capable to interpolate and predict images under viewpoints that are unobserved in the training data.",
    "authors": [
      "Zhu, Zhenyao",
      "Luo, Ping",
      "Wang, Xiaogang",
      "Tang, Xiaoou"
    ]
  },
  {
    "id": "1415db70fe9ddb119e23e9b2808cde38",
    "title": "Global Belief Recursive Neural Networks",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/1415db70fe9ddb119e23e9b2808cde38-Paper.pdf",
    "abstract": "Recursive Neural Networks have recently obtained state of the art performance on several natural language processing tasks. However, because of their feedforward architecture they cannot correctly predict phrase or word labels that are determined by context. This is a problem in tasks such as aspect-specific sentiment classification which tries to, for instance, predict that the word Android is positive in the sentence Android beats iOS. We introduce global belief recursive neural networks (GB-RNNs) which are based on the idea of extending purely feedforward neural networks to include one feedbackward step during inference. This allows phrase level predictions and representations to give feedback to words. We show the effectiveness of this model on the task of contextual sentiment analysis. We also show that dropout can improve RNN training and that a combination of unsupervised and supervised word vector representations performs better than either alone. The feedbackward step improves F1 performance by 3% over the standard RNN on this task, obtains state-of-the-art performance on the SemEval 2013 challenge and can accurately predict the sentiment of specific entities.",
    "authors": [
      "Paulus, Romain",
      "Socher, Richard",
      "Manning, Christopher D."
    ]
  },
  {
    "id": "149e9677a5989fd342ae44213df68868",
    "title": "Parallel Sampling of HDPs using Sub-Cluster Splits",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/149e9677a5989fd342ae44213df68868-Paper.pdf",
    "abstract": "We develop a sampling technique for Hierarchical Dirichlet process models. The parallel algorithm builds upon [Chang & Fisher 2013] by proposing large split and merge moves based on learned sub-clusters. The additional global split and merge moves drastically improve convergence in the experimental results. Furthermore, we discover that cross-validation techniques do not adequately determine convergence, and that previous sampling methods converge slower than were previously expected.",
    "authors": [
      "Chang, Jason",
      "Fisher III, John W."
    ]
  },
  {
    "id": "15d4e891d784977cacbfcbb00c48f133",
    "title": "Recursive Inversion Models for Permutations",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/15d4e891d784977cacbfcbb00c48f133-Paper.pdf",
    "abstract": "We develop a new exponential family probabilistic model for permutations that can capture hierarchical structure, and that has the well known Mallows and generalized Mallows models as subclasses. We describe how one can do parameter estimation and propose an approach to structure search for this class of models. We provide experimental evidence that this added flexibility both improves predictive performance and enables a deeper understanding of collections of permutations.",
    "authors": [
      "Meek, Christopher",
      "Meila, Marina"
    ]
  },
  {
    "id": "16e6a3326dd7d868cbc926602a61e4d0",
    "title": "Active Learning and Best-Response Dynamics",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/16e6a3326dd7d868cbc926602a61e4d0-Paper.pdf",
    "abstract": "We consider a setting in which low-power distributed sensors are each making highly noisy measurements of some unknown target function. A center wants to accurately learn this function by querying a small number of sensors, which ordinarily would be impossible due to the high noise rate. The question we address is whether local communication among sensors, together with natural best-response dynamics in an appropriately-de\ufb01ned game, can denoise the system without destroying the true signal and allow the center to succeed from only a small number of active queries. We prove positive (and negative) results on the denoising power of several natural dynamics, and also show experimentally that when combined with recent agnostic active learning algorithms, this process can achieve low error from very few queries, performing substantially better than active or passive learning without these denoising dynamics as well as passive learning with denoising.",
    "authors": [
      "Balcan, Maria-Florina F.",
      "Berlind, Christopher",
      "Blum, Avrim",
      "Cohen, Emma",
      "Patnaik, Kaushik",
      "Song, Le"
    ]
  },
  {
    "id": "1728efbda81692282ba642aafd57be3a",
    "title": "Multilabel Structured Output Learning with Random Spanning Trees of Max-Margin Markov Networks",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/1728efbda81692282ba642aafd57be3a-Paper.pdf",
    "abstract": "We show that the usual score function for conditional Markov networks can be written as the expectation over the scores of their spanning trees. We also show that a small random sample of these output trees can attain a significant fraction of the margin obtained by the complete graph and we provide conditions under which we can perform tractable inference. The experimental results confirm that practical learning is scalable to realistic datasets using this approach.",
    "authors": [
      "Marchand, Mario",
      "Su, Hongyu",
      "Morvant, Emilie",
      "Rousu, Juho",
      "Shawe-Taylor, John S."
    ]
  },
  {
    "id": "17e23e50bedc63b4095e3d8204ce063b",
    "title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/17e23e50bedc63b4095e3d8204ce063b-Paper.pdf",
    "abstract": "A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance.",
    "authors": [
      "Dauphin, Yann N.",
      "Pascanu, Razvan",
      "Gulcehre, Caglar",
      "Cho, Kyunghyun",
      "Ganguli, Surya",
      "Bengio, Yoshua"
    ]
  },
  {
    "id": "184260348236f9554fe9375772ff966e",
    "title": "PEWA: Patch-based Exponentially Weighted Aggregation for image denoising",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/184260348236f9554fe9375772ff966e-Paper.pdf",
    "abstract": "Patch-based methods have been widely used for noise reduction in recent years. In this paper, we propose a general statistical aggregation method which combines image patches denoised with several commonly-used algorithms. We show that weakly denoised versions of the input image obtained with standard methods, can serve to compute an efficient patch-based aggregated estimd aggregation (EWA) estimator. The resulting approach (PEWA) is based on a MCMC sampling and has a nice statistical foundation while producing denoising results that are comparable to the current state-of-the-art. We demonstrate the performance of the denoising algorithm on real images and we compare the results to several competitive methods.",
    "authors": [
      "Kervrann, Charles"
    ]
  },
  {
    "id": "185c29dc24325934ee377cfda20e414c",
    "title": "A Safe Screening Rule for Sparse Logistic Regression",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/185c29dc24325934ee377cfda20e414c-Paper.pdf",
    "abstract": "The l1-regularized logistic regression (or sparse logistic regression) is a widely used method for simultaneous classification and feature selection. Although many recent efforts have been devoted to its efficient implementation, its application to high dimensional data still poses significant challenges. In this paper, we present a fast and effective sparse logistic regression screening rule (Slores) to identify the zero components in the solution vector, which may lead to a substantial reduction in the number of features to be entered to the optimization. An appealing feature of Slores is that the data set needs to be scanned only once to run the screening and its computational cost is negligible compared to that of solving the sparse logistic regression problem. Moreover, Slores is independent of solvers for sparse logistic regression, thus Slores can be integrated with any existing solver to improve the efficiency. We have evaluated Slores using high-dimensional data sets from different applications. Extensive experimental results demonstrate that Slores outperforms the existing state-of-the-art screening rules and the efficiency of solving sparse logistic regression is improved by one magnitude in general.",
    "authors": [
      "Wang, Jie",
      "Zhou, Jiayu",
      "Liu, Jun",
      "Wonka, Peter",
      "Ye, Jieping"
    ]
  },
  {
    "id": "19b650660b253761af189682e03501dd",
    "title": "Weakly-supervised Discovery of Visual Pattern Configurations",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/19b650660b253761af189682e03501dd-Paper.pdf",
    "abstract": "The prominence of weakly labeled data gives rise to a growing demand for object detection methods that can cope with minimal supervision. We propose an approach that automatically identifies discriminative configurations of visual patterns that are characteristic of a given object class. We formulate the problem as a constrained submodular optimization problem and demonstrate the benefits of the discovered configurations in remedying mislocalizations and finding informative positive and negative training examples. Together, these lead to state-of-the-art weakly-supervised detection results on the challenging PASCAL VOC dataset.",
    "authors": [
      "Song, Hyun Oh",
      "Lee, Yong Jae",
      "Jegelka, Stefanie",
      "Darrell, Trevor"
    ]
  },
  {
    "id": "19de10adbaa1b2ee13f77f679fa1483a",
    "title": "Deep Networks with Internal Selective Attention through Feedback Connections",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/19de10adbaa1b2ee13f77f679fa1483a-Paper.pdf",
    "abstract": "Traditional convolutional neural networks (CNN) are stationary and feedforward. They neither change their parameters during evaluation nor use feedback from higher to lower layers. Real brains, however, do. So does our Deep Attention Selective Network (dasNet) architecture. DasNet's feedback structure can dynamically alter its convolutional filter sensitivities during classification. It harnesses the power of sequential processing to improve classification performance, by allowing the network to iteratively focus its internal attention on some of its convolutional filters. Feedback is trained through direct policy search in a huge million-dimensional parameter space, through scalable natural evolution strategies (SNES). On the CIFAR-10 and CIFAR-100 datasets, dasNet outperforms the previous state-of-the-art model on unaugmented datasets.",
    "authors": [
      "Stollenga, Marijn F.",
      "Masci, Jonathan",
      "Gomez, Faustino",
      "Schmidhuber, J\u00fcrgen"
    ]
  },
  {
    "id": "1bb91f73e9d31ea2830a5e73ce3ed328",
    "title": "Tight Bounds for Influence in Diffusion Networks and Application to Bond Percolation and Epidemiology",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/1bb91f73e9d31ea2830a5e73ce3ed328-Paper.pdf",
    "abstract": "In this paper, we derive theoretical bounds for the long-term influence of a node in an Independent Cascade Model (ICM). We relate these bounds to the spectral radius of a particular matrix and show that the behavior is sub-critical when this spectral radius is lower than 1. More specifically, we point out that, in general networks, the sub-critical regime behaves in O(sqrt(n)) where n is the size of the network, and that this upper bound is met for star-shaped networks. We apply our results to epidemiology and percolation on arbitrary networks, and derive a bound for the critical value beyond which a giant connected component arises. Finally, we show empirically the tightness of our bounds for a large family of networks.",
    "authors": [
      "Lemonnier, Remi",
      "Scaman, Kevin",
      "Vayatis, Nicolas"
    ]
  },
  {
    "id": "1bc0249a6412ef49b07fe6f62e6dc8de",
    "title": "A Bayesian model for identifying hierarchically organised states in neural population activity",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/1bc0249a6412ef49b07fe6f62e6dc8de-Paper.pdf",
    "abstract": "Neural population activity in cortical circuits is not solely driven by external inputs, but is also modulated by endogenous states which vary on multiple time-scales. To understand information processing in cortical circuits, we need to understand the statistical structure of internal states and their interaction with sensory inputs. Here, we present a statistical model for extracting hierarchically organised neural population states from multi-channel recordings of neural spiking activity. Population states are modelled using a hidden Markov decision tree with state-dependent tuning parameters and a generalised linear observation model. We present a variational Bayesian inference algorithm for estimating the posterior distribution over parameters from neural population recordings. On simulated data, we show that we can identify the underlying sequence of population states and reconstruct the ground truth parameters. Using population recordings from visual cortex, we find that a model with two levels of population states outperforms both a one-state and a two-state generalised linear model. Finally, we find that modelling of state-dependence also improves the accuracy with which sensory stimuli can be decoded from the population response.",
    "authors": [
      "Putzky, Patrick",
      "Franzen, Florian",
      "Bassetto, Giacomo",
      "Macke, Jakob H."
    ]
  },
  {
    "id": "1c1d4df596d01da60385f0bb17a4a9e0",
    "title": "Deep Convolutional Neural Network for Image Deconvolution",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/1c1d4df596d01da60385f0bb17a4a9e0-Paper.pdf",
    "abstract": "Many fundamental image-related problems involve deconvolution operators. Real blur degradation seldom complies with an deal linear convolution model due to camera noise, saturation, image compression, to name a few. Instead of perfectly modeling outliers, which is rather challenging from a generative model perspective, we develop a deep convolutional neural network to capture the characteristics of degradation. We note directly applying existing deep neural networks does not produce reasonable results. Our solution is to establish the connection between traditional optimization-based schemes and a neural network architecture where a novel, separable structure is introduced as a reliable support for robust deconvolution against artifacts. Our network contains two submodules, both trained in a supervised manner with proper initialization. They yield decent performance on non-blind image deconvolution compared to previous generative-model based methods.",
    "authors": [
      "Xu, Li",
      "Ren, Jimmy S.",
      "Liu, Ce",
      "Jia, Jiaya"
    ]
  },
  {
    "id": "1e6e0a04d20f50967c64dac2d639a577",
    "title": "Attentional Neural Network: Feature Selection Using Cognitive Feedback",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/1e6e0a04d20f50967c64dac2d639a577-Paper.pdf",
    "abstract": "Attentional Neural Network is a new framework that integrates top-down cognitive bias and bottom-up feature extraction in one coherent architecture. The top-down influence is especially effective when dealing with high noise or difficult segmentation problems. Our system is modular and extensible. It is also easy to train and cheap to run, and yet can accommodate complex behaviors. We obtain classification accuracy better than or competitive with state of art results on the MNIST variation dataset, and successfully disentangle overlaid digits with high success rates. We view such a general purpose framework as an essential foundation for a larger system emulating the cognitive abilities of the whole brain.",
    "authors": [
      "Wang, Qian",
      "Zhang, Jiaxing",
      "Song, Sen",
      "Zhang, Zheng"
    ]
  },
  {
    "id": "1efa39bcaec6f3900149160693694536",
    "title": "The Blinded Bandit: Learning with Adaptive Feedback",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/1efa39bcaec6f3900149160693694536-Paper.pdf",
    "abstract": "We study an online learning setting where the player is temporarily deprived of feedback each time it switches to a different action. Such model of \\emph{adaptive feedback} naturally occurs in scenarios where the environment reacts to the player's actions and requires some time to recover and stabilize after the algorithm switches actions. This motivates a variant of the multi-armed bandit problem, which we call the \\emph{blinded multi-armed bandit}, in which no feedback is given to the algorithm whenever it switches arms. We develop efficient online learning algorithms for this problem and prove that they guarantee the same asymptotic regret as the optimal algorithms for the standard multi-armed bandit problem. This result stands in stark contrast to another recent result, which states that adding a switching cost to the standard multi-armed bandit makes it substantially harder to learn, and provides a direct comparison of how feedback and loss contribute to the difficulty of an online learning problem. We also extend our results to the general prediction framework of bandit linear optimization, again attaining near-optimal regret bounds.",
    "authors": [
      "Dekel, Ofer",
      "Hazan, Elad",
      "Koren, Tomer"
    ]
  },
  {
    "id": "1f1baa5b8edac74eb4eaa329f14a0361",
    "title": "Zero-shot recognition with unreliable attributes",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/1f1baa5b8edac74eb4eaa329f14a0361-Paper.pdf",
    "abstract": "In principle, zero-shot learning makes it possible to train an object recognition model simply by specifying the category's attributes. For example, with classifiers for generic attributes like striped and four-legged, one can construct a classifier for the zebra category by enumerating which properties it possesses --- even without providing zebra training images. In practice, however, the standard zero-shot paradigm suffers because attribute predictions in novel images are hard to get right. We propose a novel random forest approach to train zero-shot models that explicitly accounts for the unreliability of attribute predictions. By leveraging statistics about each attribute\u2019s error tendencies, our method obtains more robust discriminative models for the unseen classes. We further devise extensions to handle the few-shot scenario and unreliable attribute descriptions. On three datasets, we demonstrate the benefit for visual category learning with zero or few training examples, a critical domain for rare categories or categories defined on the fly.",
    "authors": [
      "Jayaraman, Dinesh",
      "Grauman, Kristen"
    ]
  },
  {
    "id": "1ff1de774005f8da13f42943881c655f",
    "title": "Communication Efficient Distributed Machine Learning with the Parameter Server",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/1ff1de774005f8da13f42943881c655f-Paper.pdf",
    "abstract": "This paper describes a third-generation parameter server framework for distributed machine learning. This framework offers two relaxations to balance system performance and algorithm efficiency. We propose a new algorithm that takes advantage of this framework to solve non-convex non-smooth problems with convergence guarantees. We present an in-depth analysis of two large scale machine learning problems ranging from $\\ell_1$-regularized logistic regression on CPUs to reconstruction ICA on GPUs, using 636TB of real data with hundreds of billions of samples and dimensions. We demonstrate using these examples that the parameter server framework is an effective and straightforward way to scale machine learning to larger problems and systems than have been previously achieved.",
    "authors": [
      "Li, Mu",
      "Andersen, David G.",
      "Smola, Alexander J.",
      "Yu, Kai"
    ]
  },
  {
    "id": "208e43f0e45c4c78cafadb83d2888cb6",
    "title": "Beyond the Birkhoff Polytope: Convex Relaxations for Vector Permutation Problems",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/208e43f0e45c4c78cafadb83d2888cb6-Paper.pdf",
    "abstract": "The Birkhoff polytope (the convex hull of the set of permutation matrices), which is represented using $\\Theta(n^2)$ variables and constraints, is frequently invoked in formulating relaxations of optimization problems over permutations. Using a recent construction of Goemans (2010), we show that when optimizing over the convex hull of the permutation vectors (the permutahedron), we can reduce the number of variables and constraints to $\\Theta(n \\log n)$ in theory and $\\Theta(n \\log^2 n)$ in practice. We modify the recent convex formulation of the 2-SUM problem introduced by Fogel et al. (2013) to use this polytope, and demonstrate how we can attain results of similar quality in significantly less computational time for large $n$. To our knowledge, this is the first usage of Goemans' compact formulation of the permutahedron in a convex optimization problem. We also introduce a simpler regularization scheme for this convex formulation of the 2-SUM problem that yields good empirical results.",
    "authors": [
      "Lim, Cong Han",
      "Wright, Stephen"
    ]
  },
  {
    "id": "215a71a12769b056c3c32e7299f1c5ed",
    "title": "Deconvolution of High Dimensional Mixtures via Boosting, with Application to Diffusion-Weighted MRI of Human Brain",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/215a71a12769b056c3c32e7299f1c5ed-Paper.pdf",
    "abstract": "Diffusion-weighted magnetic resonance imaging (DWI) and fiber tractography are the only methods to measure the structure of the white matter in the living human brain. The diffusion signal has been modelled as the combined contribution from many individual fascicles of nerve fibers passing through each location in the white matter. Typically, this is done via basis pursuit, but estimation of the exact directions is limited due to discretization. The difficulties inherent in modeling DWI data are shared by many other problems involving fitting non-parametric mixture models. Ekanadaham et al. proposed an approach, continuous basis pursuit, to overcome discretization error in the 1-dimensional case (e.g., spike-sorting). Here, we propose a more general algorithm that fits mixture models of any dimensionality without discretization. Our algorithm uses the principles of L2-boost, together with refitting of the weights and pruning of the parameters. The addition of these steps to L2-boost both accelerates the algorithm and assures its accuracy. We refer to the resulting algorithm as elastic basis pursuit, or EBP, since it expands and contracts the active set of kernels as needed. We show that in contrast to existing approaches to fitting mixtures, our boosting framework (1) enables the selection of the optimal bias-variance tradeoff along the solution path, and (2) scales with high-dimensional problems. In simulations of DWI, we find that EBP yields better parameter estimates than a non-negative least squares (NNLS) approach, or the standard model used in DWI, the tensor model, which serves as the basis for diffusion tensor imaging (DTI). We demonstrate the utility of the method in DWI data acquired in parts of the brain containing crossings of multiple fascicles of nerve fibers.",
    "authors": [
      "Zheng, Charles Y.",
      "Pestilli, Franco",
      "Rokem, Ariel"
    ]
  },
  {
    "id": "218a0aefd1d1a4be65601cc6ddc1520e",
    "title": "On Iterative Hard Thresholding Methods for High-dimensional M-Estimation",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/218a0aefd1d1a4be65601cc6ddc1520e-Paper.pdf",
    "abstract": "The use of M-estimators in generalized linear regression models in high dimensional settings requires risk minimization with hard L_0 constraints. Of the known methods, the class of projected gradient descent (also known as iterative hard thresholding (IHT)) methods is known to offer the fastest and most scalable solutions. However, the current state-of-the-art is only able to analyze these methods in extremely restrictive settings which do not hold in high dimensional statistical models. In this work we bridge this gap by providing the first analysis for IHT-style methods in the high dimensional statistical setting. Our bounds are tight and match known minimax lower bounds. Our results rely on a general analysis framework that enables us to analyze several popular hard thresholding style algorithms (such as HTP, CoSaMP, SP) in the high dimensional regression setting. Finally, we extend our analysis to the problem of low-rank matrix recovery.",
    "authors": [
      "Jain, Prateek",
      "Tewari, Ambuj",
      "Kar, Purushottam"
    ]
  },
  {
    "id": "21fe5b8ba755eeaece7a450849876228",
    "title": "Bayesian Sampling Using Stochastic Gradient Thermostats",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/21fe5b8ba755eeaece7a450849876228-Paper.pdf",
    "abstract": "Dynamics-based sampling methods, such as Hybrid Monte Carlo (HMC) and Langevin dynamics (LD), are commonly used to sample target distributions. Recently, such approaches have been combined with stochastic gradient techniques to increase sampling efficiency when dealing with large datasets. An outstanding problem with this approach is that the stochastic gradient introduces an unknown amount of noise which can prevent proper sampling after discretization. To remedy this problem, we show that one can leverage a small number of additional variables in order to stabilize momentum fluctuations induced by the unknown noise. Our method is inspired by the idea of a thermostat in statistical physics and is justified by a general theory.",
    "authors": [
      "Ding, Nan",
      "Fang, Youhan",
      "Babbush, Ryan",
      "Chen, Changyou",
      "Skeel, Robert D.",
      "Neven, Hartmut"
    ]
  },
  {
    "id": "2291d2ec3b3048d1a6f86c2c4591b7e0",
    "title": "Encoding High Dimensional Local Features by Sparse Coding Based Fisher Vectors",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/2291d2ec3b3048d1a6f86c2c4591b7e0-Paper.pdf",
    "abstract": "Deriving from the gradient vector of a generative model of local features, Fisher vector coding (FVC) has been identified as an effective coding method for image classification. Most, if not all, FVC implementations employ the Gaussian mixture model (GMM) to characterize the generation process of local features. This choice has shown to be sufficient for traditional low dimensional local features, e.g., SIFT; and typically, good performance can be achieved with only a few hundred Gaussian distributions. However, the same number of Gaussians is insufficient to model the feature space spanned by higher dimensional local features, which have become popular recently. In order to improve the modeling capacity for high dimensional features, it turns out to be inefficient and computationally impractical to simply increase the number of Gaussians. In this paper, we propose a model in which each local feature is drawn from a Gaussian distribution whose mean vector is sampled from a subspace. With certain approximation, this model can be converted to a sparse coding procedure and the learning/inference problems can be readily solved by standard sparse coding methods. By calculating the gradient vector of the proposed model, we derive a new fisher vector encoding strategy, termed Sparse Coding based Fisher Vector Coding (SCFVC). Moreover, we adopt the recently developed Deep Convolutional Neural Network (CNN) descriptor as a high dimensional local feature and implement image classification with the proposed SCFVC. Our experimental evaluations demonstrate that our method not only significantly outperforms the traditional GMM based Fisher vector encoding but also achieves the state-of-the-art performance in generic object recognition, indoor scene, and fine-grained image classification problems.",
    "authors": [
      "Liu, Lingqiao",
      "Shen, Chunhua",
      "Wang, Lei",
      "van den Hengel, Anton",
      "Wang, Chao"
    ]
  },
  {
    "id": "25b2822c2f5a3230abfadd476e8b04c9",
    "title": "Efficient learning by implicit exploration in bandit problems with side observations",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/25b2822c2f5a3230abfadd476e8b04c9-Paper.pdf",
    "abstract": "We consider online learning problems under a a partial observability model capturing situations where the information conveyed to the learner is between full information and bandit feedback. In the simplest variant, we assume that in addition to its own loss, the learner also gets to observe losses of some other actions. The revealed losses depend on the learner's action and a directed observation system chosen by the environment. For this setting, we propose the first algorithm that enjoys near-optimal regret guarantees without having to know the observation system before selecting its actions. Along similar lines, we also define a new partial information setting that models online combinatorial optimization problems where the feedback received by the learner is between semi-bandit and full feedback. As the predictions of our first algorithm cannot be always computed efficiently in this setting, we propose another algorithm with similar properties and with the benefit of always being computationally efficient, at the price of a slightly more complicated tuning mechanism. Both algorithms rely on a novel exploration strategy called implicit exploration, which is shown to be more efficient both computationally and information-theoretically than previously studied exploration strategies for the problem.",
    "authors": [
      "Koc\u00e1k, Tom\u00e1\u0161",
      "Neu, Gergely",
      "Valko, Michal",
      "Munos, Remi"
    ]
  },
  {
    "id": "26751be1181460baf78db8d5eb7aad39",
    "title": "An Integer Polynomial Programming Based Framework for Lifted MAP Inference",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/26751be1181460baf78db8d5eb7aad39-Paper.pdf",
    "abstract": "In this paper, we present a new approach for lifted MAP inference in Markov logic networks (MLNs). The key idea in our approach is to compactly encode the MAP inference problem as an Integer Polynomial Program (IPP) by schematically applying three lifted inference steps to the MLN: lifted decomposition, lifted conditioning, and partial grounding. Our IPP encoding is lifted in the sense that an integer assignment to a variable in the IPP may represent a truth-assignment to multiple indistinguishable ground atoms in the MLN. We show how to solve the IPP by first converting it to an Integer Linear Program (ILP) and then solving the latter using state-of-the-art ILP techniques. Experiments on several benchmark MLNs show that our new algorithm is substantially superior to ground inference and existing methods in terms of computational efficiency and solution quality.",
    "authors": [
      "Sarkhel, Somdeb",
      "Venugopal, Deepak",
      "Singla, Parag",
      "Gogate, Vibhav G."
    ]
  },
  {
    "id": "26dd0dbc6e3f4c8043749885523d6a25",
    "title": "Hardness of parameter estimation in graphical models",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/26dd0dbc6e3f4c8043749885523d6a25-Paper.pdf",
    "abstract": "We consider the problem of learning the canonical parameters specifying an undirected graphical model (Markov random field) from the mean parameters. For graphical models representing a minimal exponential family, the canonical parameters are uniquely determined by the mean parameters, so the problem is feasible in principle. The goal of this paper is to investigate the computational feasibility of this statistical task. Our main result shows that parameter estimation is in general intractable: no algorithm can learn the canonical parameters of a generic pair-wise binary graphical model from the mean parameters in time bounded by a polynomial in the number of variables (unless RP = NP). Indeed, such a result has been believed to be true (see the monograph by Wainwright and Jordan) but no proof was known. Our proof gives a polynomial time reduction from approximating the partition function of the hard-core model, known to be hard, to learning approximate parameters. Our reduction entails showing that the marginal polytope boundary has an inherent repulsive property, which validates an optimization procedure over the polytope that does not use any knowledge of its structure (as required by the ellipsoid method and others).",
    "authors": [
      "Bresler, Guy",
      "Gamarnik, David",
      "Shah, Devavrat"
    ]
  },
  {
    "id": "2715518c875999308842e3455eda2fe3",
    "title": "On the Information Theoretic Limits of Learning Ising Models",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/2715518c875999308842e3455eda2fe3-Paper.pdf",
    "abstract": "We provide a general framework for computing lower-bounds on the sample complexity of recovering the underlying graphs of Ising models, given i.i.d. samples. While there have been recent results for specific graph classes, these involve fairly extensive technical arguments that are specialized to each specific graph class. In contrast, we isolate two key graph-structural ingredients that can then be used to specify sample complexity lower-bounds. Presence of these structural properties makes the graph class hard to learn. We derive corollaries of our main result that not only recover existing recent results, but also provide lower bounds for novel graph classes not considered previously. We also extend our framework to the random graph setting and derive corollaries for Erdos-Renyi graphs in a certain dense setting.",
    "authors": [
      "Tandon, Rashish",
      "Shanmugam, Karthikeyan",
      "Ravikumar, Pradeep K.",
      "Dimakis, Alexandros G."
    ]
  },
  {
    "id": "2823f4797102ce1a1aec05359cc16dd9",
    "title": "Projecting Markov Random Field Parameters for Fast Mixing",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/2823f4797102ce1a1aec05359cc16dd9-Paper.pdf",
    "abstract": "Markov chain Monte Carlo (MCMC) algorithms are simple and extremely powerful techniques to sample from almost arbitrary distributions. The flaw in practice is that it can take a large and/or unknown amount of time to converge to the stationary distribution. This paper gives sufficient conditions to guarantee that univariate Gibbs sampling on Markov Random Fields (MRFs) will be fast mixing, in a precise sense. Further, an algorithm is given to project onto this set of fast-mixing parameters in the Euclidean norm. Following recent work, we give an example use of this to project in various divergence measures, comparing of univariate marginals obtained by sampling after projection to common variational methods and Gibbs sampling on the original parameters.",
    "authors": [
      "Liu, Xianghang",
      "Domke, Justin"
    ]
  },
  {
    "id": "285ab9448d2751ee57ece7f762c39095",
    "title": "A Boosting Framework on Grounds of Online Learning",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/285ab9448d2751ee57ece7f762c39095-Paper.pdf",
    "abstract": "By exploiting the duality between boosting and online learning, we present a boosting framework which proves to be extremely powerful thanks to employing the vast knowledge available in the online learning area. Using this framework, we develop various algorithms to address multiple practically and theoretically interesting questions including sparse boosting, smooth-distribution boosting, agnostic learning and, as a by-product, some generalization to double-projection online learning algorithms.",
    "authors": [
      "Naghibi Mohamadpoor, Tofigh",
      "Pfister, Beat"
    ]
  },
  {
    "id": "287e03db1d99e0ec2edb90d079e142f3",
    "title": "Near-Optimal Density Estimation in Near-Linear Time Using Variable-Width Histograms",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/287e03db1d99e0ec2edb90d079e142f3-Paper.pdf",
    "abstract": "Let $p$ be an unknown and arbitrary probability distribution over $[0 ,1)$. We consider the problem of \\emph{density estimation}, in which a learning algorithm is given i.i.d. draws from $p$ and must (with high probability) output a hypothesis distribution that is close to $p$. The main contribution of this paper is a highly efficient density estimation algorithm for learning using a variable-width histogram, i.e., a hypothesis distribution with a piecewise constant probability density function. In more detail, for any $k$ and $\\eps$, we give an algorithm that makes $\\tilde{O}(k/\\eps^2)$ draws from $p$, runs in $\\tilde{O}(k/\\eps^2)$ time, and outputs a hypothesis distribution $h$ that is piecewise constant with $O(k \\log^2(1/\\eps))$ pieces. With high probability the hypothesis $h$ satisfies $\\dtv(p,h) \\leq C \\cdot \\opt_k(p) + \\eps$, where $\\dtv$ denotes the total variation distance (statistical distance), $C$ is a universal constant, and $\\opt_k(p)$ is the smallest total variation distance between $p$ and any $k$-piecewise constant distribution. The sample size and running time of our algorithm are both optimal up to logarithmic factors. The ``approximation factor'' $C$ that is present in our result is inherent in the problem, as we prove that no algorithm with sample size bounded in terms of $k$ and $\\eps$ can achieve $C < 2$ regardless of what kind of hypothesis distribution it uses.",
    "authors": [
      "Chan, Siu On",
      "Diakonikolas, Ilias",
      "Servedio, Rocco A.",
      "Sun, Xiaorui"
    ]
  },
  {
    "id": "28fc2782ea7ef51c1104ccf7b9bea13d",
    "title": "Efficient Minimax Signal Detection on Graphs",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/28fc2782ea7ef51c1104ccf7b9bea13d-Paper.pdf",
    "abstract": "Several problems such as network intrusion, community detection, and disease outbreak can be described by observations attributed to nodes or edges of a graph. In these applications presence of intrusion, community or disease outbreak is characterized by novel observations on some unknown connected subgraph. These problems can be formulated in terms of optimization of suitable objectives on connected subgraphs, a problem which is generally computationally difficult. We overcome the combinatorics of connectivity by embedding connected subgraphs into linear matrix inequalities (LMI). Computationally efficient tests are then realized by optimizing convex objective functions subject to these LMI constraints. We prove, by means of a novel Euclidean embedding argument, that our tests are minimax optimal for exponential family of distributions on 1-D and 2-D lattices. We show that internal conductance of the connected subgraph family plays a fundamental role in characterizing detectability.",
    "authors": [
      "Qian, Jing",
      "Saligrama, Venkatesh"
    ]
  },
  {
    "id": "291597a100aadd814d197af4f4bab3a7",
    "title": "Magnitude-sensitive preference formation`",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/291597a100aadd814d197af4f4bab3a7-Paper.pdf",
    "abstract": "Our understanding of the neural computations that underlie the ability of animals to choose among options has advanced through a synthesis of computational modeling, brain imaging and behavioral choice experiments. Yet, there remains a gulf between theories of preference learning and accounts of the real, economic choices that humans face in daily life, choices that are usually between some amount of money and an item. In this paper, we develop a theory of magnitude-sensitive preference learning that permits an agent to rationally infer its preferences for items compared with money options of different magnitudes. We show how this theory yields classical and anomalous supply-demand curves and predicts choices for a large panel of risky lotteries. Accurate replications of such phenomena without recourse to utility functions suggest that the theory proposed is both psychologically realistic and econometrically viable.",
    "authors": [
      "Srivastava, Nisheeth",
      "Vul, Ed",
      "Schrater, Paul R."
    ]
  },
  {
    "id": "2a27b8144ac02f67687f76782a3b5d8f",
    "title": "Accelerated Mini-batch Randomized Block Coordinate Descent Method",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/2a27b8144ac02f67687f76782a3b5d8f-Paper.pdf",
    "abstract": "We consider regularized empirical risk minimization problems. In particular, we minimize the sum of a smooth empirical risk function and a nonsmooth regularization function. When the regularization function is block separable, we can solve the minimization problems in a randomized block coordinate descent (RBCD) manner. Existing RBCD methods usually decrease the objective value by exploiting the partial gradient of a randomly selected block of coordinates in each iteration. Thus they need all data to be accessible so that the partial gradient of the block gradient can be exactly obtained. However, such a ``batch setting may be computationally expensive in practice. In this paper, we propose a mini-batch randomized block coordinate descent (MRBCD) method, which estimates the partial gradient of the selected block based on a mini-batch of randomly sampled data in each iteration. We further accelerate the MRBCD method by exploiting the semi-stochastic optimization scheme, which effectively reduces the variance of the partial gradient estimators. Theoretically, we show that for strongly convex functions, the MRBCD method attains lower overall iteration complexity than existing RBCD methods. As an application, we further trim the MRBCD method to solve the regularized sparse learning problems. Our numerical experiments shows that the MRBCD method naturally exploits the sparsity structure and achieves better computational performance than existing methods.\"",
    "authors": [
      "Zhao, Tuo",
      "Yu, Mo",
      "Wang, Yiming",
      "Arora, Raman",
      "Liu, Han"
    ]
  },
  {
    "id": "2a38a4a9316c49e5a833517c45d31070",
    "title": "Multiscale Fields of Patterns",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/2a38a4a9316c49e5a833517c45d31070-Paper.pdf",
    "abstract": "We describe a framework for defining high-order image models that can be used in a variety of applications. The approach involves modeling local patterns in a multiscale representation of an image. Local properties of a coarsened image reflect non-local properties of the original image. In the case of binary images local properties are defined by the binary patterns observed over small neighborhoods around each pixel. With the multiscale representation we capture the frequency of patterns observed at different scales of resolution. This framework leads to expressive priors that depend on a relatively small number of parameters. For inference and learning we use an MCMC method for block sampling with very large blocks. We evaluate the approach with two example applications. One involves contour detection. The other involves binary segmentation.",
    "authors": [
      "Felzenszwalb, Pedro",
      "Oberlin, John G."
    ]
  },
  {
    "id": "2ab56412b1163ee131e1246da0955bd1",
    "title": "How hard is my MDP?\" The distribution-norm to the rescue\"",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/2ab56412b1163ee131e1246da0955bd1-Paper.pdf",
    "abstract": "In Reinforcement Learning (RL), state-of-the-art algorithms require a large number of samples per state-action pair to estimate the transition kernel $p$. In many problems, a good approximation of $p$ is not needed. For instance, if from one state-action pair $(s,a)$, one can only transit to states with the same value, learning $p(\\cdot|s,a)$ accurately is irrelevant (only its support matters). This paper aims at capturing such behavior by defining a novel hardness measure for Markov Decision Processes (MDPs) we call the {\\em distribution-norm}. The distribution-norm w.r.t.~a measure $\\nu$ is defined on zero $\\nu$-mean functions $f$ by the standard variation of $f$ with respect to $\\nu$. We first provide a concentration inequality for the dual of the distribution-norm. This allows us to replace the generic but loose $||\\cdot||_1$ concentration inequalities used in most previous analysis of RL algorithms, to benefit from this new hardness measure. We then show that several common RL benchmarks have low hardness when measured using the new norm. The distribution-norm captures finer properties than the number of states or the diameter and can be used to assess the difficulty of MDPs.",
    "authors": [
      "Maillard, Odalric-Ambrym",
      "Mann, Timothy A.",
      "Mannor, Shie"
    ]
  },
  {
    "id": "2ac2406e835bd49c70469acae337d292",
    "title": "Spectral Learning of Mixture of Hidden Markov Models",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/2ac2406e835bd49c70469acae337d292-Paper.pdf",
    "abstract": "In this paper, we propose a learning approach for the Mixture of Hidden Markov Models (MHMM) based on the Method of Moments (MoM). Computational advantages of MoM make MHMM learning amenable for large data sets. It is not possible to directly learn an MHMM with existing learning approaches, mainly due to a permutation ambiguity in the estimation process. We show that it is possible to resolve this ambiguity using the spectral properties of a global transition matrix even in the presence of estimation noise. We demonstrate the validity of our approach on synthetic and real data.",
    "authors": [
      "Subakan, Cem",
      "Traa, Johannes",
      "Smaragdis, Paris"
    ]
  },
  {
    "id": "2afe4567e1bf64d32a5527244d104cea",
    "title": "Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/2afe4567e1bf64d32a5527244d104cea-Paper.pdf",
    "abstract": "We present techniques for speeding up the test-time evaluation of large convolutional networks, designed for object recognition tasks. These models deliver impressive accuracy, but each image evaluation requires millions of floating point operations, making their deployment on smartphones and Internet-scale clusters problematic. The computation is dominated by the convolution operations in the lower layers of the model. We exploit the redundancy present within the convolutional filters to derive approximations that significantly reduce the required computation. Using large state-of-the-art models, we demonstrate speedups of convolutional layers on both CPU and GPU by a factor of 2\u00d7, while keeping the accuracy within 1% of the original model.",
    "authors": [
      "Denton, Emily L.",
      "Zaremba, Wojciech",
      "Bruna, Joan",
      "LeCun, Yann",
      "Fergus, Rob"
    ]
  },
  {
    "id": "2b38c2df6a49b97f706ec9148ce48d86",
    "title": "Tree-structured Gaussian Process Approximations",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/2b38c2df6a49b97f706ec9148ce48d86-Paper.pdf",
    "abstract": "Gaussian process regression can be accelerated by constructing a small pseudo-dataset to summarise the observed data. This idea sits at the heart of many approximation schemes, but such an approach requires the number of pseudo-datapoints to be scaled with the range of the input space if the accuracy of the approximation is to be maintained. This presents problems in time-series settings or in spatial datasets where large numbers of pseudo-datapoints are required since computation typically scales quadratically with the pseudo-dataset size. In this paper we devise an approximation whose complexity grows linearly with the number of pseudo-datapoints. This is achieved by imposing a tree or chain structure on the pseudo-datapoints and calibrating the approximation using a Kullback-Leibler (KL) minimisation. Inference and learning can then be performed efficiently using the Gaussian belief propagation algorithm. We demonstrate the validity of our approach on a set of challenging regression tasks including missing data imputation for audio and spatial datasets. We trace out the speed-accuracy trade-off for the new method and show that the frontier dominates those obtained from a large number of existing approximation techniques.",
    "authors": [
      "Bui, Thang D.",
      "Turner, Richard E."
    ]
  },
  {
    "id": "2b8a61594b1f4c4db0902a8a395ced93",
    "title": "Optimal decision-making with time-varying evidence reliability",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/2b8a61594b1f4c4db0902a8a395ced93-Paper.pdf",
    "abstract": "Previous theoretical and experimental work on optimal decision-making was restricted to the artificial setting of a reliability of the momentary sensory evidence that remained constant within single trials. The work presented here describes the computation and characterization of optimal decision-making in the more realistic case of an evidence reliability that varies across time even within a trial. It shows that, in this case, the optimal behavior is determined by a bound in the decision maker's belief that depends only on the current, but not the past, reliability. We furthermore demonstrate that simpler heuristics fail to match the optimal performance for certain characteristics of the process that determines the time-course of this reliability, causing a drop in reward rate by more than 50%.",
    "authors": [
      "Drugowitsch, Jan",
      "Moreno-Bote, Ruben",
      "Pouget, Alexandre"
    ]
  },
  {
    "id": "2bcab9d935d219641434683dd9d18a03",
    "title": "An Autoencoder Approach to Learning Bilingual Word Representations",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/2bcab9d935d219641434683dd9d18a03-Paper.pdf",
    "abstract": "Cross-language learning allows us to use training data from one language to build models for a different language. Many approaches to bilingual learning require that we have word-level alignment of sentences from parallel corpora. In this work we explore the use of autoencoder-based methods for cross-language learning of vectorial word representations that are aligned between two languages, while not relying on word-level alignments. We show that by simply learning to reconstruct the bag-of-words representations of aligned sentences, within and between languages, we can in fact learn high-quality representations and do without word alignments. We empirically investigate the success of our approach on the problem of cross-language text classification, where a classifier trained on a given language (e.g., English) must learn to generalize to a different language (e.g., German). In experiments on 3 language pairs, we show that our approach achieves state-of-the-art performance, outperforming a method exploiting word alignments and a strong machine translation baseline.",
    "authors": [
      "Chandar A P, Sarath",
      "Lauly, Stanislas",
      "Larochelle, Hugo",
      "Khapra, Mitesh",
      "Ravindran, Balaraman",
      "Raykar, Vikas C.",
      "Saha, Amrita"
    ]
  },
  {
    "id": "2bd7f907b7f5b6bbd91822c0c7b835f6",
    "title": "Testing Unfaithful Gaussian Graphical Models",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/2bd7f907b7f5b6bbd91822c0c7b835f6-Paper.pdf",
    "abstract": "The global Markov property for Gaussian graphical models ensures graph separation implies conditional independence. Specifically if a node set $S$ graph separates nodes $u$ and $v$ then $X_u$ is conditionally independent of $X_v$ given $X_S$. The opposite direction need not be true, that is, $X_u \\perp X_v \\mid X_S$ need not imply $S$ is a node separator of $u$ and $v$. When it does, the relation $X_u \\perp X_v \\mid X_S$ is called faithful. In this paper we provide a characterization of faithful relations and then provide an algorithm to test faithfulness based only on knowledge of other conditional relations of the form $X_i \\perp X_j \\mid X_S$.",
    "authors": [
      "Soh, De Wen",
      "Tatikonda, Sekhar C."
    ]
  },
  {
    "id": "2cfd4560539f887a5e420412b370b361",
    "title": "Deep Recursive Neural Networks for Compositionality in Language",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/2cfd4560539f887a5e420412b370b361-Paper.pdf",
    "abstract": "Recursive neural networks comprise a class of architecture that can operate on structured input. They have been previously successfully applied to model compositionality in natural language using parse-tree-based structural representations. Even though these architectures are deep in structure, they lack the capacity for hierarchical representation that exists in conventional deep feed-forward networks as well as in recently investigated deep recurrent neural networks. In this work we introduce a new architecture --- a deep recursive neural network (deep RNN) --- constructed by stacking multiple recursive layers. We evaluate the proposed model on the task of fine-grained sentiment classification. Our results show that deep RNNs outperform associated shallow counterparts that employ the same number of parameters. Furthermore, our approach outperforms previous baselines on the sentiment analysis task, including a multiplicative RNN variant as well as the recently introduced paragraph vectors, achieving new state-of-the-art results. We provide exploratory analyses of the effect of multiple layers and show that they capture different aspects of compositionality in language.",
    "authors": [
      "Irsoy, Ozan",
      "Cardie, Claire"
    ]
  },
  {
    "id": "2d1b2a5ff364606ff041650887723470",
    "title": "Signal Aggregate Constraints in Additive Factorial HMMs, with Application to Energy Disaggregation",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/2d1b2a5ff364606ff041650887723470-Paper.pdf",
    "abstract": "Blind source separation problems are difficult because they are inherently unidentifiable, yet the entire goal is to identify meaningful sources. We introduce a way of incorporating domain knowledge into this problem, called signal aggregate constraints (SACs). SACs encourage the total signal for each of the unknown sources to be close to a specified value. This is based on the observation that the total signal often varies widely across the unknown sources, and we often have a good idea of what total values to expect. We incorporate SACs into an additive factorial hidden Markov model (AFHMM) to formulate the energy disaggregation problems where only one mixture signal is assumed to be observed. A convex quadratic program for approximate inference is employed for recovering those source signals. On a real-world energy disaggregation data set, we show that the use of SACs dramatically improves the original AFHMM, and significantly improves over a recent state-of-the art approach.",
    "authors": [
      "Zhong, Mingjun",
      "Goddard, Nigel",
      "Sutton, Charles"
    ]
  },
  {
    "id": "2d6cc4b2d139a53512fb8cbb3086ae2e",
    "title": "Poisson Process Jumping between an Unknown Number of Rates: Application to Neural Spike Data",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/2d6cc4b2d139a53512fb8cbb3086ae2e-Paper.pdf",
    "abstract": "We introduce a model where the rate of an inhomogeneous Poisson process is modified by a Chinese restaurant process. Applying a MCMC sampler to this model allows us to do posterior Bayesian inference about the number of states in Poisson-like data. Our sampler is shown to get accurate results for synthetic data and we apply it to V1 neuron spike data to find discrete firing rate states depending on the orientation of a stimulus.",
    "authors": [
      "Stimberg, Florian",
      "Ruttor, Andreas",
      "Opper, Manfred"
    ]
  },
  {
    "id": "2f25f6e326adb93c5787175dda209ab6",
    "title": "Low-Rank Time-Frequency Synthesis",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/2f25f6e326adb93c5787175dda209ab6-Paper.pdf",
    "abstract": "Many single-channel signal decomposition techniques rely on a low-rank factorization of a time-frequency transform. In particular, nonnegative matrix factorization (NMF) of the spectrogram -- the (power) magnitude of the short-time Fourier transform (STFT) -- has been considered in many audio applications. In this setting, NMF with the Itakura-Saito divergence was shown to underly a generative Gaussian composite model (GCM) of the STFT, a step forward from more empirical approaches based on ad-hoc transform and divergence specifications. Still, the GCM is not yet a generative model of the raw signal itself, but only of its STFT. The work presented in this paper fills in this ultimate gap by proposing a novel signal synthesis model with low-rank time-frequency structure. In particular, our new approach opens doors to multi-resolution representations, that were not possible in the traditional NMF setting. We describe two expectation-maximization algorithms for estimation in the new model and report audio signal processing results with music decomposition and speech enhancement.",
    "authors": [
      "F\u00e9votte, C\u00e9dric",
      "Kowalski, Matthieu"
    ]
  },
  {
    "id": "2f2b265625d76a6704b08093c652fd79",
    "title": "Spike Frequency Adaptation Implements Anticipative Tracking in Continuous Attractor Neural Networks",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/2f2b265625d76a6704b08093c652fd79-Paper.pdf",
    "abstract": "To extract motion information, the brain needs to compensate for time delays that are ubiquitous in neural signal transmission and processing. Here we propose a simple yet effective mechanism to implement anticipative tracking in neural systems. The proposed mechanism utilizes the property of spike-frequency adaptation (SFA), a feature widely observed in neuronal responses. We employ continuous attractor neural networks (CANNs) as the model to describe the tracking behaviors in neural systems. Incorporating SFA, a CANN exhibits intrinsic mobility, manifested by the ability of the CANN to hold self-sustained travelling waves. In tracking a moving stimulus, the interplay between the external drive and the intrinsic mobility of the network determines the tracking performance. Interestingly, we find that the regime of anticipation effectively coincides with the regime where the intrinsic speed of the travelling wave exceeds that of the external drive. Depending on the SFA amplitudes, the network can achieve either perfect tracking, with zero-lag to the input, or perfect anticipative tracking, with a constant leading time to the input. Our model successfully reproduces experimentally observed anticipative tracking behaviors, and sheds light on our understanding of how the brain processes motion information in a timely manner.",
    "authors": [
      "Mi, Yuanyuan",
      "Fung, C. C. Alan",
      "Wong, K. Y. Michael",
      "Wu, Si"
    ]
  },
  {
    "id": "301ad0e3bd5cb1627a2044908a42fdc2",
    "title": "Learning to Optimize via Information-Directed Sampling",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/301ad0e3bd5cb1627a2044908a42fdc2-Paper.pdf",
    "abstract": "We propose information-directed sampling -- a new algorithm for online optimization problems in which a decision-maker must balance between exploration and exploitation while learning from partial feedback. Each action is sampled in a manner that minimizes the ratio between the square of expected single-period regret and a measure of information gain: the mutual information between the optimal action and the next observation. We establish an expected regret bound for information-directed sampling that applies across a very general class of models and scales with the entropy of the optimal action distribution. For the widely studied Bernoulli and linear bandit models, we demonstrate simulation performance surpassing popular approaches, including upper confidence bound algorithms, Thompson sampling, and knowledge gradient. Further, we present simple analytic examples illustrating that information-directed sampling can dramatically outperform upper confidence bound algorithms and Thompson sampling due to the way it measures information gain.",
    "authors": [
      "Russo, Daniel",
      "Van Roy, Benjamin"
    ]
  },
  {
    "id": "303ed4c69846ab36c2904d3ba8573050",
    "title": "Distributed Estimation, Information Loss and Exponential Families",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/303ed4c69846ab36c2904d3ba8573050-Paper.pdf",
    "abstract": "Distributed learning of probabilistic models from multiple data repositories with minimum communication is increasingly important. We study a simple communication-efficient learning framework that first calculates the local maximum likelihood estimates (MLE) based on the data subsets, and then combines the local MLEs to achieve the best possible approximation to the global MLE, based on the whole dataset jointly. We study the statistical properties of this framework, showing that the loss of efficiency compared to the global setting relates to how much the underlying distribution families deviate from full exponential families, drawing connection to the theory of information loss by Fisher, Rao and Efron. We show that the full-exponential-family-ness\" represents the lower bound of the error rate of arbitrary combinations of local MLEs, and is achieved by a KL-divergence-based combination method but not by a more common linear combination method. We also study the empirical properties of the KL and linear combination methods, showing that the KL method significantly outperforms linear combination in practical settings with issues such as model misspecification, non-convexity, and heterogeneous data partitions.\"",
    "authors": [
      "Liu, Qiang",
      "Ihler, Alexander T."
    ]
  },
  {
    "id": "309fee4e541e51de2e41f21bebb342aa",
    "title": "A* Sampling",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/309fee4e541e51de2e41f21bebb342aa-Paper.pdf",
    "abstract": "The problem of drawing samples from a discrete distribution can be converted into a discrete optimization problem. In this work, we show how sampling from a continuous distribution can be converted into an optimization problem over continuous space. Central to the method is a stochastic process recently described in mathematical statistics that we call the Gumbel process. We present a new construction of the Gumbel process and A* sampling, a practical generic sampling algorithm that searches for the maximum of a Gumbel process using A* search. We analyze the correctness and convergence time of A* sampling and demonstrate empirically that it makes more efficient use of bound and likelihood evaluations than the most closely related adaptive rejection sampling-based algorithms.",
    "authors": [
      "Maddison, Chris J.",
      "Tarlow, Daniel",
      "Minka, Tom"
    ]
  },
  {
    "id": "30c8e1ca872524fbf7ea5c519ca397ee",
    "title": "Consistent Binary Classification with Generalized Performance Metrics",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/30c8e1ca872524fbf7ea5c519ca397ee-Paper.pdf",
    "abstract": "Performance metrics for binary classification are designed to capture tradeoffs between four fundamental population quantities: true positives, false positives, true negatives and false negatives. Despite significant interest from theoretical and applied communities, little is known about either optimal classifiers or consistent algorithms for optimizing binary classification performance metrics beyond a few special cases. We consider a fairly large family of performance metrics given by ratios of linear combinations of the four fundamental population quantities. This family includes many well known binary classification metrics such as classification accuracy, AM measure, F-measure and the Jaccard similarity coefficient as special cases. Our analysis identifies the optimal classifiers as the sign of the thresholded conditional probability of the positive class, with a performance metric-dependent threshold. The optimal threshold can be constructed using simple plug-in estimators when the performance metric is a linear combination of the population quantities, but alternative techniques are required for the general case. We propose two algorithms for estimating the optimal classifiers, and prove their statistical consistency. Both algorithms are straightforward modifications of standard approaches to address the key challenge of optimal threshold selection, thus are simple to implement in practice. The first algorithm combines a plug-in estimate of the conditional probability of the positive class with optimal threshold selection. The second algorithm leverages recent work on calibrated asymmetric surrogate losses to construct candidate classifiers. We present empirical comparisons between these algorithms on benchmark datasets.",
    "authors": [
      "Koyejo, Oluwasanmi O.",
      "Natarajan, Nagarajan",
      "Ravikumar, Pradeep K.",
      "Dhillon, Inderjit S."
    ]
  },
  {
    "id": "310ce61c90f3a46e340ee8257bc70e93",
    "title": "Asymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search (MIPS)",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/310ce61c90f3a46e340ee8257bc70e93-Paper.pdf",
    "abstract": "We present the first provably sublinear time hashing algorithm for approximate \\emph{Maximum Inner Product Search} (MIPS). Searching with (un-normalized) inner product as the underlying similarity measure is a known difficult problem and finding hashing schemes for MIPS was considered hard. While the existing Locality Sensitive Hashing (LSH) framework is insufficient for solving MIPS, in this paper we extend the LSH framework to allow asymmetric hashing schemes. Our proposal is based on a key observation that the problem of finding maximum inner products, after independent asymmetric transformations, can be converted into the problem of approximate near neighbor search in classical settings. This key observation makes efficient sublinear hashing scheme for MIPS possible. Under the extended asymmetric LSH (ALSH) framework, this paper provides an example of explicit construction of provably fast hashing scheme for MIPS. Our proposed algorithm is simple and easy to implement. The proposed hashing scheme leads to significant computational savings over the two popular conventional LSH schemes: (i) Sign Random Projection (SRP) and (ii) hashing based on $p$-stable distributions for $L_2$ norm (L2LSH), in the collaborative filtering task of item recommendations on Netflix and Movielens (10M) datasets.",
    "authors": [
      "Shrivastava, Anshumali",
      "Li, Ping"
    ]
  },
  {
    "id": "31839b036f63806cba3f47b93af8ccb5",
    "title": "Graphical Models for Recovering Probabilistic and Causal Queries from Missing Data",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/31839b036f63806cba3f47b93af8ccb5-Paper.pdf",
    "abstract": "We address the problem of deciding whether a causal or probabilistic query is estimable from data corrupted by missing entries, given a model of missingness process. We extend the results of Mohan et al, 2013 by presenting more general conditions for recovering probabilistic queries of the form P(y|x) and P(y,x) as well as causal queries of the form P(y|do(x)). We show that causal queries may be recoverable even when the factors in their identifying estimands are not recoverable. Specifically, we derive graphical conditions for recovering causal effects of the form P(y|do(x)) when Y and its missingness mechanism are not d-separable. Finally, we apply our results to problems of attrition and characterize the recovery of causal effects from data corrupted by attrition.",
    "authors": [
      "Mohan, Karthika",
      "Pearl, Judea"
    ]
  },
  {
    "id": "3295c76acbf4caaed33c36b1b5fc2cb1",
    "title": "Restricted Boltzmann machines modeling human choice",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/3295c76acbf4caaed33c36b1b5fc2cb1-Paper.pdf",
    "abstract": "We extend the multinomial logit model to represent some of the empirical phenomena that are frequently observed in the choices made by humans. These phenomena include the similarity effect, the attraction effect, and the compromise effect. We formally quantify the strength of these phenomena that can be represented by our choice model, which illuminates the flexibility of our choice model. We then show that our choice model can be represented as a restricted Boltzmann machine and that its parameters can be learned effectively from data. Our numerical experiments with real data of human choices suggest that we can train our choice model in such a way that it represents the typical phenomena of choice.",
    "authors": [
      "Osogami, Takayuki",
      "Otsuka, Makoto"
    ]
  },
  {
    "id": "32b30a250abd6331e03a2a1f16466346",
    "title": "On the Statistical Consistency of Plug-in Classifiers for Non-decomposable Performance Measures",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/32b30a250abd6331e03a2a1f16466346-Paper.pdf",
    "abstract": "We study consistency properties of algorithms for non-decomposable performance measures that cannot be expressed as a sum of losses on individual data points, such as the F-measure used in text retrieval and several other performance measures used in class imbalanced settings. While there has been much work on designing algorithms for such performance measures, there is limited understanding of the theoretical properties of these algorithms. Recently, Ye et al. (2012) showed consistency results for two algorithms that optimize the F-measure, but their results apply only to an idealized setting, where precise knowledge of the underlying probability distribution (in the form of the true' posterior class probability) is available to a learning algorithm. In this work, we consider plug-in algorithms that learn a classifier by applying an empirically determined threshold to a suitableestimate' of the class probability, and provide a general methodology to show consistency of these methods for any non-decomposable measure that can be expressed as a continuous function of true positive rate (TPR) and true negative rate (TNR), and for which the Bayes optimal classifier is the class probability function thresholded suitably. We use this template to derive consistency results for plug-in algorithms for the F-measure and for the geometric mean of TPR and precision; to our knowledge, these are the first such results for these measures. In addition, for continuous distributions, we show consistency of plug-in algorithms for any performance measure that is a continuous and monotonically increasing function of TPR and TNR. Experimental results confirm our theoretical findings.",
    "authors": [
      "Narasimhan, Harikrishna",
      "Vaish, Rohit",
      "Agarwal, Shivani"
    ]
  },
  {
    "id": "3328bdf9a4b9504b9398284244fe97c2",
    "title": "Clustering from Labels and Time-Varying Graphs",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/3328bdf9a4b9504b9398284244fe97c2-Paper.pdf",
    "abstract": "We present a general framework for graph clustering where a label is observed to each pair of nodes. This allows a very rich encoding of various types of pairwise interactions between nodes. We propose a new tractable approach to this problem based on maximum likelihood estimator and convex optimization. We analyze our algorithm under a general generative model, and provide both necessary and sufficient conditions for successful recovery of the underlying clusters. Our theoretical results cover and subsume a wide range of existing graph clustering results including planted partition, weighted clustering and partially observed graphs. Furthermore, the result is applicable to novel settings including time-varying graphs such that new insights can be gained on solving these problems. Our theoretical findings are further supported by empirical results on both synthetic and real data.",
    "authors": [
      "Lim, Shiau Hong",
      "Chen, Yudong",
      "Xu, Huan"
    ]
  },
  {
    "id": "333222170ab9edca4785c39f55221fe7",
    "title": "Unsupervised learning of an efficient short-term memory network",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/333222170ab9edca4785c39f55221fe7-Paper.pdf",
    "abstract": "Learning in recurrent neural networks has been a topic fraught with difficulties and problems. We here report substantial progress in the unsupervised learning of recurrent networks that can keep track of an input signal. Specifically, we show how these networks can learn to efficiently represent their present and past inputs, based on local learning rules only. Our results are based on several key insights. First, we develop a local learning rule for the recurrent weights whose main aim is to drive the network into a regime where, on average, feedforward signal inputs are canceled by recurrent inputs. We show that this learning rule minimizes a cost function. Second, we develop a local learning rule for the feedforward weights that, based on networks in which recurrent inputs already predict feedforward inputs, further minimizes the cost. Third, we show how the learning rules can be modified such that the network can directly encode non-whitened inputs. Fourth, we show that these learning rules can also be applied to a network that feeds a time-delayed version of the network output back into itself. As a consequence, the network starts to efficiently represent both its signal inputs and their history. We develop our main theory for linear networks, but then sketch how the learning rules could be transferred to balanced, spiking networks.",
    "authors": [
      "Vertechi, Pietro",
      "Brendel, Wieland",
      "Machens, Christian K."
    ]
  },
  {
    "id": "33e8075e9970de0cfea955afd4644bb2",
    "title": "Projective dictionary pair learning for pattern classification",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/33e8075e9970de0cfea955afd4644bb2-Paper.pdf",
    "abstract": "Discriminative dictionary learning (DL) has been widely studied in various pattern classification problems. Most of the existing DL methods aim to learn a synthesis dictionary to represent the input signal while enforcing the representation coefficients and/or representation residual to be discriminative. However, the $\\ell_0$ or $\\ell_1$-norm sparsity constraint on the representation coefficients adopted in many DL methods makes the training and testing phases time consuming. We propose a new discriminative DL framework, namely projective dictionary pair learning (DPL), which learns a synthesis dictionary and an analysis dictionary jointly to achieve the goal of signal representation and discrimination. Compared with conventional DL methods, the proposed DPL method can not only greatly reduce the time complexity in the training and testing phases, but also lead to very competitive accuracies in a variety of visual classification tasks.",
    "authors": [
      "Gu, Shuhang",
      "Zhang, Lei",
      "Zuo, Wangmeng",
      "Feng, Xiangchu"
    ]
  },
  {
    "id": "35051070e572e47d2c26c241ab88307f",
    "title": "Analysis of Learning from Positive and Unlabeled Data",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/35051070e572e47d2c26c241ab88307f-Paper.pdf",
    "abstract": "Learning a classifier from positive and unlabeled data is an important class of classification problems that are conceivable in many practical applications. In this paper, we first show that this problem can be solved by cost-sensitive learning between positive and unlabeled data. We then show that convex surrogate loss functions such as the hinge loss may lead to a wrong classification boundary due to an intrinsic bias, but the problem can be avoided by using non-convex loss functions such as the ramp loss. We next analyze the excess risk when the class prior is estimated from data, and show that the classification accuracy is not sensitive to class prior estimation if the unlabeled data is dominated by the positive data (this is naturally satisfied in inlier-based outlier detection because inliers are dominant in the unlabeled dataset). Finally, we provide generalization error bounds and show that, for an equal number of labeled and unlabeled samples, the generalization error of learning only from positive and unlabeled samples is no worse than $2\\sqrt{2}$ times the fully supervised case. These theoretical findings are also validated through experiments.",
    "authors": [
      "du Plessis, Marthinus C.",
      "Niu, Gang",
      "Sugiyama, Masashi"
    ]
  },
  {
    "id": "35464c848f410e55a13bb9d78e7fddd0",
    "title": "Learning with Fredholm Kernels",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/35464c848f410e55a13bb9d78e7fddd0-Paper.pdf",
    "abstract": "In this paper we propose a framework for supervised and semi-supervised learning based on reformulating the learning problem as a regularized Fredholm integral equation. Our approach fits naturally into the kernel framework and can be interpreted as constructing new data-dependent kernels, which we call Fredholm kernels. We proceed to discuss the noise assumption\" for semi-supervised learning and provide evidence evidence both theoretical and experimental that Fredholm kernels can effectively utilize unlabeled data under the noise assumption. We demonstrate that methods based on Fredholm learning show very competitive performance in the standard semi-supervised learning setting.\"",
    "authors": [
      "Que, Qichao",
      "Belkin, Mikhail",
      "Wang, Yusu"
    ]
  },
  {
    "id": "375c71349b295fbe2dcdca9206f20a06",
    "title": "How transferable are features in deep neural networks?",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/375c71349b295fbe2dcdca9206f20a06-Paper.pdf",
    "abstract": "Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.",
    "authors": [
      "Yosinski, Jason",
      "Clune, Jeff",
      "Bengio, Yoshua",
      "Lipson, Hod"
    ]
  },
  {
    "id": "39027dfad5138c9ca0c474d71db915c3",
    "title": "Concavity of reweighted Kikuchi approximation",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/39027dfad5138c9ca0c474d71db915c3-Paper.pdf",
    "abstract": "We analyze a reweighted version of the Kikuchi approximation for estimating the log partition function of a product distribution defined over a region graph. We establish sufficient conditions for the concavity of our reweighted objective function in terms of weight assignments in the Kikuchi expansion, and show that a reweighted version of the sum product algorithm applied to the Kikuchi region graph will produce global optima of the Kikuchi approximation whenever the algorithm converges. When the region graph has two layers, corresponding to a Bethe approximation, we show that our sufficient conditions for concavity are also necessary. Finally, we provide an explicit characterization of the polytope of concavity in terms of the cycle structure of the region graph. We conclude with simulations that demonstrate the advantages of the reweighted Kikuchi approach.",
    "authors": [
      "Loh, Po-Ling",
      "Wibisono, Andre"
    ]
  },
  {
    "id": "390e982518a50e280d8e2b535462ec1f",
    "title": "The Bayesian Case Model: A Generative Approach for Case-Based Reasoning and Prototype Classification",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/390e982518a50e280d8e2b535462ec1f-Paper.pdf",
    "abstract": "We present the Bayesian Case Model (BCM), a general framework for Bayesian case-based reasoning (CBR) and prototype classification and clustering. BCM brings the intuitive power of CBR to a Bayesian generative framework. The BCM learns prototypes, the ``quintessential observations that best represent clusters in a dataset, by performing joint inference on cluster labels, prototypes and important features. Simultaneously, BCM pursues sparsity by learning subspaces, the sets of features that play important roles in the characterization of the prototypes. The prototype and subspace representation provides quantitative benefits in interpretability while preserving classification accuracy. Human subject experiments verify statistically significant improvements to participants' understanding when using explanations produced by BCM, compared to those given by prior art.\"",
    "authors": [
      "Kim, Been",
      "Rudin, Cynthia",
      "Shah, Julie A."
    ]
  },
  {
    "id": "3948ead63a9f2944218de038d8934305",
    "title": "Advances in Learning Bayesian Networks of Bounded Treewidth",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/3948ead63a9f2944218de038d8934305-Paper.pdf",
    "abstract": "This work presents novel algorithms for learning Bayesian networks of bounded treewidth. Both exact and approximate methods are developed. The exact method combines mixed integer linear programming formulations for structure learning and treewidth computation. The approximate method consists in sampling k-trees (maximal graphs of treewidth k), and subsequently selecting, exactly or approximately, the best structure whose moral graph is a subgraph of that k-tree. The approaches are empirically compared to each other and to state-of-the-art methods on a collection of public data sets with up to 100 variables.",
    "authors": [
      "Nie, Siqi",
      "Maua, Denis D.",
      "de Campos, Cassio P.",
      "Ji, Qiang"
    ]
  },
  {
    "id": "3a066bda8c96b9478bb0512f0a43028c",
    "title": "Learning From Weakly Supervised Data by The Expectation Loss SVM (e-SVM) algorithm",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/3a066bda8c96b9478bb0512f0a43028c-Paper.pdf",
    "abstract": "In many situations we have some measurement of confidence on positiveness for a binary label. Thepositiveness\" is a continuous value whose range is a bounded interval. It quantifies the affiliation of each training data to the positive class. We propose a novel learning algorithm called \\emph{expectation loss SVM} (e-SVM) that is devoted to the problems where only the ``positiveness\" instead of a binary label of each training sample is available. Our e-SVM algorithm can also be readily extended to learn segment classifiers under weak supervision where the exact positiveness value of each training example is unobserved. In experiments, we show that the e-SVM algorithm can effectively address the segment proposal classification task under both strong supervision (e.g. the pixel-level annotations are available) and the weak supervision (e.g. only bounding-box annotations are available), and outperforms the alternative approaches. Besides, we further validate this method on two major tasks of computer vision: semantic segmentation and object detection. Our method achieves the state-of-the-art object detection performance on PASCAL VOC 2007 dataset.\"",
    "authors": [
      "Zhu, Jun",
      "Mao, Junhua",
      "Yuille, Alan L."
    ]
  },
  {
    "id": "3a0772443a0739141292a5429b952fe6",
    "title": "On the Computational Efficiency of Training Neural Networks",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/3a0772443a0739141292a5429b952fe6-Paper.pdf",
    "abstract": "It is well-known that neural networks are computationally hard to train. On the other hand, in practice, modern day neural networks are trained efficiently using SGD and a variety of tricks that include different activation functions (e.g. ReLU), over-specification (i.e., train networks which are larger than needed), and regularization. In this paper we revisit the computational complexity of training neural networks from a modern perspective. We provide both positive and negative results, some of them yield new provably efficient and practical algorithms for training neural networks.",
    "authors": [
      "Livni, Roi",
      "Shalev-Shwartz, Shai",
      "Shamir, Ohad"
    ]
  },
  {
    "id": "3a20f62a0af1aa152670bab3c602feed",
    "title": "Scaling-up Importance Sampling for Markov Logic Networks",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/3a20f62a0af1aa152670bab3c602feed-Paper.pdf",
    "abstract": "Markov Logic Networks (MLNs) are weighted first-order logic templates for generating large (ground) Markov networks. Lifted inference algorithms for them bring the power of logical inference to probabilistic inference. These algorithms operate as much as possible at the compact first-order level, grounding or propositionalizing the MLN only as necessary. As a result, lifted inference algorithms can be much more scalable than propositional algorithms that operate directly on the much larger ground network. Unfortunately, existing lifted inference algorithms suffer from two interrelated problems, which severely affects their scalability in practice. First, for most real-world MLNs having complex structure, they are unable to exploit symmetries and end up grounding most atoms (the grounding problem). Second, they suffer from the evidence problem, which arises because evidence breaks symmetries, severely diminishing the power of lifted inference. In this paper, we address both problems by presenting a scalable, lifted importance sampling-based approach that never grounds the full MLN. Specifically, we show how to scale up the two main steps in importance sampling: sampling from the proposal distribution and weight computation. Scalable sampling is achieved by using an informed, easy-to-sample proposal distribution derived from a compressed MLN-representation. Fast weight computation is achieved by only visiting a small subset of the sampled groundings of each formula instead of all of its possible groundings. We show that our new algorithm yields an asymptotically unbiased estimate. Our experiments on several MLNs clearly demonstrate the promise of our approach.",
    "authors": [
      "Venugopal, Deepak",
      "Gogate, Vibhav G."
    ]
  },
  {
    "id": "3b5dca501ee1e6d8cd7b905f4e1bf723",
    "title": "Unsupervised Transcription of Piano Music",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/3b5dca501ee1e6d8cd7b905f4e1bf723-Paper.pdf",
    "abstract": "We present a new probabilistic model for transcribing piano music from audio to a symbolic form. Our model reflects the process by which discrete musical events give rise to acoustic signals that are then superimposed to produce the observed data. As a result, the inference procedure for our model naturally resolves the source separation problem introduced by the the piano's polyphony. In order to adapt to the properties of a new instrument or acoustic environment being transcribed, we learn recording specific spectral profiles and temporal envelopes in an unsupervised fashion. Our system outperforms the best published approaches on a standard piano transcription task, achieving a 10.6% relative gain in note onset F1 on real piano audio.",
    "authors": [
      "Berg-Kirkpatrick, Taylor",
      "Andreas, Jacob",
      "Klein, Dan"
    ]
  },
  {
    "id": "3bbfdde8842a5c44a0323518eec97cbe",
    "title": "Multi-Scale Spectral Decomposition of Massive Graphs",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/3bbfdde8842a5c44a0323518eec97cbe-Paper.pdf",
    "abstract": "Computing the $k$ dominant eigenvalues and eigenvectors of massive graphs is a key operation in numerous machine learning applications; however, popular solvers suffer from slow convergence, especially when $k$ is reasonably large. In this paper, we propose and analyze a novel multi-scale spectral decomposition method (MSEIGS), which first clusters the graph into smaller clusters whose spectral decomposition can be computed efficiently and independently. We show theoretically as well as empirically that the union of all cluster's subspaces has significant overlap with the dominant subspace of the original graph, provided that the graph is clustered appropriately. Thus, eigenvectors of the clusters serve as good initializations to a block Lanczos algorithm that is used to compute spectral decomposition of the original graph. We further use hierarchical clustering to speed up the computation and adopt a fast early termination strategy to compute quality approximations. Our method outperforms widely used solvers in terms of convergence speed and approximation quality. Furthermore, our method is naturally parallelizable and exhibits significant speedups in shared-memory parallel settings. For example, on a graph with more than 82 million nodes and 3.6 billion edges, MSEIGS takes less than 3 hours on a single-core machine while Randomized SVD takes more than 6 hours, to obtain a similar approximation of the top-50 eigenvectors. Using 16 cores, we can reduce this time to less than 40 minutes.",
    "authors": [
      "Si, Si",
      "Shin, Donghyuk",
      "Dhillon, Inderjit S.",
      "Parlett, Beresford N."
    ]
  },
  {
    "id": "3cf166c6b73f030b4f67eeaeba301103",
    "title": "Dimensionality Reduction with Subspace Structure Preservation",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/3cf166c6b73f030b4f67eeaeba301103-Paper.pdf",
    "abstract": "Modeling data as being sampled from a union of independent subspaces has been widely applied to a number of real world applications. However, dimensionality reduction approaches that theoretically preserve this independence assumption have not been well studied. Our key contribution is to show that $2K$ projection vectors are sufficient for the independence preservation of any $K$ class data sampled from a union of independent subspaces. It is this non-trivial observation that we use for designing our dimensionality reduction technique. In this paper, we propose a novel dimensionality reduction algorithm that theoretically preserves this structure for a given dataset. We support our theoretical analysis with empirical results on both synthetic and real world data achieving \\textit{state-of-the-art} results compared to popular dimensionality reduction techniques.",
    "authors": [
      "Arpit, Devansh",
      "Nwogu, Ifeoma",
      "Govindaraju, Venu"
    ]
  },
  {
    "id": "3d779cae2d46cf6a8a99a35ba4167977",
    "title": "Ranking via Robust Binary Classification",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/3d779cae2d46cf6a8a99a35ba4167977-Paper.pdf",
    "abstract": "We propose RoBiRank, a ranking algorithm that is motivated by observing a close connection between evaluation metrics for learning to rank and loss functions for robust classification. The algorithm shows a very competitive performance on standard benchmark datasets against other representative algorithms in the literature. Further, in large scale problems where explicit feature vectors and scores are not given, our algorithm can be efficiently parallelized across a large number of machines; for a task that requires 386,133 x 49,824,519 pairwise interactions between items to be ranked, our algorithm finds solutions that are of dramatically higher quality than that can be found by a state-of-the-art competitor algorithm, given the same amount of wall-clock time for computation.",
    "authors": [
      "Yun, Hyokun",
      "Raman, Parameswaran",
      "Vishwanathan, S."
    ]
  },
  {
    "id": "3f67fd97162d20e6fe27748b5b372509",
    "title": "Learning the Learning Rate for Prediction with Expert Advice",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/3f67fd97162d20e6fe27748b5b372509-Paper.pdf",
    "abstract": "Most standard algorithms for prediction with expert advice depend on a parameter called the learning rate. This learning rate needs to be large enough to fit the data well, but small enough to prevent overfitting. For the exponential weights algorithm, a sequence of prior work has established theoretical guarantees for higher and higher data-dependent tunings of the learning rate, which allow for increasingly aggressive learning. But in practice such theoretical tunings often still perform worse (as measured by their regret) than ad hoc tuning with an even higher learning rate. To close the gap between theory and practice we introduce an approach to learn the learning rate. Up to a factor that is at most (poly)logarithmic in the number of experts and the inverse of the learning rate, our method performs as well as if we would know the empirically best learning rate from a large range that includes both conservative small values and values that are much higher than those for which formal guarantees were previously available. Our method employs a grid of learning rates, yet runs in linear time regardless of the size of the grid.",
    "authors": [
      "Koolen, Wouter M.",
      "van Erven, Tim",
      "Gr\u00fcnwald, Peter"
    ]
  },
  {
    "id": "3fab5890d8113d0b5a4178201dc842ad",
    "title": "Beta-Negative Binomial Process and Exchangeable \ufffcRandom Partitions for Mixed-Membership Modeling",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/3fab5890d8113d0b5a4178201dc842ad-Paper.pdf",
    "abstract": "The beta-negative binomial process (BNBP), an integer-valued stochastic process, is employed to partition a count vector into a latent random count matrix. As the marginal probability distribution of the BNBP that governs the exchangeable random partitions of grouped data has not yet been developed, current inference for the BNBP has to truncate the number of atoms of the beta process. This paper introduces an exchangeable partition probability function to explicitly describe how the BNBP clusters the data points of each group into a random number of exchangeable partitions, which are shared across all the groups. A fully collapsed Gibbs sampler is developed for the BNBP, leading to a novel nonparametric Bayesian topic model that is distinct from existing ones, with simple implementation, fast convergence, good mixing, and state-of-the-art predictive performance.",
    "authors": [
      "Zhou, Mingyuan"
    ]
  },
  {
    "id": "3fe94a002317b5f9259f82690aeea4cd",
    "title": "Learning Deep Features for Scene Recognition using Places Database",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/3fe94a002317b5f9259f82690aeea4cd-Paper.pdf",
    "abstract": "Scene recognition is one of the hallmark tasks of computer vision, allowing definition of a context for object recognition. Whereas the tremendous recent progress in object recognition tasks is due to the availability of large datasets like ImageNet and the rise of Convolutional Neural Networks (CNNs) for learning high-level features, performance at scene recognition has not attained the same level of success. This may be because current deep features trained from ImageNet are not competitive enough for such tasks. Here, we introduce a new scene-centric database called Places with over 7 million labeled pictures of scenes. We propose new methods to compare the density and diversity of image datasets and show that Places is as dense as other scene datasets and has more diversity. Using CNN, we learn deep features for scene recognition tasks, and establish new state-of-the-art results on several scene-centric datasets. A visualization of the CNN layers' responses allows us to show differences in the internal representations of object-centric and scene-centric networks.",
    "authors": [
      "Zhou, Bolei",
      "Lapedriza, Agata",
      "Xiao, Jianxiong",
      "Torralba, Antonio",
      "Oliva, Aude"
    ]
  },
  {
    "id": "40008b9a5380fcacce3976bf7c08af5b",
    "title": "Efficient Sampling for Learning Sparse Additive Models in High Dimensions",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/40008b9a5380fcacce3976bf7c08af5b-Paper.pdf",
    "abstract": "We consider the problem of learning sparse additive models, i.e., functions of the form: $f(\\vecx) = \\sum_{l \\in S} \\phi_{l}(x_l)$, $\\vecx \\in \\matR^d$ from point queries of $f$. Here $S$ is an unknown subset of coordinate variables with $\\abs{S} = k \\ll d$. Assuming $\\phi_l$'s to be smooth, we propose a set of points at which to sample $f$ and an efficient randomized algorithm that recovers a \\textit{uniform approximation} to each unknown $\\phi_l$. We provide a rigorous theoretical analysis of our scheme along with sample complexity bounds. Our algorithm utilizes recent results from compressive sensing theory along with a novel convex quadratic program for recovering robust uniform approximations to univariate functions, from point queries corrupted with arbitrary bounded noise. Lastly we theoretically analyze the impact of noise -- either arbitrary but bounded, or stochastic -- on the performance of our algorithm.",
    "authors": [
      "Tyagi, Hemant",
      "G\u00e4rtner, Bernd",
      "Krause, Andreas"
    ]
  },
  {
    "id": "4122cb13c7a474c1976c9706ae36521d",
    "title": "A framework for studying synaptic plasticity with neural spike train data",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/4122cb13c7a474c1976c9706ae36521d-Paper.pdf",
    "abstract": "Learning and memory in the brain are implemented by complex, time-varying changes in neural circuitry. The computational rules according to which synaptic weights change over time are the subject of much research, and are not precisely understood. Until recently, limitations in experimental methods have made it challenging to test hypotheses about synaptic plasticity on a large scale. However, as such data become available and these barriers are lifted, it becomes necessary to develop analysis techniques to validate plasticity models. Here, we present a highly extensible framework for modeling arbitrary synaptic plasticity rules on spike train data in populations of interconnected neurons. We treat synaptic weights as a (potentially nonlinear) dynamical system embedded in a fully-Bayesian generalized linear model (GLM). In addition, we provide an algorithm for inferring synaptic weight trajectories alongside the parameters of the GLM and of the learning rules. Using this method, we perform model comparison of two proposed variants of the well-known spike-timing-dependent plasticity (STDP) rule, where nonlinear effects play a substantial role. On synthetic data generated from the biophysical simulator NEURON, we show that we can recover the weight trajectories, the pattern of connectivity, and the underlying learning rules.",
    "authors": [
      "Linderman, Scott",
      "Stock, Christopher H.",
      "Adams, Ryan P."
    ]
  },
  {
    "id": "41a60377ba920919939d83326ebee5a1",
    "title": "Real-Time Decoding of an Integrate and Fire Encoder",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/41a60377ba920919939d83326ebee5a1-Paper.pdf",
    "abstract": "Neuronal encoding models range from the detailed biophysically-based Hodgkin Huxley model, to the statistical linear time invariant model specifying firing rates in terms of the extrinsic signal. Decoding the former becomes intractable, while the latter does not adequately capture the nonlinearities present in the neuronal encoding system. For use in practical applications, we wish to record the output of neurons, namely spikes, and decode this signal fast in order to drive a machine, for example a prosthetic device. Here, we introduce a causal, real-time decoder of the biophysically-based Integrate and Fire encoding neuron model. We show that the upper bound of the real-time reconstruction error decreases polynomially in time, and that the L2 norm of the error is bounded by a constant that depends on the density of the spikes, as well as the bandwidth and the decay of the input signal. We numerically validate the effect of these parameters on the reconstruction error.",
    "authors": [
      "Saxena, Shreya",
      "Dahleh, Munther"
    ]
  },
  {
    "id": "42a0e188f5033bc65bf8d78622277c4e",
    "title": "Parallel Direction Method of Multipliers",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/42a0e188f5033bc65bf8d78622277c4e-Paper.pdf",
    "abstract": "We consider the problem of minimizing block-separable convex functions subject to linear constraints. While the Alternating Direction Method of Multipliers (ADMM) for two-block linear constraints has been intensively studied both theoretically and empirically, in spite of some preliminary work, effective generalizations of ADMM to multiple blocks is still unclear. In this paper, we propose a parallel randomized block coordinate method named Parallel Direction Method of Multipliers (PDMM) to solve the optimization problems with multi-block linear constraints. PDMM randomly updates some blocks in parallel, behaving like parallel randomized block coordinate descent. We establish the global convergence and the iteration complexity for PDMM with constant step size. We also show that PDMM can do randomized block coordinate descent on overlapping blocks. Experimental results show that PDMM performs better than state-of-the-arts methods in two applications, robust principal component analysis and overlapping group lasso.",
    "authors": [
      "Wang, Huahua",
      "Banerjee, Arindam",
      "Luo, Zhi-Quan"
    ]
  },
  {
    "id": "43fa7f58b7eac7ac872209342e62e8f1",
    "title": "Spectral Methods for Supervised Topic Models",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/43fa7f58b7eac7ac872209342e62e8f1-Paper.pdf",
    "abstract": "Supervised topic models simultaneously model the latent topic structure of large collections of documents and a response variable associated with each document. Existing inference methods are based on either variational approximation or Monte Carlo sampling. This paper presents a novel spectral decomposition algorithm to recover the parameters of supervised latent Dirichlet allocation (sLDA) models. The Spectral-sLDA algorithm is provably correct and computationally efficient. We prove a sample complexity bound and subsequently derive a sufficient condition for the identifiability of sLDA. Thorough experiments on a diverse range of synthetic and real-world datasets verify the theory and demonstrate the practical effectiveness of the algorithm.",
    "authors": [
      "Wang, Yining",
      "Zhu, Jun"
    ]
  },
  {
    "id": "43feaeeecd7b2fe2ae2e26d917b6477d",
    "title": "Exclusive Feature Learning on Arbitrary Structures via $\\ell_{1,2}$-norm",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/43feaeeecd7b2fe2ae2e26d917b6477d-Paper.pdf",
    "abstract": "Group lasso is widely used to enforce the structural sparsity, which achieves the sparsity at inter-group level. In this paper, we propose a new formulation called ``exclusive group lasso'', which brings out sparsity at intra-group level in the context of feature selection. The proposed exclusive group lasso is applicable on any feature structures, regardless of their overlapping or non-overlapping structures. We give analysis on the properties of exclusive group lasso, and propose an effective iteratively re-weighted algorithm to solve the corresponding optimization problem with rigorous convergence analysis. We show applications of exclusive group lasso for uncorrelated feature selection. Extensive experiments on both synthetic and real-world datasets indicate the good performance of proposed methods.",
    "authors": [
      "Kong, Deguang",
      "Fujimaki, Ryohei",
      "Liu, Ji",
      "Nie, Feiping",
      "Ding, Chris"
    ]
  },
  {
    "id": "443cb001c138b2561a0d90720d6ce111",
    "title": "Non-convex Robust PCA",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/443cb001c138b2561a0d90720d6ce111-Paper.pdf",
    "abstract": "We propose a new provable method for robust PCA, where the task is to recover a low-rank matrix, which is corrupted with sparse perturbations. Our method consists of simple alternating projections onto the set of low rank and sparse matrices with intermediate de-noising steps. We prove correct recovery of the low rank and sparse components under tight recovery conditions, which match those for the state-of-art convex relaxation techniques. Our method is extremely simple to implement and has low computational complexity. For a $m \\times n$ input matrix (say m \\geq n), our method has O(r^2 mn\\log(1/\\epsilon)) running time, where $r$ is the rank of the low-rank component and $\\epsilon$ is the accuracy. In contrast, the convex relaxation methods have a running time O(mn^2/\\epsilon), which is not scalable to large problem instances. Our running time nearly matches that of the usual PCA (i.e. non robust), which is O(rmn\\log (1/\\epsilon)). Thus, we achieve ``best of both the worlds'', viz low computational complexity and provable recovery for robust PCA. Our analysis represents one of the few instances of global convergence guarantees for non-convex methods.",
    "authors": [
      "Netrapalli, Praneeth",
      "U N, Niranjan",
      "Sanghavi, Sujay",
      "Anandkumar, Animashree",
      "Jain, Prateek"
    ]
  },
  {
    "id": "4462bf0ddbe0d0da40e1e828ebebeb11",
    "title": "Expectation-Maximization for Learning Determinantal Point Processes",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/4462bf0ddbe0d0da40e1e828ebebeb11-Paper.pdf",
    "abstract": "A determinantal point process (DPP) is a probabilistic model of set diversity compactly parameterized by a positive semi-definite kernel matrix. To fit a DPP to a given task, we would like to learn the entries of its kernel matrix by maximizing the log-likelihood of the available data. However, log-likelihood is non-convex in the entries of the kernel matrix, and this learning problem is conjectured to be NP-hard. Thus, previous work has instead focused on more restricted convex learning settings: learning only a single weight for each row of the kernel matrix, or learning weights for a linear combination of DPPs with fixed kernel matrices. In this work we propose a novel algorithm for learning the full kernel matrix. By changing the kernel parameterization from matrix entries to eigenvalues and eigenvectors, and then lower-bounding the likelihood in the manner of expectation-maximization algorithms, we obtain an effective optimization procedure. We test our method on a real-world product recommendation task, and achieve relative gains of up to 16.5% in test log-likelihood compared to the naive approach of maximizing likelihood by projected gradient ascent on the entries of the kernel matrix.",
    "authors": [
      "Gillenwater, Jennifer A.",
      "Kulesza, Alex",
      "Fox, Emily",
      "Taskar, Ben"
    ]
  },
  {
    "id": "4558dbb6f6f8bb2e16d03b85bde76e2c",
    "title": "Estimation with Norm Regularization",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/4558dbb6f6f8bb2e16d03b85bde76e2c-Paper.pdf",
    "abstract": "Analysis of estimation error and associated structured statistical recovery based on norm regularized regression, e.g., Lasso, needs to consider four aspects: the norm, the loss function, the design matrix, and the noise vector. This paper presents generalizations of such estimation error analysis on all four aspects, compared to the existing literature. We characterize the restricted error set, establish relations between error sets for the constrained and regularized problems, and present an estimation error bound applicable to {\\em any} norm. Precise characterizations of the bound are presented for a variety of noise vectors, design matrices, including sub-Gaussian, anisotropic, and dependent samples, and loss functions, including least squares and generalized linear models. Gaussian widths, as a measure of size of suitable sets, and associated tools play a key role in our generalized analysis.",
    "authors": [
      "Banerjee, Arindam",
      "Chen, Sheng",
      "Fazayeli, Farideh",
      "Sivakumar, Vidyashankar"
    ]
  },
  {
    "id": "459a4ddcb586f24efd9395aa7662bc7c",
    "title": "Sparse Random Feature Algorithm as Coordinate Descent in Hilbert Space",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/459a4ddcb586f24efd9395aa7662bc7c-Paper.pdf",
    "abstract": "In this paper, we propose a Sparse Random Feature algorithm, which learns a sparse non-linear predictor by minimizing an $\\ell_1$-regularized objective function over the Hilbert Space induced from kernel function. By interpreting the algorithm as Randomized Coordinate Descent in the infinite-dimensional space, we show the proposed approach converges to a solution comparable within $\\eps$-precision to exact kernel method by drawing $O(1/\\eps)$ number of random features, contrasted to the $O(1/\\eps^2)$-type convergence achieved by Monte-Carlo analysis in current Random Feature literature. In our experiments, the Sparse Random Feature algorithm obtains sparse solution that requires less memory and prediction time while maintains comparable performance on tasks of regression and classification. In the meantime, as an approximate solver for infinite-dimensional $\\ell_1$-regularized problem, the randomized approach converges to better solution than Boosting approach when the greedy step of Boosting cannot be performed exactly.",
    "authors": [
      "Yen, Ian En-Hsu",
      "Lin, Ting-Wei",
      "Lin, Shou-De",
      "Ravikumar, Pradeep K.",
      "Dhillon, Inderjit S."
    ]
  },
  {
    "id": "4671aeaf49c792689533b00664a5c3ef",
    "title": "Nonparametric Bayesian inference on multivariate exponential families",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/4671aeaf49c792689533b00664a5c3ef-Paper.pdf",
    "abstract": "We develop a model by choosing the maximum entropy distribution from the set of models satisfying certain smoothness and independence criteria; we show that inference on this model generalizes local kernel estimation to the context of Bayesian inference on stochastic processes. Our model enables Bayesian inference in contexts when standard techniques like Gaussian process inference are too expensive to apply. Exact inference on our model is possible for any likelihood function from the exponential family. Inference is then highly efficient, requiring only O(log N) time and O(N) space at run time. We demonstrate our algorithm on several problems and show quantifiable improvement in both speed and performance relative to models based on the Gaussian process.",
    "authors": [
      "Vega-Brown, William R.",
      "Doniec, Marek",
      "Roy, Nicholas G."
    ]
  },
  {
    "id": "46771d1f432b42343f56f791422a4991",
    "title": "On Communication Cost of Distributed Statistical Estimation and Dimensionality",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/46771d1f432b42343f56f791422a4991-Paper.pdf",
    "abstract": "We explore the connection between dimensionality and communication cost in distributed learning problems. Specifically we study the problem of estimating the mean $\\vectheta$ of an unknown $d$ dimensional gaussian distribution in the distributed setting. In this problem, the samples from the unknown distribution are distributed among $m$ different machines. The goal is to estimate the mean $\\vectheta$ at the optimal minimax rate while communicating as few bits as possible. We show that in this setting, the communication cost scales linearly in the number of dimensions i.e. one needs to deal with different dimensions individually. Applying this result to previous lower bounds for one dimension in the interactive setting \\cite{ZDJW13} and to our improved bounds for the simultaneous setting, we prove new lower bounds of $\\Omega(md/\\log(m))$ and $\\Omega(md)$ for the bits of communication needed to achieve the minimax squared loss, in the interactive and simultaneous settings respectively. To complement, we also demonstrate an interactive protocol achieving the minimax squared loss with $O(md)$ bits of communication, which improves upon the simple simultaneous protocol by a logarithmic factor. Given the strong lower bounds in the general setting, we initiate the study of the distributed parameter estimation problems with structured parameters. Specifically, when the parameter is promised to be $s$-sparse, we show a simple thresholding based protocol that achieves the same squared loss while saving a $d/s$ factor of communication. We conjecture that the tradeoff between communication and squared loss demonstrated by this protocol is essentially optimal up to logarithmic factor.",
    "authors": [
      "Garg, Ankit",
      "Ma, Tengyu",
      "Nguyen, Huy"
    ]
  },
  {
    "id": "46922a0880a8f11f8f69cbb52b1396be",
    "title": "A Block-Coordinate Descent Approach for Large-scale Sparse Inverse Covariance Estimation",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/46922a0880a8f11f8f69cbb52b1396be-Paper.pdf",
    "abstract": "The sparse inverse covariance estimation problem arises in many statistical applications in machine learning and signal processing. In this problem, the inverse of a covariance matrix of a multivariate normal distribution is estimated, assuming that it is sparse. An $\\ell_1$ regularized log-determinant optimization problem is typically solved to approximate such matrices. Because of memory limitations, most existing algorithms are unable to handle large scale instances of this problem. In this paper we present a new block-coordinate descent approach for solving the problem for large-scale data sets. Our method treats the sought matrix block-by-block using quadratic approximations, and we show that this approach has advantages over existing methods in several aspects. Numerical experiments on both synthetic and real gene expression data demonstrate that our approach outperforms the existing state of the art methods, especially for large-scale problems.",
    "authors": [
      "Treister, Eran",
      "Turek, Javier S."
    ]
  },
  {
    "id": "46ba9f2a6976570b0353203ec4474217",
    "title": "Local Decorrelation For Improved Pedestrian Detection",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/46ba9f2a6976570b0353203ec4474217-Paper.pdf",
    "abstract": "Even with the advent of more sophisticated, data-hungry methods, boosted decision trees remain extraordinarily successful for fast rigid object detection, achieving top accuracy on numerous datasets. While effective, most boosted detectors use decision trees with orthogonal (single feature) splits, and the topology of the resulting decision boundary may not be well matched to the natural topology of the data. Given highly correlated data, decision trees with oblique (multiple feature) splits can be effective. Use of oblique splits, however, comes at considerable computational expense. Inspired by recent work on discriminative decorrelation of HOG features, we instead propose an efficient feature transform that removes correlations in local neighborhoods. The result is an overcomplete but locally decorrelated representation ideally suited for use with orthogonal decision trees. In fact, orthogonal trees with our locally decorrelated features outperform oblique trees trained over the original features at a fraction of the computational cost. The overall improvement in accuracy is dramatic: on the Caltech Pedestrian Dataset, we reduce false positives nearly tenfold over the previous state-of-the-art.",
    "authors": [
      "Nam, Woonhyun",
      "Dollar, Piotr",
      "Han, Joon Hee"
    ]
  },
  {
    "id": "4d6e4749289c4ec58c0063a90deb3964",
    "title": "Graph Clustering With Missing Data: Convex Algorithms and Analysis",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/4d6e4749289c4ec58c0063a90deb3964-Paper.pdf",
    "abstract": "We consider the problem of finding clusters in an unweighted graph, when the graph is partially observed. We analyze two programs, one which works for dense graphs and one which works for both sparse and dense graphs, but requires some a priori knowledge of the total cluster size, that are based on the convex optimization approach for low-rank matrix recovery using nuclear norm minimization. For the commonly used Stochastic Block Model, we obtain \\emph{explicit} bounds on the parameters of the problem (size and sparsity of clusters, the amount of observed data) and the regularization parameter characterize the success and failure of the programs. We corroborate our theoretical findings through extensive simulations. We also run our algorithm on a real data set obtained from crowdsourcing an image classification task on the Amazon Mechanical Turk, and observe significant performance improvement over traditional methods such as k-means.",
    "authors": [
      "Korlakai Vinayak, Ramya",
      "Oymak, Samet",
      "Hassibi, Babak"
    ]
  },
  {
    "id": "4e2545f819e67f0615003dd7e04a6087",
    "title": "Spatio-temporal Representations of Uncertainty in Spiking Neural Networks",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/4e2545f819e67f0615003dd7e04a6087-Paper.pdf",
    "abstract": "It has been long argued that, because of inherent ambiguity and noise, the brain needs to represent uncertainty in the form of probability distributions. The neural encoding of such distributions remains however highly controversial. Here we present a novel circuit model for representing multidimensional real-valued distributions using a spike based spatio-temporal code. Our model combines the computational advantages of the currently competing models for probabilistic codes and exhibits realistic neural responses along a variety of classic measures. Furthermore, the model highlights the challenges associated with interpreting neural activity in relation to behavioral uncertainty and points to alternative population-level approaches for the experimental validation of distributed representations.",
    "authors": [
      "Savin, Cristina",
      "Den\u00e8ve, Sophie"
    ]
  },
  {
    "id": "4e6cd95227cb0c280e99a195be5f6615",
    "title": "Hamming Ball Auxiliary Sampling for Factorial Hidden Markov Models",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/4e6cd95227cb0c280e99a195be5f6615-Paper.pdf",
    "abstract": "We introduce a novel sampling algorithm for Markov chain Monte Carlo-based Bayesian inference for factorial hidden Markov models. This algorithm is based on an auxiliary variable construction that restricts the model space allowing iterative exploration in polynomial time. The sampling approach overcomes limitations with common conditional Gibbs samplers that use asymmetric updates and become easily trapped in local modes. Instead, our method uses symmetric moves that allows joint updating of the latent sequences and improves mixing. We illustrate the application of the approach with simulated and a real data example.",
    "authors": [
      "Titsias RC AUEB, Michalis",
      "Yau, Christopher"
    ]
  },
  {
    "id": "4e732ced3463d06de0ca9a15b6153677",
    "title": "The Infinite Mixture of Infinite Gaussian Mixtures",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/4e732ced3463d06de0ca9a15b6153677-Paper.pdf",
    "abstract": "Dirichlet process mixture of Gaussians (DPMG) has been used in the literature for clustering and density estimation problems. However, many real-world data exhibit cluster distributions that cannot be captured by a single Gaussian. Modeling such data sets by DPMG creates several extraneous clusters even when clusters are relatively well-defined. Herein, we present the infinite mixture of infinite Gaussian mixtures (I2GMM) for more flexible modeling of data sets with skewed and multi-modal cluster distributions. Instead of using a single Gaussian for each cluster as in the standard DPMG model, the generative model of I2GMM uses a single DPMG for each cluster. The individual DPMGs are linked together through centering of their base distributions at the atoms of a higher level DP prior. Inference is performed by a collapsed Gibbs sampler that also enables partial parallelization. Experimental results on several artificial and real-world data sets suggest the proposed I2GMM model can predict clusters more accurately than existing variational Bayes and Gibbs sampler versions of DPMG.",
    "authors": [
      "Yerebakan, Halid Z.",
      "Rajwa, Bartek",
      "Dundar, Murat"
    ]
  },
  {
    "id": "4e87337f366f72daa424dae11df0538c",
    "title": "Partition-wise Linear Models",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/4e87337f366f72daa424dae11df0538c-Paper.pdf",
    "abstract": "Region-specific linear models are widely used in practical applications because of their non-linear but highly interpretable model representations. One of the key challenges in their use is non-convexity in simultaneous optimization of regions and region-specific models. This paper proposes novel convex region-specific linear models, which we refer to as partition-wise linear models. Our key ideas are 1) assigning linear models not to regions but to partitions (region-specifiers) and representing region-specific linear models by linear combinations of partition-specific models, and 2) optimizing regions via partition selection from a large number of given partition candidates by means of convex structured regularizations. In addition to providing initialization-free globally-optimal solutions, our convex formulation makes it possible to derive a generalization bound and to use such advanced optimization techniques as proximal methods and decomposition of the proximal maps for sparsity-inducing regularizations. Experimental results demonstrate that our partition-wise linear models perform better than or are at least competitive with state-of-the-art region-specific or locally linear models.",
    "authors": [
      "Oiwa, Hidekazu",
      "Fujimaki, Ryohei"
    ]
  },
  {
    "id": "4f398cb9d6bc79ae567298335b51ba8a",
    "title": "Convex Deep Learning via Normalized Kernels",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/4f398cb9d6bc79ae567298335b51ba8a-Paper.pdf",
    "abstract": "Deep learning has been a long standing pursuit in machine learning, which until recently was hampered by unreliable training methods before the discovery of improved heuristics for embedded layer training. A complementary research strategy is to develop alternative modeling architectures that admit efficient training methods while expanding the range of representable structures toward deep models. In this paper, we develop a new architecture for nested nonlinearities that allows arbitrarily deep compositions to be trained to global optimality. The approach admits both parametric and nonparametric forms through the use of normalized kernels to represent each latent layer. The outcome is a fully convex formulation that is able to capture compositions of trainable nonlinear layers to arbitrary depth.",
    "authors": [
      "Aslan, \u00d6zlem",
      "Zhang, Xinhua",
      "Schuurmans, Dale"
    ]
  },
  {
    "id": "4f6ffe13a5d75b2d6a3923922b3922e5",
    "title": "Discovering Structure in High-Dimensional Data Through Correlation Explanation",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/4f6ffe13a5d75b2d6a3923922b3922e5-Paper.pdf",
    "abstract": "We introduce a method to learn a hierarchy of successively more abstract representations of complex data based on optimizing an information-theoretic objective. Intuitively, the optimization searches for a set of latent factors that best explain the correlations in the data as measured by multivariate mutual information. The method is unsupervised, requires no model assumptions, and scales linearly with the number of variables which makes it an attractive approach for very high dimensional systems. We demonstrate that Correlation Explanation (CorEx) automatically discovers meaningful structure for data from diverse sources including personality tests, DNA, and human language.",
    "authors": [
      "Ver Steeg, Greg",
      "Galstyan, Aram"
    ]
  },
  {
    "id": "50905d7b2216bfeccb5b41016357176b",
    "title": "Difference of Convex Functions Programming for Reinforcement Learning",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/50905d7b2216bfeccb5b41016357176b-Paper.pdf",
    "abstract": "Large Markov Decision Processes (MDPs) are usually solved using Approximate Dynamic Programming (ADP) methods such as Approximate Value Iteration (AVI) or Approximate Policy Iteration (API). The main contribution of this paper is to show that, alternatively, the optimal state-action value function can be estimated using Difference of Convex functions (DC) Programming. To do so, we study the minimization of a norm of the Optimal Bellman Residual (OBR) $T^*Q-Q$, where $T^*$ is the so-called optimal Bellman operator. Controlling this residual allows controlling the distance to the optimal action-value function, and we show that minimizing an empirical norm of the OBR is consistant in the Vapnik sense. Finally, we frame this optimization problem as a DC program. That allows envisioning using the large related literature on DC Programming to address the Reinforcement Leaning (RL) problem.",
    "authors": [
      "Piot, Bilal",
      "Geist, Matthieu",
      "Pietquin, Olivier"
    ]
  },
  {
    "id": "522a9ae9a99880d39e5daec35375e999",
    "title": "Local Linear Convergence of Forward--Backward under Partial Smoothness",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/522a9ae9a99880d39e5daec35375e999-Paper.pdf",
    "abstract": "In this paper, we consider the Forward--Backward proximal splitting algorithm to minimize the sum of two proper closed convex functions, one of which having a Lipschitz continuous gradient and the other being partly smooth relatively to an active manifold $\\mathcal{M}$. We propose a generic framework in which we show that the Forward--Backward (i) correctly identifies the active manifold $\\mathcal{M}$ in a finite number of iterations, and then (ii) enters a local linear convergence regime that we characterize precisely. This gives a grounded and unified explanation to the typical behaviour that has been observed numerically for many problems encompassed in our framework, including the Lasso, the group Lasso, the fused Lasso and the nuclear norm regularization to name a few. These results may have numerous applications including in signal/image processing processing, sparse recovery and machine learning.",
    "authors": [
      "Liang, Jingwei",
      "Fadili, Jalal",
      "Peyr\u00e9, Gabriel"
    ]
  },
  {
    "id": "52947e0ade57a09e4a1386d08f17b656",
    "title": "Improved Distributed Principal Component Analysis",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/52947e0ade57a09e4a1386d08f17b656-Paper.pdf",
    "abstract": "We study the distributed computing setting in which there are multiple servers, each holding a set of points, who wish to compute functions on the union of their point sets. A key task in this setting is Principal Component Analysis (PCA), in which the servers would like to compute a low dimensional subspace capturing as much of the variance of the union of their point sets as possible. Given a procedure for approximate PCA, one can use it to approximately solve problems such as $k$-means clustering and low rank approximation. The essential properties of an approximate distributed PCA algorithm are its communication cost and computational efficiency for a given desired accuracy in downstream applications. We give new algorithms and analyses for distributed PCA which lead to improved communication and computational costs for $k$-means clustering and related problems. Our empirical study on real world data shows a speedup of orders of magnitude, preserving communication with only a negligible degradation in solution quality. Some of these techniques we develop, such as input-sparsity subspace embeddings with high correctness probability with a dimension and sparsity independent of the error probability, may be of independent interest.",
    "authors": [
      "Liang, Yingyu",
      "Balcan, Maria-Florina F.",
      "Kanchanapally, Vandana",
      "Woodruff, David"
    ]
  },
  {
    "id": "535ab76633d94208236a2e829ea6d888",
    "title": "Reputation-based Worker Filtering in Crowdsourcing",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/535ab76633d94208236a2e829ea6d888-Paper.pdf",
    "abstract": "In this paper, we study the problem of aggregating noisy labels from crowd workers to infer the underlying true labels of binary tasks. Unlike most prior work which has examined this problem under the random worker paradigm, we consider a much broader class of {\\em adversarial} workers with no specific assumptions on their labeling strategy. Our key contribution is the design of a computationally efficient reputation algorithm to identify and filter out these adversarial workers in crowdsourcing systems. Our algorithm uses the concept of optimal semi-matchings in conjunction with worker penalties based on label disagreements, to assign a reputation score for every worker. We provide strong theoretical guarantees for deterministic adversarial strategies as well as the extreme case of {\\em sophisticated} adversaries where we analyze the worst-case behavior of our algorithm. Finally, we show that our reputation algorithm can significantly improve the accuracy of existing label aggregation algorithms in real-world crowdsourcing datasets.",
    "authors": [
      "Jagabathula, Srikanth",
      "Subramanian, Lakshminarayanan",
      "Venkataraman, Ashwin"
    ]
  },
  {
    "id": "54229abfcfa5649e7003b83dd4755294",
    "title": "large scale canonical correlation analysis with iterative least squares",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/54229abfcfa5649e7003b83dd4755294-Paper.pdf",
    "abstract": "Canonical Correlation Analysis (CCA) is a widely used statistical tool with both well established theory and favorable performance for a wide range of machine learning problems. However, computing CCA for huge datasets can be very slow since it involves implementing QR decomposition or singular value decomposition of huge matrices. In this paper we introduce L-CCA, an iterative algorithm which can compute CCA fast on huge sparse datasets. Theory on both the asymptotic convergence and finite time accuracy of L-CCA are established. The experiments also show that L-CCA outperform other fast CCA approximation schemes on two real datasets.",
    "authors": [
      "Lu, Yichao",
      "Foster, Dean P."
    ]
  },
  {
    "id": "5487315b1286f907165907aa8fc96619",
    "title": "Analysis of Variational Bayesian Latent Dirichlet Allocation: Weaker Sparsity Than MAP",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/5487315b1286f907165907aa8fc96619-Paper.pdf",
    "abstract": "Latent Dirichlet allocation (LDA) is a popular generative model of various objects such as texts and images, where an object is expressed as a mixture of latent topics. In this paper, we theoretically investigate variational Bayesian (VB) learning in LDA. More specifically, we analytically derive the leading term of the VB free energy under an asymptotic setup, and show that there exist transition thresholds in Dirichlet hyperparameters around which the sparsity-inducing behavior drastically changes. Then we further theoretically reveal the notable phenomenon that VB tends to induce weaker sparsity than MAP in the LDA model, which is opposed to other models. We experimentally demonstrate the practical validity of our asymptotic theory on real-world Last.FM music data.",
    "authors": [
      "Nakajima, Shinichi",
      "Sato, Issei",
      "Sugiyama, Masashi",
      "Watanabe, Kazuho",
      "Kobayashi, Hiroko"
    ]
  },
  {
    "id": "555d6702c950ecb729a966504af0a635",
    "title": "Iterative Neural Autoregressive Distribution Estimator NADE-k",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/555d6702c950ecb729a966504af0a635-Paper.pdf",
    "abstract": "Training of the neural autoregressive density estimator (NADE) can be viewed as doing one step of probabilistic inference on missing values in data. We propose a new model that extends this inference scheme to multiple steps, arguing that it is easier to learn to improve a reconstruction in $k$ steps rather than to learn to reconstruct in a single inference step. The proposed model is an unsupervised building block for deep learning that combines the desirable properties of NADE and multi-predictive training: (1) Its test likelihood can be computed analytically, (2) it is easy to generate independent samples from it, and (3) it uses an inference engine that is a superset of variational inference for Boltzmann machines. The proposed NADE-k is competitive with the state-of-the-art in density estimation on the two datasets tested.",
    "authors": [
      "Raiko, Tapani",
      "Li, Yao",
      "Cho, Kyunghyun",
      "Bengio, Yoshua"
    ]
  },
  {
    "id": "556f391937dfd4398cbac35e050a2177",
    "title": "Reducing the Rank in Relational Factorization Models by Including Observable Patterns",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/556f391937dfd4398cbac35e050a2177-Paper.pdf",
    "abstract": "Tensor factorizations have become popular methods for learning from multi-relational data. In this context, the rank of a factorization is an important parameter that determines runtime as well as generalization ability. To determine conditions under which factorization is an efficient approach for learning from relational data, we derive upper and lower bounds on the rank required to recover adjacency tensors. Based on our findings, we propose a novel additive tensor factorization model for learning from latent and observable patterns in multi-relational data and present a scalable algorithm for computing the factorization. Experimentally, we show that the proposed approach does not only improve the predictive performance over pure latent variable methods but that it also reduces the required rank --- and therefore runtime and memory complexity --- significantly.",
    "authors": [
      "Nickel, Maximilian",
      "Jiang, Xueyan",
      "Tresp, Volker"
    ]
  },
  {
    "id": "56468d5607a5aaf1604ff5e15593b003",
    "title": "Deterministic Symmetric Positive Semidefinite Matrix Completion",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/56468d5607a5aaf1604ff5e15593b003-Paper.pdf",
    "abstract": "We consider the problem of recovering a symmetric, positive semidefinite (SPSD) matrix from a subset of its entries, possibly corrupted by noise. In contrast to previous matrix recovery work, we drop the assumption of a random sampling of entries in favor of a deterministic sampling of principal submatrices of the matrix. We develop a set of sufficient conditions for the recovery of a SPSD matrix from a set of its principal submatrices, present necessity results based on this set of conditions and develop an algorithm that can exactly recover a matrix when these conditions are met. The proposed algorithm is naturally generalized to the problem of noisy matrix recovery, and we provide a worst-case bound on reconstruction error for this scenario. Finally, we demonstrate the algorithm's utility on noiseless and noisy simulated datasets.",
    "authors": [
      "Bishop, William E.",
      "Yu, Byron M."
    ]
  },
  {
    "id": "5705e1164a8394aace6018e27d20d237",
    "title": "Distributed Parameter Estimation in Probabilistic Graphical Models",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/5705e1164a8394aace6018e27d20d237-Paper.pdf",
    "abstract": "This paper presents foundational theoretical results on distributed parameter estimation for undirected probabilistic graphical models. It introduces a general condition on composite likelihood decompositions of these models which guarantees the global consistency of distributed estimators, provided the local estimators are consistent.",
    "authors": [
      "Mizrahi, Yariv D.",
      "Denil, Misha",
      "de Freitas, Nando"
    ]
  },
  {
    "id": "571e0f7e2d992e738adff8b1bd43a521",
    "title": "Two-Layer Feature Reduction for Sparse-Group Lasso via Decomposition of Convex Sets",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/571e0f7e2d992e738adff8b1bd43a521-Paper.pdf",
    "abstract": "Sparse-Group Lasso (SGL) has been shown to be a powerful regression technique for simultaneously discovering group and within-group sparse patterns by using a combination of the l1 and l2 norms. However, in large-scale applications, the complexity of the regularizers entails great computational challenges. In this paper, we propose a novel two-layer feature reduction method (TLFre) for SGL via a decomposition of its dual feasible set. The two-layer reduction is able to quickly identify the inactive groups and the inactive features, respectively, which are guaranteed to be absent from the sparse representation and can be removed from the optimization. Existing feature reduction methods are only applicable for sparse models with one sparsity-inducing regularizer. To our best knowledge, TLFre is the first one that is capable of dealing with multiple sparsity-inducing regularizers. Moreover, TLFre has a very low computational cost and can be integrated with any existing solvers. Experiments on both synthetic and real data sets show that TLFre improves the efficiency of SGL by orders of magnitude.",
    "authors": [
      "Wang, Jie",
      "Ye, Jieping"
    ]
  },
  {
    "id": "5751ec3e9a4feab575962e78e006250d",
    "title": "The Large Margin Mechanism for Differentially Private Maximization",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/5751ec3e9a4feab575962e78e006250d-Paper.pdf",
    "abstract": "A basic problem in the design of privacy-preserving algorithms is the \\emph{private maximization problem}: the goal is to pick an item from a universe that (approximately) maximizes a data-dependent function, all under the constraint of differential privacy. This problem has been used as a sub-routine in many privacy-preserving algorithms for statistics and machine learning. Previous algorithms for this problem are either range-dependent---i.e., their utility diminishes with the size of the universe---or only apply to very restricted function classes. This work provides the first general purpose, range-independent algorithm for private maximization that guarantees approximate differential privacy. Its applicability is demonstrated on two fundamental tasks in data mining and machine learning.",
    "authors": [
      "Chaudhuri, Kamalika",
      "Hsu, Daniel J.",
      "Song, Shuang"
    ]
  },
  {
    "id": "57aeee35c98205091e18d1140e9f38cf",
    "title": "Causal Inference through a Witness Protection Program",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/57aeee35c98205091e18d1140e9f38cf-Paper.pdf",
    "abstract": "One of the most fundamental problems in causal inference is the estimation of a causal effect when variables are confounded. This is difficult in an observational study because one has no direct evidence that all confounders have been adjusted for. We introduce a novel approach for estimating causal effects that exploits observational conditional independencies to suggest weak'' paths in a unknown causal graph. The widely used faithfulness condition of Spirtes et al. is relaxed to allow for varying degrees ofpath cancellations'' that will imply conditional independencies but do not rule out the existence of confounding causal paths. The outcome is a posterior distribution over bounds on the average causal effect via a linear programming approach and Bayesian inference. We claim this approach should be used in regular practice to complement other default tools in observational studies.",
    "authors": [
      "Silva, Ricardo",
      "Evans, Robin"
    ]
  },
  {
    "id": "58ae749f25eded36f486bc85feb3f0ab",
    "title": "Self-Adaptable Templates for Feature Coding",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/58ae749f25eded36f486bc85feb3f0ab-Paper.pdf",
    "abstract": "Hierarchical feed-forward networks have been successfully applied in object recognition. At each level of the hierarchy, features are extracted and encoded, followed by a pooling step. Within this processing pipeline, the common trend is to learn the feature coding templates, often referred as codebook entries, filters, or over-complete basis. Recently, an approach that apparently does not use templates has been shown to obtain very promising results. This is the second-order pooling (O2P). In this paper, we analyze O2P as a coding-pooling scheme. We find that at testing phase, O2P automatically adapts the feature coding templates to the input features, rather than using templates learned during the training phase. From this finding, we are able to bring common concepts of coding-pooling schemes to O2P, such as feature quantization. This allows for significant accuracy improvements of O2P in standard benchmarks of image classification, namely Caltech101 and VOC07.",
    "authors": [
      "Boix, Xavier",
      "Roig, Gemma",
      "Diether, Salomon",
      "Gool, Luc V."
    ]
  },
  {
    "id": "58d4d1e7b1e97b258c9ed0b37e02d087",
    "title": "A Framework for Testing Identifiability of Bayesian Models of Perception",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/58d4d1e7b1e97b258c9ed0b37e02d087-Paper.pdf",
    "abstract": "Bayesian observer models are very effective in describing human performance in perceptual tasks, so much so that they are trusted to faithfully recover hidden mental representations of priors, likelihoods, or loss functions from the data. However, the intrinsic degeneracy of the Bayesian framework, as multiple combinations of elements can yield empirically indistinguishable results, prompts the question of model identifiability. We propose a novel framework for a systematic testing of the identifiability of a significant class of Bayesian observer models, with practical applications for improving experimental design. We examine the theoretical identifiability of the inferred internal representations in two case studies. First, we show which experimental designs work better to remove the underlying degeneracy in a time interval estimation task. Second, we find that the reconstructed representations in a speed perception task under a slow-speed prior are fairly robust.",
    "authors": [
      "Acerbi, Luigi",
      "Ma, Wei Ji",
      "Vijayakumar, Sethu"
    ]
  },
  {
    "id": "58e4d44e550d0f7ee0a23d6b02d9b0db",
    "title": "Low Rank Approximation Lower Bounds in Row-Update Streams",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/58e4d44e550d0f7ee0a23d6b02d9b0db-Paper.pdf",
    "abstract": "We study low-rank approximation in the streaming model in which the rows of an $n \\times d$ matrix $A$ are presented one at a time in an arbitrary order. At the end of the stream, the streaming algorithm should output a $k \\times d$ matrix $R$ so that $\\|A-AR^{\\dagger}R\\|_F^2 \\leq (1+\\eps)\\|A-A_k\\|_F^2$, where $A_k$ is the best rank-$k$ approximation to $A$. A deterministic streaming algorithm of Liberty (KDD, 2013), with an improved analysis of Ghashami and Phillips (SODA, 2014), provides such a streaming algorithm using $O(dk/\\epsilon)$ words of space. A natural question is if smaller space is possible. We give an almost matching lower bound of $\\Omega(dk/\\epsilon)$ bits of space, even for randomized algorithms which succeed only with constant probability. Our lower bound matches the upper bound of Ghashami and Phillips up to the word size, improving on a simple $\\Omega(dk)$ space lower bound.",
    "authors": [
      "Woodruff, David"
    ]
  },
  {
    "id": "59b90e1005a220e2ebc542eb9d950b1e",
    "title": "Probabilistic ODE Solvers with Runge-Kutta Means",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/59b90e1005a220e2ebc542eb9d950b1e-Paper.pdf",
    "abstract": "Runge-Kutta methods are the classic family of solvers for ordinary differential equations (ODEs), and the basis for the state of the art. Like most numerical methods, they return point estimates. We construct a family of probabilistic numerical methods that instead return a Gauss-Markov process defining a probability distribution over the ODE solution. In contrast to prior work, we construct this family such that posterior means match the outputs of the Runge-Kutta family exactly, thus inheriting their proven good properties. Remaining degrees of freedom not identified by the match to Runge-Kutta are chosen such that the posterior probability measure fits the observed structure of the ODE. Our results shed light on the structure of Runge-Kutta solvers from a new direction, provide a richer, probabilistic output, have low computational cost, and raise new research questions.",
    "authors": [
      "Schober, Michael",
      "Duvenaud, David K.",
      "Hennig, Philipp"
    ]
  },
  {
    "id": "5bce843dd76db8c939d5323dd3e54ec9",
    "title": "Learning a Concept Hierarchy from Multi-labeled Documents",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/5bce843dd76db8c939d5323dd3e54ec9-Paper.pdf",
    "abstract": "While topic models can discover patterns of word usage in large corpora, it is difficult to meld this unsupervised structure with noisy, human-provided labels, especially when the label space is large. In this paper, we present a model-Label to Hierarchy (L2H)-that can induce a hierarchy of user-generated labels and the topics associated with those labels from a set of multi-labeled documents. The model is robust enough to account for missing labels from untrained, disparate annotators and provide an interpretable summary of an otherwise unwieldy label set. We show empirically the effectiveness of L2H in predicting held-out words and labels for unseen documents.",
    "authors": [
      "Nguyen, Viet-An",
      "Ying, Jordan L.",
      "Resnik, Philip",
      "Chang, Jonathan"
    ]
  },
  {
    "id": "5c04925674920eb58467fb52ce4ef728",
    "title": "Dependent nonparametric trees for dynamic hierarchical clustering",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/5c04925674920eb58467fb52ce4ef728-Paper.pdf",
    "abstract": "Hierarchical clustering methods offer an intuitive and powerful way to model a wide variety of data sets. However, the assumption of a fixed hierarchy is often overly restrictive when working with data generated over a period of time: We expect both the structure of our hierarchy, and the parameters of the clusters, to evolve with time. In this paper, we present a distribution over collections of time-dependent, infinite-dimensional trees that can be used to model evolving hierarchies, and present an efficient and scalable algorithm for performing approximate inference in such a model. We demonstrate the efficacy of our model and inference algorithm on both synthetic data and real-world document corpora.",
    "authors": [
      "Dubey, Kumar Avinava",
      "Ho, Qirong",
      "Williamson, Sinead A.",
      "Xing, Eric P."
    ]
  },
  {
    "id": "5c50b4df4b176845cd235b6a510c6903",
    "title": "A Statistical Decision-Theoretic Framework for Social Choice",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/5c50b4df4b176845cd235b6a510c6903-Paper.pdf",
    "abstract": "In this paper, we take a statistical decision-theoretic viewpoint on social choice, putting a focus on the decision to be made on behalf of a system of agents. In our framework, we are given a statistical ranking model, a decision space, and a loss function defined on (parameter, decision) pairs, and formulate social choice mechanisms as decision rules that minimize expected loss. This suggests a general framework for the design and analysis of new social choice mechanisms. We compare Bayesian estimators, which minimize Bayesian expected loss, for the Mallows model and the Condorcet model respectively, and the Kemeny rule. We consider various normative properties, in addition to computational complexity and asymptotic behavior. In particular, we show that the Bayesian estimator for the Condorcet model satisfies some desired properties such as anonymity, neutrality, and monotonicity, can be computed in polynomial time, and is asymptotically different from the other two rules when the data are generated from the Condorcet model for some ground truth parameter.",
    "authors": [
      "Azari Soufiani, Hossein",
      "Parkes, David C.",
      "Xia, Lirong"
    ]
  },
  {
    "id": "5ca3e9b122f61f8f06494c97b1afccf3",
    "title": "Generative Adversarial Nets",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf",
    "abstract": "We propose a new framework for estimating generative models via adversarial nets, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitatively evaluation of the generated samples.",
    "authors": [
      "Goodfellow, Ian",
      "Pouget-Abadie, Jean",
      "Mirza, Mehdi",
      "Xu, Bing",
      "Warde-Farley, David",
      "Ozair, Sherjil",
      "Courville, Aaron",
      "Bengio, Yoshua"
    ]
  },
  {
    "id": "5cce8dede893813f879b873962fb669f",
    "title": "Delay-Tolerant Algorithms for Asynchronous Distributed Online Learning",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/5cce8dede893813f879b873962fb669f-Paper.pdf",
    "abstract": "We analyze new online gradient descent algorithms for distributed systems with large delays between gradient computations and the corresponding updates. Using insights from adaptive gradient methods, we develop algorithms that adapt not only to the sequence of gradients, but also to the precise update delays that occur. We first give an impractical algorithm that achieves a regret bound that precisely quantifies the impact of the delays. We then analyze AdaptiveRevision, an algorithm that is efficiently implementable and achieves comparable guarantees. The key algorithmic technique is appropriately and efficiently revising the learning rate used for previous gradient steps. Experimental results show when the delays grow large (1000 updates or more), our new algorithms perform significantly better than standard adaptive gradient methods.",
    "authors": [
      "McMahan, Brendan",
      "Streeter, Matthew"
    ]
  },
  {
    "id": "5d616dd38211ebb5d6ec52986674b6e4",
    "title": "Sequential Monte Carlo for Graphical Models",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/5d616dd38211ebb5d6ec52986674b6e4-Paper.pdf",
    "abstract": "We propose a new framework for how to use sequential Monte Carlo (SMC) algorithms for inference in probabilistic graphical models (PGM). Via a sequential decomposition of the PGM we find a sequence of auxiliary distributions defined on a monotonically increasing sequence of probability spaces. By targeting these auxiliary distributions using SMC we are able to approximate the full joint distribution defined by the PGM. One of the key merits of the SMC sampler is that it provides an unbiased estimate of the partition function of the model. We also show how it can be used within a particle Markov chain Monte Carlo framework in order to construct high-dimensional block-sampling algorithms for general PGMs.",
    "authors": [
      "Andersson Naesseth, Christian",
      "Lindsten, Fredrik",
      "Sch\u00f6n, Thomas B."
    ]
  },
  {
    "id": "62889e73828c756c961c5a6d6c01a463",
    "title": "Learning Time-Varying Coverage Functions",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/62889e73828c756c961c5a6d6c01a463-Paper.pdf",
    "abstract": "Coverage functions are an important class of discrete functions that capture laws of diminishing returns. In this paper, we propose a new problem of learning time-varying coverage functions which arise naturally from applications in social network analysis, machine learning, and algorithmic game theory. We develop a novel parametrization of the time-varying coverage function by illustrating the connections with counting processes. We present an efficient algorithm to learn the parameters by maximum likelihood estimation, and provide a rigorous theoretic analysis of its sample complexity. Empirical experiments from information diffusion in social network analysis demonstrate that with few assumptions about the underlying diffusion process, our method performs significantly better than existing approaches on both synthetic and real world data.",
    "authors": [
      "Du, Nan",
      "Liang, Yingyu",
      "Balcan, Maria-Florina F.",
      "Song, Le"
    ]
  },
  {
    "id": "63538fe6ef330c13a05a3ed7e599d5f7",
    "title": "Learning Shuffle Ideals Under Restricted Distributions",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/63538fe6ef330c13a05a3ed7e599d5f7-Paper.pdf",
    "abstract": "The class of shuffle ideals is a fundamental sub-family of regular languages. The shuffle ideal generated by a string set $U$ is the collection of all strings containing some string $u \\in U$ as a (not necessarily contiguous) subsequence. In spite of its apparent simplicity, the problem of learning a shuffle ideal from given data is known to be computationally intractable. In this paper, we study the PAC learnability of shuffle ideals and present positive results on this learning problem under element-wise independent and identical distributions and Markovian distributions in the statistical query model. A constrained generalization to learning shuffle ideals under product distributions is also provided. In the empirical direction, we propose a heuristic algorithm for learning shuffle ideals from given labeled strings under general unrestricted distributions. Experiments demonstrate the advantage for both efficiency and accuracy of our algorithm.",
    "authors": [
      "Chen, Dongqu"
    ]
  },
  {
    "id": "63923f49e5241343aa7acb6a06a751e7",
    "title": "Spectral Clustering of graphs with the Bethe Hessian",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/63923f49e5241343aa7acb6a06a751e7-Paper.pdf",
    "abstract": "Spectral clustering is a standard approach to label nodes on a graph by studying the (largest or lowest) eigenvalues of a symmetric real matrix such as e.g. the adjacency or the Laplacian. Recently, it has been argued that using instead a more complicated, non-symmetric and higher dimensional operator, related to the non-backtracking walk on the graph, leads to improved performance in detecting clusters, and even to optimal performance for the stochastic block model. Here, we propose to use instead a simpler object, a symmetric real matrix known as the Bethe Hessian operator, or deformed Laplacian. We show that this approach combines the performances of the non-backtracking operator, thus detecting clusters all the way down to the theoretical limit in the stochastic block model, with the computational, theoretical and memory advantages of real symmetric matrices.",
    "authors": [
      "Saade, Alaa",
      "Krzakala, Florent",
      "Zdeborov\u00e1, Lenka"
    ]
  },
  {
    "id": "65cc2c8205a05d7379fa3a6386f710e1",
    "title": "Optimal Regret Minimization in Posted-Price Auctions with Strategic Buyers",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/65cc2c8205a05d7379fa3a6386f710e1-Paper.pdf",
    "abstract": "We study revenue optimization learning algorithms for posted-price auctions with strategic buyers. We analyze a very broad family of monotone regret minimization algorithms for this problem, which includes the previous best known algorithm, and show that no algorithm in that family admits a strategic regret more favorable than $\\Omega(\\sqrt{T})$. We then introduce a new algorithm that achieves a strategic regret differing from the lower bound only by a factor in $O(\\log T)$, an exponential improvement upon the previous best algorithm. Our new algorithm admits a natural analysis and simpler proofs, and the ideas behind its design are general. We also report the results of empirical evaluations comparing our algorithm with the previous best algorithm and show a consistent exponential improvement in several different scenarios.",
    "authors": [
      "Mohri, Mehryar",
      "Munoz, Andres"
    ]
  },
  {
    "id": "65ded5353c5ee48d0b7d48c591b8f430",
    "title": "Fundamental Limits of Online and Distributed Algorithms for Statistical Learning and Estimation",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/65ded5353c5ee48d0b7d48c591b8f430-Paper.pdf",
    "abstract": "Many machine learning approaches are characterized by information constraints on how they interact with the training data. These include memory and sequential access constraints (e.g. fast first-order methods to solve stochastic optimization problems); communication constraints (e.g. distributed learning); partial access to the underlying data (e.g. missing features and multi-armed bandits) and more. However, currently we have little understanding how such information constraints fundamentally affect our performance, independent of the learning problem semantics. For example, are there learning problems where any algorithm which has small memory footprint (or can use any bounded number of bits from each example, or has certain communication constraints) will perform worse than what is possible without such constraints? In this paper, we describe how a single set of results implies positive answers to the above, for several different settings.",
    "authors": [
      "Shamir, Ohad"
    ]
  },
  {
    "id": "66368270ffd51418ec58bd793f2d9b1b",
    "title": "Repeated Contextual Auctions with Strategic Buyers",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/66368270ffd51418ec58bd793f2d9b1b-Paper.pdf",
    "abstract": "Motivated by real-time advertising exchanges, we analyze the problem of pricing inventory in a repeated posted-price auction. We consider both the cases of a truthful and surplus-maximizing buyer, where the former makes decisions myopically on every round, and the latter may strategically react to our algorithm, forgoing short-term surplus in order to trick the algorithm into setting better prices in the future. We further assume a buyer\u2019s valuation of a good is a function of a context vector that describes the good being sold. We give the first algorithm attaining sublinear (O(T^{2/3})) regret in the contextual setting against a surplus-maximizing buyer. We also extend this result to repeated second-price auctions with multiple buyers.",
    "authors": [
      "Amin, Kareem",
      "Rostamizadeh, Afshin",
      "Syed, Umar"
    ]
  },
  {
    "id": "66be31e4c40d676991f2405aaecc6934",
    "title": "Learning with Pseudo-Ensembles",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/66be31e4c40d676991f2405aaecc6934-Paper.pdf",
    "abstract": "We formalize the notion of a pseudo-ensemble, a (possibly infinite) collection of child models spawned from a parent model by perturbing it according to some noise process. E.g., dropout (Hinton et al, 2012) in a deep neural network trains a pseudo-ensemble of child subnetworks generated by randomly masking nodes in the parent network. We examine the relationship of pseudo-ensembles, which involve perturbation in model-space, to standard ensemble methods and existing notions of robustness, which focus on perturbation in observation-space. We present a novel regularizer based on making the behavior of a pseudo-ensemble robust with respect to the noise process generating it. In the fully-supervised setting, our regularizer matches the performance of dropout. But, unlike dropout, our regularizer naturally extends to the semi-supervised setting, where it produces state-of-the-art results. We provide a case study in which we transform the Recursive Neural Tensor Network of (Socher et al, 2013) into a pseudo-ensemble, which significantly improves its performance on a real-world sentiment analysis benchmark.",
    "authors": [
      "Bachman, Philip",
      "Alsharif, Ouais",
      "Precup, Doina"
    ]
  },
  {
    "id": "670e8a43b246801ca1eaca97b3e19189",
    "title": "Top Rank Optimization in Linear Time",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/670e8a43b246801ca1eaca97b3e19189-Paper.pdf",
    "abstract": "Bipartite ranking aims to learn a real-valued ranking function that orders positive instances before negative instances. Recent efforts of bipartite ranking are focused on optimizing ranking accuracy at the top of the ranked list. Most existing approaches are either to optimize task specific metrics or to extend the rank loss by emphasizing more on the error associated with the top ranked instances, leading to a high computational cost that is super-linear in the number of training instances. We propose a highly efficient approach, titled TopPush, for optimizing accuracy at the top that has computational complexity linear in the number of training instances. We present a novel analysis that bounds the generalization error for the top ranked instances for the proposed approach. Empirical study shows that the proposed approach is highly competitive to the state-of-the-art approaches and is 10-100 times faster.",
    "authors": [
      "Li, Nan",
      "Jin, Rong",
      "Zhou, Zhi-Hua"
    ]
  },
  {
    "id": "6766aa2750c19aad2fa1b32f36ed4aee",
    "title": "Learning Neural Network Policies with Guided Policy Search under Unknown Dynamics",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/6766aa2750c19aad2fa1b32f36ed4aee-Paper.pdf",
    "abstract": "We present a policy search method that uses iteratively refitted local linear models to optimize trajectory distributions for large, continuous problems. These trajectory distributions can be used within the framework of guided policy search to learn policies with an arbitrary parameterization. Our method fits time-varying linear dynamics models to speed up learning, but does not rely on learning a global model, which can be difficult when the dynamics are complex and discontinuous. We show that this hybrid approach requires many fewer samples than model-free methods, and can handle complex, nonsmooth dynamics that can pose a challenge for model-based techniques. We present experiments showing that our method can be used to learn complex neural network policies that successfully execute simulated robotic manipulation tasks in partially observed environments with numerous contact discontinuities and underactuation.",
    "authors": [
      "Levine, Sergey",
      "Abbeel, Pieter"
    ]
  },
  {
    "id": "678a1491514b7f1006d605e9161946b1",
    "title": "Optimizing F-Measures by Cost-Sensitive Classification",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/678a1491514b7f1006d605e9161946b1-Paper.pdf",
    "abstract": "We present a theoretical analysis of F-measures for binary, multiclass and multilabel classification. These performance measures are non-linear, but in many scenarios they are pseudo-linear functions of the per-class false negative/false positive rate. Based on this observation, we present a general reduction of F-measure maximization to cost-sensitive classification with unknown costs. We then propose an algorithm with provable guarantees to obtain an approximately optimal classifier for the F-measure by solving a series of cost-sensitive classification problems. The strength of our analysis is to be valid on any dataset and any class of classifiers, extending the existing theoretical results on F-measures, which are asymptotic in nature. We present numerical experiments to illustrate the relative importance of cost asymmetry and thresholding when learning linear classifiers on various F-measure optimization tasks.",
    "authors": [
      "Puthiya Parambath, Shameem",
      "Usunier, Nicolas",
      "Grandvalet, Yves"
    ]
  },
  {
    "id": "67d16d00201083a2b118dd5128dd6f59",
    "title": "Distributed Power-law Graph Computing: Theoretical and Empirical Analysis",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/67d16d00201083a2b118dd5128dd6f59-Paper.pdf",
    "abstract": "With the emergence of big graphs in a variety of real applications like social networks, machine learning based on distributed graph-computing~(DGC) frameworks has attracted much attention from big data machine learning community. In DGC frameworks, the graph partitioning~(GP) strategy plays a key role to affect the performance, including the workload balance and communication cost. Typically, the degree distributions of natural graphs from real applications follow skewed power laws, which makes GP a challenging task. Recently, many methods have been proposed to solve the GP problem. However, the existing GP methods cannot achieve satisfactory performance for applications with power-law graphs. In this paper, we propose a novel vertex-cut method, called \\emph{degree-based hashing}~(DBH), for GP. DBH makes effective use of the skewed degree distributions for GP. We theoretically prove that DBH can achieve lower communication cost than existing methods and can simultaneously guarantee good workload balance. Furthermore, empirical results on several large power-law graphs also show that DBH can outperform the state of the art.",
    "authors": [
      "Xie, Cong",
      "Yan, Ling",
      "Li, Wu-Jun",
      "Zhang, Zhihua"
    ]
  },
  {
    "id": "68053af2923e00204c3ca7c6a3150cf7",
    "title": "Parallel Successive Convex Approximation for Nonsmooth Nonconvex Optimization",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/68053af2923e00204c3ca7c6a3150cf7-Paper.pdf",
    "abstract": "Consider the problem of minimizing the sum of a smooth (possibly non-convex) and a convex (possibly nonsmooth) function involving a large number of variables. A popular approach to solve this problem is the block coordinate descent (BCD) method whereby at each iteration only one variable block is updated while the remaining variables are held fixed. With the recent advances in the developments of the multi-core parallel processing technology, it is desirable to parallelize the BCD method by allowing multiple blocks to be updated simultaneously at each iteration of the algorithm. In this work, we propose an inexact parallel BCD approach where at each iteration, a subset of the variables is updated in parallel by minimizing convex approximations of the original objective function. We investigate the convergence of this parallel BCD method for both randomized and cyclic variable selection rules. We analyze the asymptotic and non-asymptotic convergence behavior of the algorithm for both convex and non-convex objective functions. The numerical experiments suggest that for a special case of Lasso minimization problem, the cyclic block selection rule can outperform the randomized rule.",
    "authors": [
      "Razaviyayn, Meisam",
      "Hong, Mingyi",
      "Luo, Zhi-Quan",
      "Pang, Jong-Shi"
    ]
  },
  {
    "id": "6883966fd8f918a4aa29be29d2c386fb",
    "title": "Active Regression by Stratification",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/6883966fd8f918a4aa29be29d2c386fb-Paper.pdf",
    "abstract": "We propose a new active learning algorithm for parametric linear regression with random design. We provide finite sample convergence guarantees for general distributions in the misspecified model. This is the first active learner for this setting that provably can improve over passive learning. Unlike other learning settings (such as classification), in regression the passive learning rate of O(1/epsilon) cannot in general be improved upon. Nonetheless, the so-called `constant' in the rate of convergence, which is characterized by a distribution-dependent risk, can be improved in many cases. For a given distribution, achieving the optimal risk requires prior knowledge of the distribution. Following the stratification technique advocated in Monte-Carlo function integration, our active learner approaches a the optimal risk using piecewise constant approximations.",
    "authors": [
      "Sabato, Sivan",
      "Munos, Remi"
    ]
  },
  {
    "id": "69421f032498c97020180038fddb8e24",
    "title": "Distance-Based Network Recovery under Feature Correlation",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/69421f032498c97020180038fddb8e24-Paper.pdf",
    "abstract": "We present an inference method for Gaussian graphical models when only pairwise distances of n objects are observed. Formally, this is a problem of estimating an n x n covariance matrix from the Mahalanobis distances dMH(xi, xj), where object xi lives in a latent feature space. We solve the problem in fully Bayesian fashion by integrating over the Matrix-Normal likelihood and a Matrix-Gamma prior; the resulting Matrix-T posterior enables network recovery even under strongly correlated features. Hereby, we generalize TiWnet, which assumes Euclidean distances with strict feature independence. In spite of the greatly increased flexibility, our model neither loses statistical power nor entails more computational cost. We argue that the extension is highly relevant as it yields significantly better results in both synthetic and real-world experiments, which is successfully demonstrated for a network of biological pathways in cancer patients.",
    "authors": [
      "Adametz, David",
      "Roth, Volker"
    ]
  },
  {
    "id": "6974ce5ac660610b44d9b9fed0ff9548",
    "title": "Rounding-based Moves for Metric Labeling",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/6974ce5ac660610b44d9b9fed0ff9548-Paper.pdf",
    "abstract": "Metric labeling is a special case of energy minimization for pairwise Markov random fields. The energy function consists of arbitrary unary potentials, and pairwise potentials that are proportional to a given metric distance function over the label set. Popular methods for solving metric labeling include (i) move-making algorithms, which iteratively solve a minimum st-cut problem; and (ii) the linear programming (LP) relaxation based approach. In order to convert the fractional solution of the LP relaxation to an integer solution, several randomized rounding procedures have been developed in the literature. We consider a large class of parallel rounding procedures, and design move-making algorithms that closely mimic them. We prove that the multiplicative bound of a move-making algorithm exactly matches the approximation factor of the corresponding rounding procedure for any arbitrary distance function. Our analysis includes all known results for move-making algorithms as special cases.",
    "authors": [
      "Kumar, M. Pawan"
    ]
  },
  {
    "id": "69adc1e107f7f7d035d7baf04342e1ca",
    "title": "Transportability from Multiple Environments with Limited Experiments: Completeness Results",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/69adc1e107f7f7d035d7baf04342e1ca-Paper.pdf",
    "abstract": "This paper addresses the problem of $mz$-transportability, that is, transferring causal knowledge collected in several heterogeneous domains to a target domain in which only passive observations and limited experimental data can be collected. The paper first establishes a necessary and sufficient condition for deciding the feasibility of $mz$-transportability, i.e., whether causal effects in the target domain are estimable from the information available. It further proves that a previously established algorithm for computing transport formula is in fact complete, that is, failure of the algorithm implies non-existence of a transport formula. Finally, the paper shows that the do-calculus is complete for the $mz$-transportability class.",
    "authors": [
      "Bareinboim, Elias",
      "Pearl, Judea"
    ]
  },
  {
    "id": "6a9aeddfc689c1d0e3b9ccc3ab651bc5",
    "title": "Fast and Robust Least Squares Estimation in Corrupted Linear Models",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/6a9aeddfc689c1d0e3b9ccc3ab651bc5-Paper.pdf",
    "abstract": "Subsampling methods have been recently proposed to speed up least squares estimation in large scale settings. However, these algorithms are typically not robust to outliers or corruptions in the observed covariates. The concept of influence that was developed for regression diagnostics can be used to detect such corrupted observations as shown in this paper. This property of influence -- for which we also develop a randomized approximation -- motivates our proposed subsampling algorithm for large scale corrupted linear regression which limits the influence of data points since highly influential points contribute most to the residual error. Under a general model of corrupted observations, we show theoretically and empirically on a variety of simulated and real datasets that our algorithm improves over the current state-of-the-art approximation schemes for ordinary least squares.",
    "authors": [
      "McWilliams, Brian",
      "Krummenacher, Gabriel",
      "Lucic, Mario",
      "Buhmann, Joachim M."
    ]
  },
  {
    "id": "6aca97005c68f1206823815f66102863",
    "title": "Incremental Local Gaussian Regression",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/6aca97005c68f1206823815f66102863-Paper.pdf",
    "abstract": "Locally weighted regression (LWR) was created as a nonparametric method that can approximate a wide range of functions, is computationally efficient, and can learn continually from very large amounts of incrementally collected data. As an interesting feature, LWR can regress on non-stationary functions, a beneficial property, for instance, in control problems. However, it does not provide a proper generative model for function values, and existing algorithms have a variety of manual tuning parameters that strongly influence bias, variance and learning speed of the results. Gaussian (process) regression, on the other hand, does provide a generative model with rather black-box automatic parameter tuning, but it has higher computational cost, especially for big data sets and if a non-stationary model is required. In this paper, we suggest a path from Gaussian (process) regression to locally weighted regression, where we retain the best of both approaches. Using a localizing function basis and approximate inference techniques, we build a Gaussian (process) regression algorithm of increasingly local nature and similar computational complexity to LWR. Empirical evaluations are performed on several synthetic and real robot datasets of increasing complexity and (big) data scale, and demonstrate that we consistently achieve on par or superior performance compared to current state-of-the-art methods while retaining a principled approach to fast incremental regression with minimal manual tuning parameters.",
    "authors": [
      "Meier, Franziska",
      "Hennig, Philipp",
      "Schaal, Stefan"
    ]
  },
  {
    "id": "6c1da886822c67822bcf3679d04369fa",
    "title": "Controlling privacy in recommender systems",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/6c1da886822c67822bcf3679d04369fa-Paper.pdf",
    "abstract": "Recommender systems involve an inherent trade-off between accuracy of recommendations and the extent to which users are willing to release information about their preferences. In this paper, we explore a two-tiered notion of privacy where there is a small set of public'' users who are willing to share their preferences openly, and a large set ofprivate'' users who require privacy guarantees. We show theoretically and demonstrate empirically that a moderate number of public users with no access to private user information already suffices for reasonable accuracy. Moreover, we introduce a new privacy concept for gleaning relational information from private users while maintaining a first order deniability. We demonstrate gains from controlled access to private user preferences.",
    "authors": [
      "Xin, Yu",
      "Jaakkola, Tommi"
    ]
  },
  {
    "id": "6c29793a140a811d0c45ce03c1c93a28",
    "title": "Localized Data Fusion for Kernel k-Means Clustering with Application to Cancer Biology",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/6c29793a140a811d0c45ce03c1c93a28-Paper.pdf",
    "abstract": "In many modern applications from, for example, bioinformatics and computer vision, samples have multiple feature representations coming from different data sources. Multiview learning algorithms try to exploit all these available information to obtain a better learner in such scenarios. In this paper, we propose a novel multiple kernel learning algorithm that extends kernel k-means clustering to the multiview setting, which combines kernels calculated on the views in a localized way to better capture sample-specific characteristics of the data. We demonstrate the better performance of our localized data fusion approach on a human colon and rectal cancer data set by clustering patients. Our method finds more relevant prognostic patient groups than global data fusion methods when we evaluate the results with respect to three commonly used clinical biomarkers.",
    "authors": [
      "G\u00f6nen, Mehmet",
      "Margolin, Adam A."
    ]
  },
  {
    "id": "6c4b761a28b734fe93831e3fb400ce87",
    "title": "Object Localization based on Structural SVM using Privileged Information",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/6c4b761a28b734fe93831e3fb400ce87-Paper.pdf",
    "abstract": "We propose a structured prediction algorithm for object localization based on Support Vector Machines (SVMs) using privileged information. Privileged information provides useful high-level knowledge for image understanding and facilitates learning a reliable model even with a small number of training examples. In our setting, we assume that such information is available only at training time since it may be difficult to obtain from visual data accurately without human supervision. Our goal is to improve performance by incorporating privileged information into ordinary learning framework and adjusting model parameters for better generalization. We tackle object localization problem based on a novel structural SVM using privileged information, where an alternating loss-augmented inference procedure is employed to handle the term in the objective function corresponding to privileged information. We apply the proposed algorithm to the Caltech-UCSD Birds 200-2011 dataset, and obtain encouraging results suggesting further investigation into the benefit of privileged information in structured prediction.",
    "authors": [
      "Feyereisl, Jan",
      "Kwak, Suha",
      "Son, Jeany",
      "Han, Bohyung"
    ]
  },
  {
    "id": "6cdd60ea0045eb7a6ec44c54d29ed402",
    "title": "Robust Logistic Regression and Classification",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/6cdd60ea0045eb7a6ec44c54d29ed402-Paper.pdf",
    "abstract": "We consider logistic regression with arbitrary outliers in the covariate matrix. We propose a new robust logistic regression algorithm, called RoLR, that estimates the parameter through a simple linear programming procedure. We prove that RoLR is robust to a constant fraction of adversarial outliers. To the best of our knowledge, this is the first result on estimating logistic regression model when the covariate matrix is corrupted with any performance guarantees. Besides regression, we apply RoLR to solving binary classification problems where a fraction of training samples are corrupted.",
    "authors": [
      "Feng, Jiashi",
      "Xu, Huan",
      "Mannor, Shie",
      "Yan, Shuicheng"
    ]
  },
  {
    "id": "6d70cb65d15211726dcce4c0e971e21c",
    "title": "Flexible Transfer Learning under Support and Model Shift",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/6d70cb65d15211726dcce4c0e971e21c-Paper.pdf",
    "abstract": "Transfer learning algorithms are used when one has sufficient training data for one supervised learning task (the source/training domain) but only very limited training data for a second task (the target/test domain) that is similar but not identical to the first. Previous work on transfer learning has focused on relatively restricted settings, where specific parts of the model are considered to be carried over between tasks. Recent work on covariate shift focuses on matching the marginal distributions on observations $X$ across domains. Similarly, work on target/conditional shift focuses on matching marginal distributions on labels $Y$ and adjusting conditional distributions $P(X|Y)$, such that $P(X)$ can be matched across domains. However, covariate shift assumes that the support of test $P(X)$ is contained in the support of training $P(X)$, i.e., the training set is richer than the test set. Target/conditional shift makes a similar assumption for $P(Y)$. Moreover, not much work on transfer learning has considered the case when a few labels in the test domain are available. Also little work has been done when all marginal and conditional distributions are allowed to change while the changes are smooth. In this paper, we consider a general case where both the support and the model change across domains. We transform both $X$ and $Y$ by a location-scale shift to achieve transfer between tasks. Since we allow more flexible transformations, the proposed method yields better results on both synthetic data and real-world data.",
    "authors": [
      "Wang, Xuezhi",
      "Schneider, Jeff"
    ]
  },
  {
    "id": "6d9c547cf146054a5a720606a7694467",
    "title": "Computing Nash Equilibria in Generalized Interdependent Security Games",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/6d9c547cf146054a5a720606a7694467-Paper.pdf",
    "abstract": "We study the computational complexity of computing Nash equilibria in generalized interdependent-security (IDS) games. Like traditional IDS games, originally introduced by economists and risk-assessment experts Heal and Kunreuther about a decade ago, generalized IDS games model agents\u2019 voluntary investment decisions when facing potential direct risk and transfer risk exposure from other agents. A distinct feature of generalized IDS games, however, is that full investment can reduce transfer risk. As a result, depending on the transfer-risk reduction level, generalized IDS games may exhibit strategic complementarity (SC) or strategic substitutability (SS). We consider three variants of generalized IDS games in which players exhibit only SC, only SS, and both SC+SS. We show that determining whether there is a pure-strategy Nash equilibrium (PSNE) in SC+SS-type games is NP-complete, while computing a single PSNE in SC-type games takes worst-case polynomial time. As for the problem of computing all mixed-strategy Nash equilibria (MSNE) efficiently, we produce a partial characterization. Whenever each agent in the game is indiscriminate in terms of the transfer-risk exposure to the other agents, a case that Kearns and Ortiz originally studied in the context of traditional IDS games in their NIPS 2003 paper, we can compute all MSNE that satisfy some ordering constraints in polynomial time in all three game variants. Yet, there is a computational barrier in the general (transfer) case: we show that the computational problem is as hard as the Pure-Nash-Extension problem, also originally introduced by Kearns and Ortiz, and that it is NP complete for all three variants. Finally, we experimentally examine and discuss the practical impact that the additional protection from transfer risk allowed in generalized IDS games has on MSNE by solving several randomly-generated instances of SC+SS-type games with graph structures taken from several real-world datasets.",
    "authors": [
      "Chan, Hau",
      "Ortiz, Luis E."
    ]
  },
  {
    "id": "6d9cb7de5e8ac30bd5e8734bc96a35c1",
    "title": "Multitask learning meets tensor factorization: task imputation via convex optimization",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/6d9cb7de5e8ac30bd5e8734bc96a35c1-Paper.pdf",
    "abstract": "We study a multitask learning problem in which each task is parametrized by a weight vector and indexed by a pair of indices, which can be e.g, (consumer, time). The weight vectors can be collected into a tensor and the (multilinear-)rank of the tensor controls the amount of sharing of information among tasks. Two types of convex relaxations have recently been proposed for the tensor multilinear rank. However, we argue that both of them are not optimal in the context of multitask learning in which the dimensions or multilinear rank are typically heterogeneous. We propose a new norm, which we call the scaled latent trace norm and analyze the excess risk of all the three norms. The results apply to various settings including matrix and tensor completion, multitask learning, and multilinear multitask learning. Both the theory and experiments support the advantage of the new norm when the tensor is not equal-sized and we do not a priori know which mode is low rank.",
    "authors": [
      "Wimalawarne, Kishan",
      "Sugiyama, Masashi",
      "Tomioka, Ryota"
    ]
  },
  {
    "id": "6e2713a6efee97bacb63e52c54f0ada0",
    "title": "Mind the Nuisance: Gaussian Process Classification using Privileged Noise",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/6e2713a6efee97bacb63e52c54f0ada0-Paper.pdf",
    "abstract": "The learning with privileged information setting has recently attracted a lot of attention within the machine learning community, as it allows the integration of additional knowledge into the training process of a classifier, even when this comes in the form of a data modality that is not available at test time. Here, we show that privileged information can naturally be treated as noise in the latent function of a Gaussian process classifier (GPC). That is, in contrast to the standard GPC setting, the latent function is not just a nuisance but a feature: it becomes a natural measure of confidence about the training data by modulating the slope of the GPC probit likelihood function. Extensive experiments on public datasets show that the proposed GPC method using privileged noise, called GPC+, improves over a standard GPC without privileged knowledge, and also over the current state-of-the-art SVM-based method, SVM+. Moreover, we show that advanced neural networks and deep learning methods can be compressed as privileged information.",
    "authors": [
      "Hern\u00e1ndez-lobato, Daniel",
      "Sharmanska, Viktoriia",
      "Kersting, Kristian",
      "Lampert, Christoph H.",
      "Quadrianto, Novi"
    ]
  },
  {
    "id": "6f2268bd1d3d3ebaabb04d6b5d099425",
    "title": "On Integrated Clustering and Outlier Detection",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/6f2268bd1d3d3ebaabb04d6b5d099425-Paper.pdf",
    "abstract": "We model the joint clustering and outlier detection problem using an extension of the facility location formulation. The advantages of combining clustering and outlier selection include: (i) the resulting clusters tend to be compact and semantically coherent (ii) the clusters are more robust against data perturbations and (iii) the outliers are contextualised by the clusters and more interpretable. We provide a practical subgradient-based algorithm for the problem and also study the theoretical properties of algorithm in terms of approximation and convergence. Extensive evaluation on synthetic and real data sets attest to both the quality and scalability of our proposed method.",
    "authors": [
      "Ott, Lionel",
      "Pang, Linsey",
      "Ramos, Fabio T.",
      "Chawla, Sanjay"
    ]
  },
  {
    "id": "708f3cf8100d5e71834b1db77dfa15d6",
    "title": "Latent Support Measure Machines for Bag-of-Words Data Classification",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/708f3cf8100d5e71834b1db77dfa15d6-Paper.pdf",
    "abstract": "In many classification problems, the input is represented as a set of features, e.g., the bag-of-words (BoW) representation of documents. Support vector machines (SVMs) are widely used tools for such classification problems. The performance of the SVMs is generally determined by whether kernel values between data points can be defined properly. However, SVMs for BoW representations have a major weakness in that the co-occurrence of different but semantically similar words cannot be reflected in the kernel calculation. To overcome the weakness, we propose a kernel-based discriminative classifier for BoW data, which we call the latent support measure machine (latent SMM). With the latent SMM, a latent vector is associated with each vocabulary term, and each document is represented as a distribution of the latent vectors for words appearing in the document. To represent the distributions efficiently, we use the kernel embeddings of distributions that hold high order moment information about distributions. Then the latent SMM finds a separating hyperplane that maximizes the margins between distributions of different classes while estimating latent vectors for words to improve the classification performance. In the experiments, we show that the latent SMM achieves state-of-the-art accuracy for BoW text classification, is robust with respect to its own hyper-parameters, and is useful to visualize words.",
    "authors": [
      "Yoshikawa, Yuya",
      "Iwata, Tomoharu",
      "Sawada, Hiroshi"
    ]
  },
  {
    "id": "71a58e8cb75904f24cde464161c3e766",
    "title": "Sparse Polynomial Learning and Graph Sketching",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/71a58e8cb75904f24cde464161c3e766-Paper.pdf",
    "abstract": "Let $f: \\{-1,1\\}^n \\rightarrow \\mathbb{R}$ be a polynomial with at most $s$ non-zero real coefficients. We give an algorithm for exactly reconstructing $f$ given random examples from the uniform distribution on $\\{-1,1\\}^n$ that runs in time polynomial in $n$ and $2^{s}$ and succeeds if the function satisfies the \\textit{unique sign property}: there is one output value which corresponds to a unique set of values of the participating parities. This sufficient condition is satisfied when every coefficient of $f$ is perturbed by a small random noise, or satisfied with high probability when $s$ parity functions are chosen randomly or when all the coefficients are positive. Learning sparse polynomials over the Boolean domain in time polynomial in $n$ and $2^{s}$ is considered notoriously hard in the worst-case. Our result shows that the problem is tractable for almost all sparse polynomials. Then, we show an application of this result to hypergraph sketching which is the problem of learning a sparse (both in the number of hyperedges and the size of the hyperedges) hypergraph from uniformly drawn random cuts. We also provide experimental results on a real world dataset.",
    "authors": [
      "Kocaoglu, Murat",
      "Shanmugam, Karthikeyan",
      "Dimakis, Alexandros G.",
      "Klivans, Adam"
    ]
  },
  {
    "id": "729c68884bd359ade15d5f163166738a",
    "title": "The Noisy Power Method: A Meta Algorithm with Applications",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/729c68884bd359ade15d5f163166738a-Paper.pdf",
    "abstract": "We provide a new robust convergence analysis of the well-known power method for computing the dominant singular vectors of a matrix that we call noisy power method. Our result characterizes the convergence behavior of the algorithm when a large amount noise is introduced after each matrix-vector multiplication. The noisy power method can be seen as a meta-algorithm that has recently found a number of important applications in a broad range of machine learning problems including alternating minimization for matrix completion, streaming principal component analysis (PCA), and privacy-preserving spectral analysis. Our general analysis subsumes several existing ad-hoc convergence bounds and resolves a number of open problems in multiple applications. A recent work of Mitliagkas et al.~(NIPS 2013) gives a space-efficient algorithm for PCA in a streaming model where samples are drawn from a spiked covariance model. We give a simpler and more general analysis that applies to arbitrary distributions. Moreover, even in the spiked covariance model our result gives quantitative improvements in a natural parameter regime. As a second application, we provide an algorithm for differentially private principal component analysis that runs in nearly linear time in the input sparsity and achieves nearly tight worst-case error bounds. Complementing our worst-case bounds, we show that the error dependence of our algorithm on the matrix dimension can be replaced by an essentially tight dependence on the coherence of the matrix. This result resolves the main problem left open by Hardt and Roth (STOC 2013) and leads to strong average-case improvements over the optimal worst-case bound.",
    "authors": [
      "Hardt, Moritz",
      "Price, Eric"
    ]
  },
  {
    "id": "72da7fd6d1302c0a159f6436d01e9eb0",
    "title": "Robust Tensor Decomposition with Gross Corruption",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/72da7fd6d1302c0a159f6436d01e9eb0-Paper.pdf",
    "abstract": "In this paper, we study the statistical performance of robust tensor decomposition with gross corruption. The observations are noisy realization of the superposition of a low-rank tensor $\\mathcal{W}^*$ and an entrywise sparse corruption tensor $\\mathcal{V}^*$. Unlike conventional noise with bounded variance in previous convex tensor decomposition analysis, the magnitude of the gross corruption can be arbitrary large. We show that under certain conditions, the true low-rank tensor as well as the sparse corruption tensor can be recovered simultaneously. Our theory yields nonasymptotic Frobenius-norm estimation error bounds for each tensor separately. We show through numerical experiments that our theory can precisely predict the scaling behavior in practice.",
    "authors": [
      "Gu, Quanquan",
      "Gui, Huan",
      "Han, Jiawei"
    ]
  },
  {
    "id": "731c83db8d2ff01bdc000083fd3c3740",
    "title": "RAAM: The Benefits of Robustness in Approximating Aggregated MDPs in Reinforcement Learning",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/731c83db8d2ff01bdc000083fd3c3740-Paper.pdf",
    "abstract": "We describe how to use robust Markov decision processes for value function approximation with state aggregation. The robustness serves to reduce the sensitivity to the approximation error of sub-optimal policies in comparison to classical methods such as fitted value iteration. This results in reducing the bounds on the gamma-discounted infinite horizon performance loss by a factor of 1/(1-gamma) while preserving polynomial-time computational complexity. Our experimental results show that using the robust representation can significantly improve the solution quality with minimal additional computational cost.",
    "authors": [
      "Petrik, Marek",
      "Subramanian, Dharmashankar"
    ]
  },
  {
    "id": "74071a673307ca7459bcf75fbd024e09",
    "title": "On Prior Distributions and Approximate Inference for Structured Variables",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/74071a673307ca7459bcf75fbd024e09-Paper.pdf",
    "abstract": "We present a general framework for constructing prior distributions with structured variables. The prior is defined as the information projection of a base distribution onto distributions supported on the constraint set of interest. In cases where this projection is intractable, we propose a family of parameterized approximations indexed by subsets of the domain. We further analyze the special case of sparse structure. While the optimal prior is intractable in general, we show that approximate inference using convex subsets is tractable, and is equivalent to maximizing a submodular function subject to cardinality constraints. As a result, inference using greedy forward selection provably achieves within a factor of (1-1/e) of the optimal objective value. Our work is motivated by the predictive modeling of high-dimensional functional neuroimaging data. For this task, we employ the Gaussian base distribution induced by local partial correlations and consider the design of priors to capture the domain knowledge of sparse support. Experimental results on simulated data and high dimensional neuroimaging data show the effectiveness of our approach in terms of support recovery and predictive accuracy.",
    "authors": [
      "Koyejo, Oluwasanmi O.",
      "Khanna, Rajiv",
      "Ghosh, Joydeep",
      "Poldrack, Russell"
    ]
  },
  {
    "id": "7437d136770f5b35194cb46c1653efaa",
    "title": "A Residual Bootstrap for High-Dimensional Regression with Near Low-Rank Designs",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/7437d136770f5b35194cb46c1653efaa-Paper.pdf",
    "abstract": "We study the residual bootstrap (RB) method in the context of high-dimensional linear regression. Specifically, we analyze the distributional approximation of linear contrasts $c^{\\top}(\\hat{\\beta}_{\\rho}-\\beta)$, where $\\hat{\\beta}_{\\rho}$ is a ridge-regression estimator. When regression coefficients are estimated via least squares, classical results show that RB consistently approximates the laws of contrasts, provided that $p\\ll n$, where the design matrix is of size $n\\times p$. Up to now, relatively little work has considered how additional structure in the linear model may extend the validity of RB to the setting where $p/n\\asymp 1$. In this setting, we propose a version of RB that resamples residuals obtained from ridge regression. Our main structural assumption on the design matrix is that it is nearly low rank --- in the sense that its singular values decay according to a power-law profile. Under a few extra technical assumptions, we derive a simple criterion for ensuring that RB consistently approximates the law of a given contrast. We then specialize this result to study confidence intervals for mean response values $X_i^{\\top} \\beta$, where $X_i^{\\top}$ is the $i$th row of the design. More precisely, we show that conditionally on a Gaussian design with near low-rank structure, RB \\emph{simultaneously} approximates all of the laws $X_i^{\\top}(\\hat{\\beta}_{\\rho}-\\beta)$, $i=1,\\dots,n$. This result is also notable as it imposes no sparsity assumptions on $\\beta$. Furthermore, since our consistency results are formulated in terms of the Mallows (Kantorovich) metric, the existence of a limiting distribution is not required.",
    "authors": [
      "Lopes, Miles"
    ]
  },
  {
    "id": "74563ba21a90da13dacf2a73e3ddefa7",
    "title": "Tighten after Relax: Minimax-Optimal Sparse PCA in Polynomial Time",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/74563ba21a90da13dacf2a73e3ddefa7-Paper.pdf",
    "abstract": "We provide statistical and computational analysis of sparse Principal Component Analysis (PCA) in high dimensions. The sparse PCA problem is highly nonconvex in nature. Consequently, though its global solution attains the optimal statistical rate of convergence, such solution is computationally intractable to obtain. Meanwhile, although its convex relaxations are tractable to compute, they yield estimators with suboptimal statistical rates of convergence. On the other hand, existing nonconvex optimization procedures, such as greedy methods, lack statistical guarantees. In this paper, we propose a two-stage sparse PCA procedure that attains the optimal principal subspace estimator in polynomial time. The main stage employs a novel algorithm named sparse orthogonal iteration pursuit, which iteratively solves the underlying nonconvex problem. However, our analysis shows that this algorithm only has desired computational and statistical guarantees within a restricted region, namely the basin of attraction. To obtain the desired initial estimator that falls into this region, we solve a convex formulation of sparse PCA with early stopping. Under an integrated analytic framework, we simultaneously characterize the computational and statistical performance of this two-stage procedure. Computationally, our procedure converges at the rate of $1/\\sqrt{t}$ within the initialization stage, and at a geometric rate within the main stage. Statistically, the final principal subspace estimator achieves the minimax-optimal statistical rate of convergence with respect to the sparsity level $s^*$, dimension $d$ and sample size $n$. Our procedure motivates a general paradigm of tackling nonconvex statistical learning problems with provable statistical guarantees.",
    "authors": [
      "Wang, Zhaoran",
      "Lu, Huanran",
      "Liu, Han"
    ]
  },
  {
    "id": "757f843a169cc678064d9530d12a1881",
    "title": "Learning to Search in Branch and Bound Algorithms",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/757f843a169cc678064d9530d12a1881-Paper.pdf",
    "abstract": "Branch-and-bound is a widely used method in combinatorial optimization, including mixed integer programming, structured prediction and MAP inference. While most work has been focused on developing problem-specific techniques, little is known about how to systematically design the node searching strategy on a branch-and-bound tree. We address the key challenge of learning an adaptive node searching order for any class of problem solvable by branch-and-bound. Our strategies are learned by imitation learning. We apply our algorithm to linear programming based branch-and-bound for solving mixed integer programs (MIP). We compare our method with one of the fastest open-source solvers, SCIP; and a very efficient commercial solver, Gurobi. We demonstrate that our approach achieves better solutions faster on four MIP libraries.",
    "authors": [
      "He, He",
      "Daume III, Hal",
      "Eisner, Jason M."
    ]
  },
  {
    "id": "7634ea65a4e6d9041cfd3f7de18e334a",
    "title": "Bayesian Inference for Structured Spike and Slab Priors",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/7634ea65a4e6d9041cfd3f7de18e334a-Paper.pdf",
    "abstract": "Sparse signal recovery addresses the problem of solving underdetermined linear inverse problems subject to a sparsity constraint. We propose a novel prior formulation, the structured spike and slab prior, which allows to incorporate a priori knowledge of the sparsity pattern by imposing a spatial Gaussian process on the spike and slab probabilities. Thus, prior information on the structure of the sparsity pattern can be encoded using generic covariance functions. Furthermore, we provide a Bayesian inference scheme for the proposed model based on the expectation propagation framework. Using numerical experiments on synthetic data, we demonstrate the benefits of the model.",
    "authors": [
      "Andersen, Michael R.",
      "Winther, Ole",
      "Hansen, Lars K."
    ]
  },
  {
    "id": "76dc611d6ebaafc66cc0879c71b5db5c",
    "title": "Just-In-Time Learning for Fast and Flexible Inference",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/76dc611d6ebaafc66cc0879c71b5db5c-Paper.pdf",
    "abstract": "Much of research in machine learning has centered around the search for inference algorithms that are both general-purpose and efficient. The problem is extremely challenging and general inference remains computationally expensive. We seek to address this problem by observing that in most specific applications of a model, we typically only need to perform a small subset of all possible inference computations. Motivated by this, we introduce just-in-time learning, a framework for fast and flexible inference that learns to speed up inference at run-time. Through a series of experiments, we show how this framework can allow us to combine the flexibility of sampling with the efficiency of deterministic message-passing.",
    "authors": [
      "Eslami, S. M. Ali",
      "Tarlow, Daniel",
      "Kohli, Pushmeet",
      "Winn, John"
    ]
  },
  {
    "id": "77369e37b2aa1404f416275183ab055f",
    "title": "Fast Kernel Learning for Multidimensional Pattern Extrapolation",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/77369e37b2aa1404f416275183ab055f-Paper.pdf",
    "abstract": "The ability to automatically discover patterns and perform extrapolation is an essential quality of intelligent systems. Kernel methods, such as Gaussian processes, have great potential for pattern extrapolation, since the kernel flexibly and interpretably controls the generalisation properties of these methods. However, automatically extrapolating large scale multidimensional patterns is in general difficult, and developing Gaussian process models for this purpose involves several challenges. A vast majority of kernels, and kernel learning methods, currently only succeed in smoothing and interpolation. This difficulty is compounded by the fact that Gaussian processes are typically only tractable for small datasets, and scaling an expressive kernel learning approach poses different challenges than scaling a standard Gaussian process model. One faces additional computational constraints, and the need to retain significant model structure for expressing the rich information available in a large dataset. In this paper, we propose a Gaussian process approach for large scale multidimensional pattern extrapolation. We recover sophisticated out of class kernels, perform texture extrapolation, inpainting, and video extrapolation, and long range forecasting of land surface temperatures, all on large multidimensional datasets, including a problem with 383,400 training points. The proposed method significantly outperforms alternative scalable and flexible Gaussian process methods, in speed and accuracy. Moreover, we show that a distinct combination of expressive kernels, a fully non-parametric representation, and scalable inference which exploits existing model structure, are critical for large scale multidimensional pattern extrapolation.",
    "authors": [
      "Wilson, Andrew G.",
      "Gilboa, Elad",
      "Nehorai, Arye",
      "Cunningham, John P."
    ]
  },
  {
    "id": "7810ccd41bf26faaa2c4e1f20db70a71",
    "title": "Recursive Context Propagation Network for Semantic Scene Labeling",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/7810ccd41bf26faaa2c4e1f20db70a71-Paper.pdf",
    "abstract": "We propose a deep feed-forward neural network architecture for pixel-wise semantic scene labeling. It uses a novel recursive neural network architecture for context propagation, referred to as rCPN. It first maps the local visual features into a semantic space followed by a bottom-up aggregation of local information into a global representation of the entire image. Then a top-down propagation of the aggregated information takes place that enhances the contextual information of each local feature. Therefore, the information from every location in the image is propagated to every other location. Experimental results on Stanford background and SIFT Flow datasets show that the proposed method outperforms previous approaches. It is also orders of magnitude faster than previous methods and takes only 0.07 seconds on a GPU for pixel-wise labeling of a 256x256 image starting from raw RGB pixel values, given the super-pixel mask that takes an additional 0.3 seconds using an off-the-shelf implementation.",
    "authors": [
      "Sharma, Abhishek",
      "Tuzel, Oncel",
      "Liu, Ming-Yu"
    ]
  },
  {
    "id": "788d986905533aba051261497ecffcbb",
    "title": "Spectral Methods meet EM: A Provably Optimal Algorithm for Crowdsourcing",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/788d986905533aba051261497ecffcbb-Paper.pdf",
    "abstract": "The Dawid-Skene estimator has been widely used for inferring the true labels from the noisy labels provided by non-expert crowdsourcing workers. However, since the estimator maximizes a non-convex log-likelihood function, it is hard to theoretically justify its performance. In this paper, we propose a two-stage efficient algorithm for multi-class crowd labeling problems. The first stage uses the spectral method to obtain an initial estimate of parameters. Then the second stage refines the estimation by optimizing the objective function of the Dawid-Skene estimator via the EM algorithm. We show that our algorithm achieves the optimal convergence rate up to a logarithmic factor. We conduct extensive experiments on synthetic and real datasets. Experimental results demonstrate that the proposed algorithm is comparable to the most accurate empirical approach, while outperforming several other recently proposed methods.",
    "authors": [
      "Zhang, Yuchen",
      "Chen, Xi",
      "Zhou, Dengyong",
      "Jordan, Michael I."
    ]
  },
  {
    "id": "792c7b5aae4a79e78aaeda80516ae2ac",
    "title": "Fairness in Multi-Agent Sequential Decision-Making",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/792c7b5aae4a79e78aaeda80516ae2ac-Paper.pdf",
    "abstract": "We define a fairness solution criterion for multi-agent decision-making problems, where agents have local interests. This new criterion aims to maximize the worst performance of agents with consideration on the overall performance. We develop a simple linear programming approach and a more scalable game-theoretic approach for computing an optimal fairness policy. This game-theoretic approach formulates this fairness optimization as a two-player, zero-sum game and employs an iterative algorithm for finding a Nash equilibrium, corresponding to an optimal fairness policy. We scale up this approach by exploiting problem structure and value function approximation. Our experiments on resource allocation problems show that this fairness criterion provides a more favorable solution than the utilitarian criterion, and that our game-theoretic approach is significantly faster than linear programming.",
    "authors": [
      "Zhang, Chongjie",
      "Shah, Julie A."
    ]
  },
  {
    "id": "7bb060764a818184ebb1cc0d43d382aa",
    "title": "Multi-Class Deep Boosting",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/7bb060764a818184ebb1cc0d43d382aa-Paper.pdf",
    "abstract": "We present new ensemble learning algorithms for multi-class classification. Our algorithms can use as a base classifier set a family of deep decision trees or other rich or complex families and yet benefit from strong generalization guarantees. We give new data-dependent learning bounds for convex ensembles in the multi-class classification setting expressed in terms of the Rademacher complexities of the sub-families composing the base classifier set, and the mixture weight assigned to each sub-family. These bounds are finer than existing ones both thanks to an improved dependency on the number of classes and, more crucially, by virtue of a more favorable complexity term expressed as an average of the Rademacher complexities based on the ensemble\u2019s mixture weights. We introduce and discuss several new multi-class ensemble algorithms benefiting from these guarantees, prove positive results for the H-consistency of several of them, and report the results of experiments showing that their performance compares favorably with that of multi-class versions of AdaBoost and Logistic Regression and their L1-regularized counterparts.",
    "authors": [
      "Kuznetsov, Vitaly",
      "Mohri, Mehryar",
      "Syed, Umar"
    ]
  },
  {
    "id": "7bccfde7714a1ebadf06c5f4cea752c1",
    "title": "Depth Map Prediction from a Single Image using a Multi-Scale Deep Network",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/7bccfde7714a1ebadf06c5f4cea752c1-Paper.pdf",
    "abstract": "Predicting depth is an essential component in understanding the 3D geometry of a scene. While for stereo images local correspondence suffices for estimation, finding depth relations from a single image is less straightforward, requiring integration of both global and local information from various cues. Moreover, the task is inherently ambiguous, with a large source of uncertainty coming from the overall scale. In this paper, we present a new method that addresses this task by employing two deep network stacks: one that makes a coarse global prediction based on the entire image, and another that refines this prediction locally. We also apply a scale-invariant error to help measure depth relations rather than scale. By leveraging the raw datasets as large sources of training data, our method achieves state-of-the-art results on both NYU Depth and KITTI, and matches detailed depth boundaries without the need for superpixelation.",
    "authors": [
      "Eigen, David",
      "Puhrsch, Christian",
      "Fergus, Rob"
    ]
  },
  {
    "id": "7bcdf75ad237b8e02e301f4091fb6bc8",
    "title": "Provable Submodular Minimization using Wolfe's Algorithm",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/7bcdf75ad237b8e02e301f4091fb6bc8-Paper.pdf",
    "abstract": "Owing to several applications in large scale learning and vision problems, fast submodular function minimization (SFM) has become a critical problem. Theoretically, unconstrained SFM can be performed in polynomial time (Iwata and Orlin 2009), however these algorithms are not practical. In 1976, Wolfe proposed an algorithm to find the minimum Euclidean norm point in a polytope, and in 1980, Fujishige showed how Wolfe's algorithm can be used for SFM. For general submodular functions, the Fujishige-Wolfe minimum norm algorithm seems to have the best empirical performance. Despite its good practical performance, theoretically very little is known about Wolfe's minimum norm algorithm -- to our knowledge the only result is an exponential time analysis due to Wolfe himself. In this paper we give a maiden convergence analysis of Wolfe's algorithm. We prove that in t iterations, Wolfe's algorithm returns a O(1/t)-approximate solution to the min-norm point. We also prove a robust version of Fujishige's theorem which shows that an O(1/n^2)-approximate solution to the min-norm point problem implies exact submodular minimization. As a corollary, we get the first pseudo-polynomial time guarantee for the Fujishige-Wolfe minimum norm algorithm for submodular function minimization. In particular, we show that the min-norm point algorithm solves SFM in O(n^7F^2)-time, where $F$ is an upper bound on the maximum change a single element can cause in the function value.",
    "authors": [
      "Chakrabarty, Deeparnab",
      "Jain, Prateek",
      "Kothari, Pravesh"
    ]
  },
  {
    "id": "7d04bbbe5494ae9d2f5a76aa1c00fa2f",
    "title": "Online and Stochastic Gradient Methods for Non-decomposable Loss Functions",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/7d04bbbe5494ae9d2f5a76aa1c00fa2f-Paper.pdf",
    "abstract": "Modern applications in sensitive domains such as biometrics and medicine frequently require the use of non-decomposable loss functions such as precision@k, F-measure etc. Compared to point loss functions such as hinge-loss, these offer much more fine grained control over prediction, but at the same time present novel challenges in terms of algorithm design and analysis. In this work we initiate a study of online learning techniques for such non-decomposable loss functions with an aim to enable incremental learning as well as design scalable solvers for batch problems. To this end, we propose an online learning framework for such loss functions. Our model enjoys several nice properties, chief amongst them being the existence of efficient online learning algorithms with sublinear regret and online to batch conversion bounds. Our model is a provable extension of existing online learning models for point loss functions. We instantiate two popular losses, Prec @k and pAUC, in our model and prove sublinear regret bounds for both of them. Our proofs require a novel structural lemma over ranked lists which may be of independent interest. We then develop scalable stochastic gradient descent solvers for non-decomposable loss functions. We show that for a large family of loss functions satisfying a certain uniform convergence property (that includes Prec @k, pAUC, and F-measure), our methods provably converge to the empirical risk minimizer. Such uniform convergence results were not known for these losses and we establish these using novel proof techniques. We then use extensive experimentation on real life and benchmark datasets to establish that our method can be orders of magnitude faster than a recently proposed cutting plane method.",
    "authors": [
      "Kar, Purushottam",
      "Narasimhan, Harikrishna",
      "Jain, Prateek"
    ]
  },
  {
    "id": "7d6044e95a16761171b130dcb476a43e",
    "title": "On Model Parallelization and Scheduling Strategies for Distributed Machine Learning",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/7d6044e95a16761171b130dcb476a43e-Paper.pdf",
    "abstract": "Distributed machine learning has typically been approached from a data parallel perspective, where big data are partitioned to multiple workers and an algorithm is executed concurrently over different data subsets under various synchronization schemes to ensure speed-up and/or correctness. A sibling problem that has received relatively less attention is how to ensure efficient and correct model parallel execution of ML algorithms, where parameters of an ML program are partitioned to different workers and undergone concurrent iterative updates. We argue that model and data parallelisms impose rather different challenges for system design, algorithmic adjustment, and theoretical analysis. In this paper, we develop a system for model-parallelism, STRADS, that provides a programming abstraction for scheduling parameter updates by discovering and leveraging changing structural properties of ML programs. STRADS enables a flexible tradeoff between scheduling efficiency and fidelity to intrinsic dependencies within the models, and improves memory efficiency of distributed ML. We demonstrate the efficacy of model-parallel algorithms implemented on STRADS versus popular implementations for topic modeling, matrix factorization, and Lasso.",
    "authors": [
      "Lee, Seunghak",
      "Kim, Jin Kyu",
      "Zheng, Xun",
      "Ho, Qirong",
      "Gibson, Garth A.",
      "Xing, Eric P."
    ]
  },
  {
    "id": "7e7757b1e12abcb736ab9a754ffb617a",
    "title": "Shape and Illumination from Shading using the Generic Viewpoint Assumption",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/7e7757b1e12abcb736ab9a754ffb617a-Paper.pdf",
    "abstract": "The Generic Viewpoint Assumption (GVA) states that the position of the viewer or the light in a scene is not special. Thus, any estimated parameters from an observation should be stable under small perturbations such as object, viewpoint or light positions. The GVA has been analyzed and quantified in previous works, but has not been put to practical use in actual vision tasks. In this paper, we show how to utilize the GVA to estimate shape and illumination from a single shading image, without the use of other priors. We propose a novel linearized Spherical Harmonics (SH) shading model which enables us to obtain a computationally efficient form of the GVA term. Together with a data term, we build a model whose unknowns are shape and SH illumination. The model parameters are estimated using the Alternating Direction Method of Multipliers embedded in a multi-scale estimation framework. In this prior-free framework, we obtain competitive shape and illumination estimation results under a variety of models and lighting conditions, requiring fewer assumptions than competing methods.",
    "authors": [
      "Zoran, Daniel",
      "Krishnan, Dilip",
      "Bento, Jos\u00e9",
      "Freeman, Bill"
    ]
  },
  {
    "id": "7eb7eabbe9bd03c2fc99881d04da9cbd",
    "title": "Asynchronous Anytime Sequential Monte Carlo",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/7eb7eabbe9bd03c2fc99881d04da9cbd-Paper.pdf",
    "abstract": "We introduce a new sequential Monte Carlo algorithm we call the particle cascade. The particle cascade is an asynchronous, anytime alternative to traditional sequential Monte Carlo algorithms that is amenable to parallel and distributed implementations. It uses no barrier synchronizations which leads to improved particle throughput and memory efficiency. It is an anytime algorithm in the sense that it can be run forever to emit an unbounded number of particles while keeping within a fixed memory budget. We prove that the particle cascade provides an unbiased marginal likelihood estimator which can be straightforwardly plugged into existing pseudo-marginal methods.",
    "authors": [
      "Paige, Brooks",
      "Wood, Frank",
      "Doucet, Arnaud",
      "Teh, Yee Whye"
    ]
  },
  {
    "id": "7f39f8317fbdb1988ef4c628eba02591",
    "title": "Sparse Space-Time Deconvolution for Calcium Image Analysis",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/7f39f8317fbdb1988ef4c628eba02591-Paper.pdf",
    "abstract": "We describe a unified formulation and algorithm to find an extremely sparse representation for Calcium image sequences in terms of cell locations, cell shapes, spike timings and impulse responses. Solution of a single optimization problem yields cell segmentations and activity estimates that are on par with the state of the art, without the need for heuristic pre- or postprocessing. Experiments on real and synthetic data demonstrate the viability of the proposed method.",
    "authors": [
      "Diego Andilla, Ferran",
      "Hamprecht, Fred A."
    ]
  },
  {
    "id": "7f5d04d189dfb634e6a85bb9d9adf21e",
    "title": "From Stochastic Mixability to Fast Rates",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/7f5d04d189dfb634e6a85bb9d9adf21e-Paper.pdf",
    "abstract": "Empirical risk minimization (ERM) is a fundamental learning rule for statistical learning problems where the data is generated according to some unknown distribution $\\mathsf{P}$ and returns a hypothesis $f$ chosen from a fixed class $\\mathcal{F}$ with small loss $\\ell$. In the parametric setting, depending upon $(\\ell, \\mathcal{F},\\mathsf{P})$ ERM can have slow $(1/\\sqrt{n})$ or fast $(1/n)$ rates of convergence of the excess risk as a function of the sample size $n$. There exist several results that give sufficient conditions for fast rates in terms of joint properties of $\\ell$, $\\mathcal{F}$, and $\\mathsf{P}$, such as the margin condition and the Bernstein condition. In the non-statistical prediction with expert advice setting, there is an analogous slow and fast rate phenomenon, and it is entirely characterized in terms of the mixability of the loss $\\ell$ (there being no role there for $\\mathcal{F}$ or $\\mathsf{P}$). The notion of stochastic mixability builds a bridge between these two models of learning, reducing to classical mixability in a special case. The present paper presents a direct proof of fast rates for ERM in terms of stochastic mixability of $(\\ell,\\mathcal{F}, \\mathsf{P})$, and in so doing provides new insight into the fast-rates phenomenon. The proof exploits an old result of Kemperman on the solution to the general moment problem. We also show a partial converse that suggests a characterization of fast rates for ERM in terms of stochastic mixability is possible.",
    "authors": [
      "Mehta, Nishant A.",
      "Williamson, Robert C."
    ]
  },
  {
    "id": "7fb8ceb3bd59c7956b1df66729296a4c",
    "title": "Algorithm selection by rational metareasoning as a model of human strategy selection",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/7fb8ceb3bd59c7956b1df66729296a4c-Paper.pdf",
    "abstract": "Selecting the right algorithm is an important problem in computer science, because the algorithm often has to exploit the structure of the input to be efficient. The human mind faces the same challenge. Therefore, solutions to the algorithm selection problem can inspire models of human strategy selection and vice versa. Here, we view the algorithm selection problem as a special case of metareasoning and derive a solution that outperforms existing methods in sorting algorithm selection. We apply our theory to model how people choose between cognitive strategies and test its prediction in a behavioral experiment. We find that people quickly learn to adaptively choose between cognitive strategies. People's choices in our experiment are consistent with our model but inconsistent with previous theories of human strategy selection. Rational metareasoning appears to be a promising framework for reverse-engineering how people choose among cognitive strategies and translating the results into better solutions to the algorithm selection problem.",
    "authors": [
      "Lieder, Falk",
      "Plunkett, Dillon",
      "Hamrick, Jessica B.",
      "Russell, Stuart J.",
      "Hay, Nicholas",
      "Griffiths, Tom"
    ]
  },
  {
    "id": "7fe1f8abaad094e0b5cb1b01d712f708",
    "title": "PAC-Bayesian AUC classification and scoring",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/7fe1f8abaad094e0b5cb1b01d712f708-Paper.pdf",
    "abstract": "We develop a scoring and classification procedure based on the PAC-Bayesian approach and the AUC (Area Under Curve) criterion. We focus initially on the class of linear score functions. We derive PAC-Bayesian non-asymptotic bounds for two types of prior for the score parameters: a Gaussian prior, and a spike-and-slab prior; the latter makes it possible to perform feature selection. One important advantage of our approach is that it is amenable to powerful Bayesian computational tools. We derive in particular a Sequential Monte Carlo algorithm, as an efficient method which may be used as a gold standard, and an Expectation-Propagation algorithm, as a much faster but approximate method. We also extend our method to a class of non-linear score functions, essentially leading to a nonparametric procedure, by considering a Gaussian process prior.",
    "authors": [
      "Ridgway, James",
      "Alquier, Pierre",
      "Chopin, Nicolas",
      "Liang, Feng"
    ]
  },
  {
    "id": "7fec306d1e665bc9c748b5d2b99a6e97",
    "title": "Probabilistic Differential Dynamic Programming",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/7fec306d1e665bc9c748b5d2b99a6e97-Paper.pdf",
    "abstract": "We present a data-driven, probabilistic trajectory optimization framework for systems with unknown dynamics, called Probabilistic Differential Dynamic Programming (PDDP). PDDP takes into account uncertainty explicitly for dynamics models using Gaussian processes (GPs). Based on the second-order local approximation of the value function, PDDP performs Dynamic Programming around a nominal trajectory in Gaussian belief spaces. Different from typical gradient-based policy search methods, PDDP does not require a policy parameterization and learns a locally optimal, time-varying control policy. We demonstrate the effectiveness and efficiency of the proposed algorithm using two nontrivial tasks. Compared with the classical DDP and a state-of-the-art GP-based policy search method, PDDP offers a superior combination of data-efficiency, learning speed, and applicability.",
    "authors": [
      "Pan, Yunpeng",
      "Theodorou, Evangelos"
    ]
  },
  {
    "id": "801c14f07f9724229175b8ef8b4585a8",
    "title": "Improved Multimodal Deep Learning with Variation of Information",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/801c14f07f9724229175b8ef8b4585a8-Paper.pdf",
    "abstract": "Deep learning has been successfully applied to multimodal representation learning problems, with a common strategy to learning joint representations that are shared across multiple modalities on top of layers of modality-specific networks. Nonetheless, there still remains a question how to learn a good association between data modalities; in particular, a good generative model of multimodal data should be able to reason about missing data modality given the rest of data modalities. In this paper, we propose a novel multimodal representation learning framework that explicitly aims this goal. Rather than learning with maximum likelihood, we train the model to minimize the variation of information. We provide a theoretical insight why the proposed learning objective is sufficient to estimate the data-generating joint distribution of multimodal data. We apply our method to restricted Boltzmann machines and introduce learning methods based on contrastive divergence and multi-prediction training. In addition, we extend to deep networks with recurrent encoding structure to finetune the whole network. In experiments, we demonstrate the state-of-the-art visual recognition performance on MIR-Flickr database and PASCAL VOC 2007 database with and without text features.",
    "authors": [
      "Sohn, Kihyuk",
      "Shang, Wenling",
      "Lee, Honglak"
    ]
  },
  {
    "id": "81c650caac28cdefce4de5ddc18befa0",
    "title": "On Sparse Gaussian Chain Graph Models",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/81c650caac28cdefce4de5ddc18befa0-Paper.pdf",
    "abstract": "In this paper, we address the problem of learning the structure of Gaussian chain graph models in a high-dimensional space. Chain graph models are generalizations of undirected and directed graphical models that contain a mixed set of directed and undirected edges. While the problem of sparse structure learning has been studied extensively for Gaussian graphical models and more recently for conditional Gaussian graphical models (CGGMs), there has been little previous work on the structure recovery of Gaussian chain graph models. We consider linear regression models and a re-parameterization of the linear regression models using CGGMs as building blocks of chain graph models. We argue that when the goal is to recover model structures, there are many advantages of using CGGMs as chain component models over linear regression models, including convexity of the optimization problem, computational efficiency, recovery of structured sparsity, and ability to leverage the model structure for semi-supervised learning. We demonstrate our approach on simulated and genomic datasets.",
    "authors": [
      "McCarter, Calvin",
      "Kim, Seyoung"
    ]
  },
  {
    "id": "81ca0262c82e712e50c580c032d99b60",
    "title": "Convolutional Kernel Networks",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/81ca0262c82e712e50c580c032d99b60-Paper.pdf",
    "abstract": "An important goal in visual recognition is to devise image representations that are invariant to particular transformations. In this paper, we address this goal with a new type of convolutional neural network (CNN) whose invariance is encoded by a reproducing kernel. Unlike traditional approaches where neural networks are learned either to represent data or for solving a classification task, our network learns to approximate the kernel feature map on training data. Such an approach enjoys several benefits over classical ones. First, by teaching CNNs to be invariant, we obtain simple network architectures that achieve a similar accuracy to more complex ones, while being easy to train and robust to overfitting. Second, we bridge a gap between the neural network literature and kernels, which are natural tools to model invariance. We evaluate our methodology on visual recognition tasks where CNNs have proven to perform well, e.g., digit recognition with the MNIST dataset, and the more challenging CIFAR-10 and STL-10 datasets, where our accuracy is competitive with the state of the art.",
    "authors": [
      "Mairal, Julien",
      "Koniusz, Piotr",
      "Harchaoui, Zaid",
      "Schmid, Cordelia"
    ]
  },
  {
    "id": "81dc9bdb52d04dc20036dbd8313ed055",
    "title": "Learning Chordal Markov Networks by Dynamic Programming",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/81dc9bdb52d04dc20036dbd8313ed055-Paper.pdf",
    "abstract": "We present an algorithm for finding a chordal Markov network that maximizes any given decomposable scoring function. The algorithm is based on a recursive characterization of clique trees, and it runs in O(4^n) time for n vertices. On an eight-vertex benchmark instance, our implementation turns out to be about ten million times faster than a recently proposed, constraint satisfaction based algorithm (Corander et al., NIPS 2013). Within a few hours, it is able to solve instances up to 18 vertices, and beyond if we restrict the maximum clique size. We also study the performance of a recent integer linear programming algorithm (Bartlett and Cussens, UAI 2013). Our results suggest that, unless we bound the clique sizes, currently only the dynamic programming algorithm is guaranteed to solve instances with around 15 or more vertices.",
    "authors": [
      "Kangas, Kustaa",
      "Koivisto, Mikko",
      "Niinim\u00e4ki, Teppo"
    ]
  },
  {
    "id": "82161242827b703e6acf9c726942a1e4",
    "title": "From MAP to Marginals: Variational Inference in Bayesian Submodular Models",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/82161242827b703e6acf9c726942a1e4-Paper.pdf",
    "abstract": "Submodular optimization has found many applications in machine learning and beyond. We carry out the first systematic investigation of inference in probabilistic models defined through submodular functions, generalizing regular pairwise MRFs and Determinantal Point Processes. In particular, we present L-Field, a variational approach to general log-submodular and log-supermodular distributions based on sub- and supergradients. We obtain both lower and upper bounds on the log-partition function, which enables us to compute probability intervals for marginals, conditionals and marginal likelihoods. We also obtain fully factorized approximate posteriors, at the same computational cost as ordinary submodular optimization. Our framework results in convex problems for optimizing over differentials of submodular functions, which we show how to optimally solve. We provide theoretical guarantees of the approximation quality with respect to the curvature of the function. We further establish natural relations between our variational approach and the classical mean-field method. Lastly, we empirically demonstrate the accuracy of our inference scheme on several submodular models.",
    "authors": [
      "Djolonga, Josip",
      "Krause, Andreas"
    ]
  },
  {
    "id": "82cadb0649a3af4968404c9f6031b233",
    "title": "Algorithms for CVaR Optimization in MDPs",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/82cadb0649a3af4968404c9f6031b233-Paper.pdf",
    "abstract": "In many sequential decision-making problems we may want to manage risk by minimizing some measure of variability in costs in addition to minimizing a standard criterion. Conditional value-at-risk (CVaR) is a relatively new risk measure that addresses some of the shortcomings of the well-known variance-related risk measures, and because of its computational efficiencies has gained popularity in finance and operations research. In this paper, we consider the mean-CVaR optimization problem in MDPs. We first derive a formula for computing the gradient of this risk-sensitive objective function. We then devise policy gradient and actor-critic algorithms that each uses a specific method to estimate this gradient and updates the policy parameters in the descent direction. We establish the convergence of our algorithms to locally risk-sensitive optimal policies. Finally, we demonstrate the usefulness of our algorithms in an optimal stopping problem.",
    "authors": [
      "Chow, Yinlam",
      "Ghavamzadeh, Mohammad"
    ]
  },
  {
    "id": "838e8afb1ca34354ac209f53d90c3a43",
    "title": "Structure Regularization for Structured Prediction",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/838e8afb1ca34354ac209f53d90c3a43-Paper.pdf",
    "abstract": "While there are many studies on weight regularization, the study on structure regularization is rare. Many existing systems on structured prediction focus on increasing the level of structural dependencies within the model. However, this trend could have been misdirected, because our study suggests that complex structures are actually harmful to generalization ability in structured prediction. To control structure-based overfitting, we propose a structure regularization framework via \\emph{structure decomposition}, which decomposes training samples into mini-samples with simpler structures, deriving a model with better generalization power. We show both theoretically and empirically that structure regularization can effectively control overfitting risk and lead to better accuracy. As a by-product, the proposed method can also substantially accelerate the training speed. The method and the theoretical results can apply to general graphical models with arbitrary structures. Experiments on well-known tasks demonstrate that our method can easily beat the benchmark systems on those highly-competitive tasks, achieving record-breaking accuracies yet with substantially faster training speed.",
    "authors": [
      "Sun, Xu"
    ]
  },
  {
    "id": "839ab46820b524afda05122893c2fe8e",
    "title": "Bayes-Adaptive Simulation-based Search with Value Function Approximation",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/839ab46820b524afda05122893c2fe8e-Paper.pdf",
    "abstract": "Bayes-adaptive planning offers a principled solution to the exploration-exploitation trade-off under model uncertainty. It finds the optimal policy in belief space, which explicitly accounts for the expected effect on future rewards of reductions in uncertainty. However, the Bayes-adaptive solution is typically intractable in domains with large or continuous state spaces. We present a tractable method for approximating the Bayes-adaptive solution by combining simulation-based search with a novel value function approximation technique that generalises over belief space. Our method outperforms prior approaches in both discrete bandit tasks and simple continuous navigation and control tasks.",
    "authors": [
      "Guez, Arthur",
      "Heess, Nicolas",
      "Silver, David",
      "Dayan, Peter"
    ]
  },
  {
    "id": "84438b7aae55a0638073ef798e50b4ef",
    "title": "Optimal Teaching for Limited-Capacity Human Learners",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/84438b7aae55a0638073ef798e50b4ef-Paper.pdf",
    "abstract": "Basic decisions, such as judging a person as a friend or foe, involve categorizing novel stimuli. Recent work finds that people\u2019s category judgments are guided by a small set of examples that are retrieved from memory at decision time. This limited and stochastic retrieval places limits on human performance for probabilistic classification decisions. In light of this capacity limitation, recent work finds that idealizing training items, such that the saliency of ambiguous cases is reduced, improves human performance on novel test items. One shortcoming of previous work in idealization is that category distributions were idealized in an ad hoc or heuristic fashion. In this contribution, we take a first principles approach to constructing idealized training sets. We apply a machine teaching procedure to a cognitive model that is either limited capacity (as humans are) or unlimited capacity (as most machine learning systems are). As predicted, we find that the machine teacher recommends idealized training sets. We also find that human learners perform best when training recommendations from the machine teacher are based on a limited-capacity model. As predicted, to the extent that the learning model used by the machine teacher conforms to the true nature of human learners, the recommendations of the machine teacher prove effective. Our results provide a normative basis (given capacity constraints) for idealization procedures and offer a novel selection procedure for models of human learning.",
    "authors": [
      "Patil, Kaustubh R.",
      "Zhu, Jerry",
      "Kope\u0107, \u0141ukasz",
      "Love, Bradley C."
    ]
  },
  {
    "id": "846c260d715e5b854ffad5f70a516c88",
    "title": "Spectral Methods for Indian Buffet Process Inference",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/846c260d715e5b854ffad5f70a516c88-Paper.pdf",
    "abstract": "The Indian Buffet Process is a versatile statistical tool for modeling distributions over binary matrices. We provide an efficient spectral algorithm as an alternative to costly Variational Bayes and sampling-based algorithms. We derive a novel tensorial characterization of the moments of the Indian Buffet Process proper and for two of its applications. We give a computationally efficient iterative inference algorithm, concentration of measure bounds, and reconstruction guarantees. Our algorithm provides superior accuracy and cheaper computation than comparable Variational Bayesian approach on a number of reference problems.",
    "authors": [
      "Tung, Hsiao-Yu",
      "Smola, Alexander J."
    ]
  },
  {
    "id": "84d2004bf28a2095230e8e14993d398d",
    "title": "Deep Fragment Embeddings for Bidirectional Image Sentence Mapping",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/84d2004bf28a2095230e8e14993d398d-Paper.pdf",
    "abstract": "We introduce a model for bidirectional retrieval of images and sentences through a deep, multi-modal embedding of visual and natural language data. Unlike previous models that directly map images or sentences into a common embedding space, our model works on a finer level and embeds fragments of images (objects) and fragments of sentences (typed dependency tree relations) into a common space. We then introduce a structured max-margin objective that allows our model to explicitly associate these fragments across modalities. Extensive experimental evaluation shows that reasoning on both the global level of images and sentences and the finer level of their respective fragments improves performance on image-sentence retrieval tasks. Additionally, our model provides interpretable predictions for the image-sentence retrieval task since the inferred inter-modal alignment of fragments is explicit.",
    "authors": [
      "Karpathy, Andrej",
      "Joulin, Armand",
      "Fei-Fei, Li F."
    ]
  },
  {
    "id": "856fc81623da2150ba2210ba1b51d241",
    "title": "Greedy Subspace Clustering",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/856fc81623da2150ba2210ba1b51d241-Paper.pdf",
    "abstract": "We consider the problem of subspace clustering: given points that lie on or near the union of many low-dimensional linear subspaces, recover the subspaces. To this end, one first identifies sets of points close to the same subspace and uses the sets to estimate the subspaces. As the geometric structure of the clusters (linear subspaces) forbids proper performance of general distance based approaches such as K-means, many model-specific methods have been proposed. In this paper, we provide new simple and efficient algorithms for this problem. Our statistical analysis shows that the algorithms are guaranteed exact (perfect) clustering performance under certain conditions on the number of points and the affinity be- tween subspaces. These conditions are weaker than those considered in the standard statistical literature. Experimental results on synthetic data generated from the standard unions of subspaces model demonstrate our theory. We also show that our algorithm performs competitively against state-of-the-art algorithms on real-world applications such as motion segmentation and face clustering, with much simpler implementation and lower computational cost.",
    "authors": [
      "Park, Dohyung",
      "Caramanis, Constantine",
      "Sanghavi, Sujay"
    ]
  },
  {
    "id": "8597a6cfa74defcbde3047c891d78f90",
    "title": "Feature Cross-Substitution in Adversarial Classification",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/8597a6cfa74defcbde3047c891d78f90-Paper.pdf",
    "abstract": "The success of machine learning, particularly in supervised settings, has led to numerous attempts to apply it in adversarial settings such as spam and malware detection. The core challenge in this class of applications is that adversaries are not static data generators, but make a deliberate effort to evade the classifiers deployed to detect them. We investigate both the problem of modeling the objectives of such adversaries, as well as the algorithmic problem of accounting for rational, objective-driven adversaries. In particular, we demonstrate severe shortcomings of feature reduction in adversarial settings using several natural adversarial objective functions, an observation that is particularly pronounced when the adversary is able to substitute across similar features (for example, replace words with synonyms or replace letters in words). We offer a simple heuristic method for making learning more robust to feature cross-substitution attacks. We then present a more general approach based on mixed-integer linear programming with constraint generation, which implicitly trades off overfitting and feature selection in an adversarial setting using a sparse regularizer along with an evasion model. Our approach is the first method for combining an adversarial classification algorithm with a very general class of models of adversarial classifier evasion. We show that our algorithmic approach significantly outperforms state-of-the-art alternatives.",
    "authors": [
      "Li, Bo",
      "Vorobeychik, Yevgeniy"
    ]
  },
  {
    "id": "861dc9bd7f4e7dd3cccd534d0ae2a2e9",
    "title": "Distributed Balanced Clustering via Mapping Coresets",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/861dc9bd7f4e7dd3cccd534d0ae2a2e9-Paper.pdf",
    "abstract": "Large-scale clustering of data points in metric spaces is an important problem in mining big data sets. For many applications, we face explicit or implicit size constraints for each cluster which leads to the problem of clustering under capacity constraints or the balanced clustering'' problem. Although the balanced clustering problem has been widely studied, developing a theoretically sound distributed algorithm remains an open problem. In the present paper we develop a general framework based onmapping coresets'' to tackle this issue. For a wide range of clustering objective functions such as k-center, k-median, and k-means, our techniques give distributed algorithms for balanced clustering that match the best known single machine approximation ratios.",
    "authors": [
      "Bateni, Mohammadhossein",
      "Bhaskara, Aditya",
      "Lattanzi, Silvio",
      "Mirrokni, Vahab"
    ]
  },
  {
    "id": "865dfbde8a344b44095495f3591f7407",
    "title": "Stochastic variational inference for hidden Markov models",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/865dfbde8a344b44095495f3591f7407-Paper.pdf",
    "abstract": "Variational inference algorithms have proven successful for Bayesian analysis in large data settings, with recent advances using stochastic variational inference (SVI). However, such methods have largely been studied in independent or exchangeable data settings. We develop an SVI algorithm to learn the parameters of hidden Markov models (HMMs) in a time-dependent data setting. The challenge in applying stochastic optimization in this setting arises from dependencies in the chain, which must be broken to consider minibatches of observations. We propose an algorithm that harnesses the memory decay of the chain to adaptively bound errors arising from edge effects. We demonstrate the effectiveness of our algorithm on synthetic experiments and a large genomics dataset where a batch algorithm is computationally infeasible.",
    "authors": [
      "Foti, Nick",
      "Xu, Jason",
      "Laird, Dillon",
      "Fox, Emily"
    ]
  },
  {
    "id": "86d7c8a08b4aaa1bc7c599473f5dddda",
    "title": "Tight convex relaxations for sparse matrix factorization",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/86d7c8a08b4aaa1bc7c599473f5dddda-Paper.pdf",
    "abstract": "Based on a new atomic norm, we propose a new convex formulation for sparse matrix factorization problems in which the number of nonzero elements of the factors is assumed fixed and known. The formulation counts sparse PCA with multiple factors, subspace clustering and low-rank sparse bilinear regression as potential applications. We compute slow rates and an upper bound on the statistical dimension of the suggested norm for rank 1 matrices, showing that its statistical dimension is an order of magnitude smaller than the usual l_1-norm, trace norm and their combinations. Even though our convex formulation is in theory hard and does not lead to provably polynomial time algorithmic schemes, we propose an active set algorithm leveraging the structure of the convex problem to solve it and show promising numerical results.",
    "authors": [
      "Richard, Emile",
      "Obozinski, Guillaume R.",
      "Vert, Jean-Philippe"
    ]
  },
  {
    "id": "86df7dcfd896fcaf2674f757a2463eba",
    "title": "Extremal Mechanisms for Local Differential Privacy",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf",
    "abstract": "Local differential privacy has recently surfaced as a strong measure of privacy in contexts where personal information remains private even from data analysts. Working in a setting where the data providers and data analysts want to maximize the utility of statistical inferences performed on the released data, we study the fundamental tradeoff between local differential privacy and information theoretic utility functions. We introduce a family of extremal privatization mechanisms, which we call staircase mechanisms, and prove that it contains the optimal privatization mechanism that maximizes utility. We further show that for all information theoretic utility functions studied in this paper, maximizing utility is equivalent to solving a linear program, the outcome of which is the optimal staircase mechanism. However, solving this linear program can be computationally expensive since it has a number of variables that is exponential in the data size. To account for this, we show that two simple staircase mechanisms, the binary and randomized response mechanisms, are universally optimal in the high and low privacy regimes, respectively, and well approximate the intermediate regime.",
    "authors": [
      "Kairouz, Peter",
      "Oh, Sewoong",
      "Viswanath, Pramod"
    ]
  },
  {
    "id": "877a9ba7a98f75b90a9d49f53f15a858",
    "title": "Optimization Methods for Sparse Pseudo-Likelihood Graphical Model Selection",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/877a9ba7a98f75b90a9d49f53f15a858-Paper.pdf",
    "abstract": "Sparse high dimensional graphical model selection is a popular topic in contemporary machine learning. To this end, various useful approaches have been proposed in the context of $\\ell_1$ penalized estimation in the Gaussian framework. Though many of these approaches are demonstrably scalable and have leveraged recent advances in convex optimization, they still depend on the Gaussian functional form. To address this gap, a convex pseudo-likelihood based partial correlation graph estimation method (CONCORD) has been recently proposed. This method uses cyclic coordinate-wise minimization of a regression based pseudo-likelihood, and has been shown to have robust model selection properties in comparison with the Gaussian approach. In direct contrast to the parallel work in the Gaussian setting however, this new convex pseudo-likelihood framework has not leveraged the extensive array of methods that have been proposed in the machine learning literature for convex optimization. In this paper, we address this crucial gap by proposing two proximal gradient methods (CONCORD-ISTA and CONCORD-FISTA) for performing $\\ell_1$-regularized inverse covariance matrix estimation in the pseudo-likelihood framework. We present timing comparisons with coordinate-wise minimization and demonstrate that our approach yields tremendous pay offs for $\\ell_1$-penalized partial correlation graph estimation outside the Gaussian setting, thus yielding the fastest and most scalable approach for such problems. We undertake a theoretical analysis of our approach and rigorously demonstrate convergence, and also derive rates thereof.",
    "authors": [
      "Oh, Sang",
      "Dalal, Onkar",
      "Khare, Kshitij",
      "Rajaratnam, Bala"
    ]
  },
  {
    "id": "892c91e0a653ba19df81a90f89d99bcd",
    "title": "Unsupervised Deep Haar Scattering on Graphs",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/892c91e0a653ba19df81a90f89d99bcd-Paper.pdf",
    "abstract": "The classification of high-dimensional data defined on graphs is particularly difficult when the graph geometry is unknown. We introduce a Haar scattering transform on graphs, which computes invariant signal descriptors. It is implemented with a deep cascade of additions, subtractions and absolute values, which iteratively compute orthogonal Haar wavelet transforms. Multiscale neighborhoods of unknown graphs are estimated by minimizing an average total variation, with a pair matching algorithm of polynomial complexity. Supervised classification with dimension reduction is tested on data bases of scrambled images, and for signals sampled on unknown irregular grids on a sphere.",
    "authors": [
      "Chen, Xu",
      "Cheng, Xiuyuan",
      "Mallat, Stephane"
    ]
  },
  {
    "id": "894b77f805bd94d292574c38c5d628d5",
    "title": "Communication-Efficient Distributed Dual Coordinate Ascent",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/894b77f805bd94d292574c38c5d628d5-Paper.pdf",
    "abstract": "Communication remains the most significant bottleneck in the performance of distributed optimization algorithms for large-scale machine learning. In this paper, we propose a communication-efficient framework, COCOA, that uses local computation in a primal-dual setting to dramatically reduce the amount of necessary communication. We provide a strong convergence rate analysis for this class of algorithms, as well as experiments on real-world distributed datasets with implementations in Spark. In our experiments, we find that as compared to state-of-the-art mini-batch versions of SGD and SDCA algorithms, COCOA converges to the same .001-accurate solution quality on average 25\u00d7 as quickly.",
    "authors": [
      "Jaggi, Martin",
      "Smith, Virginia",
      "Takac, Martin",
      "Terhorst, Jonathan",
      "Krishnan, Sanjay",
      "Hofmann, Thomas",
      "Jordan, Michael I."
    ]
  },
  {
    "id": "8b16ebc056e613024c057be590b542eb",
    "title": "Learning convolution filters for inverse covariance estimation of neural network connectivity",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/8b16ebc056e613024c057be590b542eb-Paper.pdf",
    "abstract": "We consider the problem of inferring direct neural network connections from Calcium imaging time series. Inverse covariance estimation has proven to be a fast and accurate method for learning macro- and micro-scale network connectivity in the brain and in a recent Kaggle Connectomics competition inverse covariance was the main component of several top ten solutions, including our own and the winning team's algorithm. However, the accuracy of inverse covariance estimation is highly sensitive to signal preprocessing of the Calcium fluorescence time series. Furthermore, brute force optimization methods such as grid search and coordinate ascent over signal processing parameters is a time intensive process, where learning may take several days and parameters that optimize one network may not generalize to networks with different size and parameters. In this paper we show how inverse covariance estimation can be dramatically improved using a simple convolution filter prior to applying sample covariance. Furthermore, these signal processing parameters can be learned quickly using a supervised optimization algorithm. In particular, we maximize a binomial log-likelihood loss function with respect to a convolution filter of the time series and the inverse covariance regularization parameter. Our proposed algorithm is relatively fast on networks the size of those in the competition (1000 neurons), producing AUC scores with similar accuracy to the winning solution in training time under 2 hours on a cpu. Prediction on new networks of the same size is carried out in less than 15 minutes, the time it takes to read in the data and write out the solution.",
    "authors": [
      "Mohler, George"
    ]
  },
  {
    "id": "8b4066554730ddfaa0266346bdc1b202",
    "title": "General Stochastic Networks for Classification",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/8b4066554730ddfaa0266346bdc1b202-Paper.pdf",
    "abstract": "We extend generative stochastic networks to supervised learning of representations. In particular, we introduce a hybrid training objective considering a generative and discriminative cost function governed by a trade-off parameter lambda. We use a new variant of network training involving noise injection, i.e. walkback training, to jointly optimize multiple network layers. Neither additional regularization constraints, such as l1, l2 norms or dropout variants, nor pooling- or convolutional layers were added. Nevertheless, we are able to obtain state-of-the-art performance on the MNIST dataset, without using permutation invariant digits and outperform baseline models on sub-variants of the MNIST and rectangles dataset significantly.",
    "authors": [
      "Z\u00f6hrer, Matthias",
      "Pernkopf, Franz"
    ]
  },
  {
    "id": "8b6dd7db9af49e67306feb59a8bdc52c",
    "title": "Articulated Pose Estimation by a Graphical Model with Image Dependent Pairwise Relations",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/8b6dd7db9af49e67306feb59a8bdc52c-Paper.pdf",
    "abstract": "We present a method for estimating articulated human pose from a single static image based on a graphical model with novel pairwise relations that make adaptive use of local image measurements. More precisely, we specify a graphical model for human pose which exploits the fact the local image measurements can be used both to detect parts (or joints) and also to predict the spatial relationships between them (Image Dependent Pairwise Relations). These spatial relationships are represented by a mixture model. We use Deep Convolutional Neural Networks (DCNNs) to learn conditional probabilities for the presence of parts and their spatial relationships within image patches. Hence our model combines the representational flexibility of graphical models with the efficiency and statistical power of DCNNs. Our method significantly outperforms the state of the art methods on the LSP and FLIC datasets and also performs very well on the Buffy dataset without any training.",
    "authors": [
      "Chen, Xianjie",
      "Yuille, Alan L."
    ]
  },
  {
    "id": "8bb88f80d334b1869781beb89f7b73be",
    "title": "Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree Search Planning",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/8bb88f80d334b1869781beb89f7b73be-Paper.pdf",
    "abstract": "The combination of modern Reinforcement Learning and Deep Learning approaches holds the promise of making significant progress on challenging applications requiring both rich perception and policy-selection. The Arcade Learning Environment (ALE) provides a set of Atari games that represent a useful benchmark set of such applications. A recent breakthrough in combining model-free reinforcement learning with deep learning, called DQN, achieves the best real-time agents thus far. Planning-based approaches achieve far higher scores than the best model-free approaches, but they exploit information that is not available to human players, and they are orders of magnitude slower than needed for real-time play. Our main goal in this work is to build a better real-time Atari game playing agent than DQN. The central idea is to use the slow planning-based agents to provide training data for a deep-learning architecture capable of real-time play. We proposed new agents based on this idea and show that they outperform DQN.",
    "authors": [
      "Guo, Xiaoxiao",
      "Singh, Satinder",
      "Lee, Honglak",
      "Lewis, Richard L.",
      "Wang, Xiaoshi"
    ]
  },
  {
    "id": "8c19f571e251e61cb8dd3612f26d5ecf",
    "title": "Near-optimal sample compression for nearest neighbors",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/8c19f571e251e61cb8dd3612f26d5ecf-Paper.pdf",
    "abstract": "We present the first sample compression algorithm for nearest neighbors with non-trivial performance guarantees. We complement these guarantees by demonstrating almost matching hardness lower bounds, which show that our bound is nearly optimal. Our result yields new insight into margin-based nearest neighbor classification in metric spaces and allows us to significantly sharpen and simplify existing bounds. Some encouraging empirical results are also presented.",
    "authors": [
      "Gottlieb, Lee-Ad",
      "Kontorovich, Aryeh",
      "Nisnevitch, Pinhas"
    ]
  },
  {
    "id": "8c3039bd5842dca3d944faab91447818",
    "title": "Factoring Variations in Natural Images with Deep Gaussian Mixture Models",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/8c3039bd5842dca3d944faab91447818-Paper.pdf",
    "abstract": "Generative models can be seen as the swiss army knives of machine learning, as many problems can be written probabilistically in terms of the distribution of the data, including prediction, reconstruction, imputation and simulation. One of the most promising directions for unsupervised learning may lie in Deep Learning methods, given their success in supervised learning. However, one of the current problems with deep unsupervised learning methods, is that they often are harder to scale. As a result there are some easier, more scalable shallow methods, such as the Gaussian Mixture Model and the Student-t Mixture Model, that remain surprisingly competitive. In this paper we propose a new scalable deep generative model for images, called the Deep Gaussian Mixture Model, that is a straightforward but powerful generalization of GMMs to multiple layers. The parametrization of a Deep GMM allows it to efficiently capture products of variations in natural images. We propose a new EM-based algorithm that scales well to large datasets, and we show that both the Expectation and the Maximization steps can easily be distributed over multiple machines. In our density estimation experiments we show that deeper GMM architectures generalize better than more shallow ones, with results in the same ballpark as the state of the art.",
    "authors": [
      "van den Oord, Aaron",
      "Schrauwen, Benjamin"
    ]
  },
  {
    "id": "8c6744c9d42ec2cb9e8885b54ff744d0",
    "title": "Automated Variational Inference for Gaussian Process Models",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/8c6744c9d42ec2cb9e8885b54ff744d0-Paper.pdf",
    "abstract": "We develop an automated variational method for approximate inference in Gaussian process (GP) models whose posteriors are often intractable. Using a mixture of Gaussians as the variational distribution, we show that (i) the variational objective and its gradients can be approximated efficiently via sampling from univariate Gaussian distributions and (ii) the gradients of the GP hyperparameters can be obtained analytically regardless of the model likelihood. We further propose two instances of the variational distribution whose covariance matrices can be parametrized linearly in the number of observations. These results allow gradient-based optimization to be done efficiently in a black-box manner. Our approach is thoroughly verified on 5 models using 6 benchmark datasets, performing as well as the exact or hard-coded implementations while running orders of magnitude faster than the alternative MCMC sampling approaches. Our method can be a valuable tool for practitioners and researchers to investigate new models with minimal effort in deriving model-specific inference algorithms.",
    "authors": [
      "Nguyen, Trung V.",
      "Bonilla, Edwin V."
    ]
  },
  {
    "id": "8c7bbbba95c1025975e548cee86dfadc",
    "title": "Extreme bandits",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/8c7bbbba95c1025975e548cee86dfadc-Paper.pdf",
    "abstract": "In many areas of medicine, security, and life sciences, we want to allocate limited resources to different sources in order to detect extreme values. In this paper, we study an efficient way to allocate these resources sequentially under limited feedback. While sequential design of experiments is well studied in bandit theory, the most commonly optimized property is the regret with respect to the maximum mean reward. However, in other problems such as network intrusion detection, we are interested in detecting the most extreme value output by the sources. Therefore, in our work we study extreme regret which measures the efficiency of an algorithm compared to the oracle policy selecting the source with the heaviest tail. We propose the ExtremeHunter algorithm, provide its analysis, and evaluate it empirically on synthetic and real-world experiments.",
    "authors": [
      "Carpentier, Alexandra",
      "Valko, Michal"
    ]
  },
  {
    "id": "8cb22bdd0b7ba1ab13d742e22eed8da2",
    "title": "Learning Mixed Multinomial Logit Model from Ordinal Data",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/8cb22bdd0b7ba1ab13d742e22eed8da2-Paper.pdf",
    "abstract": "Motivated by generating personalized recommendations using ordinal (or preference) data, we study the question of learning a mixture of MultiNomial Logit (MNL) model, a parameterized class of distributions over permutations, from partial ordinal or preference data (e.g. pair-wise comparisons). Despite its long standing importance across disciplines including social choice, operations research and revenue management, little is known about this question. In case of single MNL models (no mixture), computationally and statistically tractable learning from pair-wise comparisons is feasible. However, even learning mixture of two MNL model is infeasible in general. Given this state of affairs, we seek conditions under which it is feasible to learn the mixture model in both computationally and statistically efficient manner. To that end, we present a sufficient condition as well as an efficient algorithm for learning mixed MNL models from partial preferences/comparisons data. In particular, a mixture of $r$ MNL components over $n$ objects can be learnt using samples whose size scales polynomially in $n$ and $r$ (concretely, $n^3 r^{3.5} \\log^4 n$, with $r \\ll n^{2/7}$ when the model parameters are sufficiently {\\em incoherent}). The algorithm has two phases: first, learn the pair-wise marginals for each component using tensor decomposition; second, learn the model parameters for each component using RankCentrality introduced by Negahban et al. In the process of proving these results, we obtain a generalization of existing analysis for tensor decomposition to a more realistic regime where only partial information about each sample is available.",
    "authors": [
      "Oh, Sewoong",
      "Shah, Devavrat"
    ]
  },
  {
    "id": "8ce6790cc6a94e65f17f908f462fae85",
    "title": "Elementary Estimators for Graphical Models",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/8ce6790cc6a94e65f17f908f462fae85-Paper.pdf",
    "abstract": "We propose a class of closed-form estimators for sparsity-structured graphical models, expressed as exponential family distributions, under high-dimensional settings. Our approach builds on observing the precise manner in which the classical graphical model MLE ``breaks down'' under high-dimensional settings. Our estimator uses a carefully constructed, well-defined and closed-form backward map, and then performs thresholding operations to ensure the desired sparsity structure. We provide a rigorous statistical analysis that shows that surprisingly our simple class of estimators recovers the same asymptotic convergence rates as those of the $\\ell_1$-regularized MLEs that are much more difficult to compute. We corroborate this statistical performance, as well as significant computational advantages via simulations of both discrete and Gaussian graphical models.",
    "authors": [
      "Yang, Eunho",
      "Lozano, Aurelie C.",
      "Ravikumar, Pradeep K."
    ]
  },
  {
    "id": "8d420fa35754d1f1c19969c88780314d",
    "title": "Efficient Minimax Strategies for Square Loss Games",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/8d420fa35754d1f1c19969c88780314d-Paper.pdf",
    "abstract": "We consider online prediction problems where the loss between the prediction and the outcome is measured by the squared Euclidean distance and its generalization, the squared Mahalanobis distance. We derive the minimax solutions for the case where the prediction and action spaces are the simplex (this setup is sometimes called the Brier game) and the $\\ell_2$ ball (this setup is related to Gaussian density estimation). We show that in both cases the value of each sub-game is a quadratic function of a simple statistic of the state, with coefficients that can be efficiently computed using an explicit recurrence relation. The resulting deterministic minimax strategy and randomized maximin strategy are linear functions of the statistic.",
    "authors": [
      "Koolen, Wouter M.",
      "Malek, Alan",
      "Bartlett, Peter L."
    ]
  },
  {
    "id": "8d6dc35e506fc23349dd10ee68dabb64",
    "title": "Generalized Higher-Order Orthogonal Iteration for Tensor Decomposition and Completion",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/8d6dc35e506fc23349dd10ee68dabb64-Paper.pdf",
    "abstract": "Low-rank tensor estimation has been frequently applied in many real-world problems. Despite successful applications, existing Schatten 1-norm minimization (SNM) methods may become very slow or even not applicable for large-scale problems. To address this difficulty, we therefore propose an efficient and scalable core tensor Schatten 1-norm minimization method for simultaneous tensor decomposition and completion, with a much lower computational complexity. We first induce the equivalence relation of Schatten 1-norm of a low-rank tensor and its core tensor. Then the Schatten 1-norm of the core tensor is used to replace that of the whole tensor, which leads to a much smaller-scale matrix SNM problem. Finally, an efficient algorithm with a rank-increasing scheme is developed to solve the proposed problem with a convergence guarantee. Extensive experimental results show that our method is usually more accurate than the state-of-the-art methods, and is orders of magnitude faster.",
    "authors": [
      "Liu, Yuanyuan",
      "Shang, Fanhua",
      "Fan, Wei",
      "Cheng, James",
      "Cheng, Hong"
    ]
  },
  {
    "id": "8d9a0adb7c204239c9635426f35c9522",
    "title": "Submodular meets Structured: Finding Diverse Subsets in Exponentially-Large Structured Item Sets",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/8d9a0adb7c204239c9635426f35c9522-Paper.pdf",
    "abstract": "To cope with the high level of ambiguity faced in domains such as Computer Vision or Natural Language processing, robust prediction methods often search for a diverse set of high-quality candidate solutions or proposals. In structured prediction problems, this becomes a daunting task, as the solution space (image labelings, sentence parses, etc.) is exponentially large. We study greedy algorithms for finding a diverse subset of solutions in structured-output spaces by drawing new connections between submodular functions over combinatorial item sets and High-Order Potentials (HOPs) studied for graphical models. Specifically, we show via examples that when marginal gains of submodular diversity functions allow structured representations, this enables efficient (sub-linear time) approximate maximization by reducing the greedy augmentation step to inference in a factor graph with appropriately constructed HOPs. We discuss benefits, tradeoffs, and show that our constructions lead to significantly better proposals.",
    "authors": [
      "Prasad, Adarsh",
      "Jegelka, Stefanie",
      "Batra, Dhruv"
    ]
  },
  {
    "id": "8dd48d6a2e2cad213179a3992c0be53c",
    "title": "Robust Bayesian Max-Margin Clustering",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/8dd48d6a2e2cad213179a3992c0be53c-Paper.pdf",
    "abstract": "We present max-margin Bayesian clustering (BMC), a general and robust framework that incorporates the max-margin criterion into Bayesian clustering models, as well as two concrete models of BMC to demonstrate its flexibility and effectiveness in dealing with different clustering tasks. The Dirichlet process max-margin Gaussian mixture is a nonparametric Bayesian clustering model that relaxes the underlying Gaussian assumption of Dirichlet process Gaussian mixtures by incorporating max-margin posterior constraints, and is able to infer the number of clusters from data. We further extend the ideas to present max-margin clustering topic model, which can learn the latent topic representation of each document while at the same time cluster documents in the max-margin fashion. Extensive experiments are performed on a number of real datasets, and the results indicate superior clustering performance of our methods compared to related baselines.",
    "authors": [
      "Chen, Changyou",
      "Zhu, Jun",
      "Zhang, Xinhua"
    ]
  },
  {
    "id": "8ebda540cbcc4d7336496819a46a1b68",
    "title": "Approximating Hierarchical MV-sets for Hierarchical Clustering",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/8ebda540cbcc4d7336496819a46a1b68-Paper.pdf",
    "abstract": "The goal of hierarchical clustering is to construct a cluster tree, which can be viewed as the modal structure of a density. For this purpose, we use a convex optimization program that can efficiently estimate a family of hierarchical dense sets in high-dimensional distributions. We further extend existing graph-based methods to approximate the cluster tree of a distribution. By avoiding direct density estimation, our method is able to handle high-dimensional data more efficiently than existing density-based approaches. We present empirical results that demonstrate the superiority of our method over existing ones.",
    "authors": [
      "Glazer, Assaf",
      "Weissbrod, Omer",
      "Lindenbaum, Michael",
      "Markovitch, Shaul"
    ]
  },
  {
    "id": "8edd72158ccd2a879f79cb2538568fdc",
    "title": "Diverse Randomized Agents Vote to Win",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/8edd72158ccd2a879f79cb2538568fdc-Paper.pdf",
    "abstract": "We investigate the power of voting among diverse, randomized software agents. With teams of computer Go agents in mind, we develop a novel theoretical model of two-stage noisy voting that builds on recent work in machine learning. This model allows us to reason about a collection of agents with different biases (determined by the first-stage noise models), which, furthermore, apply randomized algorithms to evaluate alternatives and produce votes (captured by the second-stage noise models). We analytically demonstrate that a uniform team, consisting of multiple instances of any single agent, must make a significant number of mistakes, whereas a diverse team converges to perfection as the number of agents grows. Our experiments, which pit teams of computer Go agents against strong agents, provide evidence for the effectiveness of voting when agents are diverse.",
    "authors": [
      "Jiang, Albert",
      "Soriano Marcolino, Leandro",
      "Procaccia, Ariel D.",
      "Sandholm, Tuomas",
      "Shah, Nisarg",
      "Tambe, Milind"
    ]
  },
  {
    "id": "8f121ce07d74717e0b1f21d122e04521",
    "title": "Consistency of Spectral Partitioning of Uniform Hypergraphs under Planted Partition Model",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/8f121ce07d74717e0b1f21d122e04521-Paper.pdf",
    "abstract": "Spectral graph partitioning methods have received significant attention from both practitioners and theorists in computer science. Some notable studies have been carried out regarding the behavior of these methods for infinitely large sample size (von Luxburg et al., 2008; Rohe et al., 2011), which provide sufficient confidence to practitioners about the effectiveness of these methods. On the other hand, recent developments in computer vision have led to a plethora of applications, where the model deals with multi-way affinity relations and can be posed as uniform hyper-graphs. In this paper, we view these models as random m-uniform hypergraphs and establish the consistency of spectral algorithm in this general setting. We develop a planted partition model or stochastic blockmodel for such problems using higher order tensors, present a spectral technique suited for the purpose and study its large sample behavior. The analysis reveals that the algorithm is consistent for m-uniform hypergraphs for larger values of m, and also the rate of convergence improves for increasing m. Our result provides the first theoretical evidence that establishes the importance of m-way affinities.",
    "authors": [
      "Ghoshdastidar, Debarghya",
      "Dukkipati, Ambedkar"
    ]
  },
  {
    "id": "8f19793b2671094e63a15ab883d50137",
    "title": "An Accelerated Proximal Coordinate Gradient Method",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/8f19793b2671094e63a15ab883d50137-Paper.pdf",
    "abstract": "We develop an accelerated randomized proximal coordinate gradient (APCG) method, for solving a broad class of composite convex optimization problems. In particular, our method achieves faster linear convergence rates for minimizing strongly convex functions than existing randomized proximal coordinate gradient methods. We show how to apply the APCG method to solve the dual of the regularized empirical risk minimization (ERM) problem, and devise efficient implementations that can avoid full-dimensional vector operations. For ill-conditioned ERM problems, our method obtains improved convergence rates than the state-of-the-art stochastic dual coordinate ascent (SDCA) method.",
    "authors": [
      "Lin, Qihang",
      "Lu, Zhaosong",
      "Xiao, Lin"
    ]
  },
  {
    "id": "8f1d43620bc6bb580df6e80b0dc05c48",
    "title": "Scalable Non-linear Learning with Adaptive Polynomial Expansions",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/8f1d43620bc6bb580df6e80b0dc05c48-Paper.pdf",
    "abstract": "Can we effectively learn a nonlinear representation in time comparable to linear learning? We describe a new algorithm that explicitly and adaptively expands higher-order interaction features over base linear representations. The algorithm is designed for extreme computational efficiency, and an extensive experimental study shows that its computation/prediction tradeoff ability compares very favorably against strong baselines.",
    "authors": [
      "Agarwal, Alekh",
      "Beygelzimer, Alina",
      "Hsu, Daniel J.",
      "Langford, John",
      "Telgarsky, Matus J."
    ]
  },
  {
    "id": "8fb5f8be2aa9d6c64a04e3ab9f63feee",
    "title": "Multi-Step Stochastic ADMM in High Dimensions: Applications to Sparse Optimization and Matrix Decomposition",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/8fb5f8be2aa9d6c64a04e3ab9f63feee-Paper.pdf",
    "abstract": "In this paper, we consider a multi-step version of the stochastic ADMM method with efficient guarantees for high-dimensional problems. We first analyze the simple setting, where the optimization problem consists of a loss function and a single regularizer (e.g. sparse optimization), and then extend to the multi-block setting with multiple regularizers and multiple variables (e.g. matrix decomposition into sparse and low rank components). For the sparse optimization problem, our method achieves the minimax rate of $O(s\\log d/T)$ for $s$-sparse problems in $d$ dimensions in $T$ steps, and is thus, unimprovable by any method up to constant factors. For the matrix decomposition problem with a general loss function, we analyze the multi-step ADMM with multiple blocks. We establish $O(1/T)$ rate and efficient scaling as the size of matrix grows. For natural noise models (e.g. independent noise), our convergence rate is minimax-optimal. Thus, we establish tight convergence guarantees for multi-block ADMM in high dimensions. Experiments show that for both sparse optimization and matrix decomposition problems, our algorithm outperforms the state-of-the-art methods.",
    "authors": [
      "Sedghi, Hanie",
      "Anandkumar, Anima",
      "Jonckheere, Edmond"
    ]
  },
  {
    "id": "903ce9225fca3e988c2af215d4e544d3",
    "title": "Stochastic Multi-Armed-Bandit Problem with Non-stationary Rewards",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/903ce9225fca3e988c2af215d4e544d3-Paper.pdf",
    "abstract": "In a multi-armed bandit (MAB) problem a gambler needs to choose at each round of play one of K arms, each characterized by an unknown reward distribution. Reward realizations are only observed when an arm is selected, and the gambler's objective is to maximize his cumulative expected earnings over some given horizon of play T. To do this, the gambler needs to acquire information about arms (exploration) while simultaneously optimizing immediate rewards (exploitation); the price paid due to this trade off is often referred to as the regret, and the main question is how small can this price be as a function of the horizon length T. This problem has been studied extensively when the reward distributions do not change over time; an assumption that supports a sharp characterization of the regret, yet is often violated in practical settings. In this paper, we focus on a MAB formulation which allows for a broad range of temporal uncertainties in the rewards, while still maintaining mathematical tractability. We fully characterize the (regret) complexity of this class of MAB problems by establishing a direct link between the extent of allowable reward variation\" and the minimal achievable regret, and by establishing a connection between the adversarial and the stochastic MAB frameworks.\"",
    "authors": [
      "Besbes, Omar",
      "Gur, Yonatan",
      "Zeevi, Assaf"
    ]
  },
  {
    "id": "912d2b1c7b2826caf99687388d2e8f7c",
    "title": "Efficient Structured Matrix Rank Minimization",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/912d2b1c7b2826caf99687388d2e8f7c-Paper.pdf",
    "abstract": "We study the problem of finding structured low-rank matrices using nuclear norm regularization where the structure is encoded by a linear map. In contrast to most known approaches for linearly structured rank minimization, we do not (a) use the full SVD; nor (b) resort to augmented Lagrangian techniques; nor (c) solve linear systems per iteration. Instead, we formulate the problem differently so that it is amenable to a generalized conditional gradient method, which results in a practical improvement with low per iteration computational cost. Numerical results show that our approach significantly outperforms state-of-the-art competitors in terms of running time, while effectively recovering low rank solutions in stochastic system realization and spectral compressed sensing problems.",
    "authors": [
      "Yu, Adams Wei",
      "Ma, Wanli",
      "Yu, Yaoliang",
      "Carbonell, Jaime",
      "Sra, Suvrit"
    ]
  },
  {
    "id": "918317b57931b6b7a7d29490fe5ec9f9",
    "title": "Beyond Disagreement-Based Agnostic Active Learning",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/918317b57931b6b7a7d29490fe5ec9f9-Paper.pdf",
    "abstract": "We study agnostic active learning, where the goal is to learn a classifier in a pre-specified hypothesis class interactively with as few label queries as possible, while making no assumptions on the true function generating the labels. The main algorithms for this problem are {\\em{disagreement-based active learning}}, which has a high label requirement, and {\\em{margin-based active learning}}, which only applies to fairly restricted settings. A major challenge is to find an algorithm which achieves better label complexity, is consistent in an agnostic setting, and applies to general classification problems. In this paper, we provide such an algorithm. Our solution is based on two novel contributions -- a reduction from consistent active learning to confidence-rated prediction with guaranteed error, and a novel confidence-rated predictor.",
    "authors": [
      "Zhang, Chicheng",
      "Chaudhuri, Kamalika"
    ]
  },
  {
    "id": "92af93f73faf3cefc129b6bc55a748a9",
    "title": "Optimal Neural Codes for Control and Estimation",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/92af93f73faf3cefc129b6bc55a748a9-Paper.pdf",
    "abstract": "Agents acting in the natural world aim at selecting appropriate actions based on noisy and partial sensory observations. Many behaviors leading to decision making and action selection in a closed loop setting are naturally phrased within a control theoretic framework. Within the framework of optimal Control Theory, one is usually given a cost function which is minimized by selecting a control law based on the observations. While in standard control settings the sensors are assumed fixed, biological systems often gain from the extra flexibility of optimizing the sensors themselves. However, this sensory adaptation is geared towards control rather than perception, as is often assumed. In this work we show that sensory adaptation for control differs from sensory adaptation for perception, even for simple control setups. This implies, consistently with recent experimental results, that when studying sensory adaptation, it is essential to account for the task being performed.",
    "authors": [
      "Susemihl, Alex K.",
      "Meir, Ron",
      "Opper, Manfred"
    ]
  },
  {
    "id": "941e1aaaba585b952b62c14a3a175a61",
    "title": "New Rules for Domain Independent Lifted MAP Inference",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/941e1aaaba585b952b62c14a3a175a61-Paper.pdf",
    "abstract": "Lifted inference algorithms for probabilistic first-order logic frameworks such as Markov logic networks (MLNs) have received significant attention in recent years. These algorithms use so called lifting rules to identify symmetries in the first-order representation and reduce the inference problem over a large probabilistic model to an inference problem over a much smaller model. In this paper, we present two new lifting rules, which enable fast MAP inference in a large class of MLNs. Our first rule uses the concept of single occurrence equivalence class of logical variables, which we define in the paper. The rule states that the MAP assignment over an MLN can be recovered from a much smaller MLN, in which each logical variable in each single occurrence equivalence class is replaced by a constant (i.e., an object in the domain of the variable). Our second rule states that we can safely remove a subset of formulas from the MLN if all equivalence classes of variables in the remaining MLN are single occurrence and all formulas in the subset are tautology (i.e., evaluate to true) at extremes (i.e., assignments with identical truth value for groundings of a predicate). We prove that our two new rules are sound and demonstrate via a detailed experimental evaluation that our approach is superior in terms of scalability and MAP solution quality to the state of the art approaches.",
    "authors": [
      "Mittal, Happy",
      "Goyal, Prasoon",
      "Gogate, Vibhav G.",
      "Singla, Parag"
    ]
  },
  {
    "id": "94c7bb58efc3b337800875b5d382a072",
    "title": "Sparse Multi-Task Reinforcement Learning",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/94c7bb58efc3b337800875b5d382a072-Paper.pdf",
    "abstract": "In multi-task reinforcement learning (MTRL), the objective is to simultaneously learn multiple tasks and exploit their similarity to improve the performance w.r.t.\\ single-task learning. In this paper we investigate the case when all the tasks can be accurately represented in a linear approximation space using the same small subset of the original (large) set of features. This is equivalent to assuming that the weight vectors of the task value functions are \\textit{jointly sparse}, i.e., the set of their non-zero components is small and it is shared across tasks. Building on existing results in multi-task regression, we develop two multi-task extensions of the fitted $Q$-iteration algorithm. While the first algorithm assumes that the tasks are jointly sparse in the given representation, the second one learns a transformation of the features in the attempt of finding a more sparse representation. For both algorithms we provide a sample complexity analysis and numerical simulations.",
    "authors": [
      "Calandriello, Daniele",
      "Lazaric, Alessandro",
      "Restelli, Marcello"
    ]
  },
  {
    "id": "95151403b0db4f75bfd8da0b393af853",
    "title": "The limits of squared Euclidean distance regularization",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/95151403b0db4f75bfd8da0b393af853-Paper.pdf",
    "abstract": "Some of the simplest loss functions considered in Machine Learning are the square loss, the logistic loss and the hinge loss. The most common family of algorithms, including Gradient Descent (GD) with and without Weight Decay, always predict with a linear combination of the past instances. We give a random construction for sets of examples where the target linear weight vector is trivial to learn but any algorithm from the above family is drastically sub-optimal. Our lower bound on the latter algorithms holds even if the algorithms are enhanced with an arbitrary kernel function. This type of result was known for the square loss. However, we develop new techniques that let us prove such hardness results for any loss function satisfying some minimal requirements on the loss function (including the three listed above). We also show that algorithms that regularize with the squared Euclidean distance are easily confused by random features. Finally, we conclude by discussing related open problems regarding feed forward neural networks. We conjecture that our hardness results hold for any training algorithm that is based on the squared Euclidean distance regularization (i.e. Back-propagation with the Weight Decay heuristic).",
    "authors": [
      "Derezinski, Michal",
      "Warmuth, Manfred K. K."
    ]
  },
  {
    "id": "955a1584af63a546588caae4d23840b3",
    "title": "Finding a sparse vector in a subspace: Linear sparsity using alternating directions",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/955a1584af63a546588caae4d23840b3-Paper.pdf",
    "abstract": "We consider the problem of recovering the sparsest vector in a subspace $ \\mathcal{S} \\in \\mathbb{R}^p $ with $ \\text{dim}(\\mathcal{S})=n$. This problem can be considered a homogeneous variant of the sparse recovery problem, and finds applications in sparse dictionary learning, sparse PCA, and other problems in signal processing and machine learning. Simple convex heuristics for this problem provably break down when the fraction of nonzero entries in the target sparse vector substantially exceeds $1/ \\sqrt{n}$. In contrast, we exhibit a relatively simple nonconvex approach based on alternating directions, which provably succeeds even when the fraction of nonzero entries is $\\Omega(1)$. To our knowledge, this is the first practical algorithm to achieve this linear scaling. This result assumes a planted sparse model, in which the target sparse vector is embedded in an otherwise random subspace. Empirically, our proposed algorithm also succeeds in more challenging data models arising, e.g., from sparse dictionary learning.",
    "authors": [
      "Qu, Qing",
      "Sun, Ju",
      "Wright, John"
    ]
  },
  {
    "id": "95d309f0b035d97f69902e7972c2b2e6",
    "title": "Scalable Kernel Methods via Doubly Stochastic Gradients",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/95d309f0b035d97f69902e7972c2b2e6-Paper.pdf",
    "abstract": "The general perception is that kernel methods are not scalable, so neural nets become the choice for large-scale nonlinear learning problems. Have we tried hard enough for kernel methods? In this paper, we propose an approach that scales up kernel methods using a novel concept called ``doubly stochastic functional gradients''. Based on the fact that many kernel methods can be expressed as convex optimization problems, our approach solves the optimization problems by making two unbiased stochastic approximations to the functional gradient---one using random training points and another using random features associated with the kernel---and performing descent steps with this noisy functional gradient. Our algorithm is simple, need no commit to a preset number of random features, and allows the flexibility of the function class to grow as we see more incoming data in the streaming setting. We demonstrate that a function learned by this procedure after t iterations converges to the optimal function in the reproducing kernel Hilbert space in rate O(1/t), and achieves a generalization bound of O(1/\\sqrt{t}). Our approach can readily scale kernel methods up to the regimes which are dominated by neural nets. We show competitive performances of our approach as compared to neural nets in datasets such as 2.3 million energy materials from MolecularSpace, 8 million handwritten digits from MNIST, and 1 million photos from ImageNet using convolution features.",
    "authors": [
      "Dai, Bo",
      "Xie, Bo",
      "He, Niao",
      "Liang, Yingyu",
      "Raj, Anant",
      "Balcan, Maria-Florina F.",
      "Song, Le"
    ]
  },
  {
    "id": "9683cc5f89562ea48e72bb321d9f03fb",
    "title": "Learning Mixtures of Ranking Models",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/9683cc5f89562ea48e72bb321d9f03fb-Paper.pdf",
    "abstract": "This work concerns learning probabilistic models for ranking data in a heterogeneous population. The specific problem we study is learning the parameters of a {\\em Mallows Mixture Model}. Despite being widely studied, current heuristics for this problem do not have theoretical guarantees and can get stuck in bad local optima. We present the first polynomial time algorithm which provably learns the parameters of a mixture of two Mallows models. A key component of our algorithm is a novel use of tensor decomposition techniques to learn the top-$k$ prefix in both the rankings. Before this work, even the question of {\\em identifiability} in the case of a mixture of two Mallows models was unresolved.",
    "authors": [
      "Awasthi, Pranjal",
      "Blum, Avrim",
      "Sheffet, Or",
      "Vijayaraghavan, Aravindan"
    ]
  },
  {
    "id": "96ea64f3a1aa2fd00c72faacf0cb8ac9",
    "title": "Using Convolutional Neural Networks to Recognize Rhythm \ufffcStimuli from Electroencephalography Recordings",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/96ea64f3a1aa2fd00c72faacf0cb8ac9-Paper.pdf",
    "abstract": "Electroencephalography (EEG) recordings of rhythm perception might contain enough information to distinguish different rhythm types/genres or even identify the rhythms themselves. We apply convolutional neural networks (CNNs) to analyze and classify EEG data recorded within a rhythm perception study in Kigali, Rwanda which comprises 12 East African and 12 Western rhythmic stimuli \u2013 each presented in a loop for 32 seconds to 13 participants. We investigate the impact of the data representation and the pre-processing steps for this classification tasks and compare different network structures. Using CNNs, we are able to recognize individual rhythms from the EEG with a mean classification accuracy of 24.4% (chance level 4.17%) over all subjects by looking at less than three seconds from a single channel. Aggregating predictions for multiple channels, a mean accuracy of up to 50% can be achieved for individual subjects.",
    "authors": [
      "Stober, Sebastian",
      "Cameron, Daniel J.",
      "Grahn, Jessica A."
    ]
  },
  {
    "id": "97d0145823aeb8ed80617be62e08bdcc",
    "title": "Content-based recommendations with Poisson factorization",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/97d0145823aeb8ed80617be62e08bdcc-Paper.pdf",
    "abstract": "We develop collaborative topic Poisson factorization (CTPF), a generative model of articles and reader preferences. CTPF can be used to build recommender systems by learning from reader histories and content to recommend personalized articles of interest. In detail, CTPF models both reader behavior and article texts with Poisson distributions, connecting the latent topics that represent the texts with the latent preferences that represent the readers. This provides better recommendations than competing methods and gives an interpretable latent space for understanding patterns of readership. Further, we exploit stochastic variational inference to model massive real-world datasets. For example, we can fit CPTF to the full arXiv usage dataset, which contains over 43 million ratings and 42 million word counts, within a day. We demonstrate empirically that our model outperforms several baselines, including the previous state-of-the-art approach.",
    "authors": [
      "Gopalan, Prem K.",
      "Charlin, Laurent",
      "Blei, David"
    ]
  },
  {
    "id": "98986c005e5def2da341b4e0627d4712",
    "title": "Optimizing Energy Production Using Policy Search and Predictive State Representations",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/98986c005e5def2da341b4e0627d4712-Paper.pdf",
    "abstract": "We consider the challenging practical problem of optimizing the power production of a complex of hydroelectric power plants, which involves control over three continuous action variables, uncertainty in the amount of water inflows and a variety of constraints that need to be satisfied. We propose a policy-search-based approach coupled with predictive modelling to address this problem. This approach has some key advantages compared to other alternatives, such as dynamic programming: the policy representation and search algorithm can conveniently incorporate domain knowledge; the resulting policies are easy to interpret, and the algorithm is naturally parallelizable. Our algorithm obtains a policy which outperforms the solution found by dynamic programming both quantitatively and qualitatively.",
    "authors": [
      "Grinberg, Yuri",
      "Precup, Doina",
      "Gendreau, Michel"
    ]
  },
  {
    "id": "98d6f58ab0dafbb86b083a001561bb34",
    "title": "Time--Data Tradeoffs by Aggressive Smoothing",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/98d6f58ab0dafbb86b083a001561bb34-Paper.pdf",
    "abstract": "This paper proposes a tradeoff between sample complexity and computation time that applies to statistical estimators based on convex optimization. As the amount of data increases, we can smooth optimization problems more and more aggressively to achieve accurate estimates more quickly. This work provides theoretical and experimental evidence of this tradeoff for a class of regularized linear inverse problems.",
    "authors": [
      "Bruer, John J.",
      "Tropp, Joel A.",
      "Cevher, Volkan",
      "Becker, Stephen"
    ]
  },
  {
    "id": "996009f2374006606f4c0b0fda878af1",
    "title": "Shaping Social Activity by Incentivizing Users",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/996009f2374006606f4c0b0fda878af1-Paper.pdf",
    "abstract": "Events in an online social network can be categorized roughly into endogenous events, where users just respond to the actions of their neighbors within the network, or exogenous events, where users take actions due to drives external to the network. How much external drive should be provided to each user, such that the network activity can be steered towards a target state? In this paper, we model social events using multivariate Hawkes processes, which can capture both endogenous and exogenous event intensities, and derive a time dependent linear relation between the intensity of exogenous events and the overall network activity. Exploiting this connection, we develop a convex optimization framework for determining the required level of external drive in order for the network to reach a desired activity level. We experimented with event data gathered from Twitter, and show that our method can steer the activity of the network more accurately than alternatives.",
    "authors": [
      "Farajtabar, Mehrdad",
      "Du, Nan",
      "Gomez Rodriguez, Manuel",
      "Valera, Isabel",
      "Zha, Hongyuan",
      "Song, Le"
    ]
  },
  {
    "id": "996a7fa078cc36c46d02f9af3bef918b",
    "title": "Universal Option Models",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/996a7fa078cc36c46d02f9af3bef918b-Paper.pdf",
    "abstract": "We consider the problem of learning models of options for real-time abstract planning, in the setting where reward functions can be specified at any time and their expected returns must be efficiently computed. We introduce a new model for an option that is independent of any reward function, called the {\\it universal option model (UOM)}. We prove that the UOM of an option can construct a traditional option model given a reward function, and the option-conditional return is computed directly by a single dot-product of the UOM with the reward function. We extend the UOM to linear function approximation, and we show it gives the TD solution of option returns and value functions of policies over options. We provide a stochastic approximation algorithm for incrementally learning UOMs from data and prove its consistency. We demonstrate our method in two domains. The first domain is document recommendation, where each user query defines a new reward function and a document's relevance is the expected return of a simulated random-walk through the document's references. The second domain is a real-time strategy game, where the controller must select the best game unit to accomplish dynamically-specified tasks. Our experiments show that UOMs are substantially more efficient in evaluating option returns and policies than previously known methods.",
    "authors": [
      "yao, hengshuai",
      "Szepesvari, Csaba",
      "Sutton, Richard S.",
      "Modayil, Joseph",
      "Bhatnagar, Shalabh"
    ]
  },
  {
    "id": "99bcfcd754a98ce89cb86f73acc04645",
    "title": "Discovering, Learning and Exploiting Relevance",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/99bcfcd754a98ce89cb86f73acc04645-Paper.pdf",
    "abstract": "In this paper we consider the problem of learning online what is the information to consider when making sequential decisions. We formalize this as a contextual multi-armed bandit problem where a high dimensional ($D$-dimensional) context vector arrives to a learner which needs to select an action to maximize its expected reward at each time step. Each dimension of the context vector is called a type. We assume that there exists an unknown relation between actions and types, called the relevance relation, such that the reward of an action only depends on the contexts of the relevant types. When the relation is a function, i.e., the reward of an action only depends on the context of a single type, and the expected reward of an action is Lipschitz continuous in the context of its relevant type, we propose an algorithm that achieves $\\tilde{O}(T^{\\gamma})$ regret with a high probability, where $\\gamma=2/(1+\\sqrt{2})$. Our algorithm achieves this by learning the unknown relevance relation, whereas prior contextual bandit algorithms that do not exploit the existence of a relevance relation will have $\\tilde{O}(T^{(D+1)/(D+2)})$ regret. Our algorithm alternates between exploring and exploiting, it does not require reward observations in exploitations, and it guarantees with a high probability that actions with suboptimality greater than $\\epsilon$ are never selected in exploitations. Our proposed method can be applied to a variety of learning applications including medical diagnosis, recommender systems, popularity prediction from social networks, network security etc., where at each instance of time vast amounts of different types of information are available to the decision maker, but the effect of an action depends only on a single type.",
    "authors": [
      "Tekin, Cem",
      "van der Schaar, Mihaela"
    ]
  },
  {
    "id": "99c5e07b4d5de9d18c350cdf64c5aa3d",
    "title": "Stochastic Network Design in Bidirected Trees",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/99c5e07b4d5de9d18c350cdf64c5aa3d-Paper.pdf",
    "abstract": "We investigate the problem of stochastic network design in bidirected trees. In this problem, an underlying phenomenon (e.g., a behavior, rumor, or disease) starts at multiple sources in a tree and spreads in both directions along its edges. Actions can be taken to increase the probability of propagation on edges, and the goal is to maximize the total amount of spread away from all sources. Our main result is a rounded dynamic programming approach that leads to a fully polynomial-time approximation scheme (FPTAS), that is, an algorithm that can find (1\u2212\u03b5)-optimal solutions for any problem instance in time polynomial in the input size and 1/\u03b5. Our algorithm outperforms competing approaches on a motivating problem from computational sustainability to remove barriers in river networks to restore the health of aquatic ecosystems.",
    "authors": [
      "wu, xiaojian",
      "Sheldon, Daniel R.",
      "Zilberstein, Shlomo"
    ]
  },
  {
    "id": "9a4400501febb2a95e79248486a5f6d3",
    "title": "Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/9a4400501febb2a95e79248486a5f6d3-Paper.pdf",
    "abstract": "Gaussian processes (GPs) are a powerful tool for probabilistic inference over functions. They have been applied to both regression and non-linear dimensionality reduction, and offer desirable properties such as uncertainty estimates, robustness to over-fitting, and principled ways for tuning hyper-parameters. However the scalability of these models to big datasets remains an active topic of research. We introduce a novel re-parametrisation of variational inference for sparse GP regression and latent variable models that allows for an efficient distributed algorithm. This is done by exploiting the decoupling of the data given the inducing points to re-formulate the evidence lower bound in a Map-Reduce setting. We show that the inference scales well with data and computational resources, while preserving a balanced distribution of the load among the nodes. We further demonstrate the utility in scaling Gaussian processes to big data. We show that GP performance improves with increasing amounts of data in regression (on flight data with 2 million records) and latent variable modelling (on MNIST). The results show that GPs perform better than many common models often used for big data.",
    "authors": [
      "Gal, Yarin",
      "van der Wilk, Mark",
      "Rasmussen, Carl Edward"
    ]
  },
  {
    "id": "9a96876e2f8f3dc4f3cf45f02c61c0c1",
    "title": "On the Convergence Rate of Decomposable Submodular Function Minimization",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/9a96876e2f8f3dc4f3cf45f02c61c0c1-Paper.pdf",
    "abstract": "Submodular functions describe a variety of discrete problems in machine learning, signal processing, and computer vision. However, minimizing submodular functions poses a number of algorithmic challenges. Recent work introduced an easy-to-use, parallelizable algorithm for minimizing submodular functions that decompose as the sum of simple\" submodular functions. Empirically, this algorithm performs extremely well, but no theoretical analysis was given. In this paper, we show that the algorithm converges linearly, and we provide upper and lower bounds on the rate of convergence. Our proof relies on the geometry of submodular polyhedra and draws on results from spectral graph theory.\"",
    "authors": [
      "Nishihara, Robert",
      "Jegelka, Stefanie",
      "Jordan, Michael I."
    ]
  },
  {
    "id": "9ad6aaed513b73148b7d49f70afcfb32",
    "title": "Efficient Inference of Continuous Markov Random Fields with Polynomial Potentials",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/9ad6aaed513b73148b7d49f70afcfb32-Paper.pdf",
    "abstract": "In this paper, we prove that every multivariate polynomial with even degree can be decomposed into a sum of convex and concave polynomials. Motivated by this property, we exploit the concave-convex procedure to perform inference on continuous Markov random fields with polynomial potentials. In particular, we show that the concave-convex decomposition of polynomials can be expressed as a sum-of-squares optimization, which can be efficiently solved via semidefinite programming. We demonstrate the effectiveness of our approach in the context of 3D reconstruction, shape from shading and image denoising, and show that our approach significantly outperforms existing approaches in terms of efficiency as well as the quality of the retrieved solution.",
    "authors": [
      "Wang, Shenlong",
      "Schwing, Alex",
      "Urtasun, Raquel"
    ]
  },
  {
    "id": "9c01802ddb981e6bcfbec0f0516b8e35",
    "title": "Metric Learning for Temporal Sequence Alignment",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/9c01802ddb981e6bcfbec0f0516b8e35-Paper.pdf",
    "abstract": "In this paper, we propose to learn a Mahalanobis distance to perform alignment of multivariate time series. The learning examples for this task are time series for which the true alignment is known. We cast the alignment problem as a structured prediction task, and propose realistic losses between alignments for which the optimization is tractable. We provide experiments on real data in the audio-to-audio context, where we show that the learning of a similarity measure leads to improvements in the performance of the alignment task. We also propose to use this metric learning framework to perform feature selection and, from basic audio features, build a combination of these with better alignment performance.",
    "authors": [
      "Garreau, Damien",
      "Lajugie, R\u00e9mi",
      "Arlot, Sylvain",
      "Bach, Francis"
    ]
  },
  {
    "id": "9c82c7143c102b71c593d98d96093fde",
    "title": "Extended and Unscented Gaussian Processes",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/9c82c7143c102b71c593d98d96093fde-Paper.pdf",
    "abstract": "We present two new methods for inference in Gaussian process (GP) models with general nonlinear likelihoods. Inference is based on a variational framework where a Gaussian posterior is assumed and the likelihood is linearized about the variational posterior mean using either a Taylor series expansion or statistical linearization. We show that the parameter updates obtained by these algorithms are equivalent to the state update equations in the iterative extended and unscented Kalman filters respectively, hence we refer to our algorithms as extended and unscented GPs. The unscented GP treats the likelihood as a 'black-box' by not requiring its derivative for inference, so it also applies to non-differentiable likelihood models. We evaluate the performance of our algorithms on a number of synthetic inversion problems and a binary classification dataset.",
    "authors": [
      "Steinberg, Daniel M.",
      "Bonilla, Edwin V."
    ]
  },
  {
    "id": "9c838d2e45b2ad1094d42f4ef36764f6",
    "title": "A State-Space Model for Decoding Auditory Attentional Modulation from MEG in a Competing-Speaker Environment",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/9c838d2e45b2ad1094d42f4ef36764f6-Paper.pdf",
    "abstract": "Humans are able to segregate auditory objects in a complex acoustic scene, through an interplay of bottom-up feature extraction and top-down selective attention in the brain. The detailed mechanism underlying this process is largely unknown and the ability to mimic this procedure is an important problem in artificial intelligence and computational neuroscience. We consider the problem of decoding the attentional state of a listener in a competing-speaker environment from magnetoencephalographic (MEG) recordings from the human brain. We develop a behaviorally inspired state-space model to account for the modulation of the MEG with respect to attentional state of the listener. We construct a decoder based on the maximum a posteriori (MAP) estimate of the state parameters via the Expectation-Maximization (EM) algorithm. The resulting decoder is able to track the attentional modulation of the listener with multi-second resolution using only the envelopes of the two speech streams as covariates. We present simulation studies as well as application to real MEG data from two human subjects. Our results reveal that the proposed decoder provides substantial gains in terms of temporal resolution, complexity, and decoding accuracy.",
    "authors": [
      "Akram, Sahar",
      "Simon, Jonathan Z.",
      "Shamma, Shihab A.",
      "Babadi, Behtash"
    ]
  },
  {
    "id": "9dfcd5e558dfa04aaf37f137a1d9d3e5",
    "title": "Sensory Integration and Density Estimation",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/9dfcd5e558dfa04aaf37f137a1d9d3e5-Paper.pdf",
    "abstract": "The integration of partially redundant information from multiple sensors is a standard computational problem for agents interacting with the world. In man and other primates, integration has been shown psychophysically to be nearly optimal in the sense of error minimization. An influential generalization of this notion of optimality is that populations of multisensory neurons should retain all the information from their unisensory afferents about the underlying, common stimulus [1]. More recently, it was shown empirically that a neural network trained to perform latent-variable density estimation, with the activities of the unisensory neurons as observed data, satisfies the information-preservation criterion, even though the model architecture was not designed to match the true generative process for the data [2]. We prove here an analytical connection between these seemingly different tasks, density estimation and sensory integration; that the former implies the latter for the model used in [2]; but that this does not appear to be true for all models.",
    "authors": [
      "Makin, Joseph G.",
      "Sabes, Philip N."
    ]
  },
  {
    "id": "9fe8593a8a330607d76796b35c64c600",
    "title": "Causal Strategic Inference in Networked Microfinance Economies",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/9fe8593a8a330607d76796b35c64c600-Paper.pdf",
    "abstract": "Performing interventions is a major challenge in economic policy-making. We propose \\emph{causal strategic inference} as a framework for conducting interventions and apply it to large, networked microfinance economies. The basic solution platform consists of modeling a microfinance market as a networked economy, learning the parameters of the model from the real-world microfinance data, and designing algorithms for various computational problems in question. We adopt Nash equilibrium as the solution concept for our model. For a special case of our model, we show that an equilibrium point always exists and that the equilibrium interest rates are unique. For the general case, we give a constructive proof of the existence of an equilibrium point. Our empirical study is based on the microfinance data from Bangladesh and Bolivia, which we use to first learn our models. We show that causal strategic inference can assist policy-makers by evaluating the outcomes of various types of interventions, such as removing a loss-making bank from the market, imposing an interest rate cap, and subsidizing banks.",
    "authors": [
      "Irfan, Mohammad T.",
      "Ortiz, Luis E."
    ]
  },
  {
    "id": "a0a080f42e6f13b3a2df133f073095dd",
    "title": "Exact Post Model Selection Inference for Marginal Screening",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/a0a080f42e6f13b3a2df133f073095dd-Paper.pdf",
    "abstract": "We develop a framework for post model selection inference, via marginal screening, in linear regression. At the core of this framework is a result that characterizes the exact distribution of linear functions of the response $y$, conditional on the model being selected (``condition on selection framework). This allows us to construct valid confidence intervals and hypothesis tests for regression coefficients that account for the selection procedure. In contrast to recent work in high-dimensional statistics, our results are exact (non-asymptotic) and require no eigenvalue-like assumptions on the design matrix $X$. Furthermore, the computational cost of marginal regression, constructing confidence intervals and hypothesis testing is negligible compared to the cost of linear regression, thus making our methods particularly suitable for extremely large datasets. Although we focus on marginal screening to illustrate the applicability of the condition on selection framework, this framework is much more broadly applicable. We show how to apply the proposed framework to several other selection procedures including orthogonal matching pursuit and marginal screening+Lasso.\"",
    "authors": [
      "Lee, Jason D.",
      "Taylor, Jonathan E."
    ]
  },
  {
    "id": "a14ac55a4f27472c5d894ec1c3c743d2",
    "title": "Sequence to Sequence Learning with Neural Networks",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf",
    "abstract": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.",
    "authors": [
      "Sutskever, Ilya",
      "Vinyals, Oriol",
      "Le, Quoc V."
    ]
  },
  {
    "id": "a1afc58c6ca9540d057299ec3016d726",
    "title": "Information-based learning by agents in unbounded state spaces",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/a1afc58c6ca9540d057299ec3016d726-Paper.pdf",
    "abstract": "The idea that animals might use information-driven planning to explore an unknown environment and build an internal model of it has been proposed for quite some time. Recent work has demonstrated that agents using this principle can efficiently learn models of probabilistic environments with discrete, bounded state spaces. However, animals and robots are commonly confronted with unbounded environments. To address this more challenging situation, we study information-based learning strategies of agents in unbounded state spaces using non-parametric Bayesian models. Specifically, we demonstrate that the Chinese Restaurant Process (CRP) model is able to solve this problem and that an Empirical Bayes version is able to efficiently explore bounded and unbounded worlds by relying on little prior information.",
    "authors": [
      "Mobin, Shariq A.",
      "Arnemann, James A.",
      "Sommer, Fritz"
    ]
  },
  {
    "id": "a1d50185e7426cbb0acad1e6ca74b9aa",
    "title": "Multi-Resolution Cascades for Multiclass Object Detection",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/a1d50185e7426cbb0acad1e6ca74b9aa-Paper.pdf",
    "abstract": "An algorithm for learning fast multiclass object detection cascades is introduced. It produces multi-resolution (MRes) cascades, whose early stages are binary target vs. non-target detectors that eliminate false positives, late stages multiclass classifiers that finely discriminate target classes, and middle stages have intermediate numbers of classes, determined in a data-driven manner. This MRes structure is achieved with a new structurally biased boosting algorithm (SBBoost). SBBost extends previous multiclass boosting approaches, whose boosting mechanisms are shown to implement two complementary data-driven biases: 1) the standard bias towards examples difficult to classify, and 2) a bias towards difficult classes. It is shown that structural biases can be implemented by generalizing this class-based bias, so as to encourage the desired MRes structure. This is accomplished through a generalized definition of multiclass margin, which includes a set of bias parameters. SBBoost is a boosting algorithm for maximization of this margin. It can also be interpreted as standard multiclass boosting algorithm augmented with margin thresholds or a cost-sensitive boosting algorithm with costs defined by the bias parameters. A stage adaptive bias policy is then introduced to determine bias parameters in a data driven manner. This is shown to produce MRes cascades that have high detection rate and are computationally efficient. Experiments on multiclass object detection show improved performance over previous solutions.",
    "authors": [
      "Saberian, Mohammad",
      "Vasconcelos, Nuno"
    ]
  },
  {
    "id": "a2137a2ae8e39b5002a3f8909ecb88fe",
    "title": "Generalized Dantzig Selector: Application to the k-support norm",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/a2137a2ae8e39b5002a3f8909ecb88fe-Paper.pdf",
    "abstract": "We propose a Generalized Dantzig Selector (GDS) for linear models, in which any norm encoding the parameter structure can be leveraged for estimation. We investigate both computational and statistical aspects of the GDS. Based on conjugate proximal operator, a flexible inexact ADMM framework is designed for solving GDS. Thereafter, non-asymptotic high-probability bounds are established on the estimation error, which rely on Gaussian widths of the unit norm ball and the error set. Further, we consider a non-trivial example of the GDS using k-support norm. We derive an efficient method to compute the proximal operator for k-support norm since existing methods are inapplicable in this setting. For statistical analysis, we provide upper bounds for the Gaussian widths needed in the GDS analysis, yielding the first statistical recovery guarantee for estimation with the k-support norm. The experimental results confirm our theoretical analysis.",
    "authors": [
      "Chatterjee, Soumyadeep",
      "Chen, Sheng",
      "Banerjee, Arindam"
    ]
  },
  {
    "id": "a5cdd4aa0048b187f7182f1b9ce7a6a7",
    "title": "Conditional Swap Regret and Conditional Correlated Equilibrium",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/a5cdd4aa0048b187f7182f1b9ce7a6a7-Paper.pdf",
    "abstract": "We introduce a natural extension of the notion of swap regret, conditional swap regret, that allows for action modifications conditioned on the player\u2019s action history. We prove a series of new results for conditional swap regret minimization. We present algorithms for minimizing conditional swap regret with bounded conditioning history. We further extend these results to the case where conditional swaps are considered only for a subset of actions. We also define a new notion of equilibrium, conditional correlated equilibrium, that is tightly connected to the notion of conditional swap regret: when all players follow conditional swap regret minimization strategies, then the empirical distribution approaches this equilibrium. Finally, we extend our results to the multi-armed bandit scenario.",
    "authors": [
      "Mohri, Mehryar",
      "Yang, Scott"
    ]
  },
  {
    "id": "a733fa9b25f33689e2adbe72199f0e62",
    "title": "Gaussian Process Volatility Model",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/a733fa9b25f33689e2adbe72199f0e62-Paper.pdf",
    "abstract": "The prediction of time-changing variances is an important task in the modeling of financial data. Standard econometric models are often limited as they assume rigid functional relationships for the evolution of the variance. Moreover, functional parameters are usually learned by maximum likelihood, which can lead to overfitting. To address these problems we introduce GP-Vol, a novel non-parametric model for time-changing variances based on Gaussian Processes. This new model can capture highly flexible functional relationships for the variances. Furthermore, we introduce a new online algorithm for fast inference in GP-Vol. This method is much faster than current offline inference procedures and it avoids overfitting problems by following a fully Bayesian approach. Experiments with financial data show that GP-Vol performs significantly better than current standard alternatives.",
    "authors": [
      "Wu, Yue",
      "Hern\u00e1ndez-Lobato, Jos\u00e9 Miguel",
      "Ghahramani, Zoubin"
    ]
  },
  {
    "id": "a7d8ae4569120b5bec12e7b6e9648b86",
    "title": "Fast Sampling-Based Inference in Balanced Neuronal Networks",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/a7d8ae4569120b5bec12e7b6e9648b86-Paper.pdf",
    "abstract": "Multiple lines of evidence support the notion that the brain performs probabilistic inference in multiple cognitive domains, including perception and decision making. There is also evidence that probabilistic inference may be implemented in the brain through the (quasi-)stochastic activity of neural circuits, producing samples from the appropriate posterior distributions, effectively implementing a Markov chain Monte Carlo algorithm. However, time becomes a fundamental bottleneck in such sampling-based probabilistic representations: the quality of inferences depends on how fast the neural circuit generates new, uncorrelated samples from its stationary distribution (the posterior). We explore this bottleneck in a simple, linear-Gaussian latent variable model, in which posterior sampling can be achieved by stochastic neural networks with linear dynamics. The well-known Langevin sampling (LS) recipe, so far the only sampling algorithm for continuous variables of which a neural implementation has been suggested, naturally fits into this dynamical framework. However, we first show analytically and through simulations that the symmetry of the synaptic weight matrix implied by LS yields critically slow mixing when the posterior is high-dimensional. Next, using methods from control theory, we construct and inspect networks that are optimally fast, and hence orders of magnitude faster than LS, while being far more biologically plausible. In these networks, strong -- but transient -- selective amplification of external noise generates the spatially correlated activity fluctuations prescribed by the posterior. Intriguingly, although a detailed balance of excitation and inhibition is dynamically maintained, detailed balance of Markov chain steps in the resulting sampler is violated, consistent with recent findings on how statistical irreversibility can overcome the speed limitation of random walks in other domains.",
    "authors": [
      "Hennequin, Guillaume",
      "Aitchison, Laurence",
      "Lengyel, Mate"
    ]
  },
  {
    "id": "a87ff679a2f3e71d9181a67b7542122c",
    "title": "Semi-Separable Hamiltonian Monte Carlo for Inference in Bayesian Hierarchical Models",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/a87ff679a2f3e71d9181a67b7542122c-Paper.pdf",
    "abstract": "Sampling from hierarchical Bayesian models is often difficult for MCMC methods, because of the strong correlations between the model parameters and the hyperparameters. Recent Riemannian manifold Hamiltonian Monte Carlo (RMHMC) methods have significant potential advantages in this setting, but are computationally expensive. We introduce a new RMHMC method, which we call semi-separable Hamiltonian Monte Carlo, which uses a specially designed mass matrix that allows the joint Hamiltonian over model parameters and hyperparameters to decompose into two simpler Hamiltonians. This structure is exploited by a new integrator which we call the alternating blockwise leapfrog algorithm. The resulting method can mix faster than simpler Gibbs sampling while being simpler and more efficient than previous instances of RMHMC.",
    "authors": [
      "Zhang, Yichuan",
      "Sutton, Charles"
    ]
  },
  {
    "id": "a89cf525e1d9f04d16ce31165e139a4b",
    "title": "Predicting Useful Neighborhoods for Lazy Local Learning",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/a89cf525e1d9f04d16ce31165e139a4b-Paper.pdf",
    "abstract": "Lazy local learning methods train a classifier on the fly\" at test time, using only a subset of the training instances that are most relevant to the novel test example. The goal is to tailor the classifier to the properties of the data surrounding the test example. Existing methods assume that the instances most useful for building the local model are strictly those closest to the test example. However, this fails to account for the fact that the success of the resulting classifier depends on the full distribution of selected training instances. Rather than simply gather the test example's nearest neighbors, we propose to predict the subset of training data that is jointly relevant to training its local model. We develop an approach to discover patterns between queries and their \"good\" neighborhoods using large-scale multi-label classification with compressed sensing. Given a novel test point, we estimate both the composition and size of the training subset likely to yield an accurate local model. We demonstrate the approach on image classification tasks on SUN and aPascal and show it outperforms traditional global and local approaches.\"",
    "authors": [
      "Yu, Aron",
      "Grauman, Kristen"
    ]
  },
  {
    "id": "a8baa56554f96369ab93e4f3bb068c22",
    "title": "(Almost) No Label No Cry",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/a8baa56554f96369ab93e4f3bb068c22-Paper.pdf",
    "abstract": "In Learning with Label Proportions (LLP), the objective is to learn a supervised classifier when, instead of labels, only label proportions for bags of observations are known. This setting has broad practical relevance, in particular for privacy preserving data processing. We first show that the mean operator, a statistic which aggregates all labels, is minimally sufficient for the minimization of many proper scoring losses with linear (or kernelized) classifiers without using labels. We provide a fast learning algorithm that estimates the mean operator via a manifold regularizer with guaranteed approximation bounds. Then, we present an iterative learning algorithm that uses this as initialization. We ground this algorithm in Rademacher-style generalization bounds that fit the LLP setting, introducing a generalization of Rademacher complexity and a Label Proportion Complexity measure. This latter algorithm optimizes tractable bounds for the corresponding bag-empirical risk. Experiments are provided on fourteen domains, whose size ranges up to 300K observations. They display that our algorithms are scalable and tend to consistently outperform the state of the art in LLP. Moreover, in many cases, our algorithms compete with or are just percents of AUC away from the Oracle that learns knowing all labels. On the largest domains, half a dozen proportions can suffice, i.e. roughly 40K times less than the total number of labels.",
    "authors": [
      "Patrini, Giorgio",
      "Nock, Richard",
      "Rivera, Paul",
      "Caetano, Tiberio"
    ]
  },
  {
    "id": "a8e864d04c95572d1aece099af852d0a",
    "title": "Learning Mixtures of Submodular Functions for Image Collection Summarization",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/a8e864d04c95572d1aece099af852d0a-Paper.pdf",
    "abstract": "We address the problem of image collection summarization by learning mixtures of submodular functions. We argue that submodularity is very natural to this problem, and we show that a number of previously used scoring functions are submodular \u2014 a property not explicitly mentioned in these publications. We provide classes of submodular functions capturing the necessary properties of summaries, namely coverage, likelihood, and diversity. To learn mixtures of these submodular functions as scoring functions, we formulate summarization as a supervised learning problem using large-margin structured prediction. Furthermore, we introduce a novel evaluation metric, which we call V-ROUGE, for automatic summary scoring. While a similar metric called ROUGE has been successfully applied to document summarization [14], no such metric was known for quantifying the quality of image collection summaries. We provide a new dataset consisting of 14 real-world image collections along with many human-generated ground truth summaries collected using mechanical turk. We also extensively compare our method with previously explored methods for this problem and show that our learning approach outperforms all competitors on this new dataset. This paper provides, to our knowledge, the first systematic approach for quantifying the problem of image collection summarization, along with a new dataset of image collections and human summaries.",
    "authors": [
      "Tschiatschek, Sebastian",
      "Iyer, Rishabh K.",
      "Wei, Haochen",
      "Bilmes, Jeff A."
    ]
  },
  {
    "id": "a941493eeea57ede8214fd77d41806bc",
    "title": "Distributed Bayesian Posterior Sampling via Moment Sharing",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/a941493eeea57ede8214fd77d41806bc-Paper.pdf",
    "abstract": "We propose a distributed Markov chain Monte Carlo (MCMC) inference algorithm for large scale Bayesian posterior simulation. We assume that the dataset is partitioned and stored across nodes of a cluster. Our procedure involves an independent MCMC posterior sampler at each node based on its local partition of the data. Moment statistics of the local posteriors are collected from each sampler and propagated across the cluster using expectation propagation message passing with low communication costs. The moment sharing scheme improves posterior estimation quality by enforcing agreement among the samplers. We demonstrate the speed and inference quality of our method with empirical studies on Bayesian logistic regression and sparse linear regression with a spike-and-slab prior.",
    "authors": [
      "Xu, Minjie",
      "Lakshminarayanan, Balaji",
      "Teh, Yee Whye",
      "Zhu, Jun",
      "Zhang, Bo"
    ]
  },
  {
    "id": "a9eb812238f753132652ae09963a05e9",
    "title": "Proximal Quasi-Newton for Computationally Intensive L1-regularized M-estimators",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/a9eb812238f753132652ae09963a05e9-Paper.pdf",
    "abstract": "We consider the class of optimization problems arising from computationally intensive L1-regularized M-estimators, where the function or gradient values are very expensive to compute. A particular instance of interest is the L1-regularized MLE for learning Conditional Random Fields (CRFs), which are a popular class of statistical models for varied structured prediction problems such as sequence labeling, alignment, and classification with label taxonomy. L1-regularized MLEs for CRFs are particularly expensive to optimize since computing the gradient values requires an expensive inference step. In this work, we propose the use of a carefully constructed proximal quasi-Newton algorithm for such computationally intensive M-estimation problems, where we employ an aggressive active set selection technique. In a key contribution of the paper, we show that our proximal quasi-Newton algorithm is provably super-linearly convergent, even in the absence of strong convexity, by leveraging a restricted variant of strong convexity. In our experiments, the proposed algorithm converges considerably faster than current state-of-the-art on the problems of sequence labeling and hierarchical classification.",
    "authors": [
      "Zhong, Kai",
      "Yen, Ian En-Hsu",
      "Dhillon, Inderjit S.",
      "Ravikumar, Pradeep K."
    ]
  },
  {
    "id": "aa2a77371374094fe9e0bc1de3f94ed9",
    "title": "Fast Multivariate Spatio-temporal Analysis via Low Rank Tensor Learning",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/aa2a77371374094fe9e0bc1de3f94ed9-Paper.pdf",
    "abstract": "Accurate and efficient analysis of multivariate spatio-temporal data is critical in climatology, geology, and sociology applications. Existing models usually assume simple inter-dependence among variables, space, and time, and are computationally expensive. We propose a unified low rank tensor learning framework for multivariate spatio-temporal analysis, which can conveniently incorporate different properties in spatio-temporal data, such as spatial clustering and shared structure among variables. We demonstrate how the general framework can be applied to cokriging and forecasting tasks, and develop an efficient greedy algorithm to solve the resulting optimization problem with convergence guarantee. We conduct experiments on both synthetic datasets and real application datasets to demonstrate that our method is not only significantly faster than existing methods but also achieves lower estimation error.",
    "authors": [
      "Bahadori, Mohammad Taha",
      "Yu, Qi (Rose)",
      "Liu, Yan"
    ]
  },
  {
    "id": "ac1dd209cbcc5e5d1c6e28598e8cbbe8",
    "title": "Multi-scale Graphical Models for Spatio-Temporal Processes",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/ac1dd209cbcc5e5d1c6e28598e8cbbe8-Paper.pdf",
    "abstract": "Learning the dependency structure between spatially distributed observations of a spatio-temporal process is an important problem in many fields such as geology, geophysics, atmospheric sciences, oceanography, etc. . However, estimation of such systems is complicated by the fact that they exhibit dynamics at multiple scales of space and time arising due to a combination of diffusion and convection/advection. As we show, time-series graphical models based on vector auto-regressive processes are inef\ufb01cient in capturing such multi-scale structure. In this paper, we present a hierarchical graphical model with physically derived priors that better represents the multi-scale character of these dynamical systems. We also propose algorithms to ef\ufb01ciently estimate the interaction structure from data. We demonstrate results on a general class of problems arising in exploration geophysics by discovering graphical structure that is physically meaningful and provide evidence of its advantages over alternative approaches.",
    "authors": [
      "janoos, firdaus",
      "Denli, Huseyin",
      "Subrahmanya, Niranjan"
    ]
  },
  {
    "id": "ad71c82b22f4f65b9398f76d8be4c615",
    "title": "Bregman Alternating Direction Method of Multipliers",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/ad71c82b22f4f65b9398f76d8be4c615-Paper.pdf",
    "abstract": "The mirror descent algorithm (MDA) generalizes gradient descent by using a Bregman divergence to replace squared Euclidean distance. In this paper, we similarly generalize the alternating direction method of multipliers (ADMM) to Bregman ADMM (BADMM), which allows the choice of different Bregman divergences to exploit the structure of problems. BADMM provides a unified framework for ADMM and its variants, including generalized ADMM, inexact ADMM and Bethe ADMM. We establish the global convergence and the $O(1/T)$ iteration complexity for BADMM. In some cases, BADMM can be faster than ADMM by a factor of $O(n/\\ln n)$ where $n$ is the dimensionality. In solving the linear program of mass transportation problem, BADMM leads to massive parallelism and can easily run on GPU. BADMM is several times faster than highly optimized commercial software Gurobi.",
    "authors": [
      "Wang, Huahua",
      "Banerjee, Arindam"
    ]
  },
  {
    "id": "ad972f10e0800b49d76fed33a21f6698",
    "title": "Bounded Regret for Finite-Armed Structured Bandits",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/ad972f10e0800b49d76fed33a21f6698-Paper.pdf",
    "abstract": "We study a new type of K-armed bandit problem where the expected return of one arm may depend on the returns of other arms. We present a new algorithm for this general class of problems and show that under certain circumstances it is possible to achieve finite expected cumulative regret. We also give problem-dependent lower bounds on the cumulative regret showing that at least in special cases the new algorithm is nearly optimal.",
    "authors": [
      "Lattimore, Tor",
      "Munos, Remi"
    ]
  },
  {
    "id": "af5afd7f7c807171981d443ad4f4f648",
    "title": "Exponential Concentration of a Density Functional Estimator",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/af5afd7f7c807171981d443ad4f4f648-Paper.pdf",
    "abstract": "We analyse a plug-in estimator for a large class of integral functionals of one or more continuous probability densities. This class includes important families of entropy, divergence, mutual information, and their conditional versions. For densities on the d-dimensional unit cube [0,1]^d that lie in a beta-Holder smoothness class, we prove our estimator converges at the rate O(n^(1/(beta+d))). Furthermore, we prove that the estimator obeys an exponential concentration inequality about its mean, whereas most previous related results have bounded only expected error of estimators. Finally, we demonstrate our bounds to the case of conditional Renyi mutual information.",
    "authors": [
      "Singh, Shashank",
      "Poczos, Barnabas"
    ]
  },
  {
    "id": "afda332245e2af431fb7b672a68b659d",
    "title": "Decomposing Parameter Estimation Problems",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/afda332245e2af431fb7b672a68b659d-Paper.pdf",
    "abstract": "We propose a technique for decomposing the parameter learning problem in Bayesian networks into independent learning problems. Our technique applies to incomplete datasets and exploits variables that are either hidden or observed in the given dataset. We show empirically that the proposed technique can lead to orders-of-magnitude savings in learning time. We explain, analytically and empirically, the reasons behind our reported savings, and compare the proposed technique to related ones that are sometimes used by inference algorithms.",
    "authors": [
      "Refaat, Khaled S.",
      "Choi, Arthur",
      "Darwiche, Adnan"
    ]
  },
  {
    "id": "aff1621254f7c1be92f64550478c56e6",
    "title": "Convex Optimization Procedure for Clustering: Theoretical Revisit",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/aff1621254f7c1be92f64550478c56e6-Paper.pdf",
    "abstract": "In this paper, we present theoretical analysis of SON~--~a convex optimization procedure for clustering using a sum-of-norms (SON) regularization recently proposed in \\cite{ICML2011Hocking_419,SON, Lindsten650707, pelckmans2005convex}. In particular, we show if the samples are drawn from two cubes, each being one cluster, then SON can provably identify the cluster membership provided that the distance between the two cubes is larger than a threshold which (linearly) depends on the size of the cube and the ratio of numbers of samples in each cluster. To the best of our knowledge, this paper is the first to provide a rigorous analysis to understand why and when SON works. We believe this may provide important insights to develop novel convex optimization based algorithms for clustering.",
    "authors": [
      "Zhu, Changbo",
      "Xu, Huan",
      "Leng, Chenlei",
      "Yan, Shuicheng"
    ]
  },
  {
    "id": "b056eb1587586b71e2da9acfe4fbd19e",
    "title": "Submodular Attribute Selection for Action Recognition in Video",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/b056eb1587586b71e2da9acfe4fbd19e-Paper.pdf",
    "abstract": "In real-world action recognition problems, low-level features cannot adequately characterize the rich spatial-temporal structures in action videos. In this work, we encode actions based on attributes that describes actions as high-level concepts: \\textit{e.g.}, jump forward and motion in the air. We base our analysis on two types of action attributes. One type of action attributes is generated by humans. The second type is data-driven attributes, which is learned from data using dictionary learning methods. Attribute-based representation may exhibit high variance due to noisy and redundant attributes. We propose a discriminative and compact attribute-based representation by selecting a subset of discriminative attributes from a large attribute set. Three attribute selection criteria are proposed and formulated as a submodular optimization problem. A greedy optimization algorithm is presented and guaranteed to be at least (1-1/e)-approximation to the optimum. Experimental results on the Olympic Sports and UCF101 datasets demonstrate that the proposed attribute-based representation can significantly boost the performance of action recognition algorithms and outperform most recently proposed recognition approaches.",
    "authors": [
      "Zheng, Jingjing",
      "Jiang, Zhuolin",
      "Chellappa, Rama",
      "Phillips, Jonathon P."
    ]
  },
  {
    "id": "b139e104214a08ae3f2ebcce149cdf6e",
    "title": "Quantized Estimation of Gaussian Sequence Models in Euclidean Balls",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/b139e104214a08ae3f2ebcce149cdf6e-Paper.pdf",
    "abstract": "A central result in statistical theory is Pinsker's theorem, which characterizes the minimax rate in the normal means model of nonparametric estimation. In this paper, we present an extension to Pinsker's theorem where estimation is carried out under storage or communication constraints. In particular, we place limits on the number of bits used to encode an estimator, and analyze the excess risk in terms of this constraint, the signal size, and the noise level. We give sharp upper and lower bounds for the case of a Euclidean ball, which establishes the Pareto-optimal minimax tradeoff between storage and risk in this setting.",
    "authors": [
      "Zhu, Yuancheng",
      "Lafferty, John"
    ]
  },
  {
    "id": "b147a61c1d07c1c999560f62add6dbc7",
    "title": "Large-Margin Convex Polytope Machine",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/b147a61c1d07c1c999560f62add6dbc7-Paper.pdf",
    "abstract": "We present the Convex Polytope Machine (CPM), a novel non-linear learning algorithm for large-scale binary classification tasks. The CPM finds a large margin convex polytope separator which encloses one class. We develop a stochastic gradient descent based algorithm that is amenable to massive datasets, and augment it with a heuristic procedure to avoid sub-optimal local minima. Our experimental evaluations of the CPM on large-scale datasets from distinct domains (MNIST handwritten digit recognition, text topic, and web security) demonstrate that the CPM trains models faster, sometimes several orders of magnitude, than state-of-the-art similar approaches and kernel-SVM methods while achieving comparable or better classification performance. Our empirical results suggest that, unlike prior similar approaches, we do not need to control the number of sub-classifiers (sides of the polytope) to avoid overfitting.",
    "authors": [
      "Kantchelian, Alex",
      "Tschantz, Michael C.",
      "Huang, Ling",
      "Bartlett, Peter L.",
      "Joseph, Anthony D.",
      "Tygar, J. D."
    ]
  },
  {
    "id": "b1563a78ec59337587f6ab6397699afc",
    "title": "A provable SVD-based algorithm for learning topics in dominant admixture corpus",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/b1563a78ec59337587f6ab6397699afc-Paper.pdf",
    "abstract": "Topic models, such as Latent Dirichlet Allocation (LDA), posit that documents are drawn from admixtures of distributions over words, known as topics. The inference problem of recovering topics from such a collection of documents drawn from admixtures, is NP-hard. Making a strong assumption called separability, [4] gave the first provable algorithm for inference. For the widely used LDA model, [6] gave a provable algorithm using clever tensor-methods. But [4, 6] do not learn topic vectors with bounded $l_1$ error (a natural measure for probability vectors). Our aim is to develop a model which makes intuitive and empirically supported assumptions and to design an algorithm with natural, simple components such as SVD, which provably solves the inference problem for the model with bounded $l_1$ error. A topic in LDA and other models is essentially characterized by a group of co-occurring words. Motivated by this, we introduce topic specific Catchwords, a group of words which occur with strictly greater frequency in a topic than any other topic individually and are required to have high frequency together rather than individually. A major contribution of the paper is to show that under this more realistic assumption, which is empirically verified on real corpora, a singular value decomposition (SVD) based algorithm with a crucial pre-processing step of thresholding, can provably recover the topics from a collection of documents drawn from Dominant admixtures. Dominant admixtures are convex combination of distributions in which one distribution has a significantly higher contribution than the others. Apart from the simplicity of the algorithm, the sample complexity has near optimal dependence on $w_0$, the lowest probability that a topic is dominant, and is better than [4]. Empirical evidence shows that on several real world corpora, both Catchwords and Dominant admixture assumptions hold and the proposed algorithm substantially outperforms the state of the art [5].",
    "authors": [
      "Bansal, Trapit",
      "Bhattacharyya, Chiranjib",
      "Kannan, Ravindran"
    ]
  },
  {
    "id": "b1eec33c726a60554bc78518d5f9b32c",
    "title": "Divide-and-Conquer Learning by Anchoring a Conical Hull",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/b1eec33c726a60554bc78518d5f9b32c-Paper.pdf",
    "abstract": "We reduce a broad class of machine learning problems, usually addressed by EM or sampling, to the problem of finding the $k$ extremal rays spanning the conical hull of a data point set. These $k$ ``anchors'' lead to a global solution and a more interpretable model that can even outperform EM and sampling on generalization error. To find the $k$ anchors, we propose a novel divide-and-conquer learning scheme ``DCA'' that distributes the problem to $\\mathcal O(k\\log k)$ same-type sub-problems on different low-D random hyperplanes, each can be solved by any solver. For the 2D sub-problem, we present a non-iterative solver that only needs to compute an array of cosine values and its max/min entries. DCA also provides a faster subroutine for other methods to check whether a point is covered in a conical hull, which improves algorithm design in multiple dimensions and brings significant speedup to learning. We apply our method to GMM, HMM, LDA, NMF and subspace clustering, then show its competitive performance and scalability over other methods on rich datasets.",
    "authors": [
      "Zhou, Tianyi",
      "Bilmes, Jeff A.",
      "Guestrin, Carlos"
    ]
  },
  {
    "id": "b3b43aeeacb258365cc69cdaf42a68af",
    "title": "Discriminative Metric Learning by Neighborhood Gerrymandering",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/b3b43aeeacb258365cc69cdaf42a68af-Paper.pdf",
    "abstract": "We formulate the problem of metric learning for k nearest neighbor classification as a large margin structured prediction problem, with a latent variable representing the choice of neighbors and the task loss directly corresponding to classification error. We describe an efficient algorithm for exact loss augmented inference,and a fast gradient descent algorithm for learning in this model. The objective drives the metric to establish neighborhood boundaries that benefit the true class labels for the training points. Our approach, reminiscent of gerrymandering (redrawing of political boundaries to provide advantage to certain parties), is more direct in its handling of optimizing classification accuracy than those previously proposed. In experiments on a variety of data sets our method is shown to achieve excellent results compared to current state of the art in metric learning.",
    "authors": [
      "Trivedi, Shubhendu",
      "Mcallester, David",
      "Shakhnarovich, Greg"
    ]
  },
  {
    "id": "b432f34c5a997c8e7c806a895ecc5e25",
    "title": "Learning on graphs using Orthonormal Representation is Statistically Consistent",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/b432f34c5a997c8e7c806a895ecc5e25-Paper.pdf",
    "abstract": "Existing research \\cite{reg} suggests that embedding graphs on a unit sphere can be beneficial in learning labels on the vertices of a graph. However the choice of optimal embedding remains an open issue. \\emph{Orthonormal representation} of graphs, a class of embeddings over the unit sphere, was introduced by Lov\\'asz \\cite{lovasz_shannon}. In this paper, we show that there exists orthonormal representations which are statistically consistent over a large class of graphs, including power law and random graphs. This result is achieved by extending the notion of consistency designed in the inductive setting to graph transduction. As part of the analysis, we explicitly derive relationships between the Rademacher complexity measure and structural properties of graphs, such as the chromatic number. We further show the fraction of vertices of a graph $G$, on $n$ nodes, that need to be labelled for the learning algorithm to be consistent, also known as labelled sample complexity, is $ \\Omega\\left(\\frac{\\vartheta(G)}{n}\\right)^{\\frac{1}{4}}$ where $\\vartheta(G)$ is the famous Lov\\'asz~$\\vartheta$ function of the graph. This, for the first time, relates labelled sample complexity to graph connectivity properties, such as the density of graphs. In the multiview setting, whenever individual views are expressed by a graph, it is a well known heuristic that a convex combination of Laplacians \\cite{lap_mv1} tend to improve accuracy. The analysis presented here easily extends to Multiple graph transduction, and helps develop a sound statistical understanding of the heuristic, previously unavailable.",
    "authors": [
      "Shivanna, Rakesh",
      "Bhattacharyya, Chiranjib"
    ]
  },
  {
    "id": "b51a15f382ac914391a58850ab343b00",
    "title": "Generalized Unsupervised Manifold Alignment",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/b51a15f382ac914391a58850ab343b00-Paper.pdf",
    "abstract": "In this paper, we propose a generalized Unsupervised Manifold Alignment (GUMA) method to build the connections between different but correlated datasets without any known correspondences. Based on the assumption that datasets of the same theme usually have similar manifold structures, GUMA is formulated into an explicit integer optimization problem considering the structure matching and preserving criteria, as well as the feature comparability of the corresponding points in the mutual embedding space. The main benefits of this model include: (1) simultaneous discovery and alignment of manifold structures; (2) fully unsupervised matching without any pre-specified correspondences; (3) efficient iterative alignment without computations in all permutation cases. Experimental results on dataset matching and real-world applications demonstrate the effectiveness and the practicability of our manifold alignment method.",
    "authors": [
      "Cui, Zhen",
      "Chang, Hong",
      "Shan, Shiguang",
      "Chen, Xilin"
    ]
  },
  {
    "id": "b53b3a3d6ab90ce0268229151c9bde11",
    "title": "Grouping-Based Low-Rank Trajectory Completion and 3D Reconstruction",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/b53b3a3d6ab90ce0268229151c9bde11-Paper.pdf",
    "abstract": "Extracting 3D shape of deforming objects in monocular videos, a task known as non-rigid structure-from-motion (NRSfM), has so far been studied only on synthetic datasets and controlled environments. Typically, the objects to reconstruct are pre-segmented, they exhibit limited rotations and occlusions, or full-length trajectories are assumed. In order to integrate NRSfM into current video analysis pipelines, one needs to consider as input realistic -thus incomplete- tracking, and perform spatio-temporal grouping to segment the objects from their surroundings. Furthermore, NRSfM needs to be robust to noise in both segmentation and tracking, e.g., drifting, segmentation leaking'', optical flowbleeding'' etc. In this paper, we make a first attempt towards this goal, and propose a method that combines dense optical flow tracking, motion trajectory clustering and NRSfM for 3D reconstruction of objects in videos. For each trajectory cluster, we compute multiple reconstructions by minimizing the reprojection error and the rank of the 3D shape under different rank bounds of the trajectory matrix. We show that dense 3D shape is extracted and trajectories are completed across occlusions and low textured regions, even under mild relative motion between the object and the camera. We achieve competitive results on a public NRSfM benchmark while using fixed parameters across all sequences and handling incomplete trajectories, in contrast to existing approaches. We further test our approach on popular video segmentation datasets. To the best of our knowledge, our method is the first to extract dense object models from realistic videos, such as those found in Youtube or Hollywood movies, without object-specific priors.",
    "authors": [
      "Fragkiadaki, Katerina",
      "Salas, Marta",
      "Arbelaez, Pablo",
      "Malik, Jitendra"
    ]
  },
  {
    "id": "b5488aeff42889188d03c9895255cecc",
    "title": "A statistical model for tensor PCA",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/b5488aeff42889188d03c9895255cecc-Paper.pdf",
    "abstract": "We consider the Principal Component Analysis problem for large tensors of arbitrary order k under a single-spike (or rank-one plus noise) model. On the one hand, we use information theory, and recent results in probability theory to establish necessary and sufficient conditions under which the principal component can be estimated using unbounded computational resources. It turns out that this is possible as soon as the signal-to-noise ratio beta becomes larger than C\\sqrt{k log k} (and in particular beta can remain bounded has the problem dimensions increase). On the other hand, we analyze several polynomial-time estimation algorithms, based on tensor unfolding, power iteration and message passing ideas from graphical models. We show that, unless the signal-to-noise ratio diverges in the system dimensions, none of these approaches succeeds. This is possibly related to a fundamental limitation of computationally tractable estimators for this problem. For moderate dimensions, we propose an hybrid approach that uses unfolding together with power iteration, and show that it outperforms significantly baseline methods. Finally, we consider the case in which additional side information is available about the unknown signal. We characterize the amount of side information that allow the iterative algorithms to converge to a good estimate.",
    "authors": [
      "Richard, Emile",
      "Montanari, Andrea"
    ]
  },
  {
    "id": "b55ec28c52d5f6205684a473a2193564",
    "title": "Making Pairwise Binary Graphical Models Attractive",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/b55ec28c52d5f6205684a473a2193564-Paper.pdf",
    "abstract": "Computing the partition function (i.e., the normalizing constant) of a given pairwise binary graphical model is NP-hard in general. As a result, the partition function is typically estimated by approximate inference algorithms such as belief propagation (BP) and tree-reweighted belief propagation (TRBP). The former provides reasonable estimates in practice but has convergence issues. The later has better convergence properties but typically provides poorer estimates. In this work, we propose a novel scheme that has better convergence properties than BP and provably provides better partition function estimates in many instances than TRBP. In particular, given an arbitrary pairwise binary graphical model, we construct a specific ``attractive'' 2-cover. We explore the properties of this special cover and show that it can be used to construct an algorithm with the desired properties.",
    "authors": [
      "Ruozzi, Nicholas",
      "Jebara, Tony"
    ]
  },
  {
    "id": "b571ecea16a9824023ee1af16897a582",
    "title": "Subspace Embeddings for the Polynomial Kernel",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/b571ecea16a9824023ee1af16897a582-Paper.pdf",
    "abstract": "Sketching is a powerful dimensionality reduction tool for accelerating statistical learning algorithms. However, its applicability has been limited to a certain extent since the crucial ingredient, the so-called oblivious subspace embedding, can only be applied to data spaces with an explicit representation as the column span or row span of a matrix, while in many settings learning is done in a high-dimensional space implicitly defined by the data matrix via a kernel transformation. We propose the first {\\em fast} oblivious subspace embeddings that are able to embed a space induced by a non-linear kernel {\\em without} explicitly mapping the data to the high-dimensional space. In particular, we propose an embedding for mappings induced by the polynomial kernel. Using the subspace embeddings, we obtain the fastest known algorithms for computing an implicit low rank approximation of the higher-dimension mapping of the data matrix, and for computing an approximate kernel PCA of the data, as well as doing approximate kernel principal component regression.",
    "authors": [
      "Avron, Haim",
      "Nguyen, Huy",
      "Woodruff, David"
    ]
  },
  {
    "id": "b59c67bf196a4758191e42f76670ceba",
    "title": "On the relations of LFPs & Neural Spike Trains",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/b59c67bf196a4758191e42f76670ceba-Paper.pdf",
    "abstract": "One of the goals of neuroscience is to identify neural networks that correlate with important behaviors, environments, or genotypes. This work proposes a strategy for identifying neural networks characterized by time- and frequency-dependent connectivity patterns, using convolutional dictionary learning that links spike-train data to local field potentials (LFPs) across multiple areas of the brain. Analytical contributions are: (i) modeling dynamic relationships between LFPs and spikes; (ii) describing the relationships between spikes and LFPs, by analyzing the ability to predict LFP data from one region based on spiking information from across the brain; and (iii) development of a clustering methodology that allows inference of similarities in neurons from multiple regions. Results are based on data sets in which spike and LFP data are recorded simultaneously from up to 16 brain regions in a mouse.",
    "authors": [
      "Carlson, David E.",
      "Schaich Borg, Jana",
      "Dzirasa, Kafui",
      "Carin, Lawrence"
    ]
  },
  {
    "id": "b6a1085a27ab7bff7550f8a3bd017df8",
    "title": "Online Optimization for Max-Norm Regularization",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/b6a1085a27ab7bff7550f8a3bd017df8-Paper.pdf",
    "abstract": "Max-norm regularizer has been extensively studied in the last decade as it promotes an effective low rank estimation of the underlying data. However, max-norm regularized problems are typically formulated and solved in a batch manner, which prevents it from processing big data due to possible memory bottleneck. In this paper, we propose an online algorithm for solving max-norm regularized problems that is scalable to large problems. Particularly, we consider the matrix decomposition problem as an example, although our analysis can also be applied in other problems such as matrix completion. The key technique in our algorithm is to reformulate the max-norm into a matrix factorization form, consisting of a basis component and a coefficients one. In this way, we can solve the optimal basis and coefficients alternatively. We prove that the basis produced by our algorithm converges to a stationary point asymptotically. Experiments demonstrate encouraging results for the effectiveness and robustness of our algorithm. See the full paper at arXiv:1406.3190.",
    "authors": [
      "Shen, Jie",
      "Xu, Huan",
      "Li, Ping"
    ]
  },
  {
    "id": "b7087c1f4f89e63af8d46f3b20271153",
    "title": "Cone-Constrained Principal Component Analysis",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/b7087c1f4f89e63af8d46f3b20271153-Paper.pdf",
    "abstract": "Estimating a vector from noisy quadratic observations is a task that arises naturally in many contexts, from dimensionality reduction, to synchronization and phase retrieval problems. It is often the case that additional information is available about the unknown vector (for instance, sparsity, sign or magnitude of its entries). Many authors propose non-convex quadratic optimization problems that aim at exploiting optimally this information. However, solving these problems is typically NP-hard. We consider a simple model for noisy quadratic observation of an unknown vector $\\bvz$. The unknown vector is constrained to belong to a cone $\\Cone \\ni \\bvz$. While optimal estimation appears to be intractable for the general problems in this class, we provide evidence that it is tractable when $\\Cone$ is a convex cone with an efficient projection. This is surprising, since the corresponding optimization problem is non-convex and --from a worst case perspective-- often NP hard. We characterize the resulting minimax risk in terms of the statistical dimension of the cone $\\delta(\\Cone)$. This quantity is already known to control the risk of estimation from gaussian observations and random linear measurements. It is rather surprising that the same quantity plays a role in the estimation risk from quadratic measurements.",
    "authors": [
      "Deshpande, Yash",
      "Montanari, Andrea",
      "Richard, Emile"
    ]
  },
  {
    "id": "b710915795b9e9c02cf10d6d2bdb688c",
    "title": "Fast Training of Pose Detectors in the Fourier Domain",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/b710915795b9e9c02cf10d6d2bdb688c-Paper.pdf",
    "abstract": "In many datasets, the samples are related by a known image transformation, such as rotation, or a repeatable non-rigid deformation. This applies to both datasets with the same objects under different viewpoints, and datasets augmented with virtual samples. Such datasets possess a high degree of redundancy, because geometrically-induced transformations should preserve intrinsic properties of the objects. Likewise, ensembles of classifiers used for pose estimation should also share many characteristics, since they are related by a geometric transformation. By assuming that this transformation is norm-preserving and cyclic, we propose a closed-form solution in the Fourier domain that can eliminate most redundancies. It can leverage off-the-shelf solvers with no modification (e.g. libsvm), and train several pose classifiers simultaneously at no extra cost. Our experiments show that training a sliding-window object detector and pose estimator can be sped up by orders of magnitude, for transformations as diverse as planar rotation, the walking motion of pedestrians, and out-of-plane rotations of cars.",
    "authors": [
      "Henriques, Jo\u00e3o F.",
      "Martins, Pedro",
      "Caseiro, Rui F.",
      "Batista, Jorge"
    ]
  },
  {
    "id": "b73dfe25b4b8714c029b37a6ad3006fa",
    "title": "Optimistic Planning in Markov Decision Processes Using a Generative Model",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/b73dfe25b4b8714c029b37a6ad3006fa-Paper.pdf",
    "abstract": "We consider the problem of online planning in a Markov decision process with discounted rewards for any given initial state. We consider the PAC sample complexity problem of computing, with probability $1-\\delta$, an $\\epsilon$-optimal action using the smallest possible number of calls to the generative model (which provides reward and next-state samples). We design an algorithm, called StOP (for Stochastic-Optimistic Planning), based on the optimism in the face of uncertainty\" principle. StOP can be used in the general setting, requires only a generative model, and enjoys a complexity bound that only depends on the local structure of the MDP.\"",
    "authors": [
      "Sz\u00f6r\u00e9nyi, Bal\u00e1zs",
      "Kedenburg, Gunnar",
      "Munos, Remi"
    ]
  },
  {
    "id": "b7892fb3c2f009c65f686f6355c895b5",
    "title": "Bayesian Nonlinear Support Vector Machines and Discriminative Factor Modeling",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/b7892fb3c2f009c65f686f6355c895b5-Paper.pdf",
    "abstract": "A new Bayesian formulation is developed for nonlinear support vector machines (SVMs), based on a Gaussian process and with the SVM hinge loss expressed as a scaled mixture of normals. We then integrate the Bayesian SVM into a factor model, in which feature learning and nonlinear classifier design are performed jointly; almost all previous work on such discriminative feature learning has assumed a linear classifier. Inference is performed with expectation conditional maximization (ECM) and Markov Chain Monte Carlo (MCMC). An extensive set of experiments demonstrate the utility of using a nonlinear Bayesian SVM within discriminative feature learning and factor modeling, from the standpoints of accuracy and interpretability",
    "authors": [
      "Henao, Ricardo",
      "Yuan, Xin",
      "Carin, Lawrence"
    ]
  },
  {
    "id": "b865367fc4c0845c0682bd466e6ebf4c",
    "title": "Feedback Detection for Live Predictors",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/b865367fc4c0845c0682bd466e6ebf4c-Paper.pdf",
    "abstract": "A predictor that is deployed in a live production system may perturb the features it uses to make predictions. Such a feedback loop can occur, for example, when a model that predicts a certain type of behavior ends up causing the behavior it predicts, thus creating a self-fulfilling prophecy. In this paper we analyze predictor feedback detection as a causal inference problem, and introduce a local randomization scheme that can be used to detect non-linear feedback in real-world problems. We conduct a pilot study for our proposed methodology using a predictive system currently deployed as a part of a search engine.",
    "authors": [
      "Wager, Stefan",
      "Chamandy, Nick",
      "Muralidharan, Omkar",
      "Najmi, Amir"
    ]
  },
  {
    "id": "b86e8d03fe992d1b0e19656875ee557c",
    "title": "Do Convnets Learn Correspondence?",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/b86e8d03fe992d1b0e19656875ee557c-Paper.pdf",
    "abstract": "Convolutional neural nets (convnets) trained from massive labeled datasets have substantially improved the state-of-the-art in image classification and object detection. However, visual understanding requires establishing correspondence on a finer level than object category. Given their large pooling regions and training from whole-image labels, it is not clear that convnets derive their success from an accurate correspondence model which could be used for precise localization. In this paper, we study the effectiveness of convnet activation features for tasks requiring correspondence. We present evidence that convnet features localize at a much finer scale than their receptive field sizes, that they can be used to perform intraclass aligment as well as conventional hand-engineered features, and that they outperform conventional features in keypoint prediction on objects from PASCAL VOC 2011.",
    "authors": [
      "Long, Jonathan L.",
      "Zhang, Ning",
      "Darrell, Trevor"
    ]
  },
  {
    "id": "b9d487a30398d42ecff55c228ed5652b",
    "title": "Convolutional Neural Network Architectures for Matching Natural Language Sentences",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/b9d487a30398d42ecff55c228ed5652b-Paper.pdf",
    "abstract": "Semantic matching is of central importance to many natural language tasks \\cite{bordes2014semantic,RetrievalQA}. A successful matching algorithm needs to adequately model the internal structures of language objects and the interaction between them. As a step toward this goal, we propose convolutional neural network models for matching two sentences, by adapting the convolutional strategy in vision and speech. The proposed models not only nicely represent the hierarchical structures of sentences with their layer-by-layer composition and pooling, but also capture the rich matching patterns at different levels. Our models are rather generic, requiring no prior knowledge on language, and can hence be applied to matching tasks of different nature and in different languages. The empirical study on a variety of matching tasks demonstrates the efficacy of the proposed model on a variety of matching tasks and its superiority to competitor models.",
    "authors": [
      "Hu, Baotian",
      "Lu, Zhengdong",
      "Li, Hang",
      "Chen, Qingcai"
    ]
  },
  {
    "id": "b9f94c77652c9a76fc8a442748cd54bd",
    "title": "Conditional Random Field Autoencoders for Unsupervised Structured Prediction",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/b9f94c77652c9a76fc8a442748cd54bd-Paper.pdf",
    "abstract": "We introduce a framework for unsupervised learning of structured predictors with overlapping, global features. Each input's latent representation is predicted conditional on the observed data using a feature-rich conditional random field (CRF). Then a reconstruction of the input is (re)generated, conditional on the latent structure, using a generative model which factorizes similarly to the CRF. The autoencoder formulation enables efficient exact inference without resorting to unrealistic independence assumptions or restricting the kinds of features that can be used. We illustrate insightful connections to traditional autoencoders, posterior regularization and multi-view learning. Finally, we show competitive results with instantiations of the framework for two canonical tasks in natural language processing: part-of-speech induction and bitext word alignment, and show that training our model can be substantially more efficient than comparable feature-rich baselines.",
    "authors": [
      "Ammar, Waleed",
      "Dyer, Chris",
      "Smith, Noah A."
    ]
  },
  {
    "id": "bb7946e7d85c81a9e69fee1cea4a087c",
    "title": "Optimal rates for k-NN density and mode estimation",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/bb7946e7d85c81a9e69fee1cea4a087c-Paper.pdf",
    "abstract": "We present two related contributions of independent interest: (1) high-probability finite sample rates for $k$-NN density estimation, and (2) practical mode estimators -- based on $k$-NN -- which attain minimax-optimal rates under surprisingly general distributional conditions.",
    "authors": [
      "Dasgupta, Sanjoy",
      "Kpotufe, Samory"
    ]
  },
  {
    "id": "bca82e41ee7b0833588399b1fcd177c7",
    "title": "Coresets for k-Segmentation of Streaming Data",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/bca82e41ee7b0833588399b1fcd177c7-Paper.pdf",
    "abstract": "Life-logging video streams, financial time series, and Twitter tweets are a few examples of high-dimensional signals over practically unbounded time. We consider the problem of computing optimal segmentation of such signals by k-piecewise linear function, using only one pass over the data by maintaining a coreset for the signal. The coreset enables fast further analysis such as automatic summarization and analysis of such signals. A coreset (core-set) is a compact representation of the data seen so far, which approximates the data well for a specific task -- in our case, segmentation of the stream. We show that, perhaps surprisingly, the segmentation problem admits coresets of cardinality only linear in the number of segments k, independently of both the dimension d of the signal, and its number n of points. More precisely, we construct a representation of size O(klog n /eps^2) that provides a (1+eps)-approximation for the sum of squared distances to any given k-piecewise linear function. Moreover, such coresets can be constructed in a parallel streaming approach. Our results rely on a novel eduction of statistical estimations to problems in computational geometry. We empirically evaluate our algorithms on very large synthetic and real data sets from GPS, video and financial domains, using 255 machines in Amazon cloud.",
    "authors": [
      "Rosman, Guy",
      "Volkov, Mikhail",
      "Feldman, Dan",
      "Fisher III, John W.",
      "Rus, Daniela"
    ]
  },
  {
    "id": "be3159ad04564bfb90db9e32851ebf9c",
    "title": "Message Passing Inference for Large Scale Graphical Models with High Order Potentials",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/be3159ad04564bfb90db9e32851ebf9c-Paper.pdf",
    "abstract": "To keep up with the Big Data challenge, parallelized algorithms based on dual decomposition have been proposed to perform inference in Markov random fields. Despite this parallelization, current algorithms struggle when the energy has high order terms and the graph is densely connected. In this paper we propose a partitioning strategy followed by a message passing algorithm which is able to exploit pre-computations. It only updates the high-order factors when passing messages across machines. We demonstrate the effectiveness of our approach on the task of joint layout and semantic segmentation estimation from single images, and show that our approach is orders of magnitude faster than current methods.",
    "authors": [
      "Zhang, Jian",
      "Schwing, Alex",
      "Urtasun, Raquel"
    ]
  },
  {
    "id": "be53ee61104935234b174e62a07e53cf",
    "title": "Weighted importance sampling for off-policy learning with linear function approximation",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/be53ee61104935234b174e62a07e53cf-Paper.pdf",
    "abstract": "Importance sampling is an essential component of off-policy model-free reinforcement learning algorithms. However, its most effective variant, \\emph{weighted} importance sampling, does not carry over easily to function approximation and, because of this, it is not utilized in existing off-policy learning algorithms. In this paper, we take two steps toward bridging this gap. First, we show that weighted importance sampling can be viewed as a special case of weighting the error of individual training samples, and that this weighting has theoretical and empirical benefits similar to those of weighted importance sampling. Second, we show that these benefits extend to a new weighted-importance-sampling version of off-policy LSTD(lambda). We show empirically that our new WIS-LSTD(lambda) algorithm can result in much more rapid and reliable convergence than conventional off-policy LSTD(lambda) (Yu 2010, Bertsekas & Yu 2009).",
    "authors": [
      "Mahmood, A. Rupam",
      "van Hasselt, Hado P.",
      "Sutton, Richard S."
    ]
  },
  {
    "id": "be83ab3ecd0db773eb2dc1b0a17836a1",
    "title": "Incremental Clustering: The Case for Extra Clusters",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/be83ab3ecd0db773eb2dc1b0a17836a1-Paper.pdf",
    "abstract": "The explosion in the amount of data available for analysis often necessitates a transition from batch to incremental clustering methods, which process one element at a time and typically store only a small subset of the data. In this paper, we initiate the formal analysis of incremental clustering methods focusing on the types of cluster structure that they are able to detect. We find that the incremental setting is strictly weaker than the batch model, proving that a fundamental class of cluster structures that can readily be detected in the batch setting is impossible to identify using any incremental method. Furthermore, we show how the limitations of incremental clustering can be overcome by allowing additional clusters.",
    "authors": [
      "Ackerman, Margareta",
      "Dasgupta, Sanjoy"
    ]
  },
  {
    "id": "beed13602b9b0e6ecb5b568ff5058f07",
    "title": "Positive Curvature and Hamiltonian Monte Carlo",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/beed13602b9b0e6ecb5b568ff5058f07-Paper.pdf",
    "abstract": "The Jacobi metric introduced in mathematical physics can be used to analyze Hamiltonian Monte Carlo (HMC). In a geometrical setting, each step of HMC corresponds to a geodesic on a Riemannian manifold with a Jacobi metric. Our calculation of the sectional curvature of this HMC manifold allows us to see that it is positive in cases such as sampling from a high dimensional multivariate Gaussian. We show that positive curvature can be used to prove theoretical concentration results for HMC Markov chains.",
    "authors": [
      "Seiler, Christof",
      "Rubinstein-Salzedo, Simon",
      "Holmes, Susan"
    ]
  },
  {
    "id": "c06d06da9666a219db15cf575aff2824",
    "title": "Inferring sparse representations of continuous signals with continuous orthogonal matching pursuit",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/c06d06da9666a219db15cf575aff2824-Paper.pdf",
    "abstract": "Many signals, such as spike trains recorded in multi-channel electrophysiological recordings, may be represented as the sparse sum of translated and scaled copies of waveforms whose timing and amplitudes are of interest. From the aggregate signal, one may seek to estimate the identities, amplitudes, and translations of the waveforms that compose the signal. Here we present a fast method for recovering these identities, amplitudes, and translations. The method involves greedily selecting component waveforms and then refining estimates of their amplitudes and translations, moving iteratively between these steps in a process analogous to the well-known Orthogonal Matching Pursuit (OMP) algorithm. Our approach for modeling translations borrows from Continuous Basis Pursuit (CBP), which we extend in several ways: by selecting a subspace that optimally captures translated copies of the waveforms, replacing the convex optimization problem with a greedy approach, and moving to the Fourier domain to more precisely estimate time shifts. We test the resulting method, which we call Continuous Orthogonal Matching Pursuit (COMP), on simulated and neural data, where it shows gains over CBP in both speed and accuracy.",
    "authors": [
      "Knudson, Karin C.",
      "Yates, Jacob",
      "Huk, Alexander",
      "Pillow, Jonathan W."
    ]
  },
  {
    "id": "c0c7c76d30bd3dcaefc96f40275bdc0a",
    "title": "Zeta Hull Pursuits: Learning Nonconvex Data Hulls",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/c0c7c76d30bd3dcaefc96f40275bdc0a-Paper.pdf",
    "abstract": "Selecting a small informative subset from a given dataset, also called column sampling, has drawn much attention in machine learning. For incorporating structured data information into column sampling, research efforts were devoted to the cases where data points are fitted with clusters, simplices, or general convex hulls. This paper aims to study nonconvex hull learning which has rarely been investigated in the literature. In order to learn data-adaptive nonconvex hulls, a novel approach is proposed based on a graph-theoretic measure that leverages graph cycles to characterize the structural complexities of input data points. Employing this measure, we present a greedy algorithmic framework, dubbed Zeta Hulls, to perform structured column sampling. The process of pursuing a Zeta hull involves the computation of matrix inverse. To accelerate the matrix inversion computation and reduce its space complexity as well, we exploit a low-rank approximation to the graph adjacency matrix by using an efficient anchor graph technique. Extensive experimental results show that data representation learned by Zeta Hulls can achieve state-of-the-art accuracy in text and image classification tasks.",
    "authors": [
      "Xiong, Yuanjun",
      "Liu, Wei",
      "Zhao, Deli",
      "Tang, Xiaoou"
    ]
  },
  {
    "id": "c0f168ce8900fa56e57789e2a2f2c9d0",
    "title": "Near-Optimal-Sample Estimators for Spherical Gaussian Mixtures",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/c0f168ce8900fa56e57789e2a2f2c9d0-Paper.pdf",
    "abstract": "Many important distributions are high dimensional, and often they can be modeled as Gaussian mixtures. We derive the first sample-efficient polynomial-time estimator for high-dimensional spherical Gaussian mixtures. Based on intuitive spectral reasoning, it approximates mixtures of $k$ spherical Gaussians in $d$-dimensions to within$\\ell_1$ distance $\\epsilon$ using $\\mathcal{O}({dk^9(\\log^2 d)}/{\\epsilon^4})$ samples and $\\mathcal{O}_{k,\\epsilon}(d^3\\log^5 d)$ computation time. Conversely, we show that any estimator requires $\\Omega\\bigl({dk}/{\\epsilon^2}\\bigr)$ samples, hence the algorithm's sample complexity is nearly optimal in the dimension. The implied time-complexity factor \\mathcal{O}_{k,\\epsilon}$ is exponential in $k$, but much smaller than previously known. We also construct a simple estimator for one-dimensional Gaussian mixtures that uses $\\tilde\\mathcal{O}(k /\\epsilon^2)$ samples and $\\tilde\\mathcal{O}((k/\\epsilon)^{3k+1})$ computation time.",
    "authors": [
      "Suresh, Ananda Theertha",
      "Orlitsky, Alon",
      "Acharya, Jayadev",
      "Jafarpour, Ashkan"
    ]
  },
  {
    "id": "c15da1f2b5e5ed6e6837a3802f0d1593",
    "title": "Provable Tensor Factorization with Missing Data",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/c15da1f2b5e5ed6e6837a3802f0d1593-Paper.pdf",
    "abstract": "We study the problem of low-rank tensor factorization in the presence of missing data. We ask the following question: how many sampled entries do we need, to efficiently and exactly reconstruct a tensor with a low-rank orthogonal decomposition? We propose a novel alternating minimization based method which iteratively refines estimates of the singular vectors. We show that under certain standard assumptions, our method can recover a three-mode $n\\times n\\times n$ dimensional rank-$r$ tensor exactly from $O(n^{3/2} r^5 \\log^4 n)$ randomly sampled entries. In the process of proving this result, we solve two challenging sub-problems for tensors with missing data. First, in analyzing the initialization step, we prove a generalization of a celebrated result by Szemer\\'edie et al. on the spectrum of random graphs. Next, we prove global convergence of alternating minimization with a good initialization. Simulations suggest that the dependence of the sample size on dimensionality $n$ is indeed tight.",
    "authors": [
      "Jain, Prateek",
      "Oh, Sewoong"
    ]
  },
  {
    "id": "c22abfa379f38b5b0411bc11fa9bf92f",
    "title": "Learning Generative Models with Visual Attention",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/c22abfa379f38b5b0411bc11fa9bf92f-Paper.pdf",
    "abstract": "Attention has long been proposed by psychologists to be important for efficiently dealing with the massive amounts of sensory stimulus in the neocortex. Inspired by the attention models in visual neuroscience and the need for object-centered data for generative models, we propose a deep-learning based generative framework using attention. The attentional mechanism propagates signals from the region of interest in a scene to an aligned canonical representation for generative modeling. By ignoring scene background clutter, the generative model can concentrate its resources on the object of interest. A convolutional neural net is employed to provide good initializations during posterior inference which uses Hamiltonian Monte Carlo. Upon learning images of faces, our model can robustly attend to the face region of novel test subjects. More importantly, our model can learn generative models of new faces from a novel dataset of large images where the face locations are not known.",
    "authors": [
      "Tang, Charlie",
      "Srivastava, Nitish",
      "Salakhutdinov, Russ R."
    ]
  },
  {
    "id": "c399862d3b9d6b76c8436e924a68c45b",
    "title": "Bandit Convex Optimization: Towards Tight Bounds",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf",
    "abstract": "Bandit Convex Optimization (BCO) is a fundamental framework for decision making under uncertainty, which generalizes many problems from the realm of online and statistical learning. While the special case of linear cost functions is well understood, a gap on the attainable regret for BCO with nonlinear losses remains an important open question. In this paper we take a step towards understanding the best attainable regret bounds for BCO: we give an efficient and near-optimal regret algorithm for BCO with strongly-convex and smooth loss functions. In contrast to previous works on BCO that use time invariant exploration schemes, our method employs an exploration scheme that shrinks with time.",
    "authors": [
      "Hazan, Elad",
      "Levy, Kfir"
    ]
  },
  {
    "id": "c59b469d724f7919b7d35514184fdc0f",
    "title": "A Latent Source Model for Online Collaborative Filtering",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/c59b469d724f7919b7d35514184fdc0f-Paper.pdf",
    "abstract": "Despite the prevalence of collaborative filtering in recommendation systems, there has been little theoretical development on why and how well it works, especially in the ``online'' setting, where items are recommended to users over time. We address this theoretical gap by introducing a model for online recommendation systems, cast item recommendation under the model as a learning problem, and analyze the performance of a cosine-similarity collaborative filtering method. In our model, each of $n$ users either likes or dislikes each of $m$ items. We assume there to be $k$ types of users, and all the users of a given type share a common string of probabilities determining the chance of liking each item. At each time step, we recommend an item to each user, where a key distinction from related bandit literature is that once a user consumes an item (e.g., watches a movie), then that item cannot be recommended to the same user again. The goal is to maximize the number of likable items recommended to users over time. Our main result establishes that after nearly $\\log(km)$ initial learning time steps, a simple collaborative filtering algorithm achieves essentially optimal performance without knowing $k$. The algorithm has an exploitation step that uses cosine similarity and two types of exploration steps, one to explore the space of items (standard in the literature) and the other to explore similarity between users (novel to this work).",
    "authors": [
      "Bresler, Guy",
      "Chen, George H.",
      "Shah, Devavrat"
    ]
  },
  {
    "id": "c60d060b946d6dd6145dcbad5c4ccf6f",
    "title": "Self-Paced Learning with Diversity",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/c60d060b946d6dd6145dcbad5c4ccf6f-Paper.pdf",
    "abstract": "Self-paced learning (SPL) is a recently proposed learning regime inspired by the learning process of humans and animals that gradually incorporates easy to more complex samples into training. Existing methods are limited in that they ignore an important aspect in learning: diversity. To incorporate this information, we propose an approach called self-paced learning with diversity (SPLD) which formalizes the preference for both easy and diverse samples into a general regularizer. This regularization term is independent of the learning objective, and thus can be easily generalized into various learning tasks. Albeit non-convex, the optimization of the variables included in this SPLD regularization term for sample selection can be globally solved in linearithmic time. We demonstrate that our method significantly outperforms the conventional SPL on three real-world datasets. Specifically, SPLD achieves the best MAP so far reported in literature on the Hollywood2 and Olympic Sports datasets.",
    "authors": [
      "Jiang, Lu",
      "Meng, Deyu",
      "Yu, Shoou-I",
      "Lan, Zhenzhong",
      "Shan, Shiguang",
      "Hauptmann, Alexander"
    ]
  },
  {
    "id": "c7635bfd99248a2cdef8249ef7bfbef4",
    "title": "Inference by Learning: Speeding-up Graphical Model Optimization via a Coarse-to-Fine Cascade of Pruning Classifiers",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/c7635bfd99248a2cdef8249ef7bfbef4-Paper.pdf",
    "abstract": "We propose a general and versatile framework that significantly speeds-up graphical model optimization while maintaining an excellent solution accuracy. The proposed approach, refereed as Inference by Learning or IbyL, relies on a multi-scale pruning scheme that progressively reduces the solution space by use of a coarse-to-fine cascade of learnt classifiers. We thoroughly experiment with classic computer vision related MRF problems, where our novel framework constantly yields a significant time speed-up (with respect to the most efficient inference methods) and obtains a more accurate solution than directly optimizing the MRF. We make our code available on-line.",
    "authors": [
      "Conejo, Bruno",
      "Komodakis, Nikos",
      "Leprince, Sebastien",
      "Avouac, Jean Philippe"
    ]
  },
  {
    "id": "c7af0926b294e47e52e46cfebe173f20",
    "title": "Capturing Semantically Meaningful Word Dependencies with an Admixture of Poisson MRFs",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/c7af0926b294e47e52e46cfebe173f20-Paper.pdf",
    "abstract": "We develop a fast algorithm for the Admixture of Poisson MRFs (APM) topic model and propose a novel metric to directly evaluate this model. The APM topic model recently introduced by Inouye et al. (2014) is the first topic model that allows for word dependencies within each topic unlike in previous topic models like LDA that assume independence between words within a topic. Research in both the semantic coherence of a topic models (Mimno et al. 2011, Newman et al. 2010) and measures of model fitness (Mimno & Blei 2011) provide strong support that explicitly modeling word dependencies---as in APM---could be both semantically meaningful and essential for appropriately modeling real text data. Though APM shows significant promise for providing a better topic model, APM has a high computational complexity because $O(p^2)$ parameters must be estimated where $p$ is the number of words (Inouye et al. could only provide results for datasets with $p = 200$). In light of this, we develop a parallel alternating Newton-like algorithm for training the APM model that can handle $p = 10^4$ as an important step towards scaling to large datasets. In addition, Inouye et al. only provided tentative and inconclusive results on the utility of APM. Thus, motivated by simple intuitions and previous evaluations of topic models, we propose a novel evaluation metric based on human evocation scores between word pairs (i.e. how much one word brings to mind\" another word (Boyd-Graber et al. 2006)). We provide compelling quantitative and qualitative results on the BNC corpus that demonstrate the superiority of APM over previous topic models for identifying semantically meaningful word dependencies. (MATLAB code available at: http://bigdata.ices.utexas.edu/software/apm/)\"",
    "authors": [
      "Inouye, David I.",
      "Ravikumar, Pradeep K.",
      "Dhillon, Inderjit S."
    ]
  },
  {
    "id": "c81e728d9d4c2f636f067f89cc14862c",
    "title": "Kernel Mean Estimation via Spectral Filtering",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/c81e728d9d4c2f636f067f89cc14862c-Paper.pdf",
    "abstract": "The problem of estimating the kernel mean in a reproducing kernel Hilbert space (RKHS) is central to kernel methods in that it is used by classical approaches (e.g., when centering a kernel PCA matrix), and it also forms the core inference step of modern kernel methods (e.g., kernel-based non-parametric tests) that rely on embedding probability distributions in RKHSs. Previous work [1] has shown that shrinkage can help in constructing \u201cbetter\u201d estimators of the kernel mean than the empirical estimator. The present paper studies the consistency and admissibility of the estimators in [1], and proposes a wider class of shrinkage estimators that improve upon the empirical estimator by considering appropriate basis functions. Using the kernel PCA basis, we show that some of these estimators can be constructed using spectral filtering algorithms which are shown to be consistent under some technical assumptions. Our theoretical analysis also reveals a fundamental connection to the kernel-based supervised learning framework. The proposed estimators are simple to implement and perform well in practice.",
    "authors": [
      "Muandet, Krikamol",
      "Sriperumbudur, Bharath",
      "Sch\u00f6lkopf, Bernhard"
    ]
  },
  {
    "id": "c8ba76c279269b1c6bc8a07e38e78fa4",
    "title": "A Dual Algorithm for Olfactory Computation in the Locust Brain",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/c8ba76c279269b1c6bc8a07e38e78fa4-Paper.pdf",
    "abstract": "We study the early locust olfactory system in an attempt to explain its well-characterized structure and dynamics. We first propose its computational function as recovery of high-dimensional sparse olfactory signals from a small number of measurements. Detailed experimental knowledge about this system rules out standard algorithmic solutions to this problem. Instead, we show that solving a dual formulation of the corresponding optimisation problem yields structure and dynamics in good agreement with biological data. Further biological constraints lead us to a reduced form of this dual formulation in which the system uses independent component analysis to continuously adapt to its olfactory environment to allow accurate sparse recovery. Our work demonstrates the challenges and rewards of attempting detailed understanding of experimentally well-characterized systems.",
    "authors": [
      "Tootoonian, Sina",
      "Lengyel, Mate"
    ]
  },
  {
    "id": "c930eecd01935feef55942cc445f708f",
    "title": "Online combinatorial optimization with stochastic decision sets and adversarial losses",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/c930eecd01935feef55942cc445f708f-Paper.pdf",
    "abstract": "Most work on sequential learning assumes a fixed set of actions that are available all the time. However, in practice, actions can consist of picking subsets of readings from sensors that may break from time to time, road segments that can be blocked or goods that are out of stock. In this paper we study learning algorithms that are able to deal with stochastic availability of such unreliable composite actions. We propose and analyze algorithms based on the Follow-The-Perturbed-Leader prediction method for several learning settings differing in the feedback provided to the learner. Our algorithms rely on a novel loss estimation technique that we call Counting Asleep Times. We deliver regret bounds for our algorithms for the previously studied full information and (semi-)bandit settings, as well as a natural middle point between the two that we call the restricted information setting. A special consequence of our results is a significant improvement of the best known performance guarantees achieved by an efficient algorithm for the sleeping bandit problem with stochastic availability. Finally, we evaluate our algorithms empirically and show their improvement over the known approaches.",
    "authors": [
      "Neu, Gergely",
      "Valko, Michal"
    ]
  },
  {
    "id": "ca9c267dad0305d1a6308d2a0cf1c39c",
    "title": "Learning Multiple Tasks in Parallel with a Shared Annotator",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/ca9c267dad0305d1a6308d2a0cf1c39c-Paper.pdf",
    "abstract": "We introduce a new multi-task framework, in which $K$ online learners are sharing a single annotator with limited bandwidth. On each round, each of the $K$ learners receives an input, and makes a prediction about the label of that input. Then, a shared (stochastic) mechanism decides which of the $K$ inputs will be annotated. The learner that receives the feedback (label) may update its prediction rule, and we proceed to the next round. We develop an online algorithm for multi-task binary classification that learns in this setting, and bound its performance in the worst-case setting. Additionally, we show that our algorithm can be used to solve two bandits problems: contextual bandits, and dueling bandits with context, both allowed to decouple exploration and exploitation. Empirical study with OCR data, vowel prediction (VJ project) and document classification, shows that our algorithm outperforms other algorithms, one of which uses uniform allocation, and essentially makes more (accuracy) for the same labour of the annotator.",
    "authors": [
      "Cohen, Haim",
      "Crammer, Koby"
    ]
  },
  {
    "id": "caf1a3dfb505ffed0d024130f58c5cfa",
    "title": "A Complete Variational Tracker",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/caf1a3dfb505ffed0d024130f58c5cfa-Paper.pdf",
    "abstract": "We introduce a novel probabilistic tracking algorithm that incorporates combinatorial data association constraints and model-based track management using variational Bayes. We use a Bethe entropy approximation to incorporate data association constraints that are often ignored in previous probabilistic tracking algorithms. Noteworthy aspects of our method include a model-based mechanism to replace heuristic logic typically used to initiate and destroy tracks, and an assignment posterior with linear computation cost in window length as opposed to the exponential scaling of previous MAP-based approaches. We demonstrate the applicability of our method on radar tracking and computer vision problems.",
    "authors": [
      "Turner, Ryan D.",
      "Bottone, Steven",
      "Avasarala, Bhargav"
    ]
  },
  {
    "id": "cb70ab375662576bd1ac5aaf16b3fca4",
    "title": "Low-dimensional models of neural population activity in sensory cortical circuits",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/cb70ab375662576bd1ac5aaf16b3fca4-Paper.pdf",
    "abstract": "Neural responses in visual cortex are influenced by visual stimuli and by ongoing spiking activity in local circuits. An important challenge in computational neuroscience is to develop models that can account for both of these features in large multi-neuron recordings and to reveal how stimulus representations interact with and depend on cortical dynamics. Here we introduce a statistical model of neural population activity that integrates a nonlinear receptive field model with a latent dynamical model of ongoing cortical activity. This model captures the temporal dynamics, effective network connectivity in large population recordings, and correlations due to shared stimulus drive as well as common noise. Moreover, because the nonlinear stimulus inputs are mixed by the ongoing dynamics, the model can account for a relatively large number of idiosyncratic receptive field shapes with a small number of nonlinear inputs to a low-dimensional latent dynamical model. We introduce a fast estimation method using online expectation maximization with Laplace approximations. Inference scales linearly in both population size and recording duration. We apply this model to multi-channel recordings from primary visual cortex and show that it accounts for a large number of individual neural receptive fields using a small number of nonlinear inputs and a low-dimensional dynamical model.",
    "authors": [
      "Archer, Evan W.",
      "Koster, Urs",
      "Pillow, Jonathan W.",
      "Macke, Jakob H."
    ]
  },
  {
    "id": "cb79f8fa58b91d3af6c9c991f63962d3",
    "title": "Spectral k-Support Norm Regularization",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/cb79f8fa58b91d3af6c9c991f63962d3-Paper.pdf",
    "abstract": "The $k$-support norm has successfully been applied to sparse vector prediction problems. We observe that it belongs to a wider class of norms, which we call the box-norms. Within this framework we derive an efficient algorithm to compute the proximity operator of the squared norm, improving upon the original method for the $k$-support norm. We extend the norms from the vector to the matrix setting and we introduce the spectral $k$-support norm. We study its properties and show that it is closely related to the multitask learning cluster norm. We apply the norms to real and synthetic matrix completion datasets. Our findings indicate that spectral $k$-support norm regularization gives state of the art performance, consistently improving over trace norm regularization and the matrix elastic net.",
    "authors": [
      "McDonald, Andrew M.",
      "Pontil, Massimiliano",
      "Stamos, Dimitris"
    ]
  },
  {
    "id": "cc1aa436277138f61cda703991069eaf",
    "title": "Learning Optimal Commitment to Overcome Insecurity",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/cc1aa436277138f61cda703991069eaf-Paper.pdf",
    "abstract": "Game-theoretic algorithms for physical security have made an impressive real-world impact. These algorithms compute an optimal strategy for the defender to commit to in a Stackelberg game, where the attacker observes the defender's strategy and best-responds. In order to build the game model, though, the payoffs of potential attackers for various outcomes must be estimated; inaccurate estimates can lead to significant inefficiencies. We design an algorithm that optimizes the defender's strategy with no prior information, by observing the attacker's responses to randomized deployments of resources and learning his priorities. In contrast to previous work, our algorithm requires a number of queries that is polynomial in the representation of the game.",
    "authors": [
      "Blum, Avrim",
      "Haghtalab, Nika",
      "Procaccia, Ariel D."
    ]
  },
  {
    "id": "ccb0989662211f61edae2e26d58ea92f",
    "title": "A Drifting-Games Analysis for Online Learning and Applications to Boosting",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/ccb0989662211f61edae2e26d58ea92f-Paper.pdf",
    "abstract": "We provide a general mechanism to design online learning algorithms based on a minimax analysis within a drifting-games framework. Different online learning settings (Hedge, multi-armed bandit problems and online convex optimization) are studied by converting into various kinds of drifting games. The original minimax analysis for drifting games is then used and generalized by applying a series of relaxations, starting from choosing a convex surrogate of the 0-1 loss function. With different choices of surrogates, we not only recover existing algorithms, but also propose new algorithms that are totally parameter-free and enjoy other useful properties. Moreover, our drifting-games framework naturally allows us to study high probability bounds without resorting to any concentration results, and also a generalized notion of regret that measures how good the algorithm is compared to all but the top small fraction of candidates. Finally, we translate our new Hedge algorithm into a new adaptive boosting algorithm that is computationally faster as shown in experiments, since it ignores a large number of examples on each round.",
    "authors": [
      "Luo, Haipeng",
      "Schapire, Robert E."
    ]
  },
  {
    "id": "cd0dce8fca267bf1fb86cf43e18d5598",
    "title": "Dynamic Rank Factor Model for Text Streams",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/cd0dce8fca267bf1fb86cf43e18d5598-Paper.pdf",
    "abstract": "We propose a semi-parametric and dynamic rank factor model for topic modeling, capable of (1) discovering topic prevalence over time, and (2) learning contemporary multi-scale dependence structures, providing topic and word correlations as a byproduct. The high-dimensional and time-evolving ordinal/rank observations (such as word counts), after an arbitrary monotone transformation, are well accommodated through an underlying dynamic sparse factor model. The framework naturally admits heavy-tailed innovations, capable of inferring abrupt temporal jumps in the importance of topics. Posterior inference is performed through straightforward Gibbs sampling, based on the forward-filtering backward-sampling algorithm. Moreover, an efficient data subsampling scheme is leveraged to speed up inference on massive datasets. The modeling framework is illustrated on two real datasets: the US State of the Union Address and the JSTOR collection from Science.",
    "authors": [
      "Han, Shaobo",
      "Du, Lin",
      "Salazar, Esther",
      "Carin, Lawrence"
    ]
  },
  {
    "id": "cd14821dab219ea06e2fd1a2df2e3582",
    "title": "Consistency of weighted majority votes",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/cd14821dab219ea06e2fd1a2df2e3582-Paper.pdf",
    "abstract": "We revisit from a statistical learning perspective the classical decision-theoretic problem of weighted expert voting. In particular, we examine the consistency (both asymptotic and finitary) of the optimal Nitzan-Paroush weighted majority and related rules. In the case of known expert competence levels, we give sharp error estimates for the optimal rule. When the competence levels are unknown, they must be empirically estimated. We provide frequentist and Bayesian analyses for this situation. Some of our proof techniques are non-standard and may be of independent interest. The bounds we derive are nearly optimal, and several challenging open problems are posed. Experimental results are provided to illustrate the theory.",
    "authors": [
      "Berend, Daniel",
      "Kontorovich, Aryeh"
    ]
  },
  {
    "id": "cd89fef7ffdd490db800357f47722b20",
    "title": "Modeling Deep Temporal Dependencies with Recurrent Grammar Cells\"\"",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/cd89fef7ffdd490db800357f47722b20-Paper.pdf",
    "abstract": "We propose modeling time series by representing the transformations that take a frame at time t to a frame at time t+1. To this end we show how a bi-linear model of transformations, such as a gated autoencoder, can be turned into a recurrent network, by training it to predict future frames from the current one and the inferred transformation using backprop-through-time. We also show how stacking multiple layers of gating units in a recurrent pyramid makes it possible to represent the \u201dsyntax\u201d of complicated time series, and that it can outperform standard recurrent neural networks in terms of prediction accuracy on a variety of tasks.",
    "authors": [
      "Michalski, Vincent",
      "Memisevic, Roland",
      "Konda, Kishore"
    ]
  },
  {
    "id": "cf9a242b70f45317ffd281241fa66502",
    "title": "Augur: Data-Parallel Probabilistic Modeling",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/cf9a242b70f45317ffd281241fa66502-Paper.pdf",
    "abstract": "Implementing inference procedures for each new probabilistic model is time-consuming and error-prone. Probabilistic programming addresses this problem by allowing a user to specify the model and then automatically generating the inference procedure. To make this practical it is important to generate high performance inference code. In turn, on modern architectures, high performance requires parallel execution. In this paper we present Augur, a probabilistic modeling language and compiler for Bayesian networks designed to make effective use of data-parallel architectures such as GPUs. We show that the compiler can generate data-parallel inference code scalable to thousands of GPU cores by making use of the conditional independence relationships in the Bayesian network.",
    "authors": [
      "Tristan, Jean-Baptiste",
      "Huang, Daniel",
      "Tassarotti, Joseph",
      "Pocock, Adam C.",
      "Green, Stephen",
      "Steele, Guy L."
    ]
  },
  {
    "id": "d1c38a09acc34845c6be3a127a5aacaf",
    "title": "Augmentative Message Passing for Traveling Salesman Problem and Graph Partitioning",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/d1c38a09acc34845c6be3a127a5aacaf-Paper.pdf",
    "abstract": "The cutting plane method is an augmentative constrained optimization procedure that is often used with continuous-domain optimization techniques such as linear and convex programs. We investigate the viability of a similar idea within message passing -- for integral solutions -- in the context of two combinatorial problems: 1) For Traveling Salesman Problem (TSP), we propose a factor-graph based on Held-Karp formulation, with an exponential number of constraint factors, each of which has an exponential but sparse tabular form. 2) For graph-partitioning (a.k.a. community mining) using modularity optimization, we introduce a binary variable model with a large number of constraints that enforce formation of cliques. In both cases we are able to derive surprisingly simple message updates that lead to competitive solutions on benchmark instances. In particular for TSP we are able to find near-optimal solutions in the time that empirically grows with $N^3$, demonstrating that augmentation is practical and efficient.",
    "authors": [
      "Ravanbakhsh, Siamak",
      "Rabbany, Reihaneh",
      "Greiner, Russell"
    ]
  },
  {
    "id": "d1dc3a8270a6f9394f88847d7f0050cf",
    "title": "Mondrian Forests: Efficient Online Random Forests",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/d1dc3a8270a6f9394f88847d7f0050cf-Paper.pdf",
    "abstract": "Ensembles of randomized decision trees, usually referred to as random forests, are widely used for classification and regression tasks in machine learning and statistics. Random forests achieve competitive predictive performance and are computationally efficient to train and test, making them excellent candidates for real-world prediction tasks. The most popular random forest variants (such as Breiman's random forest and extremely randomized trees) operate on batches of training data. Online methods are now in greater demand. Existing online random forests, however, require more training data than their batch counterpart to achieve comparable predictive performance. In this work, we use Mondrian processes (Roy and Teh, 2009) to construct ensembles of random decision trees we call Mondrian forests. Mondrian forests can be grown in an incremental/online fashion and remarkably, the distribution of online Mondrian forests is the same as that of batch Mondrian forests. Mondrian forests achieve competitive predictive performance comparable with existing online random forests and periodically re-trained batch random forests, while being more than an order of magnitude faster, thus representing a better computation vs accuracy tradeoff.",
    "authors": [
      "Lakshminarayanan, Balaji",
      "Roy, Daniel M.",
      "Teh, Yee Whye"
    ]
  },
  {
    "id": "d38901788c533e8286cb6400b40b386d",
    "title": "A Probabilistic Framework for Multimodal Retrieval using Integrative Indian Buffet Process",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/d38901788c533e8286cb6400b40b386d-Paper.pdf",
    "abstract": "We propose a multimodal retrieval procedure based on latent feature models. The procedure consists of a nonparametric Bayesian framework for learning underlying semantically meaningful abstract features in a multimodal dataset, a probabilistic retrieval model that allows cross-modal queries and an extension model for relevance feedback. Experiments on two multimodal datasets, PASCAL-Sentence and SUN-Attribute, demonstrate the effectiveness of the proposed retrieval procedure in comparison to the state-of-the-art algorithms for learning binary codes.",
    "authors": [
      "Ozdemir, Bahadir",
      "Davis, Larry S."
    ]
  },
  {
    "id": "d516b13671a4179d9b7b458a6ebdeb92",
    "title": "A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/d516b13671a4179d9b7b458a6ebdeb92-Paper.pdf",
    "abstract": "We propose a method for automatically answering questions about images by bringing together recent advances from natural language processing and computer vision. We combine discrete reasoning with uncertain predictions by a multi-world approach that represents uncertainty about the perceived world in a bayesian framework. Our approach can handle human questions of high complexity about realistic scenes and replies with range of answer like counts, object classes, instances and lists of them. The system is directly trained from question-answer pairs. We establish a first benchmark for this task that can be seen as a modern attempt at a visual turing test.",
    "authors": [
      "Malinowski, Mateusz",
      "Fritz, Mario"
    ]
  },
  {
    "id": "d523773c6b194f37b938d340d5d02232",
    "title": "Semi-supervised Learning with Deep Generative Models",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/d523773c6b194f37b938d340d5d02232-Paper.pdf",
    "abstract": "The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.",
    "authors": [
      "Kingma, Durk P.",
      "Mohamed, Shakir",
      "Jimenez Rezende, Danilo",
      "Welling, Max"
    ]
  },
  {
    "id": "d54e99a6c03704e95e6965532dec148b",
    "title": "Biclustering Using Message Passing",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/d54e99a6c03704e95e6965532dec148b-Paper.pdf",
    "abstract": "Biclustering is the analog of clustering on a bipartite graph. Existent methods infer biclusters through local search strategies that find one cluster at a time; a common technique is to update the row memberships based on the current column memberships, and vice versa. We propose a biclustering algorithm that maximizes a global objective function using message passing. Our objective function closely approximates a general likelihood function, separating a cluster size penalty term into row- and column-count penalties. Because we use a global optimization framework, our approach excels at resolving the overlaps between biclusters, which are important features of biclusters in practice. Moreover, Expectation-Maximization can be used to learn the model parameters if they are unknown. In simulations, we find that our method outperforms two of the best existing biclustering algorithms, ISA and LAS, when the planted clusters overlap. Applied to three gene expression datasets, our method finds coregulated gene clusters that have high quality in terms of cluster size and density.",
    "authors": [
      "O'Connor, Luke",
      "Feizi, Soheil"
    ]
  },
  {
    "id": "d554f7bb7be44a7267068a7df88ddd20",
    "title": "Stochastic Proximal Gradient Descent with Acceleration Techniques",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/d554f7bb7be44a7267068a7df88ddd20-Paper.pdf",
    "abstract": "Proximal gradient descent (PGD) and stochastic proximal gradient descent (SPGD) are popular methods for solving regularized risk minimization problems in machine learning and statistics. In this paper, we propose and analyze an accelerated variant of these methods in the mini-batch setting. This method incorporates two acceleration techniques: one is Nesterov's acceleration method, and the other is a variance reduction for the stochastic gradient. Accelerated proximal gradient descent (APG) and proximal stochastic variance reduction gradient (Prox-SVRG) are in a trade-off relationship. We show that our method, with the appropriate mini-batch size, achieves lower overall complexity than both APG and Prox-SVRG.",
    "authors": [
      "Nitanda, Atsushi"
    ]
  },
  {
    "id": "d5cfead94f5350c12c322b5b664544c1",
    "title": "DFacTo: Distributed Factorization of Tensors",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/d5cfead94f5350c12c322b5b664544c1-Paper.pdf",
    "abstract": "We present a technique for significantly speeding up Alternating Least Squares (ALS) and Gradient Descent (GD), two widely used algorithms for tensor factorization. By exploiting properties of the Khatri-Rao product, we show how to efficiently address a computationally challenging sub-step of both algorithms. Our algorithm, DFacTo, only requires two matrix-vector products and is easy to parallelize. DFacTo is not only scalable but also on average 4 to 10 times faster than competing algorithms on a variety of datasets. For instance, DFacTo only takes 480 seconds on 4 machines to perform one iteration of the ALS algorithm and 1,143 seconds to perform one iteration of the GD algorithm on a 6.5 million x 2.5 million x 1.5 million dimensional tensor with 1.2 billion non-zero entries.",
    "authors": [
      "Choi, Joon Hee",
      "Vishwanathan, S."
    ]
  },
  {
    "id": "d67d8ab4f4c10bf22aa353e27879133c",
    "title": "Robust Classification Under Sample Selection Bias",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/d67d8ab4f4c10bf22aa353e27879133c-Paper.pdf",
    "abstract": "In many important machine learning applications, the source distribution used to estimate a probabilistic classifier differs from the target distribution on which the classifier will be used to make predictions. Due to its asymptotic properties, sample-reweighted loss minimization is a commonly employed technique to deal with this difference. However, given finite amounts of labeled source data, this technique suffers from significant estimation errors in settings with large sample selection bias. We develop a framework for robustly learning a probabilistic classifier that adapts to different sample selection biases using a minimax estimation formulation. Our approach requires only accurate estimates of statistics under the source distribution and is otherwise as robust as possible to unknown properties of the conditional label distribution, except when explicit generalization assumptions are incorporated. We demonstrate the behavior and effectiveness of our approach on synthetic and UCI binary classification tasks.",
    "authors": [
      "Liu, Anqi",
      "Ziebart, Brian"
    ]
  },
  {
    "id": "d81f9c1be2e08964bf9f24b15f0e4900",
    "title": "Deep Joint Task Learning for Generic Object Extraction",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/d81f9c1be2e08964bf9f24b15f0e4900-Paper.pdf",
    "abstract": "This paper investigates how to extract objects-of-interest without relying on hand-craft features and sliding windows approaches, that aims to jointly solve two sub-tasks: (i) rapidly localizing salient objects from images, and (ii) accurately segmenting the objects based on the localizations. We present a general joint task learning framework, in which each task (either object localization or object segmentation) is tackled via a multi-layer convolutional neural network, and the two networks work collaboratively to boost performance. In particular, we propose to incorporate latent variables bridging the two networks in a joint optimization manner. The first network directly predicts the positions and scales of salient objects from raw images, and the latent variables adjust the object localizations to feed the second network that produces pixelwise object masks. An EM-type method is then studied for the joint optimization, iterating with two steps: (i) by using the two networks, it estimates the latent variables by employing an MCMC-based sampling method; (ii) it optimizes the parameters of the two networks unitedly via back propagation, with the fixed latent variables. Extensive experiments demonstrate that our joint learning framework significantly outperforms other state-of-the-art approaches in both accuracy and efficiency (e.g., 1000 times faster than competing approaches).",
    "authors": [
      "Wang, Xiaolong",
      "Zhang, Liliang",
      "Lin, Liang",
      "Liang, Zhujin",
      "Zuo, Wangmeng"
    ]
  },
  {
    "id": "d840cc5d906c3e9c84374c8919d2074e",
    "title": "Automatic Discovery of Cognitive Skills to Improve the Prediction of Student Learning",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/d840cc5d906c3e9c84374c8919d2074e-Paper.pdf",
    "abstract": "To master a discipline such as algebra or physics, students must acquire a set of cognitive skills. Traditionally, educators and domain experts manually determine what these skills are and then select practice exercises to hone a particular skill. We propose a technique that uses student performance data to automatically discover the skills needed in a discipline. The technique assigns a latent skill to each exercise such that a student's expected accuracy on a sequence of same-skill exercises improves monotonically with practice. Rather than discarding the skills identified by experts, our technique incorporates a nonparametric prior over the exercise-skill assignments that is based on the expert-provided skills and a weighted Chinese restaurant process. We test our technique on datasets from five different intelligent tutoring systems designed for students ranging in age from middle school through college. We obtain two surprising results. First, in three of the five datasets, the skills inferred by our technique support significantly improved predictions of student performance over the expert-provided skills. Second, the expert-provided skills have little value: our technique predicts student performance nearly as well when it ignores the domain expertise as when it attempts to leverage it. We discuss explanations for these surprising results and also the relationship of our skill-discovery technique to alternative approaches.",
    "authors": [
      "Lindsey, Robert V.",
      "Khajah, Mohammad",
      "Mozer, Michael C."
    ]
  },
  {
    "id": "d86ea612dec96096c5e0fcc8dd42ab6d",
    "title": "General Table Completion using a Bayesian Nonparametric Model",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/d86ea612dec96096c5e0fcc8dd42ab6d-Paper.pdf",
    "abstract": "Even though heterogeneous databases can be found in a broad variety of applications, there exists a lack of tools for estimating missing data in such databases. In this paper, we provide an efficient and robust table completion tool, based on a Bayesian nonparametric latent feature model. In particular, we propose a general observation model for the Indian buffet process (IBP) adapted to mixed continuous (real-valued and positive real-valued) and discrete (categorical, ordinal and count) observations. Then, we propose an inference algorithm that scales linearly with the number of observations. Finally, our experiments over five real databases show that the proposed approach provides more robust and accurate estimates than the standard IBP and the Bayesian probabilistic matrix factorization with Gaussian observations.",
    "authors": [
      "Valera, Isabel",
      "Ghahramani, Zoubin"
    ]
  },
  {
    "id": "d96409bf894217686ba124d7356686c9",
    "title": "A Representation Theory for Ranking Functions",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/d96409bf894217686ba124d7356686c9-Paper.pdf",
    "abstract": "This paper presents a representation theory for permutation-valued functions, which in their general form can also be called listwise ranking functions. Pointwise ranking functions assign a score to each object independently, without taking into account the other objects under consideration; whereas listwise loss functions evaluate the set of scores assigned to all objects as a whole. In many supervised learning to rank tasks, it might be of interest to use listwise ranking functions instead; in particular, the Bayes Optimal ranking functions might themselves be listwise, especially if the loss function is listwise. A key caveat to using listwise ranking functions has been the lack of an appropriate representation theory for such functions. We show that a natural symmetricity assumption that we call exchangeability allows us to explicitly characterize the set of such exchangeable listwise ranking functions. Our analysis draws from the theories of tensor analysis, functional analysis and De Finetti theorems. We also present experiments using a novel reranking method motivated by our representation theory.",
    "authors": [
      "Pareek, Harsh H.",
      "Ravikumar, Pradeep K."
    ]
  },
  {
    "id": "da4fb5c6e93e74d3df8527599fa62642",
    "title": "Multivariate Regression with Calibration",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/da4fb5c6e93e74d3df8527599fa62642-Paper.pdf",
    "abstract": "We propose a new method named calibrated multivariate regression (CMR) for fitting high dimensional multivariate regression models. Compared to existing methods, CMR calibrates the regularization for each regression task with respect to its noise level so that it is simultaneously tuning insensitive and achieves an improved finite-sample performance. Computationally, we develop an efficient smoothed proximal gradient algorithm which has a worst-case iteration complexity $O(1/\\epsilon)$, where $\\epsilon$ is a pre-specified numerical accuracy. Theoretically, we prove that CMR achieves the optimal rate of convergence in parameter estimation. We illustrate the usefulness of CMR by thorough numerical simulations and show that CMR consistently outperforms other high dimensional multivariate regression methods. We also apply CMR on a brain activity prediction problem and find that CMR is as competitive as the handcrafted model created by human experts.",
    "authors": [
      "Liu, Han",
      "Wang, Lie",
      "Zhao, Tuo"
    ]
  },
  {
    "id": "da8ce53cf0240070ce6c69c48cd588ee",
    "title": "Blossom Tree Graphical Models",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/da8ce53cf0240070ce6c69c48cd588ee-Paper.pdf",
    "abstract": "We combine the ideas behind trees and Gaussian graphical models to form a new nonparametric family of graphical models. Our approach is to attach nonparanormal blossoms\", with arbitrary graphs, to a collection of nonparametric trees. The tree edges are chosen to connect variables that most violate joint Gaussianity. The non-tree edges are partitioned into disjoint groups, and assigned to tree nodes using a nonparametric partial correlation statistic. A nonparanormal blossom is then \"grown\" for each group using established methods based on the graphical lasso. The result is a factorization with respect to the union of the tree branches and blossoms, defining a high-dimensional joint density that can be efficiently estimated and evaluated on test points. Theoretical properties and experiments with simulated and real data demonstrate the effectiveness of blossom trees.\"",
    "authors": [
      "Liu, Zhe",
      "Lafferty, John"
    ]
  },
  {
    "id": "daca41214b39c5dc66674d09081940f0",
    "title": "Scalable Methods for Nonnegative Matrix Factorizations of Near-separable Tall-and-skinny Matrices",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/daca41214b39c5dc66674d09081940f0-Paper.pdf",
    "abstract": "Numerous algorithms are used for nonnegative matrix factorization under the assumption that the matrix is nearly separable. In this paper, we show how to make these algorithms scalable for data matrices that have many more rows than columns, so-called tall-and-skinny matrices.\" One key component to these improved methods is an orthogonal matrix transformation that preserves the separability of the NMF problem. Our final methods need to read the data matrix only once and are suitable for streaming, multi-core, and MapReduce architectures. We demonstrate the efficacy of these algorithms on terabyte-sized matrices from scientific computing and bioinformatics.\"",
    "authors": [
      "Benson, Austin R.",
      "Lee, Jason D.",
      "Rajwa, Bartek",
      "Gleich, David F."
    ]
  },
  {
    "id": "db957c626a8cd7a27231adfbf51e20eb",
    "title": "Rates of Convergence for Nearest Neighbor Classification",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/db957c626a8cd7a27231adfbf51e20eb-Paper.pdf",
    "abstract": "We analyze the behavior of nearest neighbor classification in metric spaces and provide finite-sample, distribution-dependent rates of convergence under minimal assumptions. These are more general than existing bounds, and enable us, as a by-product, to establish the universal consistency of nearest neighbor in a broader range of data spaces than was previously known. We illustrate our upper and lower bounds by introducing a new smoothness class customized for nearest neighbor classification. We find, for instance, that under the Tsybakov margin condition the convergence rate of nearest neighbor matches recently established lower bounds for nonparametric classification.",
    "authors": [
      "Chaudhuri, Kamalika",
      "Dasgupta, Sanjoy"
    ]
  },
  {
    "id": "dbe272bab69f8e13f14b405e038deb64",
    "title": "Inferring synaptic conductances from spike trains with a biophysically inspired point process model",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/dbe272bab69f8e13f14b405e038deb64-Paper.pdf",
    "abstract": "A popular approach to neural characterization describes neural responses in terms of a cascade of linear and nonlinear stages: a linear filter to describe stimulus integration, followed by a nonlinear function to convert the filter output to spike rate. However, real neurons respond to stimuli in a manner that depends on the nonlinear integration of excitatory and inhibitory synaptic inputs. Here we introduce a biophysically inspired point process model that explicitly incorporates stimulus-induced changes in synaptic conductance in a dynamical model of neuronal membrane potential. Our work makes two important contributions. First, on a theoretical level, it offers a novel interpretation of the popular generalized linear model (GLM) for neural spike trains. We show that the classic GLM is a special case of our conductance-based model in which the stimulus linearly modulates excitatory and inhibitory conductances in an equal and opposite \u201cpush-pull\u201d fashion. Our model can therefore be viewed as a direct extension of the GLM in which we relax these constraints; the resulting model can exhibit shunting as well as hyperpolarizing inhibition, and time-varying changes in both gain and membrane time constant. Second, on a practical level, we show that our model provides a tractable model of spike responses in early sensory neurons that is both more accurate and more interpretable than the GLM. Most importantly, we show that we can accurately infer intracellular synaptic conductances from extracellularly recorded spike trains. We validate these estimates using direct intracellular measurements of excitatory and inhibitory conductances in parasol retinal ganglion cells. We show that the model fit to extracellular spike trains can predict excitatory and inhibitory conductances elicited by novel stimuli with nearly the same accuracy as a model trained directly with intracellular conductances.",
    "authors": [
      "Latimer, Kenneth W.",
      "Chichilnisky, E.J.",
      "Rieke, Fred",
      "Pillow, Jonathan W."
    ]
  },
  {
    "id": "dc09c97fd73d7a324bdbfe7c79525f64",
    "title": "Scalable Inference for Neuronal Connectivity from Calcium Imaging",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/dc09c97fd73d7a324bdbfe7c79525f64-Paper.pdf",
    "abstract": "Fluorescent calcium imaging provides a potentially powerful tool for inferring connectivity in neural circuits with up to thousands of neurons. However, a key challenge in using calcium imaging for connectivity detection is that current systems often have a temporal response and frame rate that can be orders of magnitude slower than the underlying neural spiking process. Bayesian inference based on expectation-maximization (EM) have been proposed to overcome these limitations, but they are often computationally demanding since the E-step in the EM procedure typically involves state estimation in a high-dimensional nonlinear dynamical system. In this work, we propose a computationally fast method for the state estimation based on a hybrid of loopy belief propagation and approximate message passing (AMP). The key insight is that a neural system as viewed through calcium imaging can be factorized into simple scalar dynamical systems for each neuron with linear interconnections between the neurons. Using the structure, the updates in the proposed hybrid AMP methodology can be computed by a set of one-dimensional state estimation procedures and linear transforms with the connectivity matrix. This yields a computationally scalable method for inferring connectivity of large neural circuits. Simulations of the method on realistic neural networks demonstrate good accuracy with computation times that are potentially significantly faster than current approaches based on Markov Chain Monte Carlo methods.",
    "authors": [
      "Fletcher, Alyson K.",
      "Rangan, Sundeep"
    ]
  },
  {
    "id": "dc5689792e08eb2e219dce49e64c885b",
    "title": "Minimax-optimal Inference from Partial Rankings",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/dc5689792e08eb2e219dce49e64c885b-Paper.pdf",
    "abstract": "This paper studies the problem of rank aggregation under the Plackett-Luce model. The goal is to infer a global ranking and related scores of the items, based on partial rankings provided by multiple users over multiple subsets of items. A question of particular interest is how to optimally assign items to users for ranking and how many item assignments are needed to achieve a target estimation error. Without any assumptions on how the items are assigned to users, we derive an oracle lower bound and the Cram\\'er-Rao lower bound of the estimation error. We prove an upper bound on the estimation error achieved by the maximum likelihood estimator, and show that both the upper bound and the Cram\\'er-Rao lower bound inversely depend on the spectral gap of the Laplacian of an appropriately defined comparison graph. Since random comparison graphs are known to have large spectral gaps, this suggests the use of random assignments when we have the control. Precisely, the matching oracle lower bound and the upper bound on the estimation error imply that the maximum likelihood estimator together with a random assignment is minimax-optimal up to a logarithmic factor. We further analyze a popular rank-breaking scheme that decompose partial rankings into pairwise comparisons. We show that even if one applies the mismatched maximum likelihood estimator that assumes independence (on pairwise comparisons that are now dependent due to rank-breaking), minimax optimal performance is still achieved up to a logarithmic factor.",
    "authors": [
      "Hajek, Bruce",
      "Oh, Sewoong",
      "Xu, Jiaming"
    ]
  },
  {
    "id": "dc58e3a306451c9d670adcd37004f48f",
    "title": "Neurons as Monte Carlo Samplers: Bayesian \ufffcInference and Learning in Spiking Networks",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/dc58e3a306451c9d670adcd37004f48f-Paper.pdf",
    "abstract": "We propose a two-layer spiking network capable of performing approximate inference and learning for a hidden Markov model. The lower layer sensory neurons detect noisy measurements of hidden world states. The higher layer neurons with recurrent connections infer a posterior distribution over world states from spike trains generated by sensory neurons. We show how such a neuronal network with synaptic plasticity can implement a form of Bayesian inference similar to Monte Carlo methods such as particle filtering. Each spike in the population of inference neurons represents a sample of a particular hidden world state. The spiking activity across the neural population approximates the posterior distribution of hidden state. The model provides a functional explanation for the Poisson-like noise commonly observed in cortical responses. Uncertainties in spike times provide the necessary variability for sampling during inference. Unlike previous models, the hidden world state is not observed by the sensory neurons, and the temporal dynamics of the hidden state is unknown. We demonstrate how this network can sequentially learn the hidden Markov model using a spike-timing dependent Hebbian learning rule and achieve power-law convergence rates.",
    "authors": [
      "Huang, Yanping",
      "Rao, Rajesh P."
    ]
  },
  {
    "id": "dca5672ff3444c7e997aa9a2c4eb2094",
    "title": "Simple MAP Inference via Low-Rank Relaxations",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/dca5672ff3444c7e997aa9a2c4eb2094-Paper.pdf",
    "abstract": "We focus on the problem of maximum a posteriori (MAP) inference in Markov random fields with binary variables and pairwise interactions. For this common subclass of inference tasks, we consider low-rank relaxations that interpolate between the discrete problem and its full-rank semidefinite relaxation, followed by randomized rounding. We develop new theoretical bounds studying the effect of rank, showing that as the rank grows, the relaxed objective increases but saturates, and that the fraction in objective value retained by the rounded discrete solution decreases. In practice, we show two algorithms for optimizing the low-rank objectives which are simple to implement, enjoy ties to the underlying theory, and outperform existing approaches on benchmark MAP inference tasks.",
    "authors": [
      "Frostig, Roy",
      "Wang, Sida",
      "Liang, Percy S.",
      "Manning, Christopher D."
    ]
  },
  {
    "id": "de03beffeed9da5f3639a621bcab5dd4",
    "title": "Fast Prediction for Large-Scale Kernel Machines",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/de03beffeed9da5f3639a621bcab5dd4-Paper.pdf",
    "abstract": "Kernel machines such as kernel SVM and kernel ridge regression usually construct high quality models; however, their use in real-world applications remains limited due to the high prediction cost. In this paper, we present two novel insights for improving the prediction efficiency of kernel machines. First, we show that by adding \u201cpseudo landmark points\u201d to the classical Nystr\u00a8om kernel approximation in an elegant way, we can significantly reduce the prediction error without much additional prediction cost. Second, we provide a new theoretical analysis on bounding the error of the solution computed by using Nystr\u00a8om kernel approximation method, and show that the error is related to the weighted kmeans objective function where the weights are given by the model computed from the original kernel. This theoretical insight suggests a new landmark point selection technique for the situation where we have knowledge of the original model. Based on these two insights, we provide a divide-and-conquer framework for improving the prediction speed. First, we divide the whole problem into smaller local subproblems to reduce the problem size. In the second phase, we develop a kernel approximation based fast prediction approach within each subproblem. We apply our algorithm to real world large-scale classification and regression datasets, and show that the proposed algorithm is consistently and significantly better than other competitors. For example, on the Covertype classification problem, in terms of prediction time, our algorithm achieves more than 10000 times speedup over the full kernel SVM, and a two-fold speedup over the state-of-the-art LDKL approach, while obtaining much higher prediction accuracy than LDKL (95.2% vs. 89.53%).",
    "authors": [
      "Hsieh, Cho-Jui",
      "Si, Si",
      "Dhillon, Inderjit S."
    ]
  },
  {
    "id": "df0aab058ce179e4f7ab135ed4e641a9",
    "title": "Median Selection Subset Aggregation for Parallel Inference",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/df0aab058ce179e4f7ab135ed4e641a9-Paper.pdf",
    "abstract": "For massive data sets, efficient computation commonly relies on distributed algorithms that store and process subsets of the data on different machines, minimizing communication costs. Our focus is on regression and classification problems involving many features. A variety of distributed algorithms have been proposed in this context, but challenges arise in defining an algorithm with low communication, theoretical guarantees and excellent practical performance in general settings. We propose a MEdian Selection Subset AGgregation Estimator (message) algorithm, which attempts to solve these problems. The algorithm applies feature selection in parallel for each subset using Lasso or another method, calculates the `median' feature inclusion index, estimates coefficients for the selected features in parallel for each subset, and then averages these estimates. The algorithm is simple, involves very minimal communication, scales efficiently in both sample and feature size, and has theoretical guarantees. In particular, we show model selection consistency and coefficient estimation efficiency. Extensive experiments show excellent performance in variable selection, estimation, prediction, and computation time relative to usual competitors.",
    "authors": [
      "Wang, Xiangyu",
      "Peng, Peichao",
      "Dunson, David B."
    ]
  },
  {
    "id": "dfd7468ac613286cdbb40872c8ef3b06",
    "title": "Design Principles of the Hippocampal Cognitive Map",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/dfd7468ac613286cdbb40872c8ef3b06-Paper.pdf",
    "abstract": "Hippocampal place fields have been shown to reflect behaviorally relevant aspects of space. For instance, place fields tend to be skewed along commonly traveled directions, they cluster around rewarded locations, and they are constrained by the geometric structure of the environment. We hypothesize a set of design principles for the hippocampal cognitive map that explain how place fields represent space in a way that facilitates navigation and reinforcement learning. In particular, we suggest that place fields encode not just information about the current location, but also predictions about future locations under the current transition distribution. Under this model, a variety of place field phenomena arise naturally from the structure of rewards, barriers, and directional biases as reflected in the transition policy. Furthermore, we demonstrate that this representation of space can support efficient reinforcement learning. We also propose that grid cells compute the eigendecomposition of place fields in part because is useful for segmenting an enclosure along natural boundaries. When applied recursively, this segmentation can be used to discover a hierarchical decomposition of space. Thus, grid cells might be involved in computing subgoals for hierarchical reinforcement learning.",
    "authors": [
      "Stachenfeld, Kimberly L.",
      "Botvinick, Matthew",
      "Gershman, Samuel J."
    ]
  },
  {
    "id": "e00406144c1e7e35240afed70f34166a",
    "title": "Smoothed Gradients for Stochastic Variational Inference",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/e00406144c1e7e35240afed70f34166a-Paper.pdf",
    "abstract": "Stochastic variational inference (SVI) lets us scale up Bayesian computation to massive data. It uses stochastic optimization to fit a variational distribution, following easy-to-compute noisy natural gradients. As with most traditional stochastic optimization methods, SVI takes precautions to use unbiased stochastic gradients whose expectations are equal to the true gradients. In this paper, we explore the idea of following biased stochastic gradients in SVI. Our method replaces the natural gradient with a similarly constructed vector that uses a fixed-window moving average of some of its previous terms. We will demonstrate the many advantages of this technique. First, its computational cost is the same as for SVI and storage requirements only multiply by a constant factor. Second, it enjoys significant variance reduction over the unbiased estimates, smaller bias than averaged gradients, and leads to smaller mean-squared error against the full gradient. We test our method on latent Dirichlet allocation with three large corpora.",
    "authors": [
      "Mandt, Stephan",
      "Blei, David"
    ]
  },
  {
    "id": "e034fb6b66aacc1d48f445ddfb08da98",
    "title": "A Multiplicative Model for Learning Distributed Text-Based Attribute Representations",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/e034fb6b66aacc1d48f445ddfb08da98-Paper.pdf",
    "abstract": "In this paper we propose a general framework for learning distributed representations of attributes: characteristics of text whose representations can be jointly learned with word embeddings. Attributes can correspond to a wide variety of concepts, such as document indicators (to learn sentence vectors), language indicators (to learn distributed language representations), meta-data and side information (such as the age, gender and industry of a blogger) or representations of authors. We describe a third-order model where word context and attribute vectors interact multiplicatively to predict the next word in a sequence. This leads to the notion of conditional word similarity: how meanings of words change when conditioned on different attributes. We perform several experimental tasks including sentiment classification, cross-lingual document classification, and blog authorship attribution. We also qualitatively evaluate conditional word neighbours and attribute-conditioned text generation.",
    "authors": [
      "Kiros, Ryan",
      "Zemel, Richard",
      "Salakhutdinov, Russ R."
    ]
  },
  {
    "id": "e077e1a544eec4f0307cf5c3c721d944",
    "title": "Feedforward Learning of Mixture Models",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/e077e1a544eec4f0307cf5c3c721d944-Paper.pdf",
    "abstract": "We develop a biologically-plausible learning rule that provably converges to the class means of general mixture models. This rule generalizes the classical BCM neural rule within a tensor framework, substantially increasing the generality of the learning problem it solves. It achieves this by incorporating triplets of samples from the mixtures, which provides a novel information processing interpretation to spike-timing-dependent plasticity. We provide both proofs of convergence, and a close fit to experimental data on STDP.",
    "authors": [
      "Lawlor, Matthew",
      "Zucker, Steven W."
    ]
  },
  {
    "id": "e1d5be1c7f2f456670de3d53c7b54f4a",
    "title": "Searching for Higgs Boson Decay Modes with Deep Learning",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/e1d5be1c7f2f456670de3d53c7b54f4a-Paper.pdf",
    "abstract": "Particle colliders enable us to probe the fundamental nature of matter by observing exotic particles produced by high-energy collisions. Because the experimental measurements from these collisions are necessarily incomplete and imprecise, machine learning algorithms play a major role in the analysis of experimental data. The high-energy physics community typically relies on standardized machine learning software packages for this analysis, and devotes substantial effort towards improving statistical power by hand crafting high-level features derived from the raw collider measurements. In this paper, we train artificial neural networks to detect the decay of the Higgs boson to tau leptons on a dataset of 82 million simulated collision events. We demonstrate that deep neural network architectures are particularly well-suited for this task with the ability to automatically discover high-level features from the data and increase discovery significance.",
    "authors": [
      "Sadowski, Peter J.",
      "Whiteson, Daniel",
      "Baldi, Pierre"
    ]
  },
  {
    "id": "e2a2dcc36a08a345332c751b2f2e476c",
    "title": "Decoupled Variational Gaussian Inference",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/e2a2dcc36a08a345332c751b2f2e476c-Paper.pdf",
    "abstract": "Variational Gaussian (VG) inference methods that optimize a lower bound to the marginal likelihood are a popular approach for Bayesian inference. These methods are fast and easy to use, while being reasonably accurate. A difficulty remains in computation of the lower bound when the latent dimensionality $L$ is large. Even though the lower bound is concave for many models, its computation requires optimization over $O(L^2)$ variational parameters. Efficient reparameterization schemes can reduce the number of parameters, but give inaccurate solutions or destroy concavity leading to slow convergence. We propose decoupled variational inference that brings the best of both worlds together. First, it maximizes a Lagrangian of the lower bound reducing the number of parameters to $O(N)$, where $N$ is the number of data examples. The reparameterization obtained is unique and recovers maxima of the lower-bound even when the bound is not concave. Second, our method maximizes the lower bound using a sequence of convex problems, each of which is parallellizable over data examples and computes gradient efficiently. Overall, our approach avoids all direct computations of the covariance, only requiring its linear projections. Theoretically, our method converges at the same rate as existing methods in the case of concave lower bounds, while remaining convergent at a reasonable rate for the non-concave case.",
    "authors": [
      "Khan, Mohammad Emtiyaz E."
    ]
  },
  {
    "id": "e3251075554389fe91d17a794861d47b",
    "title": "On Multiplicative Multitask Feature Learning",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/e3251075554389fe91d17a794861d47b-Paper.pdf",
    "abstract": "We investigate a general framework of multiplicative multitask feature learning which decomposes each task's model parameters into a multiplication of two components. One of the components is used across all tasks and the other component is task-specific. Several previous methods have been proposed as special cases of our framework. We study the theoretical properties of this framework when different regularization conditions are applied to the two decomposed components. We prove that this framework is mathematically equivalent to the widely used multitask feature learning methods that are based on a joint regularization of all model parameters, but with a more general form of regularizers. Further, an analytical formula is derived for the across-task component as related to the task-specific component for all these regularizers, leading to a better understanding of the shrinkage effect. Study of this framework motivates new multitask learning algorithms. We propose two new learning formulations by varying the parameters in the proposed framework. Empirical studies have revealed the relative advantages of the two new formulations by comparing with the state of the art, which provides instructive insights into the feature learning problem with multiple tasks.",
    "authors": [
      "Wang, Xin",
      "Bi, Jinbo",
      "Yu, Shipeng",
      "Sun, Jiangwen"
    ]
  },
  {
    "id": "e449b9317dad920c0dd5ad0a2a2d5e49",
    "title": "Learning Distributed Representations for Structured Output Prediction",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/e449b9317dad920c0dd5ad0a2a2d5e49-Paper.pdf",
    "abstract": "In recent years, distributed representations of inputs have led to performance gains in many applications by allowing statistical information to be shared across inputs. However, the predicted outputs (labels, and more generally structures) are still treated as discrete objects even though outputs are often not discrete units of meaning. In this paper, we present a new formulation for structured prediction where we represent individual labels in a structure as dense vectors and allow semantically similar labels to share parameters. We extend this representation to larger structures by defining compositionality using tensor products to give a natural generalization of standard structured prediction approaches. We define a learning objective for jointly learning the model parameters and the label vectors and propose an alternating minimization algorithm for learning. We show that our formulation outperforms structural SVM baselines in two tasks: multiclass document classification and part-of-speech tagging.",
    "authors": [
      "Srikumar, Vivek",
      "Manning, Christopher D."
    ]
  },
  {
    "id": "e49b8b4053df9505e1f48c3a701c0682",
    "title": "Large-scale L-BFGS using MapReduce",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/e49b8b4053df9505e1f48c3a701c0682-Paper.pdf",
    "abstract": "L-BFGS has been applied as an effective parameter estimation method for various machine learning algorithms since 1980s. With an increasing demand to deal with massive instances and variables, it is important to scale up and parallelize L-BFGS effectively in a distributed system. In this paper, we study the problem of parallelizing the L-BFGS algorithm in large clusters of tens of thousands of shared-nothing commodity machines. First, we show that a naive implementation of L-BFGS using Map-Reduce requires either a significant amount of memory or a large number of map-reduce steps with negative performance impact. Second, we propose a new L-BFGS algorithm, called Vector-free L-BFGS, which avoids the expensive dot product operations in the two loop recursion and greatly improves computation efficiency with a great degree of parallelism. The algorithm scales very well and enables a variety of machine learning algorithms to handle a massive number of variables over large datasets. We prove the mathematical equivalence of the new Vector-free L-BFGS and demonstrate its excellent performance and scalability using real-world machine learning problems with billions of variables in production clusters.",
    "authors": [
      "Chen, Weizhu",
      "Wang, Zhenghao",
      "Zhou, Jingren"
    ]
  },
  {
    "id": "e4a6222cdb5b34375400904f03d8e6a5",
    "title": "Sparse PCA via Covariance Thresholding",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/e4a6222cdb5b34375400904f03d8e6a5-Paper.pdf",
    "abstract": "In sparse principal component analysis we are given noisy observations of a low-rank matrix of dimension $n\\times p$ and seek to reconstruct it under additional sparsity assumptions. In particular, we assume here that the principal components $\\bv_1,\\dots,\\bv_r$ have at most $k_1, \\cdots, k_q$ non-zero entries respectively, and study the high-dimensional regime in which $p$ is of the same order as $n$. In an influential paper, Johnstone and Lu \\cite{johnstone2004sparse} introduced a simple algorithm that estimates the support of the principal vectors $\\bv_1,\\dots,\\bv_r$ by the largest entries in the diagonal of the empirical covariance. This method can be shown to succeed with high probability if $k_q \\le C_1\\sqrt{n/\\log p}$, and to fail with high probability if $k_q\\ge C_2 \\sqrt{n/\\log p}$ for two constants $0 < C_1,C_2 < \\infty$. Despite a considerable amount of work over the last ten years, no practical algorithm exists with provably better support recovery guarantees. Here we analyze a covariance thresholding algorithm that was recently proposed by Krauthgamer, Nadler and Vilenchik \\cite{KrauthgamerSPCA}. We confirm empirical evidence presented by these authors and rigorously prove that the algorithm succeeds with high probability for $k$ of order $\\sqrt{n}$. Recent conditional lower bounds \\cite{berthet2013computational} suggest that it might be impossible to do significantly better. The key technical component of our analysis develops new bounds on the norm of kernel random matrices, in regimes that were not considered before.",
    "authors": [
      "Deshpande, Yash",
      "Montanari, Andrea"
    ]
  },
  {
    "id": "e53a0a2978c28872a4505bdb51db06dc",
    "title": "Randomized Experimental Design for Causal Graph Discovery",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/e53a0a2978c28872a4505bdb51db06dc-Paper.pdf",
    "abstract": "We examine the number of controlled experiments required to discover a causal graph. Hauser and Buhlmann showed that the number of experiments required is logarithmic in the cardinality of maximum undirected clique in the essential graph. Their lower bounds, however, assume that the experiment designer cannot use randomization in selecting the experiments. We show that significant improvements are possible with the aid of randomization \u2013 in an adversarial (worst-case) setting, the designer can then recover the causal graph using at most O(log log n) experiments in expectation. This bound cannot be improved; we show it is tight for some causal graphs. We then show that in a non-adversarial (average-case) setting, even larger improvements are possible: if the causal graph is chosen uniformly at random under a Erd\u00f6s-R\u00e9nyi model then the expected number of experiments to discover the causal graph is constant. Finally, we present computer simulations to complement our theoretic results. Our work exploits a structural characterization of essential graphs by Andersson et al. Their characterization is based upon a set of orientation forcing operations. Our results show a distinction between which forcing operations are most important in worst-case and average-case settings.",
    "authors": [
      "Hu, Huining",
      "Li, Zhentao",
      "Vetta, Adrian R."
    ]
  },
  {
    "id": "e56954b4f6347e897f954495eab16a88",
    "title": "Combinatorial Pure Exploration of Multi-Armed Bandits",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/e56954b4f6347e897f954495eab16a88-Paper.pdf",
    "abstract": "We study the {\\em combinatorial pure exploration (CPE)} problem in the stochastic multi-armed bandit setting, where a learner explores a set of arms with the objective of identifying the optimal member of a \\emph{decision class}, which is a collection of subsets of arms with certain combinatorial structures such as size-$K$ subsets, matchings, spanning trees or paths, etc. The CPE problem represents a rich class of pure exploration tasks which covers not only many existing models but also novel cases where the object of interest has a non-trivial combinatorial structure. In this paper, we provide a series of results for the general CPE problem. We present general learning algorithms which work for all decision classes that admit offline maximization oracles in both fixed confidence and fixed budget settings. We prove problem-dependent upper bounds of our algorithms. Our analysis exploits the combinatorial structures of the decision classes and introduces a new analytic tool. We also establish a general problem-dependent lower bound for the CPE problem. Our results show that the proposed algorithms achieve the optimal sample complexity (within logarithmic factors) for many decision classes. In addition, applying our results back to the problems of top-$K$ arms identification and multiple bandit best arms identification, we recover the best available upper bounds up to constant factors and partially resolve a conjecture on the lower bounds.",
    "authors": [
      "Chen, Shouyuan",
      "Lin, Tian",
      "King, Irwin",
      "Lyu, Michael R.",
      "Chen, Wei"
    ]
  },
  {
    "id": "e5e63da79fcd2bebbd7cb8bf1c1d0274",
    "title": "Deep Learning Face Representation by Joint Identification-Verification",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/e5e63da79fcd2bebbd7cb8bf1c1d0274-Paper.pdf",
    "abstract": "The key challenge of face recognition is to develop effective feature representations for reducing intra-personal variations while enlarging inter-personal differences. In this paper, we show that it can be well solved with deep learning and using both face identification and verification signals as supervision. The Deep IDentification-verification features (DeepID2) are learned with carefully designed deep convolutional networks. The face identification task increases the inter-personal variations by drawing DeepID2 features extracted from different identities apart, while the face verification task reduces the intra-personal variations by pulling DeepID2 features extracted from the same identity together, both of which are essential to face recognition. The learned DeepID2 features can be well generalized to new identities unseen in the training data. On the challenging LFW dataset, 99.15% face verification accuracy is achieved. Compared with the best previous deep learning result on LFW, the error rate has been significantly reduced by 67%.",
    "authors": [
      "Sun, Yi",
      "Chen, Yuheng",
      "Wang, Xiaogang",
      "Tang, Xiaoou"
    ]
  },
  {
    "id": "e744f91c29ec99f0e662c9177946c627",
    "title": "Joint Training of a Convolutional Network and a Graphical Model for Human Pose Estimation",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/e744f91c29ec99f0e662c9177946c627-Paper.pdf",
    "abstract": "This paper proposes a new hybrid architecture that consists of a deep Convolutional Network and a Markov Random Field. We show how this architecture is successfully applied to the challenging problem of articulated human pose estimation in monocular images. The architecture can exploit structural domain constraints such as geometric relationships between body joint locations. We show that joint training of these two model paradigms improves performance and allows us to significantly outperform existing state-of-the-art techniques.",
    "authors": [
      "Tompson, Jonathan J.",
      "Jain, Arjun",
      "LeCun, Yann",
      "Bregler, Christoph"
    ]
  },
  {
    "id": "e7b24b112a44fdd9ee93bdf998c6ca0e",
    "title": "Permutation Diffusion Maps (PDM) with Application to the Image Association Problem in Computer Vision",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/e7b24b112a44fdd9ee93bdf998c6ca0e-Paper.pdf",
    "abstract": "Consistently matching keypoints across images, and the related problem of finding clusters of nearby images, are critical components of various tasks in Computer Vision, including Structure from Motion (SfM). Unfortunately, occlusion and large repetitive structures tend to mislead most currently used matching algorithms, leading to characteristic pathologies in the final output. In this paper we introduce a new method, Permutations Diffusion Maps (PDM), to solve the matching problem, as well as a related new affinity measure, derived using ideas from harmonic analysis on the symmetric group. We show that just by using it as a preprocessing step to existing SfM pipelines, PDM can greatly improve reconstruction quality on difficult datasets.",
    "authors": [
      "Pachauri, Deepti",
      "Kondor, Risi",
      "Sargur, Gautam",
      "Singh, Vikas"
    ]
  },
  {
    "id": "e816c635cad85a60fabd6b97b03cbcc9",
    "title": "Structure learning of antiferromagnetic Ising models",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/e816c635cad85a60fabd6b97b03cbcc9-Paper.pdf",
    "abstract": "In this paper we investigate the computational complexity of learning the graph structure underlying a discrete undirected graphical model from i.i.d. samples. Our first result is an unconditional computational lower bound of $\\Omega (p^{d/2})$ for learning general graphical models on $p$ nodes of maximum degree $d$, for the class of statistical algorithms recently introduced by Feldman et al. The construction is related to the notoriously difficult learning parities with noise problem in computational learning theory. Our lower bound shows that the $\\widetilde O(p^{d+2})$ runtime required by Bresler, Mossel, and Sly's exhaustive-search algorithm cannot be significantly improved without restricting the class of models. Aside from structural assumptions on the graph such as it being a tree, hypertree, tree-like, etc., most recent papers on structure learning assume that the model has the correlation decay property. Indeed, focusing on ferromagnetic Ising models, Bento and Montanari showed that all known low-complexity algorithms fail to learn simple graphs when the interaction strength exceeds a number related to the correlation decay threshold. Our second set of results gives a class of repelling (antiferromagnetic) models that have the \\emph{opposite} behavior: very strong repelling allows efficient learning in time $\\widetilde O(p^2)$. We provide an algorithm whose performance interpolates between $\\widetilde O(p^2)$ and $\\widetilde O(p^{d+2})$ depending on the strength of the repulsion.",
    "authors": [
      "Bresler, Guy",
      "Gamarnik, David",
      "Shah, Devavrat"
    ]
  },
  {
    "id": "e8dfff4676a47048d6f0c4ef899593dd",
    "title": "Clustered factor analysis of multineuronal spike data",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/e8dfff4676a47048d6f0c4ef899593dd-Paper.pdf",
    "abstract": "High-dimensional, simultaneous recordings of neural spiking activity are often explored, analyzed and visualized with the help of latent variable or factor models. Such models are however ill-equipped to extract structure beyond shared, distributed aspects of firing activity across multiple cells. Here, we extend unstructured factor models by proposing a model that discovers subpopulations or groups of cells from the pool of recorded neurons. The model combines aspects of mixture of factor analyzer models for capturing clustering structure, and aspects of latent dynamical system models for capturing temporal dependencies. In the resulting model, we infer the subpopulations and the latent factors from data using variational inference and model parameters are estimated by Expectation Maximization (EM). We also address the crucial problem of initializing parameters for EM by extending a sparse subspace clustering algorithm to integer-valued spike count observations. We illustrate the merits of the proposed model by applying it to calcium-imaging data from spinal cord neurons, and we show that it uncovers meaningful clustering structure in the data.",
    "authors": [
      "Buesing, Lars",
      "Machado, Timothy A.",
      "Cunningham, John P.",
      "Paninski, Liam"
    ]
  },
  {
    "id": "e94550c93cd70fe748e6982b3439ad3b",
    "title": "Mode Estimation for High Dimensional Discrete Tree Graphical Models",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/e94550c93cd70fe748e6982b3439ad3b-Paper.pdf",
    "abstract": "This paper studies the following problem: given samples from a high dimensional discrete distribution, we want to estimate the leading $(\\delta,\\rho)$-modes of the underlying distributions. A point is defined to be a $(\\delta,\\rho)$-mode if it is a local optimum of the density within a $\\delta$-neighborhood under metric $\\rho$. As we increase the ``scale'' parameter $\\delta$, the neighborhood size increases and the total number of modes monotonically decreases. The sequence of the $(\\delta,\\rho)$-modes reveal intrinsic topographical information of the underlying distributions. Though the mode finding problem is generally intractable in high dimensions, this paper unveils that, if the distribution can be approximated well by a tree graphical model, mode characterization is significantly easier. An efficient algorithm with provable theoretical guarantees is proposed and is applied to applications like data analysis and multiple predictions.",
    "authors": [
      "Chen, Chao",
      "Liu, Han",
      "Metaxas, Dimitris",
      "Zhao, Tianqi"
    ]
  },
  {
    "id": "e94f63f579e05cb49c05c2d050ead9c0",
    "title": "Sampling for Inference in Probabilistic Models with Fast Bayesian Quadrature",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/e94f63f579e05cb49c05c2d050ead9c0-Paper.pdf",
    "abstract": "We propose a novel sampling framework for inference in probabilistic models: an active learning approach that converges more quickly (in wall-clock time) than Markov chain Monte Carlo (MCMC) benchmarks. The central challenge in probabilistic inference is numerical integration, to average over ensembles of models or unknown (hyper-)parameters (for example to compute marginal likelihood or a partition function). MCMC has provided approaches to numerical integration that deliver state-of-the-art inference, but can suffer from sample inefficiency and poor convergence diagnostics. Bayesian quadrature techniques offer a model-based solution to such problems, but their uptake has been hindered by prohibitive computation costs. We introduce a warped model for probabilistic integrands (likelihoods) that are known to be non-negative, permitting a cheap active learning scheme to optimally select sample locations. Our algorithm is demonstrated to offer faster convergence (in seconds) relative to simple Monte Carlo and annealed importance sampling on both synthetic and real-world examples.",
    "authors": [
      "Gunter, Tom",
      "Osborne, Michael A.",
      "Garnett, Roman",
      "Hennig, Philipp",
      "Roberts, Stephen J."
    ]
  },
  {
    "id": "ea8fcd92d59581717e06eb187f10666d",
    "title": "Do Deep Nets Really Need to be Deep?",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/ea8fcd92d59581717e06eb187f10666d-Paper.pdf",
    "abstract": "Currently, deep neural networks are the state of the art on problems such as speech recognition and computer vision. In this paper we empirically demonstrate that shallow feed-forward nets can learn the complex functions previously learned by deep nets and achieve accuracies previously only achievable with deep models. Moreover, in some cases the shallow nets can learn these deep functions using the same number of parameters as the original deep models. On the TIMIT phoneme recognition and CIFAR-10 image recognition tasks, shallow nets can be trained that perform similarly to complex, well-engineered, deeper convolutional models.",
    "authors": [
      "Ba, Jimmy",
      "Caruana, Rich"
    ]
  },
  {
    "id": "eae27d77ca20db309e056e3d2dcd7d69",
    "title": "A Unified Semantic Embedding: Relating Taxonomies and Attributes",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/eae27d77ca20db309e056e3d2dcd7d69-Paper.pdf",
    "abstract": "We propose a method that learns a discriminative yet semantic space for object categorization, where we also embed auxiliary semantic entities such as supercategories and attributes. Contrary to prior work which only utilized them as side information, we explicitly embed the semantic entities into the same space where we embed categories, which enables us to represent a category as their linear combination. By exploiting such a unified model for semantics, we enforce each category to be generated as a sparse combination of a supercategory + attributes, with an additional exclusive regularization to learn discriminative composition. The proposed reconstructive regularization guides the discriminative learning process to learn a better generalizing model, as well as generates compact semantic description of each category, which enables humans to analyze what has been learned.",
    "authors": [
      "Hwang, Sung Ju",
      "Sigal, Leonid"
    ]
  },
  {
    "id": "eb160de1de89d9058fcb0b968dbbbd68",
    "title": "Parallel Double Greedy Submodular Maximization",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/eb160de1de89d9058fcb0b968dbbbd68-Paper.pdf",
    "abstract": "Many machine learning problems can be reduced to the maximization of submodular functions. Although well understood in the serial setting, the parallel maximization of submodular functions remains an open area of research with recent results only addressing monotone functions. The optimal algorithm for maximizing the more general class of non-monotone submodular functions was introduced by Buchbinder et al. and follows a strongly serial double-greedy logic and program analysis. In this work, we propose two methods to parallelize the double-greedy algorithm. The first, coordination-free approach emphasizes speed at the cost of a weaker approximation guarantee. The second, concurrency control approach guarantees a tight 1/2-approximation, at the quantifiable cost of additional coordination and reduced parallelism. As a consequence we explore the trade off space between guaranteed performance and objective optimality. We implement and evaluate both algorithms on multi-core hardware and billion edge graphs, demonstrating both the scalability and tradeoffs of each approach.",
    "authors": [
      "Pan, Xinghao",
      "Jegelka, Stefanie",
      "Gonzalez, Joseph E.",
      "Bradley, Joseph K.",
      "Jordan, Michael I."
    ]
  },
  {
    "id": "eb86d510361fc23b59f18c1bc9802cc6",
    "title": "Efficient Optimization for Average Precision SVM",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/eb86d510361fc23b59f18c1bc9802cc6-Paper.pdf",
    "abstract": "The accuracy of information retrieval systems is often measured using average precision (AP). Given a set of positive (relevant) and negative (non-relevant) samples, the parameters of a retrieval system can be estimated using the AP-SVM framework, which minimizes a regularized convex upper bound on the empirical AP loss. However, the high computational complexity of loss-augmented inference, which is required for learning an AP-SVM, prohibits its use with large training datasets. To alleviate this deficiency, we propose three complementary approaches. The first approach guarantees an asymptotic decrease in the computational complexity of loss-augmented inference by exploiting the problem structure. The second approach takes advantage of the fact that we do not require a full ranking during loss-augmented inference. This helps us to avoid the expensive step of sorting the negative samples according to their individual scores. The third approach approximates the AP loss over all samples by the AP loss over difficult samples (for example, those that are incorrectly classified by a binary SVM), while ensuring the correct classification of the remaining samples. Using the PASCAL VOC action classification and object detection datasets, we show that our approaches provide significant speed-ups during training without degrading the test accuracy of AP-SVM.",
    "authors": [
      "Mohapatra, Pritish",
      "Jawahar, C.V.",
      "Kumar, M. Pawan"
    ]
  },
  {
    "id": "ede7e2b6d13a41ddf9f4bdef84fdc737",
    "title": "SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/ede7e2b6d13a41ddf9f4bdef84fdc737-Paper.pdf",
    "abstract": "In this work we introduce a new fast incremental gradient method SAGA, in the spirit of SAG, SDCA, MISO and SVRG. SAGA improves on the theory behind SAG and SVRG, with better theoretical convergence rates, and support for composite objectives where a proximal operator is used on the regulariser. Unlike SDCA, SAGA supports non-strongly convex problems directly, and is adaptive to any inherent strong convexity of the problem. We give experimental results showing the effectiveness of our method.",
    "authors": [
      "Defazio, Aaron",
      "Bach, Francis",
      "Lacoste-Julien, Simon"
    ]
  },
  {
    "id": "f09696910bdd874a99cd74c8f05b5c44",
    "title": "A Differential Equation for Modeling Nesterov\u2019s Accelerated Gradient Method: Theory and Insights",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/f09696910bdd874a99cd74c8f05b5c44-Paper.pdf",
    "abstract": "We derive a second-order ordinary differential equation (ODE), which is the limit of Nesterov\u2019s accelerated gradient method. This ODE exhibits approximate equivalence to Nesterov\u2019s scheme and thus can serve as a tool for analysis. We show that the continuous time ODE allows for a better understanding of Nesterov\u2019s scheme. As a byproduct, we obtain a family of schemes with similar convergence rates. The ODE interpretation also suggests restarting Nesterov\u2019s scheme leading to an algorithm, which can be rigorously proven to converge at a linear rate whenever the objective is strongly convex.",
    "authors": [
      "Su, Weijie",
      "Boyd, Stephen",
      "Candes, Emmanuel"
    ]
  },
  {
    "id": "f0adc8838f4bdedde4ec2cfad0515589",
    "title": "Sparse PCA with Oracle Property",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/f0adc8838f4bdedde4ec2cfad0515589-Paper.pdf",
    "abstract": "In this paper, we study the estimation of the $k$-dimensional sparse principal subspace of covariance matrix $\\Sigma$ in the high-dimensional setting. We aim to recover the oracle principal subspace solution, i.e., the principal subspace estimator obtained assuming the true support is known a priori. To this end, we propose a family of estimators based on the semidefinite relaxation of sparse PCA with novel regularizations. In particular, under a weak assumption on the magnitude of the population projection matrix, one estimator within this family exactly recovers the true support with high probability, has exact rank-$k$, and attains a $\\sqrt{s/n}$ statistical rate of convergence with $s$ being the subspace sparsity level and $n$ the sample size. Compared to existing support recovery results for sparse PCA, our approach does not hinge on the spiked covariance model or the limited correlation condition. As a complement to the first estimator that enjoys the oracle property, we prove that, another estimator within the family achieves a sharper statistical rate of convergence than the standard semidefinite relaxation of sparse PCA, even when the previous assumption on the magnitude of the projection matrix is violated. We validate the theoretical results by numerical experiments on synthetic datasets.",
    "authors": [
      "Gu, Quanquan",
      "Wang, Zhaoran",
      "Liu, Han"
    ]
  },
  {
    "id": "f0e52b27a7a5d6a1a87373dffa53dbe5",
    "title": "SerialRank: Spectral Ranking using Seriation",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/f0e52b27a7a5d6a1a87373dffa53dbe5-Paper.pdf",
    "abstract": "We describe a seriation algorithm for ranking a set of n items given pairwise comparisons between these items. Intuitively, the algorithm assigns similar rankings to items that compare similarly with all others. It does so by constructing a similarity matrix from pairwise comparisons, using seriation methods to reorder this matrix and construct a ranking. We first show that this spectral seriation algorithm recovers the true ranking when all pairwise comparisons are observed and consistent with a total order. We then show that ranking reconstruction is still exact even when some pairwise comparisons are corrupted or missing, and that seriation based spectral ranking is more robust to noise than other scoring methods. An additional benefit of the seriation formulation is that it allows us to solve semi-supervised ranking problems. Experiments on both synthetic and real datasets demonstrate that seriation based spectral ranking achieves competitive and in some cases superior performance compared to classical ranking methods.",
    "authors": [
      "Fogel, Fajwel",
      "d'Aspremont, Alexandre",
      "Vojnovic, Milan"
    ]
  },
  {
    "id": "f0fcf351df4eb6786e9bb6fc4e2dee02",
    "title": "Pre-training of Recurrent Neural Networks via Linear Autoencoders",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/f0fcf351df4eb6786e9bb6fc4e2dee02-Paper.pdf",
    "abstract": "We propose a pre-training technique for recurrent neural networks based on linear autoencoder networks for sequences, i.e. linear dynamical systems modelling the target sequences. We start by giving a closed form solution for the definition of the optimal weights of a linear autoencoder given a training set of sequences. This solution, however, is computationally very demanding, so we suggest a procedure to get an approximate solution for a given number of hidden units. The weights obtained for the linear autoencoder are then used as initial weights for the input-to-hidden connections of a recurrent neural network, which is then trained on the desired task. Using four well known datasets of sequences of polyphonic music, we show that the proposed pre-training approach is highly effective, since it allows to largely improve the state of the art results on all the considered datasets.",
    "authors": [
      "Pasa, Luca",
      "Sperduti, Alessandro"
    ]
  },
  {
    "id": "f29c21d4897f78948b91f03172341b7b",
    "title": "Stochastic Gradient Descent, Weighted Sampling, and the Randomized Kaczmarz algorithm",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/f29c21d4897f78948b91f03172341b7b-Paper.pdf",
    "abstract": "We improve a recent gurantee of Bach and Moulines on the linear convergence of SGD for smooth and strongly convex objectives, reducing a quadratic dependence on the strong convexity to a linear dependence. Furthermore, we show how reweighting the sampling distribution (i.e. importance sampling) is necessary in order to further improve convergence, and obtain a linear dependence on average smoothness, dominating previous results, and more broadly discus how importance sampling for SGD can improve convergence also in other scenarios. Our results are based on a connection we make between SGD and the randomized Kaczmarz algorithm, which allows us to transfer ideas between the separate bodies of literature studying each of the two methods.",
    "authors": [
      "Needell, Deanna",
      "Ward, Rachel",
      "Srebro, Nati"
    ]
  },
  {
    "id": "f387624df552cea2f369918c5e1e12bc",
    "title": "Best-Arm Identification in Linear Bandits",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/f387624df552cea2f369918c5e1e12bc-Paper.pdf",
    "abstract": "We study the best-arm identification problem in linear bandit, where the rewards of the arms depend linearly on an unknown parameter $\\theta^*$ and the objective is to return the arm with the largest reward. We characterize the complexity of the problem and introduce sample allocation strategies that pull arms to identify the best arm with a fixed confidence, while minimizing the sample budget. In particular, we show the importance of exploiting the global linear structure to improve the estimate of the reward of near-optimal arms. We analyze the proposed strategies and compare their empirical performance. Finally, as a by-product of our analysis, we point out the connection to the $G$-optimality criterion used in optimal experimental design.",
    "authors": [
      "Soare, Marta",
      "Lazaric, Alessandro",
      "Munos, Remi"
    ]
  },
  {
    "id": "f4573fc71c731d5c362f0d7860945b88",
    "title": "Multivariate f-divergence Estimation With Confidence",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/f4573fc71c731d5c362f0d7860945b88-Paper.pdf",
    "abstract": "The problem of f-divergence estimation is important in the fields of machine learning, information theory, and statistics. While several divergence estimators exist, relatively few have known convergence properties. In particular, even for those estimators whose MSE convergence rates are known, the asymptotic distributions are unknown. We establish the asymptotic normality of a recently proposed ensemble estimator of f-divergence between two distributions from a finite number of samples. This estimator has MSE convergence rate of O(1/T), is simple to implement, and performs well in high dimensions. This theory enables us to perform divergence-based inference tasks such as testing equality of pairs of distributions based on empirical samples. We experimentally validate our theoretical results and, as an illustration, use them to empirically bound the best achievable classification error.",
    "authors": [
      "Moon, Kevin",
      "Hero, Alfred"
    ]
  },
  {
    "id": "f4a4da9aa7eadfd23c7bdb7cf57b3112",
    "title": "Online Decision-Making in General Combinatorial Spaces",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/f4a4da9aa7eadfd23c7bdb7cf57b3112-Paper.pdf",
    "abstract": "We study online combinatorial decision problems, where one must make sequential decisions in some combinatorial space without knowing in advance the cost of decisions on each trial; the goal is to minimize the total regret over some sequence of trials relative to the best fixed decision in hindsight. Such problems have been studied mostly in settings where decisions are represented by Boolean vectors and costs are linear in this representation. Here we study a general setting where costs may be linear in any suitable low-dimensional vector representation of elements of the decision space. We give a general algorithm for such problems that we call low-dimensional online mirror descent (LDOMD); the algorithm generalizes both the Component Hedge algorithm of Koolen et al. (2010), and a recent algorithm of Suehiro et al. (2012). Our study offers a unification and generalization of previous work, and emphasizes the role of the convex polytope arising from the vector representation of the decision space; while Boolean representations lead to 0-1 polytopes, more general vector representations lead to more general polytopes. We study several examples of both types of polytopes. Finally, we demonstrate the benefit of having a general framework for such problems via an application to an online transportation problem; the associated transportation polytopes generalize the Birkhoff polytope of doubly stochastic matrices, and the resulting algorithm generalizes the PermELearn algorithm of Helmbold and Warmuth (2009).",
    "authors": [
      "Rajkumar, Arun",
      "Agarwal, Shivani"
    ]
  },
  {
    "id": "f4b9ec30ad9f68f89b29639786cb62ef",
    "title": "Altitude Training: Strong Bounds for Single-Layer Dropout",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/f4b9ec30ad9f68f89b29639786cb62ef-Paper.pdf",
    "abstract": "Dropout training, originally designed for deep neural networks, has been successful on high-dimensional single-layer natural language tasks. This paper proposes a theoretical explanation for this phenomenon: we show that, under a generative Poisson topic model with long documents, dropout training improves the exponent in the generalization bound for empirical risk minimization. Dropout achieves this gain much like a marathon runner who practices at altitude: once a classifier learns to perform reasonably well on training examples that have been artificially corrupted by dropout, it will do very well on the uncorrupted test set. We also show that, under similar conditions, dropout preserves the Bayes decision boundary and should therefore induce minimal bias in high dimensions.",
    "authors": [
      "Wager, Stefan",
      "Fithian, William",
      "Wang, Sida",
      "Liang, Percy S."
    ]
  },
  {
    "id": "f57a2f557b098c43f11ab969efe1504b",
    "title": "Probabilistic low-rank matrix completion on finite alphabets",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/f57a2f557b098c43f11ab969efe1504b-Paper.pdf",
    "abstract": "The task of reconstructing a matrix given a sample of observed entries is known as the \\emph{matrix completion problem}. Such a consideration arises in a wide variety of problems, including recommender systems, collaborative filtering, dimensionality reduction, image processing, quantum physics or multi-class classification to name a few. Most works have focused on recovering an unknown real-valued low-rank matrix from randomly sub-sampling its entries. Here, we investigate the case where the observations take a finite numbers of values, corresponding for examples to ratings in recommender systems or labels in multi-class classification. We also consider a general sampling scheme (non-necessarily uniform) over the matrix entries. The performance of a nuclear-norm penalized estimator is analyzed theoretically. More precisely, we derive bounds for the Kullback-Leibler divergence between the true and estimated distributions. In practice, we have also proposed an efficient algorithm based on lifted coordinate gradient descent in order to tackle potentially high dimensional settings.",
    "authors": [
      "Lafond, Jean",
      "Klopp, Olga",
      "Moulines, Eric",
      "Salmon, Joseph"
    ]
  },
  {
    "id": "f60bb6bb4c96d4df93c51bd69dcc15a0",
    "title": "Tight Continuous Relaxation of the Balanced k-Cut Problem",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/f60bb6bb4c96d4df93c51bd69dcc15a0-Paper.pdf",
    "abstract": "Spectral Clustering as a relaxation of the normalized/ratio cut has become one of the standard graph-based clustering methods. Existing methods for the computation of multiple clusters, corresponding to a balanced k-cut of the graph, are either based on greedy techniques or heuristics which have weak connection to the original motivation of minimizing the normalized cut. In this paper we propose a new tight continuous relaxation for any balanced k-cut problem and show that a related recently proposed relaxation is in most cases loose leading to poor performance in practice. For the optimization of our tight continuous relaxation we propose a new algorithm for the hard sum-of-ratios minimization problem which achieves monotonic descent. Extensive comparisons show that our method beats all existing approaches for ratio cut and other balanced k-cut criteria.",
    "authors": [
      "Rangapuram, Syama Sundar",
      "Mudrakarta, Pramod Kaushik",
      "Hein, Matthias"
    ]
  },
  {
    "id": "f63f65b503e22cb970527f23c9ad7db1",
    "title": "Discrete Graph Hashing",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/f63f65b503e22cb970527f23c9ad7db1-Paper.pdf",
    "abstract": "Hashing has emerged as a popular technique for fast nearest neighbor search in gigantic databases. In particular, learning based hashing has received considerable attention due to its appealing storage and search efficiency. However, the performance of most unsupervised learning based hashing methods deteriorates rapidly as the hash code length increases. We argue that the degraded performance is due to inferior optimization procedures used to achieve discrete binary codes. This paper presents a graph-based unsupervised hashing model to preserve the neighborhood structure of massive data in a discrete code space. We cast the graph hashing problem into a discrete optimization framework which directly learns the binary codes. A tractable alternating maximization algorithm is then proposed to explicitly deal with the discrete constraints, yielding high-quality codes to well capture the local neighborhoods. Extensive experiments performed on four large datasets with up to one million samples show that our discrete optimization based graph hashing method obtains superior search accuracy over state-of-the-art unsupervised hashing methods, especially for longer codes.",
    "authors": [
      "Liu, Wei",
      "Mu, Cun",
      "Kumar, Sanjiv",
      "Chang, Shih-Fu"
    ]
  },
  {
    "id": "f670ef5d2d6bdf8f29450a970494dd64",
    "title": "Orbit Regularization",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/f670ef5d2d6bdf8f29450a970494dd64-Paper.pdf",
    "abstract": "We propose a general framework for regularization based on group majorization. In this framework, a group is defined to act on the parameter space and an orbit is fixed; to control complexity, the model parameters are confined to lie in the convex hull of this orbit (the orbitope). Common regularizers are recovered as particular cases, and a connection is revealed between the recent sorted 1 -norm and the hyperoctahedral group. We derive the properties a group must satisfy for being amenable to optimization with conditional and projected gradient algorithms. Finally, we suggest a continuation strategy for orbit exploration, presenting simulation results for the symmetric and hyperoctahedral groups.",
    "authors": [
      "Negrinho, Renato",
      "Martins, Andre"
    ]
  },
  {
    "id": "f718499c1c8cef6730f9fd03c8125cab",
    "title": "A Synaptical Story of Persistent Activity with Graded Lifetime in a Neural System",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/f718499c1c8cef6730f9fd03c8125cab-Paper.pdf",
    "abstract": "Persistent activity refers to the phenomenon that cortical neurons keep firing even after the stimulus triggering the initial neuronal responses is moved. Persistent activity is widely believed to be the substrate for a neural system retaining a memory trace of the stimulus information. In a conventional view, persistent activity is regarded as an attractor of the network dynamics, but it faces a challenge of how to be closed properly. Here, in contrast to the view of attractor, we consider that the stimulus information is encoded in a marginally unstable state of the network which decays very slowly and exhibits persistent firing for a prolonged duration. We propose a simple yet effective mechanism to achieve this goal, which utilizes the property of short-term plasticity (STP) of neuronal synapses. STP has two forms, short-term depression (STD) and short-term facilitation (STF), which have opposite effects on retaining neuronal responses. We find that by properly combining STF and STD, a neural system can hold persistent activity of graded lifetime, and that persistent activity fades away naturally without relying on an external drive. The implications of these results on neural information representation are discussed.",
    "authors": [
      "Mi, Yuanyuan",
      "Li, Luozheng",
      "Wang, Dahui",
      "Wu, Si"
    ]
  },
  {
    "id": "f7664060cc52bc6f3d620bcedc94a4b6",
    "title": "Log-Hilbert-Schmidt metric between positive definite operators on Hilbert spaces",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/f7664060cc52bc6f3d620bcedc94a4b6-Paper.pdf",
    "abstract": "This paper introduces a novel mathematical and computational framework, namely {\\it Log-Hilbert-Schmidt metric} between positive definite operators on a Hilbert space. This is a generalization of the Log-Euclidean metric on the Riemannian manifold of positive definite matrices to the infinite-dimensional setting. The general framework is applied in particular to compute distances between covariance operators on a Reproducing Kernel Hilbert Space (RKHS), for which we obtain explicit formulas via the corresponding Gram matrices. Empirically, we apply our formulation to the task of multi-category image classification, where each image is represented by an infinite-dimensional RKHS covariance operator. On several challenging datasets, our method significantly outperforms approaches based on covariance matrices computed directly on the original input features, including those using the Log-Euclidean metric, Stein and Jeffreys divergences, achieving new state of the art results.",
    "authors": [
      "Ha Quang, Minh",
      "San Biagio, Marco",
      "Murino, Vittorio"
    ]
  },
  {
    "id": "f76a89f0cb91bc419542ce9fa43902dc",
    "title": "Constant Nullspace Strong Convexity and Fast Convergence of Proximal Methods under High-Dimensional Settings",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/f76a89f0cb91bc419542ce9fa43902dc-Paper.pdf",
    "abstract": "State of the art statistical estimators for high-dimensional problems take the form of regularized, and hence non-smooth, convex programs. A key facet of thesestatistical estimation problems is that these are typically not strongly convex under a high-dimensional sampling regime when the Hessian matrix becomes rank-deficient. Under vanilla convexity however, proximal optimization methods attain only a sublinear rate. In this paper, we investigate a novel variant of strong convexity, which we call Constant Nullspace Strong Convexity (CNSC), where we require that the objective function be strongly convex only over a constant subspace. As we show, the CNSC condition is naturally satisfied by high-dimensional statistical estimators. We then analyze the behavior of proximal methods under this CNSC condition: we show global linear convergence of Proximal Gradient and local quadratic convergence of Proximal Newton Method, when the regularization function comprising the statistical estimator is decomposable. We corroborate our theory via numerical experiments, and show a qualitative difference in the convergence rates of the proximal algorithms when the loss function does satisfy the CNSC condition.",
    "authors": [
      "Yen, Ian En-Hsu",
      "Hsieh, Cho-Jui",
      "Ravikumar, Pradeep K.",
      "Dhillon, Inderjit S."
    ]
  },
  {
    "id": "f9a40a4780f5e1306c46f1c8daecee3b",
    "title": "Sparse Bayesian structure learning with \u201cdependent relevance determination\u201d priors",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/f9a40a4780f5e1306c46f1c8daecee3b-Paper.pdf",
    "abstract": "In many problem settings, parameter vectors are not merely sparse, but dependent in such a way that non-zero coefficients tend to cluster together. We refer to this form of dependency as \u201cregion sparsity\u201d. Classical sparse regression methods, such as the lasso and automatic relevance determination (ARD), model parameters as independent a priori, and therefore do not exploit such dependencies. Here we introduce a hierarchical model for smooth, region-sparse weight vectors and tensors in a linear regression setting. Our approach represents a hierarchical extension of the relevance determination framework, where we add a transformed Gaussian process to model the dependencies between the prior variances of regression weights. We combine this with a structured model of the prior variances of Fourier coefficients, which eliminates unnecessary high frequencies. The resulting prior encourages weights to be region-sparse in two different bases simultaneously. We develop efficient approximate inference methods and show substantial improvements over comparable methods (e.g., group lasso and smooth RVM) for both simulated and real datasets from brain imaging.",
    "authors": [
      "Wu, Anqi",
      "Park, Mijung",
      "Koyejo, Oluwasanmi O.",
      "Pillow, Jonathan W."
    ]
  },
  {
    "id": "f9be311e65d81a9ad8150a60844bb94c",
    "title": "Deep Symmetry Networks",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/f9be311e65d81a9ad8150a60844bb94c-Paper.pdf",
    "abstract": "The chief difficulty in object recognition is that objects' classes are obscured by a large number of extraneous sources of variability, such as pose and part deformation. These sources of variation can be represented by symmetry groups, sets of composable transformations that preserve object identity. Convolutional neural networks (convnets) achieve a degree of translational invariance by computing feature maps over the translation group, but cannot handle other groups. As a result, these groups' effects have to be approximated by small translations, which often requires augmenting datasets and leads to high sample complexity. In this paper, we introduce deep symmetry networks (symnets), a generalization of convnets that forms feature maps over arbitrary symmetry groups. Symnets use kernel-based interpolation to tractably tie parameters and pool over symmetry spaces of any dimension. Like convnets, they are trained with backpropagation. The composition of feature transformations through the layers of a symnet provides a new approach to deep learning. Experiments on NORB and MNIST-rot show that symnets over the affine group greatly reduce sample complexity relative to convnets by better capturing the symmetries in the data.",
    "authors": [
      "Gens, Robert",
      "Domingos, Pedro M."
    ]
  },
  {
    "id": "fa83a11a198d5a7f0bf77a1987bcd006",
    "title": "Covariance shrinkage for autocorrelated data",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/fa83a11a198d5a7f0bf77a1987bcd006-Paper.pdf",
    "abstract": "The accurate estimation of covariance matrices is essential for many signal processing and machine learning algorithms. In high dimensional settings the sample covariance is known to perform poorly, hence regularization strategies such as analytic shrinkage of Ledoit/Wolf are applied. In the standard setting, i.i.d. data is assumed, however, in practice, time series typically exhibit strong autocorrelation structure, which introduces a pronounced estimation bias. Recent work by Sancetta has extended the shrinkage framework beyond i.i.d. data. We contribute in this work by showing that the Sancetta estimator, while being consistent in the high-dimensional limit, suffers from a high bias in finite sample sizes. We propose an alternative estimator, which is (1) unbiased, (2) less sensitive to hyperparameter choice and (3) yields superior performance in simulations on toy data and on a real world data set from an EEG-based Brain-Computer-Interfacing experiment.",
    "authors": [
      "Bartz, Daniel",
      "M\u00fcller, Klaus-Robert"
    ]
  },
  {
    "id": "facf9f743b083008a894eee7baa16469",
    "title": "Scale Adaptive Blind Deblurring",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/facf9f743b083008a894eee7baa16469-Paper.pdf",
    "abstract": "The presence of noise and small scale structures usually leads to large kernel estimation errors in blind image deblurring empirically, if not a total failure. We present a scale space perspective on blind deblurring algorithms, and introduce a cascaded scale space formulation for blind deblurring. This new formulation suggests a natural approach robust to noise and small scale structures through tying the estimation across multiple scales and balancing the contributions of different scales automatically by learning from data. The proposed formulation also allows to handle non-uniform blur with a straightforward extension. Experiments are conducted on both benchmark dataset and real-world images to validate the effectiveness of the proposed method. One surprising finding based on our approach is that blur kernel estimation is not necessarily best at the finest scale.",
    "authors": [
      "Zhang, Haichao",
      "Yang, Jianchao"
    ]
  },
  {
    "id": "fb8feff253bb6c834deb61ec76baa893",
    "title": "Parallel Feature Selection Inspired by Group Testing",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/fb8feff253bb6c834deb61ec76baa893-Paper.pdf",
    "abstract": "This paper presents a parallel feature selection method for classification that scales up to very high dimensions and large data sizes. Our original method is inspired by group testing theory, under which the feature selection procedure consists of a collection of randomized tests to be performed in parallel. Each test corresponds to a subset of features, for which a scoring function may be applied to measure the relevance of the features in a classification task. We develop a general theory providing sufficient conditions under which true features are guaranteed to be correctly identified. Superior performance of our method is demonstrated on a challenging relation extraction task from a very large data set that have both redundant features and sample size in the order of millions. We present comprehensive comparisons with state-of-the-art feature selection methods on a range of data sets, for which our method exhibits competitive performance in terms of running time and accuracy. Moreover, it also yields substantial speedup when used as a pre-processing step for most other existing methods.",
    "authors": [
      "Zhou, Yingbo",
      "Porwal, Utkarsh",
      "Zhang, Ce",
      "Ngo, Hung Q.",
      "Nguyen, XuanLong",
      "R\u00e9, Christopher",
      "Govindaraju, Venu"
    ]
  },
  {
    "id": "fc528592c3858f90196fbfacc814f235",
    "title": "Streaming, Memory Limited Algorithms for Community Detection",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/fc528592c3858f90196fbfacc814f235-Paper.pdf",
    "abstract": "In this paper, we consider sparse networks consisting of a finite number of non-overlapping communities, i.e. disjoint clusters, so that there is higher density within clusters than across clusters. Both the intra- and inter-cluster edge densities vanish when the size of the graph grows large, making the cluster reconstruction problem nosier and hence difficult to solve. We are interested in scenarios where the network size is very large, so that the adjacency matrix of the graph is hard to manipulate and store. The data stream model in which columns of the adjacency matrix are revealed sequentially constitutes a natural framework in this setting. For this model, we develop two novel clustering algorithms that extract the clusters asymptotically accurately. The first algorithm is {\\it offline}, as it needs to store and keep the assignments of nodes to clusters, and requires a memory that scales linearly with the network size. The second algorithm is {\\it online}, as it may classify a node when the corresponding column is revealed and then discard this information. This algorithm requires a memory growing sub-linearly with the network size. To construct these efficient streaming memory-limited clustering algorithms, we first address the problem of clustering with partial information, where only a small proportion of the columns of the adjacency matrix is observed and develop, for this setting, a new spectral algorithm which is of independent interest.",
    "authors": [
      "Yun, Se-Young",
      "lelarge, marc",
      "Proutiere, Alexandre"
    ]
  },
  {
    "id": "fd5c905bcd8c3348ad1b35d7231ee2b1",
    "title": "Analysis of Brain States from Multi-Region LFP Time-Series",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/fd5c905bcd8c3348ad1b35d7231ee2b1-Paper.pdf",
    "abstract": "The local field potential (LFP) is a source of information about the broad patterns of brain activity, and the frequencies present in these time-series measurements are often highly correlated between regions. It is believed that these regions may jointly constitute a ``brain state,'' relating to cognition and behavior. An infinite hidden Markov model (iHMM) is proposed to model the evolution of brain states, based on electrophysiological LFP data measured at multiple brain regions. A brain state influences the spectral content of each region in the measured LFP. A new state-dependent tensor factorization is employed across brain regions, and the spectral properties of the LFPs are characterized in terms of Gaussian processes (GPs). The LFPs are modeled as a mixture of GPs, with state- and region-dependent mixture weights, and with the spectral content of the data encoded in GP spectral mixture covariance kernels. The model is able to estimate the number of brain states and the number of mixture components in the mixture of GPs. A new variational Bayesian split-merge algorithm is employed for inference. The model infers state changes as a function of external covariates in two novel electrophysiological datasets, using LFP data recorded simultaneously from multiple brain regions in mice; the results are validated and interpreted by subject-matter experts.",
    "authors": [
      "Ulrich, Kyle R.",
      "Carlson, David E.",
      "Lian, Wenzhao",
      "Borg, Jana S.",
      "Dzirasa, Kafui",
      "Carin, Lawrence"
    ]
  },
  {
    "id": "fde9264cf376fffe2ee4ddf4a988880d",
    "title": "Clamping Variables and Approximate Inference",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/fde9264cf376fffe2ee4ddf4a988880d-Paper.pdf",
    "abstract": "It was recently proved using graph covers (Ruozzi, 2012) that the Bethe partition function is upper bounded by the true partition function for a binary pairwise model that is attractive. Here we provide a new, arguably simpler proof from first principles. We make use of the idea of clamping a variable to a particular value. For an attractive model, we show that summing over the Bethe partition functions for each sub-model obtained after clamping any variable can only raise (and hence improve) the approximation. In fact, we derive a stronger result that may have other useful implications. Repeatedly clamping until we obtain a model with no cycles, where the Bethe approximation is exact, yields the result. We also provide a related lower bound on a broad class of approximate partition functions of general pairwise multi-label models that depends only on the topology. We demonstrate that clamping a few wisely chosen variables can be of practical value by dramatically reducing approximation error.",
    "authors": [
      "Weller, Adrian",
      "Jebara, Tony"
    ]
  },
  {
    "id": "feab05aa91085b7a8012516bc3533958",
    "title": "Neural Word Embedding as Implicit Matrix Factorization",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/feab05aa91085b7a8012516bc3533958-Paper.pdf",
    "abstract": "We analyze skip-gram with negative-sampling (SGNS), a word embedding method introduced by Mikolov et al., and show that it is implicitly factorizing a word-context matrix, whose cells are the pointwise mutual information (PMI) of the respective word and context pairs, shifted by a global constant. We find that another embedding method, NCE, is implicitly factorizing a similar matrix, where each cell is the (shifted) log conditional probability of a word given its context. We show that using a sparse Shifted Positive PMI word-context matrix to represent words improves results on two word similarity tasks and one of two analogy tasks. When dense low-dimensional vectors are preferred, exact factorization with SVD can achieve solutions that are at least as good as SGNS's solutions for word similarity tasks. On analogy questions SGNS remains superior to SVD. We conjecture that this stems from the weighted nature of SGNS's factorization.",
    "authors": [
      "Levy, Omer",
      "Goldberg, Yoav"
    ]
  },
  {
    "id": "ff4d5fbbafdf976cfdc032e3bde78de5",
    "title": "Constrained convex minimization via model-based excessive gap",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/ff4d5fbbafdf976cfdc032e3bde78de5-Paper.pdf",
    "abstract": "We introduce a model-based excessive gap technique to analyze first-order primal- dual methods for constrained convex minimization. As a result, we construct first- order primal-dual methods with optimal convergence rates on the primal objec- tive residual and the primal feasibility gap of their iterates separately. Through a dual smoothing and prox-center selection strategy, our framework subsumes the augmented Lagrangian, alternating direction, and dual fast-gradient methods as special cases, where our rates apply.",
    "authors": [
      "Tran-Dinh, Quoc",
      "Cevher, Volkan"
    ]
  },
  {
    "id": "ffeed84c7cb1ae7bf4ec4bd78275bb98",
    "title": "A Filtering Approach to Stochastic Variational Inference",
    "year": "2014",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2014/file/ffeed84c7cb1ae7bf4ec4bd78275bb98-Paper.pdf",
    "abstract": "Stochastic variational inference (SVI) uses stochastic optimization to scale up Bayesian computation to massive data. We present an alternative perspective on SVI as approximate parallel coordinate ascent. SVI trades-off bias and variance to step close to the unknown true coordinate optimum given by batch variational Bayes (VB). We define a model to automate this process. The model infers the location of the next VB optimum from a sequence of noisy realizations. As a consequence of this construction, we update the variational parameters using Bayes rule, rather than a hand-crafted optimization schedule. When our model is a Kalman filter this procedure can recover the original SVI algorithm and SVI with adaptive steps. We may also encode additional assumptions in the model, such as heavy-tailed noise. By doing so, our algorithm outperforms the original SVI schedule and a state-of-the-art adaptive SVI algorithm in two diverse domains.",
    "authors": [
      "Houlsby, Neil",
      "Blei, David"
    ]
  }
]