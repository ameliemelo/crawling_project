[{
    "id": "000c076c390a4c357313fca29e390ece",
    "title": "Beyond Value-Function Gaps: Improved Instance-Dependent Regret Bounds for Episodic Reinforcement Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/000c076c390a4c357313fca29e390ece-Paper.pdf",
    "abstract": "We provide improved gap-dependent regret bounds for reinforcement learning in finite episodic Markov decision processes. Compared to prior work, our bounds depend on alternative definitions of gaps. These definitions are based on the insight that, in order to achieve a favorable regret, an algorithm does not need to learn how to behave optimally in states that are not reached by an optimal policy. We prove tighter upper regret bounds for optimistic algorithms and accompany them with new information-theoretic lower bounds for a large class of MDPs. Our results show that optimistic algorithms can not achieve the information-theoretic lower bounds even in deterministic MDPs unless there is a unique optimal policy.",
    "authors": [
      "Dann, Christoph",
      "Marinov, Teodor Vanislavov",
      "Mohri, Mehryar",
      "Zimmert, Julian"
    ]
  },
  {
    "id": "003dd617c12d444ff9c80f717c3fa982",
    "title": "Learning One Representation to Optimize All Rewards",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/003dd617c12d444ff9c80f717c3fa982-Paper.pdf",
    "abstract": "We introduce the forward-backward (FB) representation of the dynamics of a reward-free Markov decision process. It provides explicit near-optimal policies for any reward specified a posteriori. During an unsupervised phase, we use reward-free interactions with the environment to learn two representations via off-the-shelf deep learning methods and temporal difference (TD) learning. In the test phase, a reward representation is estimated either from reward observations or an explicit reward description (e.g., a target state). The optimal policy for thatreward is directly obtained from these representations, with no planning. We assume access to an exploration scheme or replay buffer for the first phase.The corresponding unsupervised loss is well-principled: if training is perfect, the policies obtained are provably optimal for any reward function.  With imperfect training, the sub-optimality is proportional to the unsupervised approximation error. The FB representation learns long-range relationships between states and actions, via a predictive occupancy map, without having to synthesize states as in model-based approaches.This is a step towards learning controllable agents in arbitrary black-box stochastic environments. This approach compares well to goal-oriented RL algorithms on discrete and continuous mazes, pixel-based MsPacman, and the FetchReach virtual robot arm. We also illustrate how the agent can immediately adapt to new tasks beyond goal-oriented RL.",
    "authors": [
      "Touati, Ahmed",
      "Ollivier, Yann"
    ]
  },
  {
    "id": "007ff380ee5ac49ffc34442f5c2a2b86",
    "title": "Matrix factorisation and the interpretation of geodesic distance",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/007ff380ee5ac49ffc34442f5c2a2b86-Paper.pdf",
    "abstract": "Given a graph or similarity matrix, we consider the problem of recovering a notion of true distance between the nodes, and so their true positions. We show that this can be accomplished in two steps: matrix factorisation, followed by nonlinear dimension reduction. This combination is effective because the point cloud obtained in the first step lives close to a manifold in which latent distance is encoded as geodesic distance. Hence, a nonlinear dimension reduction tool, approximating geodesic distance, can recover the latent positions, up to a simple transformation. We give a detailed account of the case where spectral embedding is used, followed by Isomap, and provide encouraging experimental evidence for other combinations of techniques.",
    "authors": [
      "Whiteley, Nick",
      "Gray, Annie",
      "Rubin-Delanchy, Patrick"
    ]
  },
  {
    "id": "0084ae4bc24c0795d1e6a4f58444d39b",
    "title": "UniDoc: Unified Pretraining Framework for Document Understanding",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0084ae4bc24c0795d1e6a4f58444d39b-Paper.pdf",
    "abstract": "Document intelligence automates the extraction of information from documents and supports many business applications. Recent self-supervised learning methods on large-scale unlabeled document datasets have opened up promising directions towards reducing annotation efforts by training models with self-supervised objectives. However, most of the existing document pretraining methods are still language-dominated. We present UDoc, a new unified pretraining framework for document understanding. UDoc is designed to support most document understanding tasks, extending the Transformer to take multimodal embeddings as input. Each input element is composed of words and visual features from a semantic region of the input document image. An important feature of UDoc is that it learns a generic representation by making use of three self-supervised losses, encouraging the representation to model sentences, learn similarities, and align modalities. Extensive empirical analysis demonstrates that the pretraining procedure learns better joint representations and leads to improvements in downstream tasks.",
    "authors": [
      "Gu, Jiuxiang",
      "Kuen, Jason",
      "Morariu, Vlad I",
      "Zhao, Handong",
      "Jain, Rajiv",
      "Barmpalios, Nikolaos",
      "Nenkova, Ani",
      "Sun, Tong"
    ]
  },
  {
    "id": "008bd5ad93b754d500338c253d9c1770",
    "title": "Finding Discriminative Filters for Specific Degradations in Blind Super-Resolution",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/008bd5ad93b754d500338c253d9c1770-Paper.pdf",
    "abstract": "Recent blind super-resolution (SR) methods typically consist of two branches, one for degradation prediction and the other for conditional restoration. However, our experiments show that a one-branch network can achieve comparable performance to the two-branch scheme. Then we wonder: how can one-branch networks automatically learn to distinguish degradations? To find the answer, we propose a new diagnostic tool -- Filter Attribution method based on Integral Gradient (FAIG). Unlike previous integral gradient methods, our FAIG aims at finding the most discriminative filters instead of input pixels/features for degradation removal in blind SR networks. With the discovered filters, we further develop a simple yet effective method to predict the degradation of an input image. Based on FAIG, we show that, in one-branch blind SR networks, 1) we could find a very small number of (1%) discriminative filters for each specific degradation; 2) The weights, locations and connections of the discovered filters are all important to determine the specific network function. 3) The task of degradation prediction can be implicitly realized by these discriminative filters without explicit supervised learning. Our findings can not only help us better understand network behaviors inside one-branch blind SR networks, but also provide guidance on designing more efficient architectures and diagnosing networks for blind SR.",
    "authors": [
      "Xie, Liangbin",
      "Wang, Xintao",
      "Dong, Chao",
      "Qi, Zhongang",
      "Shan, Ying"
    ]
  },
  {
    "id": "009c434cab57de48a31f6b669e7ba266",
    "title": "Counterfactual Explanations Can Be Manipulated",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/009c434cab57de48a31f6b669e7ba266-Paper.pdf",
    "abstract": "Counterfactual explanations are emerging as an attractive option for providing recourse to individuals adversely impacted by algorithmic decisions.  As they are deployed in critical applications (e.g. law enforcement, financial lending), it becomes important to ensure that we clearly understand the vulnerabilties of these methods and find ways to address them. However, there is little understanding of the vulnerabilities and shortcomings of counterfactual explanations. In this work, we introduce the first framework that describes the vulnerabilities of counterfactual explanations and shows how they can be manipulated. More specifically, we show counterfactual explanations may converge to drastically different counterfactuals under a small perturbation indicating they are not robust.  Leveraging this insight, we introduce a novel objective to train seemingly fair models where counterfactual explanations find much lower cost recourse under a slight perturbation.  We describe how these models can unfairly provide low-cost recourse for specific subgroups in the data while appearing fair to auditors. We perform experiments on loan and violent crime prediction data sets where certain subgroups achieve up to 20x lower cost recourse under the perturbation. These results raise concerns regarding the dependability of current counterfactual explanation techniques, which we hope will inspire investigations in robust counterfactual explanations.",
    "authors": [
      "Slack, Dylan",
      "Hilgard, Anna",
      "Lakkaraju, Himabindu",
      "Singh, Sameer"
    ]
  },
  {
    "id": "00ac8ed3b4327bdd4ebbebcb2ba10a00",
    "title": "From Canonical Correlation Analysis to Self-supervised Graph Neural Networks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/00ac8ed3b4327bdd4ebbebcb2ba10a00-Paper.pdf",
    "abstract": "We introduce a conceptually simple yet effective model for self-supervised representation learning with graph data. It follows the previous methods that generate two views of an input graph through data augmentation. However, unlike contrastive methods that focus on instance-level discrimination, we optimize an innovative feature-level objective inspired by classical Canonical Correlation Analysis. Compared with other works, our approach requires none of the parameterized mutual information estimator, additional projector, asymmetric structures, and most importantly, negative samples which can be costly. We show that the new objective essentially 1) aims at discarding augmentation-variant information by learning invariant representations, and 2) can prevent degenerated solutions by decorrelating features in different dimensions. Our theoretical analysis further provides an understanding for the new objective which can be equivalently seen as an instantiation of the Information Bottleneck Principle under the self-supervised setting. Despite its simplicity, our method performs competitively on seven public graph datasets.",
    "authors": [
      "Zhang, Hengrui",
      "Wu, Qitian",
      "Yan, Junchi",
      "Wipf, David",
      "Yu, Philip S"
    ]
  },
  {
    "id": "00b76fddeaaa7d8c2c43d504b2babd8a",
    "title": "BAST: Bayesian Additive Regression Spanning Trees for Complex Constrained Domain",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/00b76fddeaaa7d8c2c43d504b2babd8a-Paper.pdf",
    "abstract": "Nonparametric regression on complex domains has been a challenging task as most existing methods, such as ensemble models based on binary decision trees, are not designed to account for intrinsic geometries and domain boundaries. This article proposes a Bayesian additive regression spanning trees (BAST) model for nonparametric regression on manifolds, with an emphasis on complex constrained domains or irregularly shaped spaces embedded in Euclidean spaces. Our model is built upon a random spanning tree manifold partition model as each weak learner, which is capable of capturing any irregularly shaped spatially contiguous partitions while respecting intrinsic geometries and domain boundary constraints. Utilizing many nice properties of spanning tree structures, we design an efficient Bayesian inference algorithm. Equipped with a soft prediction scheme, BAST is demonstrated to significantly outperform other competing methods in simulation experiments and in an application to the chlorophyll data in Aral Sea, due to its strong local adaptivity to different levels of smoothness. ",
    "authors": [
      "Luo, Zhao Tang",
      "Sang, Huiyan",
      "Mallick, Bani"
    ]
  },
  {
    "id": "01259a0cb2431834302abe2df60a1327",
    "title": "Hyperbolic Busemann Learning with Ideal Prototypes",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/01259a0cb2431834302abe2df60a1327-Paper.pdf",
    "abstract": "Hyperbolic space has become a popular choice of manifold for representation learning of various datatypes from tree-like structures and text to graphs. Building on the success of deep learning with prototypes in Euclidean and hyperspherical spaces, a few recent works have proposed hyperbolic prototypes for classification. Such approaches enable effective learning in low-dimensional output spaces and can exploit hierarchical relations amongst classes, but require privileged information about class labels to position the hyperbolic prototypes. In this work, we propose Hyperbolic Busemann Learning. The main idea behind our approach is to position prototypes on the ideal boundary of the Poincar\\'{e} ball, which does not require prior label knowledge. To be able to compute proximities to ideal prototypes, we introduce the penalised Busemann loss. We provide theory supporting the use of ideal prototypes and the proposed loss by proving its equivalence to logistic regression in the one-dimensional case. Empirically, we show that our approach provides a natural interpretation of classification confidence, while outperforming recent hyperspherical and hyperbolic prototype approaches.",
    "authors": [
      "Ghadimi Atigh, Mina",
      "Keller-Ressel, Martin",
      "Mettes, Pascal"
    ]
  },
  {
    "id": "012d9fe15b2493f21902cd55603382ec",
    "title": "Backward-Compatible Prediction Updates: A Probabilistic Approach",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/012d9fe15b2493f21902cd55603382ec-Paper.pdf",
    "abstract": "When machine learning systems meet real world applications, accuracy is only one of several requirements. In this paper, we assay a complementary perspective originating from the increasing availability of pre-trained and regularly improving state-of-the-art models. While new improved models develop at a fast pace, downstream tasks vary more slowly or stay constant. Assume that we have a large unlabelled data set for which we want to maintain accurate predictions. Whenever a new and presumably better ML models becomes available, we encounter two problems: (i) given a limited budget, which data points should be re-evaluated using the new model?; and (ii) if the new predictions differ from the current ones, should we update? Problem (i) is about compute cost, which matters for very large data sets and models. Problem (ii) is about maintaining consistency of the predictions, which can be highly relevant for downstream applications; our demand is to avoid negative flips, i.e., changing correct to incorrect predictions. In this paper, we formalize the Prediction Update Problem and present an efficient probabilistic approach as answer to the above questions. In extensive experiments on standard classification benchmark data sets, we show that our method outperforms alternative strategies along key metrics for backward-compatible prediction updates.",
    "authors": [
      "Tr\u00e4uble, Frederik",
      "von K\u00fcgelgen, Julius",
      "Kleindessner, Matth\u00e4us",
      "Locatello, Francesco",
      "Sch\u00f6lkopf, Bernhard",
      "Gehler, Peter"
    ]
  },
  {
    "id": "01632f7b7a127233fa1188bd6c2e42e1",
    "title": "Truncated Marginal Neural Ratio Estimation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/01632f7b7a127233fa1188bd6c2e42e1-Paper.pdf",
    "abstract": "Parametric stochastic simulators are ubiquitous in science, often featuring high-dimensional input parameters and/or an intractable likelihood. Performing Bayesian parameter inference in this context can be challenging. We present a neural simulation-based inference algorithm which simultaneously offers simulation efficiency and fast empirical posterior testability, which is unique among modern algorithms. Our approach is simulation efficient by simultaneously estimating low-dimensional marginal posteriors instead of the joint posterior and by proposing simulations targeted to an observation of interest via a prior suitably truncated by an indicator function.  Furthermore, by estimating a locally amortized posterior our algorithm enables efficient empirical tests of the robustness of the inference results. Since scientists cannot access the ground truth, these tests are necessary for trusting inference in real-world applications. We perform experiments on a marginalized version of the simulation-based inference benchmark and two complex and narrow posteriors, highlighting the simulator efficiency of our algorithm as well as the quality of the estimated marginal posteriors.",
    "authors": [
      "Miller, Benjamin K",
      "Cole, Alex",
      "Forr\u00e9, Patrick",
      "Louppe, Gilles",
      "Weniger, Christoph"
    ]
  },
  {
    "id": "01894d6f048493d2cacde3c579c315a3",
    "title": "ReAct: Out-of-distribution Detection With Rectified Activations",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/01894d6f048493d2cacde3c579c315a3-Paper.pdf",
    "abstract": "Out-of-distribution (OOD) detection has received much attention lately due to its practical importance in enhancing the safe deployment of neural networks. One of the primary challenges is that models often produce highly confident predictions on OOD data, which undermines the driving principle in OOD detection that the model should only be confident about in-distribution samples. In this work, we propose ReAct\u2014a simple and effective technique for reducing model overconfidence on OOD data. Our method is motivated by novel analysis on internal activations of neural networks, which displays highly distinctive signature patterns for OOD distributions. Our method can generalize effectively to different network architectures and different OOD detection scores. We empirically demonstrate that ReAct achieves competitive detection performance on a comprehensive suite of benchmark datasets, and give theoretical explication for our method\u2019s efficacy. On the ImageNet benchmark, ReAct reduces the false positive rate (FPR95) by 25.05% compared to the previous best method.",
    "authors": [
      "Sun, Yiyou",
      "Guo, Chuan",
      "Li, Yixuan"
    ]
  },
  {
    "id": "018b59ce1fd616d874afad0f44ba338d",
    "title": "Non-local Latent Relation Distillation for Self-Adaptive 3D Human Pose Estimation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/018b59ce1fd616d874afad0f44ba338d-Paper.pdf",
    "abstract": "Available 3D human pose estimation approaches leverage different forms of strong (2D/3D pose) or weak (multi-view or depth) paired supervision. Barring synthetic or in-studio domains, acquiring such supervision for each new target environment is highly inconvenient. To this end, we cast 3D pose learning as a self-supervised adaptation problem that aims to transfer the task knowledge from a labeled source domain to a completely unpaired target. We propose to infer image-to-pose via two explicit mappings viz. image-to-latent and latent-to-pose where the latter is a pre-learned decoder obtained from a prior-enforcing generative adversarial auto-encoder. Next, we introduce relation distillation as a means to align the unpaired cross-modal samples i.e., the unpaired target videos and unpaired 3D pose sequences. To this end, we propose a new set of non-local relations in order to characterize long-range latent pose interactions, unlike general contrastive relations where positive couplings are limited to a local neighborhood structure. Further, we provide an objective way to quantify non-localness in order to select the most effective relation set. We evaluate different self-adaptation settings and demonstrate state-of-the-art 3D human pose estimation performance on standard benchmarks.",
    "authors": [
      "Kundu, Jogendra Nath",
      "Seth, Siddharth",
      "Jamkhandi, Anirudh",
      "YM, Pradyumna",
      "Jampani, Varun",
      "Chakraborty, Anirban",
      "R, Venkatesh Babu"
    ]
  },
  {
    "id": "01931a6925d3de09e5f87419d9d55055",
    "title": "Fast Training of Neural Lumigraph Representations using Meta Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/01931a6925d3de09e5f87419d9d55055-Paper.pdf",
    "abstract": "Novel view synthesis is a long-standing problem in machine learning and computer vision. Significant progress has recently been made in developing neural scene representations and rendering techniques that synthesize photorealistic images from arbitrary views. These representations, however, are extremely slow to train and often also slow to render. Inspired by neural variants of image-based rendering, we develop a new neural rendering approach with the goal of quickly learning a high-quality representation which can also be rendered in real-time. Our approach, MetaNLR++, accomplishes this by using a unique combination of a neural shape representation and 2D CNN-based image feature extraction, aggregation, and re-projection. To push representation convergence times down to minutes, we leverage meta learning to learn neural shape and image feature priors which accelerate training. The optimized shape and image features can then be extracted using traditional graphics techniques and rendered in real time. We show that MetaNLR++ achieves similar or better novel view synthesis results in a fraction of the time that competing methods require.",
    "authors": [
      "Bergman, Alexander",
      "Kellnhofer, Petr",
      "Wetzstein, Gordon"
    ]
  },
  {
    "id": "019f8b946a256d9357eadc5ace2c8678",
    "title": "Analytical Study of Momentum-Based Acceleration Methods in Paradigmatic High-Dimensional Non-Convex Problems",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/019f8b946a256d9357eadc5ace2c8678-Paper.pdf",
    "abstract": "The optimization step in many machine learning problems rarely relies on vanilla gradient descent but it is common practice to use momentum-based accelerated methods. Despite these algorithms being widely applied to arbitrary loss functions, their behaviour in generically non-convex, high dimensional landscapes is poorly understood.In this work, we use dynamical mean field theory techniques to describe analytically the average dynamics of these methods in a prototypical non-convex model: the (spiked) matrix-tensor model. We derive a closed set of equations that describe the behaviour of heavy-ball momentum and Nesterov acceleration in the infinite dimensional limit. By numerical integration of these equations, we observe that these methods speed up the dynamics but do not improve the algorithmic threshold with respect to gradient descent in the spiked model.",
    "authors": [
      "Sarao Mannelli, Stefano",
      "Urbani, Pierfrancesco"
    ]
  },
  {
    "id": "01b7575c38dac42f3cfb7d500438b875",
    "title": "Multimodal Few-Shot Learning with Frozen Language Models",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/01b7575c38dac42f3cfb7d500438b875-Paper.pdf",
    "abstract": "When trained at sufficient scale, auto-regressive language models exhibit the notable ability to learn a new language task after being prompted with just a few examples. Here, we present a simple, yet effective, approach for transferring this few-shot learning ability to a multimodal setting (vision and language). Using aligned image and caption data, we train a vision encoder to represent each image as a sequence of continuous embeddings, such that a pre-trained, frozen language model presented with this prefix generates the appropriate caption. The resulting system is a multimodal few-shot learner, with the surprising ability to learn a variety of new tasks when conditioned on examples, represented as a sequence of any number of interleaved image and text embeddings. We demonstrate that it can rapidly learn words for new objects and novel visual categories, do visual question-answering with only a handful of examples, and make use of outside knowledge, by measuring a single model on a variety of established and new benchmarks.",
    "authors": [
      "Tsimpoukelli, Maria",
      "Menick, Jacob L",
      "Cabi, Serkan",
      "Eslami, S. M. Ali",
      "Vinyals, Oriol",
      "Hill, Felix"
    ]
  },
  {
    "id": "01d8bae291b1e4724443375634ccfa0e",
    "title": "Approximating the Permanent with Deep Rejection Sampling",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/01d8bae291b1e4724443375634ccfa0e-Paper.pdf",
    "abstract": "We present a randomized approximation scheme for the permanent of a matrix with nonnegative entries. Our scheme extends a recursive rejection sampling method of Huber and Law (SODA 2008) by replacing the permanent upper bound with a linear combination of the subproblem bounds at a moderately large depth of the recursion tree. This method, we call deep rejection sampling, is empirically shown to outperform the basic, depth-zero variant, as well as a related method by Kuck et al. (NeurIPS 2019).  We analyze the expected running time of the scheme on random $(0, 1)$-matrices where each entry is independently $1$ with probability $p$. Our bound is superior to a previous one for $p$ less than $1/5$, matching another bound that was only known to hold when every row and column has density exactly $p$.",
    "authors": [
      "Harviainen, Juha",
      "R\u00f6ysk\u00f6, Antti",
      "Koivisto, Mikko"
    ]
  },
  {
    "id": "01ded4259d101feb739b06c399e9cd9c",
    "title": "Revisiting Model Stitching to Compare Neural Representations",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/01ded4259d101feb739b06c399e9cd9c-Paper.pdf",
    "abstract": "We revisit and extend model stitching (Lenc & Vedaldi 2015) as a methodology to study the internal representations of neural networks. Given two trained and frozen models $A$ and $B$, we consider a \"stitched model\" formed by connecting the bottom-layers of $A$ to the top-layers of $B$, with a simple trainable layer between them.  We argue that model stitching is a powerful and perhaps under-appreciated tool, which reveals aspects of representations that measures such as centered kernel alignment (CKA) cannot. Through extensive experiments, we use model stitching to obtain quantitative verifications for intuitive statements such as \"good networks learn similar representations\", by demonstrating that good networks of the same architecture, but trained in very different ways (eg: supervised vs. self-supervised learning), can be stitched to each other without drop in performance. We also give evidence for the intuition that \"more is better\" by showing that representations learnt with (1) more data, (2) bigger width, or (3) more training time can be \"plugged in\" to weaker models to improve performance. Finally, our experiments reveal a new structural property of SGD which we call \"stitching connectivity\", akin to mode-connectivity: typical minima reached by SGD are all \"stitching-connected\" to each other.",
    "authors": [
      "Bansal, Yamini",
      "Nakkiran, Preetum",
      "Barak, Boaz"
    ]
  },
  {
    "id": "01e9565cecc4e989123f9620c1d09c09",
    "title": "AugMax: Adversarial Composition of Random Augmentations for Robust Training",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/01e9565cecc4e989123f9620c1d09c09-Paper.pdf",
    "abstract": "Data augmentation is a simple yet effective way to improve the robustness of deep neural networks (DNNs). Diversity and hardness are two complementary dimensions of data augmentation to achieve robustness. For example, AugMix explores random compositions of a diverse set of augmentations to enhance broader coverage, while adversarial training generates adversarially hard samples to spot the weakness. Motivated by this, we propose a data augmentation framework, termed AugMax, to unify the two aspects of diversity and hardness. AugMax first randomly samples multiple augmentation operators and then learns an adversarial mixture of the selected operators. Being a stronger form of data augmentation, AugMax leads to a significantly augmented input distribution which makes model training more challenging. To solve this problem, we further design a disentangled normalization module, termed DuBIN (Dual-Batch-and-Instance Normalization), that disentangles the instance-wise feature heterogeneity arising from AugMax. Experiments show that AugMax-DuBIN leads to significantly improved out-of-distribution robustness, outperforming prior arts by 3.03%, 3.49%, 1.82% and 0.71% on CIFAR10-C, CIFAR100-C, Tiny ImageNet-C and ImageNet-C. Codes and pretrained models are available: https://github.com/VITA-Group/AugMax.",
    "authors": [
      "Wang, Haotao",
      "Xiao, Chaowei",
      "Kossaifi, Jean",
      "Yu, Zhiding",
      "Anandkumar, Anima",
      "Wang, Zhangyang"
    ]
  },
  {
    "id": "021bbc7ee20b71134d53e20206bd6feb",
    "title": "Habitat 2.0: Training Home Assistants to Rearrange their Habitat",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/021bbc7ee20b71134d53e20206bd6feb-Paper.pdf",
    "abstract": "We introduce Habitat 2.0 (H2.0), a simulation platform for training virtual robots in interactive 3D environments and complex physics-enabled scenarios. We make comprehensive contributions to all levels of the embodied AI stack \u2013 data, simulation, and benchmark tasks. Specifically, we present: (i) ReplicaCAD: an artist-authored, annotated, reconfigurable 3D dataset of apartments (matching real spaces) with articulated objects (e.g. cabinets and drawers that can open/close); (ii) H2.0: a high-performance physics-enabled 3D simulator with speeds exceeding 25,000 simulation steps per second (850x real-time) on an 8-GPU node, representing 100x speed-ups over prior work; and, (iii) Home Assistant Benchmark (HAB): a suite of common tasks for assistive robots (tidy the house, stock groceries, set the table) that test a range of mobile manipulation capabilities. These large-scale engineering contributions allow us to systematically compare deep reinforcement learning (RL) at scale and classical sense-plan-act (SPA) pipelines in long-horizon structured tasks, with an emphasis on generalization to new objects, receptacles, and layouts. We find that (1) flat RL policies struggle on HAB compared to hierarchical ones; (2) a hierarchy with independent skills suffers from \u2018hand-off problems\u2019, and (3) SPA pipelines are more brittle than RL policies.",
    "authors": [
      "Szot, Andrew",
      "Clegg, Alexander",
      "Undersander, Eric",
      "Wijmans, Erik",
      "Zhao, Yili",
      "Turner, John",
      "Maestre, Noah",
      "Mukadam, Mustafa",
      "Chaplot, Devendra Singh",
      "Maksymets, Oleksandr",
      "Gokaslan, Aaron",
      "Vondru\u0161, Vladim\u00edr",
      "Dharur, Sameer",
      "Meier, Franziska",
      "Galuba, Wojciech",
      "Chang, Angel",
      "Kira, Zsolt",
      "Koltun, Vladlen",
      "Malik, Jitendra",
      "Savva, Manolis",
      "Batra, Dhruv"
    ]
  },
  {
    "id": "024677efb8e4aee2eaeef17b54695bbe",
    "title": "Time Discretization-Invariant Safe Action Repetition for Policy Gradient Methods",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/024677efb8e4aee2eaeef17b54695bbe-Paper.pdf",
    "abstract": "In reinforcement learning, continuous time is often discretized by a time scale $\\delta$, to which the resulting performance is known to be highly sensitive. In this work, we seek to find a $\\delta$-invariant algorithm for policy gradient (PG) methods, which performs well regardless of the value of $\\delta$. We first identify the underlying reasons that cause PG methods to fail as $\\delta \\to 0$, proving that the variance of the PG estimator can diverge to infinity in stochastic environments under a certain assumption of stochasticity. While durative actions or action repetition can be employed to have $\\delta$-invariance, previous action repetition methods cannot immediately react to unexpected situations in stochastic environments. We thus propose a novel $\\delta$-invariant method named Safe Action Repetition (SAR) applicable to any existing PG algorithm. SAR can handle the stochasticity of environments by adaptively reacting to changes in states during action repetition. We empirically show that our method is not only $\\delta$-invariant but also robust to stochasticity, outperforming previous $\\delta$-invariant approaches on eight MuJoCo environments with both deterministic and stochastic settings. Our code is available at https://vision.snu.ac.kr/projects/sar.",
    "authors": [
      "Park, Seohong",
      "Kim, Jaekyeom",
      "Kim, Gunhee"
    ]
  },
  {
    "id": "024d2d699e6c1a82c9ba986386f4d824",
    "title": "Meta-Learning Reliable Priors in the Function Space",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/024d2d699e6c1a82c9ba986386f4d824-Paper.pdf",
    "abstract": "Meta-Learning promises to enable more data-efficient inference by harnessing previous experience from related learning tasks. While existing meta-learning methods help us to improve the accuracy of our predictions in face of data scarcity, they fail to supply reliable uncertainty estimates, often being grossly overconfident in their predictions. Addressing these shortcomings, we introduce a novel meta-learning framework, called F-PACOH, that treats meta-learned priors as stochastic processes and performs meta-level regularization directly in the function space. This allows us to directly steer the probabilistic predictions of the meta-learner towards high epistemic uncertainty in regions of insufficient meta-training data and, thus, obtain well-calibrated uncertainty estimates. Finally, we showcase how our approach can be integrated with sequential decision making, where reliable uncertainty quantification is imperative. In our benchmark study on meta-learning for Bayesian Optimization (BO), F-PACOH significantly outperforms all other meta-learners and standard baselines.  Even in a challenging lifelong BO setting, where optimization tasks arrive one at a time and the meta-learner needs to build up informative prior knowledge incrementally, our proposed method demonstrates strong positive transfer.",
    "authors": [
      "Rothfuss, Jonas",
      "Heyn, Dominique",
      "Chen, jinfan",
      "Krause, Andreas"
    ]
  },
  {
    "id": "0266e33d3f546cb5436a10798e657d97",
    "title": "VoiceMixer: Adversarial Voice Style Mixup",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0266e33d3f546cb5436a10798e657d97-Paper.pdf",
    "abstract": "Although recent advances in voice conversion have shown significant improvement, there still remains a gap between the converted voice and target voice. A key factor that maintains this gap is the insufficient decomposition of content and voice style from the source speech. This insufficiency leads to the converted speech containing source speech style or losing source speech content. In this paper, we present VoiceMixer which can effectively decompose and transfer voice style through a novel information bottleneck and adversarial feedback. With self-supervised representation learning, the proposed information bottleneck can decompose the content and style with only a small loss of content information. Also, for adversarial feedback of each information, the discriminator is decomposed into content and style discriminator with self-supervision, which enable our model to achieve better generalization to the voice style of the converted speech. The experimental results show the superiority of our model in disentanglement and transfer performance, and improve audio quality by preserving content information.",
    "authors": [
      "Lee, Sang-Hoon",
      "Kim, Ji-Hoon",
      "Chung, Hyunseung",
      "Lee, Seong-Whan"
    ]
  },
  {
    "id": "02e656adee09f8394b402d9958389b7d",
    "title": "Predicting What You Already Know Helps: Provable Self-Supervised Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/02e656adee09f8394b402d9958389b7d-Paper.pdf",
    "abstract": "Self-supervised representation learning solves auxiliary prediction tasks (known as pretext tasks), that do not require labeled data, to learn semantic representations. These pretext tasks are created solely using the input features, such as predicting a missing image patch, recovering the color channels of an image from context, or predicting missing words, yet predicting this \\textit{known} information helps in learning representations effective for downstream prediction tasks. This paper posits a mechanism based on approximate conditional independence to formalize how solving certain pretext tasks can learn representations that provably decrease the sample complexity of downstream supervised tasks. Formally, we quantify how the approximate independence between the components of the pretext task (conditional on the label and latent variables) allows us to learn representations that can solve the downstream task with drastically reduced sample complexity by just training a linear layer on top of the learned representation. ",
    "authors": [
      "Lee, Jason D.",
      "Lei, Qi",
      "Saunshi, Nikunj",
      "ZHUO, JIACHENG"
    ]
  },
  {
    "id": "030e65da2b1c944090548d36b244b28d",
    "title": "Oracle Complexity in Nonsmooth Nonconvex Optimization",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/030e65da2b1c944090548d36b244b28d-Paper.pdf",
    "abstract": "It is well-known that given a smooth, bounded-from-below, and possibly nonconvex function, standard gradient-based methods can find $\\epsilon$-stationary points (with gradient norm less than $\\epsilon$) in $\\mathcal{O}(1/\\epsilon^2)$ iterations. However, many important nonconvex optimization problems, such as those associated with training modern neural networks, are inherently not smooth, making these results inapplicable. In this paper, we study nonsmooth nonconvex optimization from an oracle complexity viewpoint, where the algorithm is assumed to be given access only to local information about the function at various points. We provide two main results (under mild assumptions): First, we consider the problem of getting \\emph{near} $\\epsilon$-stationary points. This is perhaps the most natural relaxation of \\emph{finding} $\\epsilon$-stationary points, which is impossible in the nonsmooth nonconvex case. We prove that this relaxed goal cannot be achieved efficiently, for any distance and $\\epsilon$ smaller than some constants. Our second result deals with the possibility of tackling nonsmooth nonconvex optimization by reduction to smooth optimization: Namely, applying smooth optimization methods on a smooth approximation of the objective function. For this approach, we prove an inherent trade-off between oracle complexity and smoothness: On the one hand, smoothing a nonsmooth nonconvex function can be done very efficiently (e.g., by randomized smoothing), but with dimension-dependent factors in the smoothness parameter, which can strongly affect iteration complexity when plugging into standard smooth optimization methods. On the other hand, these dimension factors can be  eliminated with suitable smoothing methods, but only by making the oracle complexity of the smoothing process exponentially large.",
    "authors": [
      "Kornowski, Guy",
      "Shamir, Ohad"
    ]
  },
  {
    "id": "03227b950778ab86436ff79fe975b596",
    "title": "CentripetalText: An Efficient Text Instance Representation for Scene Text Detection",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/03227b950778ab86436ff79fe975b596-Paper.pdf",
    "abstract": "Scene text detection remains a grand challenge due to the variation in text curvatures, orientations, and aspect ratios. One of the hardest problems in this task is how to represent text instances of arbitrary shapes. Although many methods have been proposed to model irregular texts in a flexible manner, most of them lose simplicity and robustness. Their complicated post-processings and the regression under Dirac delta distribution undermine the detection performance and the generalization ability. In this paper, we propose an efficient text instance representation named CentripetalText (CT), which decomposes text instances into the combination of text kernels and centripetal shifts. Specifically, we utilize the centripetal shifts to implement pixel aggregation, guiding the external text pixels to the internal text kernels. The relaxation operation is integrated into the dense regression for centripetal shifts, allowing the correct prediction in a range instead of a specific value. The convenient reconstruction of text contours and the tolerance of prediction errors in our method guarantee the high detection accuracy and the fast inference speed, respectively. Besides, we shrink our text detector into a proposal generation module, namely CentripetalText Proposal Network (CPN), replacing Segmentation Proposal Network (SPN) in Mask TextSpotter v3 and producing more accurate proposals. To validate the effectiveness of our method, we conduct experiments on several commonly used scene text benchmarks, including both curved and multi-oriented text datasets. For the task of scene text detection, our approach achieves superior or competitive performance compared to other existing methods, e.g., F-measure of 86.3% at 40.0 FPS on Total-Text, F-measure of 86.1% at 34.8 FPS on MSRA-TD500, etc. For the task of end-to-end scene text recognition, our method outperforms Mask TextSpotter v3 by 1.1% in F-measure on Total-Text.",
    "authors": [
      "Sheng, Tao",
      "Chen, Jie",
      "Lian, Zhouhui"
    ]
  },
  {
    "id": "032abcd424b4312e7087f434ef1c0094",
    "title": "Learning to Select Exogenous Events for Marked Temporal Point Process",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/032abcd424b4312e7087f434ef1c0094-Paper.pdf",
    "abstract": "Marked temporal point processes (MTPPs) have emerged as a powerful modelingtool for a wide variety of applications which are characterized using discreteevents localized in continuous time. In this context, the events are of two typesendogenous events which occur due to the influence of the previous events andexogenous events which occur due to the effect of the externalities. However, inpractice, the events do not come with endogenous or exogenous labels. To thisend, our goal in this paper is to identify the set of exogenous events from a set ofunlabelled events. To do so, we first formulate the parameter estimation problemin conjunction with exogenous event set selection problem and show that thisproblem is NP hard. Next, we prove that the underlying objective is a monotoneand \\alpha-submodular set function, with respect to the candidate set of exogenousevents. Such a characterization subsequently allows us to use a stochastic greedyalgorithm which was originally proposed in~\\cite{greedy}for submodular maximization.However, we show that it also admits an approximation guarantee for maximizing\\alpha-submodular set function, even when the learning algorithm provides an imperfectestimates of the trained parameters. Finally, our experiments with synthetic andreal data show that our method performs better than the existing approaches builtupon superposition of endogenous and exogenous MTPPs.",
    "authors": [
      "Zhang, Ping",
      "Iyer, Rishabh",
      "Tendulkar, Ashish",
      "Aggarwal, Gaurav",
      "De, Abir"
    ]
  },
  {
    "id": "0397758f8990c1b41b81b43ac389ab9f",
    "title": "DRIVE: One-bit Distributed Mean Estimation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0397758f8990c1b41b81b43ac389ab9f-Paper.pdf",
    "abstract": "We consider the problem where $n$ clients transmit $d$-dimensional real-valued vectors using $d(1+o(1))$ bits each, in a manner that allows the receiver to approximately reconstruct their mean. Such compression problems naturally arise in distributed and federated learning. We provide novel mathematical results and derive computationally efficient algorithms that are more accurate than previous compression techniques.  We evaluate our methods on a collection of distributed and federated learning tasks, using a variety of datasets, and show a consistent improvement over the state of the art.",
    "authors": [
      "Vargaftik, Shay",
      "Ben-Basat, Ran",
      "Portnoy, Amit",
      "Mendelson, Gal",
      "Ben-Itzhak, Yaniv",
      "Mitzenmacher, Michael"
    ]
  },
  {
    "id": "03a3655fff3e9bdea48de9f49e938e32",
    "title": "Learning Space Partitions for Path Planning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/03a3655fff3e9bdea48de9f49e938e32-Paper.pdf",
    "abstract": "Path planning, the problem of efficiently discovering high-reward trajectories, often requires optimizing a high-dimensional and multimodal reward function. Popular approaches like CEM and CMA-ES greedily focus on promising regions of the search space and may get trapped in local maxima. DOO and VOOT balance exploration and exploitation, but use space partitioning strategies independent of the reward function to be optimized. Recently, LaMCTS empirically learns to partition the search space in a reward-sensitive manner for black-box optimization. In this paper, we develop a novel formal regret analysis for when and why such an adaptive region partitioning scheme works. We also propose a new path planning method LaP3 which improves the function value estimation within each sub-region, and uses a latent representation of the search space. Empirically, LaP3 outperforms existing path planning methods in 2D navigation tasks, especially in the presence of difficult-to-escape local optima, and shows benefits when plugged into the planning components of model-based RL such as PETS. These gains transfer to highly multimodal real-world tasks, where we outperform strong baselines in compiler phase ordering by up to 39% on average across 9 tasks, and in molecular design by up to 0.4 on properties on a 0-1 scale. Code is available at https://github.com/yangkevin2/neurips2021-lap3.",
    "authors": [
      "Yang, Kevin",
      "Zhang, Tianjun",
      "Cummins, Chris",
      "Cui, Brandon",
      "Steiner, Benoit",
      "Wang, Linnan",
      "Gonzalez, Joseph E.",
      "Klein, Dan",
      "Tian, Yuandong"
    ]
  },
  {
    "id": "03b2ceb73723f8b53cd533e4fba898ee",
    "title": "Progressive Feature Interaction Search for Deep Sparse Network",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/03b2ceb73723f8b53cd533e4fba898ee-Paper.pdf",
    "abstract": "Deep sparse networks (DSNs), of which the crux is exploring the high-order feature interactions, have become the state-of-the-art on the prediction task with high-sparsity features. However, these models suffer from low computation efficiency, including large model size and slow model inference, which largely limits these models' application value. In this work, we approach this problem with neural architecture search by automatically searching the critical component in DSNs, the feature-interaction layer. We propose a distilled search space to cover the desired architectures with fewer parameters. We then develop a progressive search algorithm for efficient search on the space and well capture the order-priority property in sparse prediction tasks. Experiments on three real-world benchmark datasets show promising results of PROFIT in both accuracy and efficiency. Further studies validate the feasibility of our designed search space and search algorithm.",
    "authors": [
      "Gao, Chen",
      "Li, Yinfeng",
      "Yao, Quanming",
      "Jin, Depeng",
      "Li, Yong"
    ]
  },
  {
    "id": "03b92cd507ff5870df0db7f074728830",
    "title": "Local Explanation of Dialogue Response Generation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/03b92cd507ff5870df0db7f074728830-Paper.pdf",
    "abstract": "In comparison to the interpretation of classification models, the explanation of sequence generation models is also an important problem, however it has seen little attention. In this work, we study model-agnostic explanations of a representative text generation task -- dialogue response generation. Dialog response generation is challenging with its open-ended sentences and multiple acceptable responses. To gain insights into the reasoning process of a generation model, we propose a new method, local explanation of response generation (LERG) that regards the explanations as the mutual interaction of segments in input and output sentences. LERG views the sequence prediction as uncertainty estimation of a human response and then creates explanations by perturbing the input and calculating the certainty change over the human response. We show that LERG adheres to desired properties of explanations for text generation including unbiased approximation, consistency and cause identification. Empirically, our results show that our method consistently improves other widely used methods on proposed automatic- and human- evaluation metrics for this new task by $4.4$-$12.8$\\%. Our analysis demonstrates that LERG can extract both explicit and implicit relations between input and output segments.",
    "authors": [
      "Tuan, Yi-Lin",
      "Pryor, Connor",
      "Chen, Wenhu",
      "Getoor, Lise",
      "Wang, William Yang"
    ]
  },
  {
    "id": "03e4d3f831100d4355663f3d425d716b",
    "title": "Scalable Inference in SDEs by Direct Matching of the Fokker\u2013Planck\u2013Kolmogorov Equation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/03e4d3f831100d4355663f3d425d716b-Paper.pdf",
    "abstract": "Simulation-based techniques such as variants of stochastic Runge\u2013Kutta are the de facto approach for inference with stochastic differential equations (SDEs) in machine learning. These methods are general-purpose and used with parametric and non-parametric models, and neural SDEs. Stochastic Runge\u2013Kutta relies on the use of sampling schemes that can be inefficient in high dimensions. We address this issue by revisiting the classical SDE literature and derive direct approximations to the (typically intractable) Fokker\u2013Planck\u2013Kolmogorov equation by matching moments. We show how this workflow is fast, scales to high-dimensional latent spaces, and is applicable to scarce-data applications, where a non-parametric SDE with a driving Gaussian process velocity field specifies the model. ",
    "authors": [
      "Solin, Arno",
      "Tamir, Ella",
      "Verma, Prakhar"
    ]
  },
  {
    "id": "040a99f23e8960763e680041c601acab",
    "title": "The Complexity of Bayesian Network Learning: Revisiting the Superstructure",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/040a99f23e8960763e680041c601acab-Paper.pdf",
    "abstract": "We investigate the parameterized complexity of Bayesian Network Structure Learning (BNSL), a classical problem that has received significant attention in empirical but also purely theoretical studies. We follow up on previous works that have analyzed the complexity of BNSL w.r.t. the so-called superstructure of the input. While known results imply that BNSL is unlikely to be fixed-parameter tractable even when parameterized by the size of a vertex cover in the superstructure, here we show that a different kind of parameterization - notably by the size of a feedback edge set - yields fixed-parameter tractability. We proceed by showing that this result can be strengthened to a localized version of the feedback edge set, and provide corresponding lower bounds that complement previous results to provide a complexity classification of BNSL w.r.t. virtually all well-studied graph parameters.We then analyze how the complexity of BNSL depends on the representation of the input. In particular, while the bulk of past theoretical work on the topic assumed the use of the so-called non-zero representation, here we prove that if an additive representation can be used instead then BNSL becomes fixed-parameter tractable even under significantly milder restrictions to the superstructure, notably when parameterized by the treewidth alone. Last but not least, we show how our results can be extended to the closely related problem of Polytree Learning.",
    "authors": [
      "Ganian, Robert",
      "Korchemna, Viktoriia"
    ]
  },
  {
    "id": "040ca38cefb1d9226d79c05dd25469cb",
    "title": "Fast Tucker Rank Reduction for Non-Negative Tensors Using Mean-Field Approximation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/040ca38cefb1d9226d79c05dd25469cb-Paper.pdf",
    "abstract": "We present an efficient low-rank approximation algorithm for non-negative tensors. The algorithm is derived from our two findings: First,  we show that rank-1 approximation for tensors can be viewed as a mean-field approximation by treating each tensor as a probability distribution. Second, we theoretically provide a sufficient condition for distribution parameters to reduce Tucker ranks of tensors; interestingly, this sufficient condition can be achieved by iterative application of the mean-field approximation. Since the mean-field approximation is always given as a closed formula, our findings lead to a fast low-rank approximation algorithm without using a gradient method. We empirically demonstrate that our algorithm is faster than the existing non-negative Tucker rank reduction methods and achieves competitive or better approximation of given tensors.",
    "authors": [
      "Ghalamkari, Kazu",
      "Sugiyama, Mahito"
    ]
  },
  {
    "id": "0415740eaa4d9decbc8da001d3fd805f",
    "title": "Learning Stochastic Majority Votes by Minimizing a PAC-Bayes Generalization Bound",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0415740eaa4d9decbc8da001d3fd805f-Paper.pdf",
    "abstract": "We investigate a stochastic counterpart of majority votes over finite ensembles of classifiers, and study its generalization properties. While our approach holds for arbitrary distributions, we instantiate it with Dirichlet distributions: this allows for a closed-form and differentiable expression for the expected risk, which then turns the generalization bound into a tractable training objective.The resulting stochastic majority vote learning algorithm achieves state-of-the-art accuracy and benefits from (non-vacuous) tight generalization bounds, in a series of numerical experiments when compared to competing algorithms which also minimize PAC-Bayes objectives -- both with uninformed (data-independent) and informed (data-dependent) priors.",
    "authors": [
      "Zantedeschi, Valentina",
      "Viallard, Paul",
      "Morvant, Emilie",
      "Emonet, R\u00e9mi",
      "Habrard, Amaury",
      "Germain, Pascal",
      "Guedj, Benjamin"
    ]
  },
  {
    "id": "043ab21fc5a1607b381ac3896176dac6",
    "title": "Numerical influence of ReLU\u2019(0) on backpropagation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/043ab21fc5a1607b381ac3896176dac6-Paper.pdf",
    "abstract": "In theory, the choice of ReLU(0) in [0, 1] for a neural network has a negligible influence both on backpropagation and training. Yet, in the real world, 32 bits default precision combined with the size of deep learning problems makes it a hyperparameter of training methods. We investigate the importance of the value of ReLU'(0) for several precision levels (16, 32, 64 bits), on various networks (fully connected, VGG, ResNet) and datasets (MNIST, CIFAR10, SVHN, ImageNet). We observe considerable variations of backpropagation outputs which occur around half of the time in 32 bits precision. The effect disappears with double precision, while it is systematic at 16 bits. For vanilla SGD training, the choice ReLU'(0) = 0 seems to be the most efficient. For our experiments on ImageNet the gain in test accuracy over ReLU'(0) = 1 was more than 10 points (two runs). We also evidence that reconditioning approaches as batch-norm or ADAM tend to buffer the influence of ReLU'(0)\u2019s value. Overall, the message we convey is that algorithmic differentiation of nonsmooth problems potentially hides parameters that could be tuned advantageously.",
    "authors": [
      "Bertoin, David",
      "Bolte, J\u00e9r\u00f4me",
      "Gerchinovitz, S\u00e9bastien",
      "Pauwels, Edouard"
    ]
  },
  {
    "id": "0496604c1d80f66fbeb963c12e570a26",
    "title": "A Contrastive Learning Approach for Training Variational Autoencoder Priors",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0496604c1d80f66fbeb963c12e570a26-Paper.pdf",
    "abstract": "Variational autoencoders (VAEs) are one of the powerful likelihood-based generative models with applications in many domains. However, they struggle to generate high-quality images, especially when samples are obtained from the prior without any tempering. One explanation for VAEs' poor generative quality is the prior hole problem: the prior distribution fails to match the aggregate approximate posterior. Due to this mismatch, there exist areas in the latent space with high density under the prior that do not correspond to any encoded image. Samples from those areas are decoded to corrupted images. To tackle this issue, we propose an energy-based prior defined by the product of a base prior distribution and a reweighting factor, designed to bring the base closer to the aggregate posterior. We train the reweighting factor by noise contrastive estimation, and we generalize it to hierarchical VAEs with many latent variable groups. Our experiments confirm that the proposed noise contrastive priors improve the generative performance of state-of-the-art VAEs by a large margin on the MNIST, CIFAR-10, CelebA 64, and CelebA HQ 256 datasets. Our method is simple and can be applied to a wide variety of VAEs to improve the expressivity of their prior distribution.",
    "authors": [
      "Aneja, Jyoti",
      "Schwing, Alex",
      "Kautz, Jan",
      "Vahdat, Arash"
    ]
  },
  {
    "id": "04a1bf2d968f1ce381cf1f9184a807a9",
    "title": "What training reveals about neural network complexity",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/04a1bf2d968f1ce381cf1f9184a807a9-Paper.pdf",
    "abstract": "This work explores the Benevolent Training Hypothesis (BTH) which argues that the complexity of the function a deep neural network (NN) is learning can be deduced by its training dynamics. Our analysis provides evidence for BTH by relating the NN's Lipschitz constant at different regions of the input space with the behavior of the stochastic training procedure.  We first observe that the Lipschitz constant close to the training data affects various aspects of the parameter trajectory, with more complex networks having a longer trajectory, bigger variance, and often veering further from their initialization. We then show that NNs whose 1st layer bias is trained more steadily (i.e., slowly and with little variation) have bounded complexity even in regions of the input space that are far from any training point. Finally, we find that steady training with Dropout implies a training- and data-dependent generalization bound that grows poly-logarithmically with the number of parameters. Overall, our results support the intuition that good training behavior can be a useful bias towards good generalization.",
    "authors": [
      "Loukas, Andreas",
      "Poiitis, Marinos",
      "Jegelka, Stefanie"
    ]
  },
  {
    "id": "04da4aea8e38ac933ab23cb2389dddef",
    "title": "Class-agnostic Reconstruction of Dynamic Objects from Videos",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/04da4aea8e38ac933ab23cb2389dddef-Paper.pdf",
    "abstract": "We introduce REDO, a class-agnostic framework to REconstruct the Dynamic Objects from RGBD or calibrated videos. Compared to prior work, our problem setting is more realistic yet more challenging for three reasons: 1) due to occlusion or camera settings an object of interest may never be entirely visible, but we aim to reconstruct the complete shape; 2) we aim to handle different object dynamics including rigid motion, non-rigid motion, and articulation; 3) we aim to reconstruct different  categories  of  objects  with  one  unified  framework. To  address  these challenges, we develop two novel modules.  First, we introduce a canonical 4D implicit function which is pixel-aligned with aggregated temporal visual cues. Second, we develop a 4D transformation module which captures object dynamics to support temporal propagation and aggregation. We study the efficacy of REDO in extensive experiments on synthetic RGBD video datasets SAIL-VOS 3D and DeformingThings4D++,  and on real-world video data 3DPW. We find REDO outperforms state-of-the-art dynamic reconstruction methods by a margin. In ablation studies we validate each developed component.",
    "authors": [
      "Ren, Zhongzheng",
      "Zhao, Xiaoming",
      "Schwing, Alex"
    ]
  },
  {
    "id": "051928341be67dcba03f0e04104d9047",
    "title": "Unique sparse decomposition of low rank matrices",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/051928341be67dcba03f0e04104d9047-Paper.pdf",
    "abstract": " The problem of finding the unique low dimensional decomposition of a given matrix has been a fundamental and recurrent problem in many areas. In this paper, we study the problem of seeking a unique decomposition of a low-rank matrix $Y\\in \\mathbb{R}^{p\\times n}$ that admits a sparse representation. Specifically, we consider $ Y =  AX\\in  \\mathbb{R}^{p\\times n}$ where the matrix $A\\in  \\mathbb{R}^{p\\times r}$ has full column rank, with $r < \\min\\{n,p\\}$, and the matrix $X\\in  \\mathbb{R}^{r\\times n}$ is element-wise sparse.  We prove that this sparse decomposition of $Y$ can be uniquely identified by recovering ground-truth $A$ column by column, up to some intrinsic signed permutation. Our approach relies on solving a nonconvex optimization problem constrained over the unit sphere. Our geometric analysis for the nonconvex optimization landscape shows that any {\\em strict} local solution is close to the ground truth solution, and can be recovered by a simple data-driven initialization followed with any second-order descent algorithm. At last, we corroborate these theoretical results with numerical experiments",
    "authors": [
      "Jin, Dian",
      "Bing, Xin",
      "Zhang, Yuqian"
    ]
  },
  {
    "id": "05311655a15b75fab86956663e1819cd",
    "title": "Neighborhood Reconstructing Autoencoders",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/05311655a15b75fab86956663e1819cd-Paper.pdf",
    "abstract": "Vanilla autoencoders often produce manifolds that overfit to noisy training data, or have the wrong local connectivity and geometry. Autoencoder regularization techniques, e.g., the denoising autoencoder, have had some success in reducing overfitting, whereas recent graph-based methods that exploit local connectivity information provided by neighborhood graphs have had some success in mitigating local connectivity errors. Neither of these two approaches satisfactorily reduce both overfitting and connectivity errors; moreover, graph-based methods typically involve considerable preprocessing and tuning. To simultaneously address the two issues of overfitting and local connectivity, we propose a new graph-based autoencoder, the Neighborhood Reconstructing Autoencoder (NRAE). Unlike existing graph-based methods that attempt to encode the training data to some prescribed latent space distribution -- one consequence being that only the encoder is the object of the regularization -- NRAE merges local connectivity information contained in the neighborhood graphs with local quadratic approximations of the decoder function to formulate a new neighborhood reconstruction loss. Compared to existing graph-based methods, our new loss function is simple and easy to implement, and the resulting algorithm is scalable and computationally efficient; the only required preprocessing step is the construction of the neighborhood graph. Extensive experiments with standard datasets demonstrate that, compared to existing methods, NRAE improves both overfitting and local connectivity in the learned manifold, in some cases by significant margins. Code for NRAE is available at https://github.com/Gabe-YHLee/NRAE-public.",
    "authors": [
      "LEE, Yonghyeon",
      "Kwon, Hyeokjun",
      "Park, Frank"
    ]
  },
  {
    "id": "0537fb40a68c18da59a35c2bfe1ca554",
    "title": "TopicNet: Semantic Graph-Guided Topic Discovery",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0537fb40a68c18da59a35c2bfe1ca554-Paper.pdf",
    "abstract": "Existing deep hierarchical topic models are able to extract semantically meaningful topics from a text corpus  in an unsupervised manner and automatically organize them into a topic hierarchy.  However, it is unclear how to incorporate prior belief such as knowledge graph to guide the learning of the topic hierarchy. To address this issue, we introduce TopicNet as a deep hierarchical topic model that can inject prior structural knowledge as inductive bias to influence the learning. TopicNet represents each topic as a Gaussian-distributed embedding vector, projects the topics of all layers into a shared embedding space, and explores both the symmetric and asymmetric similarities between Gaussian embedding vectors to incorporate prior semantic hierarchies. With a variational auto-encoding inference network,  the model parameters are optimized by minimizing the evidence lower bound and supervised loss via stochastic gradient descent. Experiments on widely used benchmark show that TopicNet outperforms related deep topic models on discovering deeper interpretable topics and mining better document representations. ",
    "authors": [
      "Duan, Zhibin",
      "Xu, Yi.shi",
      "Chen, Bo",
      "wang, dongsheng",
      "Wang, Chaojie",
      "Zhou, Mingyuan"
    ]
  },
  {
    "id": "054ab897023645cd7ad69525c46992a0",
    "title": "(Almost) Free Incentivized Exploration from Decentralized Learning Agents",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/054ab897023645cd7ad69525c46992a0-Paper.pdf",
    "abstract": "Incentivized exploration in multi-armed bandits (MAB) has witnessed increasing interests and many progresses in recent years, where a principal offers bonuses to agents to do explorations on her behalf. However, almost all existing studies are confined to temporary myopic agents. In this work, we break this barrier and study incentivized exploration with multiple and long-term strategic agents, who have more complicated behaviors that often appear in real-world applications. An important observation of this work is that strategic agents' intrinsic needs of learning benefit (instead of harming) the principal's explorations by providing \"free pulls\". Moreover, it turns out that increasing the population of agents significantly lowers the principal's burden of incentivizing. The key and somewhat surprising insight revealed from our results is that when there are sufficiently many learning agents involved, the exploration process of the principal can be (almost) free. Our main results are built upon three novel components which may be of independent interest: (1) a simple yet provably effective incentive-provision strategy; (2) a carefully crafted best arm identification algorithm for rewards aggregated under unequal confidences; (3) a high-probability finite-time lower bound of UCB algorithms. Experimental results are provided to complement the theoretical analysis.",
    "authors": [
      "Shi, Chengshuai",
      "Xu, Haifeng",
      "Xiong, Wei",
      "Shen, Cong"
    ]
  },
  {
    "id": "05546b0e38ab9175cd905eebcc6ebb76",
    "title": "Combining Recurrent, Convolutional, and Continuous-time Models with Linear State Space Layers",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/05546b0e38ab9175cd905eebcc6ebb76-Paper.pdf",
    "abstract": "Recurrent neural networks (RNNs), temporal convolutions, and neural differential equations (NDEs) are popular families of deep learning models for time-series data, each with unique strengths and tradeoffs in modeling power and computational efficiency.  We introduce a simple sequence model inspired by control systems that generalizes these approaches while addressing their shortcomings.  The Linear State-Space Layer (LSSL) maps a sequence $u \\mapsto y$ by simply simulating a linear continuous-time state-space representation $\\dot{x} = Ax + Bu, y = Cx + Du$.  Theoretically, we show that LSSL models are closely related to the three aforementioned families of models and inherit their strengths.  For example, they generalize convolutions to continuous-time, explain common RNN heuristics, and share features of NDEs such as time-scale adaptation.  We then incorporate and generalize recent theory on continuous-time memorization to introduce a trainable subset of structured matrices $A$ that endow LSSLs with long-range memory.  Empirically, stacking LSSL layers into a simple deep neural network obtains state-of-the-art results across time series benchmarks for long dependencies in sequential image classification, real-world healthcare regression tasks, and speech.  On a difficult speech classification task with length-16000 sequences, LSSL outperforms prior approaches by 24 accuracy points, and even outperforms baselines that use hand-crafted features on 100x shorter sequences.",
    "authors": [
      "Gu, Albert",
      "Johnson, Isys",
      "Goel, Karan",
      "Saab, Khaled",
      "Dao, Tri",
      "Rudra, Atri",
      "R\u00e9, Christopher"
    ]
  },
  {
    "id": "055e31fa43e652cb4ab6c0ee845c8d36",
    "title": "Revisiting Hilbert-Schmidt Information Bottleneck for Adversarial Robustness",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/055e31fa43e652cb4ab6c0ee845c8d36-Paper.pdf",
    "abstract": "We investigate the HSIC (Hilbert-Schmidt independence criterion) bottleneck as a regularizer for learning an adversarially robust deep neural network classifier. In addition to the usual cross-entropy loss, we add regularization terms for every intermediate layer to ensure that the latent representations retain useful information for output prediction while reducing redundant information. We show that the HSIC bottleneck enhances robustness to adversarial attacks both theoretically and experimentally. In particular, we prove that the HSIC bottleneck regularizer reduces the sensitivity of the classifier to adversarial examples. Our experiments on multiple benchmark datasets and architectures demonstrate that incorporating an HSIC bottleneck regularizer attains competitive natural accuracy and improves adversarial robustness, both with and without adversarial examples during training. Our code and adversarially robust models are publicly available.",
    "authors": [
      "Wang, Zifeng",
      "Jian, Tong",
      "Masoomi, Aria",
      "Ioannidis, Stratis",
      "Dy, Jennifer"
    ]
  },
  {
    "id": "05a70454516ecd9194c293b0e415777f",
    "title": "T-LoHo: A Bayesian Regularization Model for Structured Sparsity and Smoothness on Graphs",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/05a70454516ecd9194c293b0e415777f-Paper.pdf",
    "abstract": "Graphs have been commonly used to represent complex data structures. In models dealing with graph-structured data, multivariate parameters may not only exhibit sparse patterns but have structured sparsity and smoothness in the sense that both zero and non-zero parameters tend to cluster together. We propose a new prior for high-dimensional parameters with graphical relations, referred to as the Tree-based Low-rank Horseshoe (T-LoHo) model, that generalizes the popular univariate Bayesian horseshoe shrinkage prior to the multivariate setting to detect structured sparsity and smoothness simultaneously. The T-LoHo prior can be embedded in many high-dimensional hierarchical models. To illustrate its utility, we apply it to regularize a Bayesian high-dimensional regression problem where the regression coefficients are linked by a graph, so that the resulting clusters have flexible shapes and satisfy the cluster contiguity constraint with respect to the graph. We design an efficient Markov chain Monte Carlo algorithm that delivers full Bayesian inference with uncertainty measures for model parameters such as the number of clusters. We offer theoretical investigations of the clustering effects and posterior concentration results. Finally, we illustrate the performance of the model with simulation studies and a real data application for anomaly detection on a road network. The results indicate substantial improvements over other competing methods such as the sparse fused lasso.",
    "authors": [
      "Lee, Changwoo",
      "Luo, Zhao Tang",
      "Sang, Huiyan"
    ]
  },
  {
    "id": "05d74c48b5b30514d8e9bd60320fc8f6",
    "title": "The Utility of Explainable AI in Ad Hoc Human-Machine Teaming",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/05d74c48b5b30514d8e9bd60320fc8f6-Paper.pdf",
    "abstract": "Recent advances in machine learning have led to growing interest in Explainable AI (xAI) to enable humans to gain insight into the decision-making of machine learning models. Despite this recent interest, the utility of xAI techniques has not yet been characterized in human-machine teaming. Importantly, xAI offers the promise of enhancing team situational awareness (SA) and shared mental model development, which are the key characteristics of effective human-machine teams. Rapidly developing such mental models is especially critical in ad hoc human-machine teaming, where agents do not have a priori knowledge of others' decision-making strategies. In this paper, we present two novel human-subject experiments quantifying the benefits of deploying xAI techniques within a human-machine teaming scenario. First, we show that xAI techniques can support SA ($p<0.05)$. Second, we examine how different SA levels induced via a collaborative AI policy abstraction affect ad hoc human-machine teaming performance. Importantly, we find that the benefits of xAI are not universal, as there is a strong dependence on the composition of the human-machine team. Novices benefit from xAI providing increased SA ($p<0.05$) but are susceptible to cognitive overhead ($p<0.05$). On the other hand, expert performance degrades with the addition of xAI-based support ($p<0.05$), indicating that the cost of paying attention to the xAI outweighs the benefits obtained from being provided additional information to enhance SA. Our results demonstrate that researchers must deliberately design and deploy the right xAI techniques in the right scenario by carefully considering human-machine team composition and how the xAI method augments SA.",
    "authors": [
      "Paleja, Rohan",
      "Ghuy, Muyleng",
      "Ranawaka Arachchige, Nadun",
      "Jensen, Reed",
      "Gombolay, Matthew"
    ]
  },
  {
    "id": "05d8cccb5f47e5072f0a05b5f514941a",
    "title": "Subgoal Search For Complex Reasoning Tasks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/05d8cccb5f47e5072f0a05b5f514941a-Paper.pdf",
    "abstract": "Humans excel in solving complex reasoning tasks through a mental process of moving from one idea to a related one. Inspired by this, we propose Subgoal Search (kSubS) method. Its key component is a learned subgoal generator that produces a diversity of subgoals that are both achievable and closer to the solution. Using subgoals reduces the search space and induces a high-level search graph suitable for efficient planning. In this paper, we implement kSubS using a transformer-based subgoal module coupled with the classical best-first search framework. We show that a simple approach of generating $k$-th step ahead subgoals is surprisingly efficient on three challenging domains: two popular puzzle games, Sokoban and the Rubik's Cube, and an inequality proving benchmark INT. kSubS achieves strong results including state-of-the-art on INT within a modest computational budget.",
    "authors": [
      "Czechowski, Konrad",
      "Odrzyg\u00f3\u017ad\u017a, Tomasz",
      "Zbysi\u0144ski, Marek",
      "Zawalski, Micha\u0142",
      "Olejnik, Krzysztof",
      "Wu, Yuhuai",
      "Kuci\u0144ski, \u0141ukasz",
      "Mi\u0142o\u015b, Piotr"
    ]
  },
  {
    "id": "05f971b5ec196b8c65b75d2ef8267331",
    "title": "MCMC Variational Inference via Uncorrected Hamiltonian Annealing",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/05f971b5ec196b8c65b75d2ef8267331-Paper.pdf",
    "abstract": "Given an unnormalized target distribution we want to obtain approximate samples from it and a tight lower bound on its (log) normalization constant log Z. Annealed Importance Sampling (AIS) with Hamiltonian MCMC is a powerful method that can be used to do this. Its main drawback is that it uses non-differentiable transition kernels, which makes tuning its many parameters hard. We propose a framework to use an AIS-like procedure with Uncorrected Hamiltonian MCMC, called Uncorrected Hamiltonian Annealing. Our method leads to tight and differentiable lower bounds on log Z. We show empirically that our method yields better performances than other competing approaches, and that the ability to tune its parameters using reparameterization gradients may lead to large performance improvements.",
    "authors": [
      "Geffner, Tomas",
      "Domke, Justin"
    ]
  },
  {
    "id": "0602940f23884f782058efac46f64b0f",
    "title": "Landmark-RxR: Solving Vision-and-Language Navigation with Fine-Grained Alignment Supervision",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0602940f23884f782058efac46f64b0f-Paper.pdf",
    "abstract": "In Vision-and-Language Navigation (VLN) task, an agent is asked to navigate inside 3D indoor environments following given instructions. Cross-modal alignment is one of the most critical challenges in VLN because the predicted trajectory needs to match the given instruction accurately. In this paper, we address the cross-modal alignment challenge from the perspective of fine-grain. Firstly, to alleviate weak cross-modal alignment supervision from coarse-grained data, we introduce a human-annotated fine-grained VLN dataset, namely Landmark-RxR. Secondly, to further enhance local cross-modal alignment under fine-grained supervision, we investigate the focal-oriented rewards with soft and hard forms, by focusing on the critical points sampled from fine-grained Landmark-RxR. Moreover, to fully evaluate the navigation process, we also propose a re-initialization mechanism that makes metrics insensitive to difficult points, which can cause the agent to deviate from the correct trajectories. Experimental results show that our agent has superior navigation performance on Landmark-RxR, en-RxR and R2R. Our dataset and code are available at https://github.com/hekj/Landmark-RxR.",
    "authors": [
      "He, Keji",
      "Huang, Yan",
      "Wu, Qi",
      "Yang, Jianhua",
      "An, Dong",
      "Sima, Shuanglin",
      "Wang, Liang"
    ]
  },
  {
    "id": "0607f4c705595b911a4f3e7a127b44e0",
    "title": "A Winning Hand: Compressing Deep Networks Can Improve Out-of-Distribution Robustness",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0607f4c705595b911a4f3e7a127b44e0-Paper.pdf",
    "abstract": "Successful adoption of deep learning (DL) in the wild requires models to be: (1) compact, (2) accurate, and (3) robust to distributional shifts. Unfortunately, efforts towards simultaneously meeting these requirements have mostly been unsuccessful. This raises an important question: Is the inability to create Compact, Accurate, and Robust Deep neural networks (CARDs) fundamental? To answer this question, we perform a large-scale analysis of popular model compression techniques which uncovers several intriguing patterns. Notably, in contrast to traditional pruning approaches (e.g., fine tuning and gradual magnitude pruning), we find that ``lottery ticket-style'' approaches can surprisingly be used to produce CARDs, including binary-weight CARDs. Specifically, we are able to create extremely compact CARDs that, compared to their larger counterparts, have similar test accuracy and matching (or better) robustness---simply by pruning and (optionally) quantizing. Leveraging the compactness of CARDs, we develop a simple domain-adaptive test-time ensembling approach (CARD-Decks) that uses a gating module to dynamically select appropriate CARDs from the CARD-Deck based on their spectral-similarity with test samples. The proposed approach builds a \"winning hand'' of CARDs that establishes a new state-of-the-art (on RobustBench) on CIFAR-10-C accuracies (i.e., 96.8% standard and 92.75% robust) and CIFAR-100-C accuracies (80.6% standard and 71.3% robust) with better memory usage than non-compressed baselines (pretrained CARDs and CARD-Decks available at https://github.com/RobustBench/robustbench). Finally, we provide theoretical support for our empirical findings. ",
    "authors": [
      "Diffenderfer, James",
      "Bartoldson, Brian",
      "Chaganti, Shreya",
      "Zhang, Jize",
      "Kailkhura, Bhavya"
    ]
  },
  {
    "id": "063e26c670d07bb7c4d30e6fc69fe056",
    "title": "On the Importance of Gradients for Detecting Distributional Shifts in the Wild",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/063e26c670d07bb7c4d30e6fc69fe056-Paper.pdf",
    "abstract": "Detecting out-of-distribution (OOD) data has become a critical component in ensuring the safe deployment of machine learning models in the real world. Existing OOD detection approaches primarily rely on the output or feature space for deriving OOD scores, while largely overlooking information from the gradient space. In this paper, we present GradNorm, a simple and effective approach for detecting OOD inputs by utilizing information extracted from the gradient space. GradNorm directly employs the vector norm of gradients, backpropagated from the KL divergence between the softmax output and a uniform probability distribution. Our key idea is that the magnitude of gradients is higher for in-distribution (ID) data than that for OOD data, making it informative for OOD detection. GradNorm demonstrates superior performance, reducing the average FPR95 by up to 16.33% compared to the previous best method.",
    "authors": [
      "Huang, Rui",
      "Geng, Andrew",
      "Li, Yixuan"
    ]
  },
  {
    "id": "0678c572b0d5597d2d4a6b5bd135754c",
    "title": "Iterative Methods for Private Synthetic Data: Unifying Framework and New Methods",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0678c572b0d5597d2d4a6b5bd135754c-Paper.pdf",
    "abstract": "We study private synthetic data generation for query release, where the goal is to construct a sanitized version of a sensitive dataset, subject to differential privacy, that approximately preserves the answers to a large collection of statistical queries. We first present an algorithmic framework that unifies a long line of iterative algorithms in the literature. Under this framework, we propose two new methods. The first method, private entropy projection (PEP), can be viewed as an advanced variant of MWEM that adaptively reuses past query measurements to boost accuracy. Our second method, generative networks with the exponential mechanism (GEM), circumvents computational bottlenecks in algorithms such as MWEM and PEP by optimizing over generative models parameterized by neural networks, which capture a rich family of distributions while enabling fast gradient-based optimization. We demonstrate that PEP and GEM empirically outperform existing algorithms. Furthermore, we show that GEM nicely incorporates prior information from public data while overcoming limitations of PMW^Pub, the existing state-of-the-art method that also leverages public data.",
    "authors": [
      "Liu, Terrance",
      "Vietri, Giuseppe",
      "Wu, Steven Z."
    ]
  },
  {
    "id": "067a26d87265ea39030f5bd82408ce7c",
    "title": "Understanding End-to-End Model-Based Reinforcement Learning Methods as Implicit Parameterization",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/067a26d87265ea39030f5bd82408ce7c-Paper.pdf",
    "abstract": "Estimating the per-state expected cumulative rewards is a critical aspect of reinforcement learning approaches, however the experience is obtained, but standard deep neural-network function-approximation methods are often inefficient in this setting. An alternative approach, exemplified by value iteration networks, is to learn transition and reward models of a latent Markov decision process whose value predictions fit the data. This approach has been shown empirically to converge faster to a more robust solution in many cases, but there has been little theoretical study of this phenomenon. In this paper, we explore such implicit representations of value functions via theory and focused experimentation. We prove that, for a linear parametrization, gradient descent converges to global optima despite non-linearity and non-convexity introduced by the implicit representation. Furthermore, we derive convergence rates for both cases which allow us to identify conditions under which stochastic gradient descent (SGD) with this implicit representation converges substantially faster than its explicit counterpart. Finally, we provide empirical results in some simple domains that illustrate the theoretical findings.",
    "authors": [
      "Gehring, Clement",
      "Kawaguchi, Kenji",
      "Huang, Jiaoyang",
      "Kaelbling, Leslie"
    ]
  },
  {
    "id": "069090145d54bf4aa3894133f7e89873",
    "title": "Mirror Langevin Monte Carlo: the Case Under Isoperimetry",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/069090145d54bf4aa3894133f7e89873-Paper.pdf",
    "abstract": "Motivated by the connection between sampling and optimization, we study a mirror descent analogue of Langevin dynamics and analyze three different discretization schemes, giving nonasymptotic convergence rate under functional inequalities such as Log-Sobolev in the corresponding metric. Compared to the Euclidean setting, the result reveals intricate relationship between the underlying geometry and the target distribution and suggests that care might need to be taken in order for the discretized algorithm to achieve vanishing bias with diminishing stepsize for sampling from potentials under weaker smoothness/convexity regularity conditions. ",
    "authors": [
      "Jiang, Qijia"
    ]
  },
  {
    "id": "06997f04a7db92466a2baa6ebc8b872d",
    "title": "Do Different Tracking Tasks Require Different Appearance Models?",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/06997f04a7db92466a2baa6ebc8b872d-Paper.pdf",
    "abstract": "Tracking objects of interest in a video is one of the most popular and widely applicable problems in computer vision. However, with the years, a Cambrian explosion of use cases and benchmarks has fragmented the problem in a multitude of different experimental setups. As a consequence, the literature has fragmented too, and now novel approaches proposed by the community are usually specialised to fit only one specific setup. To understand to what extent this specialisation is necessary, in this work we present UniTrack, a solution to address five different tasks within the same framework. UniTrack consists of a single and task-agnostic appearance model, which can be learned in a supervised or self-supervised fashion, and multiple ``heads'' that address individual tasks and do not require training. We show how most tracking tasks can be solved within this framework, and that the same appearance model can be successfully used to obtain results that are competitive against specialised methods for most of the tasks considered. The framework also allows us to analyse appearance models obtained with the most recent self-supervised methods, thus extending their evaluation and comparison to a larger variety of important problems. ",
    "authors": [
      "Wang, Zhongdao",
      "Zhao, Hengshuang",
      "Li, Ya-Li",
      "Wang, Shengjin",
      "Torr, Philip",
      "Bertinetto, Luca"
    ]
  },
  {
    "id": "06a9d51e04213572ef0720dd27a84792",
    "title": "Towards robust vision by multi-task learning on monkey visual cortex",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/06a9d51e04213572ef0720dd27a84792-Paper.pdf",
    "abstract": "Deep neural networks set the state-of-the-art across many tasks in computer vision, but their generalization ability to simple image distortions is surprisingly fragile. In contrast, the mammalian visual system is robust to a wide range of perturbations. Recent work suggests that this generalization ability can be explained by useful inductive biases encoded in the representations of visual stimuli throughout the visual cortex. Here, we successfully leveraged these inductive biases with a multi-task learning approach: we jointly trained a deep network to perform image classification and to predict neural activity in macaque primary visual cortex (V1) in response to the same natural stimuli. We measured the out-of-distribution generalization abilities of our resulting network by testing its robustness to common image distortions. We found that co-training on monkey V1 data indeed leads to increased robustness despite the absence of those distortions during training. Additionally, we showed that our network's robustness is often very close to that of an Oracle network where parts of the architecture are directly trained on noisy images. Our results also demonstrated that the network's representations become more brain-like as their robustness improves. Using a novel constrained reconstruction analysis, we investigated what makes our brain-regularized network more robust. We found that our monkey co-trained network is more sensitive to content than noise when compared to a Baseline network that we trained for image classification alone. Using DeepGaze-predicted saliency maps for ImageNet images, we found that the monkey co-trained network tends to be more sensitive to salient regions in a scene, reminiscent of existing theories on the role of V1 in the detection of object borders and bottom-up saliency. Overall, our work expands the promising research avenue of transferring inductive biases from biological to artificial neural networks on the representational level, and provides a novel analysis of the effects of our transfer.",
    "authors": [
      "Safarani, Shahd",
      "Nix, Arne",
      "Willeke, Konstantin",
      "Cadena, Santiago",
      "Restivo, Kelli",
      "Denfield, George",
      "Tolias, Andreas",
      "Sinz, Fabian"
    ]
  },
  {
    "id": "06c284d3f757b15c02f47f3ff06dc275",
    "title": "Arbitrary Conditional Distributions with Energy",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/06c284d3f757b15c02f47f3ff06dc275-Paper.pdf",
    "abstract": "Modeling distributions of covariates, or density estimation, is a core challenge in unsupervised learning. However, the majority of work only considers the joint distribution, which has limited relevance to practical situations. A more general and useful problem is arbitrary conditional density estimation, which aims to model any possible conditional distribution over a set of covariates, reflecting the more realistic setting of inference based on prior knowledge. We propose a novel method, Arbitrary Conditioning with Energy (ACE), that can simultaneously estimate the distribution $p(\\mathbf{x}_u \\mid \\mathbf{x}_o)$ for all possible subsets of unobserved features $\\mathbf{x}_u$ and observed features $\\mathbf{x}_o$. ACE is designed to avoid unnecessary bias and complexity --- we specify densities with a highly expressive energy function and reduce the problem to only learning one-dimensional conditionals (from which more complex distributions can be recovered during inference). This results in an approach that is both simpler and higher-performing than prior methods. We show that ACE achieves state-of-the-art for arbitrary conditional likelihood estimation and data imputation on standard benchmarks.",
    "authors": [
      "Strauss, Ryan",
      "Oliva, Junier B."
    ]
  },
  {
    "id": "06d172404821f7d01060cc9629171b2e",
    "title": "Learning Domain Invariant Representations in Goal-conditioned Block MDPs",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/06d172404821f7d01060cc9629171b2e-Paper.pdf",
    "abstract": "Deep Reinforcement Learning (RL) is successful in solving many complex Markov Decision Processes (MDPs) problems. However, agents often face unanticipated environmental changes after deployment in the real world. These changes are often spurious and unrelated to the underlying problem, such as background shifts for visual input agents. Unfortunately, deep RL policies are usually sensitive to these changes and fail to act robustly against them. This resembles the problem of domain generalization in supervised learning. In this work, we study this problem for goal-conditioned RL agents. We propose a theoretical framework in the Block MDP setting that characterizes the generalizability of goal-conditioned policies to new environments. Under this framework, we develop a practical method PA-SkewFit that enhances domain generalization. The empirical evaluation shows that our goal-conditioned RL agent can perform well in various unseen test environments, improving by 50\\% over baselines.",
    "authors": [
      "Han, Beining",
      "Zheng, Chongyi",
      "Chan, Harris",
      "Paster, Keiran",
      "Zhang, Michael",
      "Ba, Jimmy"
    ]
  },
  {
    "id": "06d5ae105ea1bea4d800bc96491876e9",
    "title": "Near-Optimal Multi-Perturbation Experimental Design for Causal Structure Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/06d5ae105ea1bea4d800bc96491876e9-Paper.pdf",
    "abstract": "Causal structure learning is a key problem in many domains. Causal structures can be learnt by performing experiments on the system of interest. We address the largely unexplored problem of designing a batch of experiments that each simultaneously intervene on multiple variables. While potentially more informative than the commonly considered single-variable interventions, selecting such interventions is algorithmically much more challenging, due to the doubly-exponential combinatorial search space over sets of composite interventions. In this paper, we develop efficient algorithms for optimizing different objective functions quantifying the informativeness of a budget-constrained batch of experiments. By establishing novel submodularity properties of these objectives, we provide approximation guarantees for our algorithms. Our algorithms empirically perform superior to both random interventions and algorithms that only select single-variable interventions. ",
    "authors": [
      "Sussex, Scott",
      "Uhler, Caroline",
      "Krause, Andreas"
    ]
  },
  {
    "id": "06f2e099b4f87109d52e15d7c05f0084",
    "title": "Fuzzy Clustering with Similarity Queries",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/06f2e099b4f87109d52e15d7c05f0084-Paper.pdf",
    "abstract": "The fuzzy or soft $k$-means objective is a popular generalization of the well-known $k$-means problem, extending the clustering capability of the $k$-means to datasets that are uncertain, vague and otherwise hard to cluster. In this paper, we propose a semi-supervised active clustering framework, where the learner is allowed to interact with an oracle (domain expert), asking for the similarity between a certain set of chosen items. We study the query and computational complexities of clustering in this framework. We prove that having a few of such similarity queries enables one to get a polynomial-time approximation algorithm to an otherwise conjecturally NP-hard problem. In particular, we provide algorithms for fuzzy clustering in this setting that ask $O(\\mathsf{poly}(k)\\log n)$ similarity queries and run with polynomial-time-complexity, where $n$ is the number of items. The fuzzy $k$-means objective is nonconvex, with $k$-means as a special case, and is equivalent to some other generic nonconvex problem such as non-negative matrix factorization. The ubiquitous Lloyd-type algorithms (or alternating-minimization algorithms) can get stuck at a local minima. Our results show that by making few similarity queries, the problem becomes easier to solve. Finally, we test our algorithms over real-world datasets, showing their effectiveness in real-world applications. ",
    "authors": [
      "Huleihel, Wasim",
      "Mazumdar, Arya",
      "Pal, Soumyabrata"
    ]
  },
  {
    "id": "06fe1c234519f6812fc4c1baae25d6af",
    "title": "Improving black-box optimization in VAE latent space using decoder uncertainty",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/06fe1c234519f6812fc4c1baae25d6af-Paper.pdf",
    "abstract": "Optimization in the latent space of variational autoencoders is a promising approach to generate high-dimensional discrete objects that maximize an expensive black-box property (e.g., drug-likeness in molecular generation, function approximation with arithmetic expressions). However, existing methods lack robustness as they may decide to explore areas of the latent space for which no data was available during training and where the decoder can be unreliable, leading to the generation of unrealistic or invalid objects. We propose to leverage the epistemic uncertainty of the decoder to guide the optimization process. This is not trivial though, as a naive estimation of uncertainty in the high-dimensional and structured settings we consider would result in high estimator variance. To solve this problem, we introduce an importance sampling-based estimator that provides more robust estimates of epistemic uncertainty. Our uncertainty-guided optimization approach does not require modifications of the model architecture nor the training process. It produces samples with a better trade-off between black-box objective and validity of the generated samples, sometimes improving both simultaneously. We illustrate these advantages across several experimental settings in digit generation, arithmetic expression approximation and molecule generation for drug design.",
    "authors": [
      "Notin, Pascal",
      "Hern\u00e1ndez-Lobato, Jos\u00e9 Miguel",
      "Gal, Yarin"
    ]
  },
  {
    "id": "07563a3fe3bbe7e3ba84431ad9d055af",
    "title": "Sample Selection for Fair and Robust Training",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/07563a3fe3bbe7e3ba84431ad9d055af-Paper.pdf",
    "abstract": "Fairness and robustness are critical elements of Trustworthy AI that need to be addressed together. Fairness is about learning an unbiased model while robustness is about learning from corrupted data, and it is known that addressing only one of them may have an adverse affect on the other. In this work, we propose a sample selection-based algorithm for fair and robust training. To this end, we formulate a combinatorial optimization problem for the unbiased selection of samples in the presence of data corruption. Observing that solving this optimization problem is strongly NP-hard, we propose a greedy algorithm that is efficient and effective in practice. Experiments show that our method obtains fairness and robustness that are better than or comparable to the state-of-the-art technique, both on synthetic and benchmark real datasets. Moreover, unlike other fair and robust training baselines, our algorithm can be used by only modifying the sampling step in batch selection without changing the training algorithm or leveraging additional clean data.",
    "authors": [
      "Roh, Yuji",
      "Lee, Kangwook",
      "Whang, Steven",
      "Suh, Changho"
    ]
  },
  {
    "id": "0768281a05da9f27df178b5c39a51263",
    "title": "NeurWIN: Neural Whittle Index Network For Restless Bandits Via Deep RL",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0768281a05da9f27df178b5c39a51263-Paper.pdf",
    "abstract": "Whittle index policy is a powerful tool to obtain asymptotically optimal solutions for the notoriously intractable problem of restless bandits. However, finding the Whittle indices remains a difficult problem for many practical restless bandits with convoluted transition kernels. This paper proposes NeurWIN, a neural Whittle index network that seeks to learn the Whittle indices for any restless bandits by leveraging mathematical properties of the Whittle indices. We show that a neural network that produces the Whittle index is also one that produces the optimal control for a set of Markov decision problems. This property motivates using deep reinforcement learning for the training of NeurWIN. We demonstrate the utility of NeurWIN by evaluating its performance for three recently studied restless bandit problems.Our experiment results show that the performance of NeurWIN is significantly better than other RL algorithms.",
    "authors": [
      "Nakhleh, Khaled",
      "Ganji, Santosh",
      "Hsieh, Ping-Chun",
      "Hou, I-Hong",
      "Shakkottai, Srinivas"
    ]
  },
  {
    "id": "076a8133735eb5d7552dc195b125a454",
    "title": "Sageflow: Robust Federated Learning against Both Stragglers and Adversaries",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/076a8133735eb5d7552dc195b125a454-Paper.pdf",
    "abstract": "While federated learning (FL) allows efficient model training with local data at edge devices, among major issues still to be resolved are: slow devices known as stragglers and malicious attacks launched by adversaries.   While the presence of both of these issues raises serious concerns in practical FL systems, no known schemes or combinations of schemes effectively address them at the same time. We propose Sageflow, staleness-aware grouping with entropy-based filtering and loss-weighted averaging, to handle both stragglers and adversaries simultaneously. Model grouping and weighting according to staleness (arrival delay) provides robustness against stragglers, while entropy-based filtering and loss-weighted averaging, working in a highly complementary fashion at each grouping stage,  counter a wide range of adversary attacks. A theoretical bound is established to provide key insights into the convergence behavior of Sageflow. Extensive experimental results show that Sageflow outperforms various existing methods aiming to handle stragglers/adversaries.",
    "authors": [
      "Park, Jungwuk",
      "Han, Dong-Jun",
      "Choi, Minseok",
      "Moon, Jaekyun"
    ]
  },
  {
    "id": "076ccd93ad68be51f23707988e934906",
    "title": "Alias-Free Generative Adversarial Networks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/076ccd93ad68be51f23707988e934906-Paper.pdf",
    "abstract": "We observe that despite their hierarchical convolutional nature, the synthesis process of typical generative adversarial networks depends on absolute pixel coordinates in an unhealthy manner. This manifests itself as, e.g., detail appearing to be glued to image coordinates instead of the surfaces of depicted objects. We trace the root cause to careless signal processing that causes aliasing in the generator network. Interpreting all signals in the network as continuous, we derive generally applicable, small architectural changes that guarantee that unwanted information cannot leak into the hierarchical synthesis process. The resulting networks match the FID of StyleGAN2 but differ dramatically in their internal representations, and they are fully equivariant to translation and rotation even at subpixel scales. Our results pave the way for generative models better suited for video and animation.",
    "authors": [
      "Karras, Tero",
      "Aittala, Miika",
      "Laine, Samuli",
      "H\u00e4rk\u00f6nen, Erik",
      "Hellsten, Janne",
      "Lehtinen, Jaakko",
      "Aila, Timo"
    ]
  },
  {
    "id": "077b83af57538aa183971a2fe0971ec1",
    "title": "Noise2Score: Tweedie\u2019s Approach to Self-Supervised Image Denoising without Clean Images",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/077b83af57538aa183971a2fe0971ec1-Paper.pdf",
    "abstract": "Recently, there has  been extensive research interest in training  deep networks to denoise images without clean reference.However, the representative approaches such as Noise2Noise, Noise2Void, Stein's unbiased risk estimator (SURE), etc.  seem to differ from one another and it is difficult to find the coherent mathematical structure. To address this, here we present a novel approach, called Noise2Score, which reveals a missing link in order to unite these seemingly different approaches.Specifically, we  show that   image denoising  problems  without clean images can be addressed by finding the mode of the posterior distribution and that the Tweedie's formula offers an explicit solution through the score function (i.e. the gradient of loglikelihood). Our method then uses the  recent finding that  the score function  can be stably estimated from the noisy images using the amortized residual denoising autoencoder, the method of which is closely related to Noise2Noise or Nose2Void. Our Noise2Score approach is so universal  that the same network training can be used to remove noises from images that are corrupted by any exponential family distributions and noise parameters. Using extensive  experiments with Gaussian, Poisson, and Gamma noises, we show  that  Noise2Score significantly outperforms the state-of-the-art self-supervised denoising methods in the benchmark data set such as (C)BSD68, Set12, and Kodak, etc. ",
    "authors": [
      "Kim, Kwanyoung",
      "Ye, Jong Chul"
    ]
  },
  {
    "id": "07811dc6c422334ce36a09ff5cd6fe71",
    "title": "Continuous Mean-Covariance Bandits",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/07811dc6c422334ce36a09ff5cd6fe71-Paper.pdf",
    "abstract": "Existing risk-aware multi-armed bandit models typically focus on risk measures of individual options such as variance. As a result, they cannot be directly applied to important real-world online decision making problems with correlated options. In this paper, we propose a novel Continuous Mean-Covariance Bandit (CMCB) model to explicitly take into account option correlation. Specifically, in CMCB, there is a learner who sequentially chooses weight vectors on given options and observes random feedback according to the decisions. The agent's  objective is to  achieve the best trade-off between reward and risk, measured with option covariance. To capture different reward observation scenarios in practice, we consider three feedback settings, i.e., full-information, semi-bandit and full-bandit feedback. We propose novel algorithms with optimal regrets (within logarithmic factors), and provide matching lower bounds to validate their optimalities. The experimental results also demonstrate the superiority of our algorithms.  To the best of our knowledge, this is the first work that considers option correlation in risk-aware bandits and explicitly quantifies how arbitrary covariance structures impact the learning performance.The novel analytical techniques we developed for exploiting the estimated covariance to build concentration and bounding the risk of selected actions based on sampling strategy properties can likely find applications in other bandit analysis and be of independent interests. ",
    "authors": [
      "Du, Yihan",
      "Wang, Siwei",
      "Fang, Zhixuan",
      "Huang, Longbo"
    ]
  },
  {
    "id": "07845cd9aefa6cde3f8926d25138a3a2",
    "title": "Dynamic Visual Reasoning by Learning Differentiable Physics Models from Video and Language",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/07845cd9aefa6cde3f8926d25138a3a2-Paper.pdf",
    "abstract": "In this work, we propose a unified framework, called Visual Reasoning with Differ-entiable Physics (VRDP), that can jointly learn visual concepts and infer physics models of objects and their interactions from videos and language. This is achieved by seamlessly integrating three components: a visual perception module, a concept learner, and a differentiable physics engine. The visual perception module parses each video frame into object-centric trajectories and represents them as latent scene representations. The concept learner grounds visual concepts (e.g., color, shape, and material) from these object-centric representations based on the language, thus providing prior knowledge for the physics engine. The differentiable physics model, implemented as an impulse-based differentiable rigid-body simulator, performs differentiable physical simulation based on the grounded concepts to infer physical properties, such as mass, restitution, and velocity, by fitting the simulated trajectories into the video observations. Consequently, these learned concepts and physical models can explain what we have seen and imagine what is about to happen in future and counterfactual scenarios. Integrating differentiable physics into the dynamic reasoning framework offers several appealing benefits.  More accurate dynamics prediction in learned physics models enables state-of-the-art performance on both synthetic and real-world benchmarks while still maintaining high transparency and interpretability; most notably, VRDP improves the accuracy of predictive and counterfactual questions by 4.5% and 11.5% compared to its best counterpart. VRDP is also highly data-efficient: physical parameters can be optimized from very few videos, and even a single video can be sufficient. Finally, with all physical parameters inferred, VRDP can quickly learn new concepts from a few examples.",
    "authors": [
      "Ding, Mingyu",
      "Chen, Zhenfang",
      "Du, Tao",
      "Luo, Ping",
      "Tenenbaum, Josh",
      "Gan, Chuang"
    ]
  },
  {
    "id": "07a4e20a7bbeeb7a736682b26b16ebe8",
    "title": "Solving Soft Clustering Ensemble via $k$-Sparse Discrete Wasserstein Barycenter",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/07a4e20a7bbeeb7a736682b26b16ebe8-Paper.pdf",
    "abstract": "Clustering ensemble is one of the most important problems in  ensemble learning. Though it has been extensively studied in the past decades, the existing methods often suffer from the issues like high computational complexity and the difficulty on understanding the consensus. In this paper, we study the more general soft clustering ensemble problem where each individual solution is a soft clustering. We connect it to the well-known discrete Wasserstein barycenter problem in geometry. Based on some novel geometric insights in high dimensions, we propose the sampling-based algorithms with provable quality guarantees. We also provide the systematical analysis on the consensus of our model. Finally, we conduct the experiments  to evaluate our proposed algorithms.  ",
    "authors": [
      "Qin, Ruizhe",
      "Li, Mengying",
      "Ding, Hu"
    ]
  },
  {
    "id": "07ac7cd13fd0eb1654ccdbd222b81437",
    "title": "Bayesian Adaptation for Covariate Shift",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/07ac7cd13fd0eb1654ccdbd222b81437-Paper.pdf",
    "abstract": "When faced with distribution shift at test time, deep neural networks often make inaccurate predictions with unreliable uncertainty estimates.While improving the robustness of neural networks is one promising approach to mitigate this issue, an appealing alternate to robustifying networks against all possible test-time shifts is to instead directly adapt them to unlabeled inputs from the particular distribution shift we encounter at test time.However, this poses a challenging question: in the standard Bayesian model for supervised learning, unlabeled inputs are conditionally independent of model parameters when the labels are unobserved, so what can unlabeled data tell us about the model parameters at test-time? In this paper, we derive a Bayesian model that provides for a well-defined relationship between unlabeled inputs under distributional shift and model parameters, and show how approximate inference in this model can be instantiated with a simple regularized entropy minimization procedure at test-time. We evaluate our method on a variety of distribution shifts for image classification, including image corruptions, natural distribution shifts, and domain adaptation settings, and show that our method improves both accuracy and uncertainty estimation.",
    "authors": [
      "Zhou, Aurick",
      "Levine, Sergey"
    ]
  },
  {
    "id": "07b1c04a30f798b5506c1ec5acfb9031",
    "title": "Perturb-and-max-product: Sampling and learning in discrete energy-based models",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/07b1c04a30f798b5506c1ec5acfb9031-Paper.pdf",
    "abstract": "Perturb-and-MAP offers an elegant approach to approximately sample from a energy-based model (EBM) by computing the maximum-a-posteriori (MAP) configuration of a perturbed version of the model. Sampling in turn enables learning. However, this line of research has been hindered by the general intractability of the MAP computation. Very few works venture outside tractable models, and when they do, they use linear programming approaches, which as we will show, have several limitations. In this work we present perturb-and-max-product (PMP), a parallel and scalable mechanism for sampling and learning in discrete EBMs. Models can be arbitrary as long as they are built using tractable factors. We show that (a) for Ising models, PMP is orders of magnitude faster than Gibbs and Gibbs-with-Gradients (GWG) at learning and generating samples of similar or better quality; (b) PMP is able to learn and sample from RBMs; (c) in a large, entangled graphical model in which Gibbs and GWG fail to mix, PMP succeeds.",
    "authors": [
      "Lazaro-Gredilla, Miguel",
      "Dedieu, Antoine",
      "George, Dileep"
    ]
  },
  {
    "id": "07bba581a2dd8d098a3be0f683560643",
    "title": "Towards Unifying Behavioral and Response Diversity for Open-ended Learning in Zero-sum Games",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/07bba581a2dd8d098a3be0f683560643-Paper.pdf",
    "abstract": "Measuring and promoting policy diversity is critical for solving games with strong non-transitive dynamics where strategic cycles exist, and there is no consistent winner (e.g., Rock-Paper-Scissors). With that in mind, maintaining a pool of diverse policies via open-ended learning is an attractive solution, which can generate auto-curricula to avoid being exploited. However, in conventional open-ended learning algorithms, there are no widely accepted definitions for diversity, making it hard to construct and evaluate the diverse policies. In this work, we summarize previous concepts of diversity and work towards offering a unified measure of diversity in multi-agent open-ended learning to include all elements in Markov games, based on both Behavioral Diversity (BD) and Response Diversity (RD). At the trajectory distribution level, we re-define BD in the state-action space as the discrepancies of occupancy measures. For the reward dynamics, we propose RD to characterize diversity through the responses of policies when encountering different opponents. We also show that many current diversity measures fall in one of the categories of BD or RD but not both. With this unified diversity measure, we design the corresponding diversity-promoting objective and population effectivity when seeking the best responses in open-ended learning. We validate our methods in both relatively simple games like matrix game, non-transitive mixture model, and the complex \\textit{Google Research Football} environment. The population found by our methods reveals the lowest exploitability, highest population effectivity in matrix game and non-transitive mixture model, as well as the largest goal difference when interacting with opponents of various levels in \\textit{Google Research Football}.",
    "authors": [
      "Liu, Xiangyu",
      "Jia, Hangtian",
      "Wen, Ying",
      "Hu, Yujing",
      "Chen, Yingfeng",
      "Fan, Changjie",
      "HU, ZHIPENG",
      "Yang, Yaodong"
    ]
  },
  {
    "id": "07c5807d0d927dcd0980f86024e5208b",
    "title": "Towards Better Understanding of Training Certifiably Robust Models against Adversarial Examples",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/07c5807d0d927dcd0980f86024e5208b-Paper.pdf",
    "abstract": "We study the problem of training certifiably robust models against adversarial examples. Certifiable training minimizes an upper bound on the worst-case loss over the allowed perturbation, and thus the tightness of the upper bound is an important factor in building certifiably robust models. However, many studies have shown that Interval Bound Propagation (IBP) training uses much looser bounds but outperforms other models that use tighter bounds. We identify another key factor that influences the performance of certifiable training: \\textit{smoothness of the loss landscape}. We find significant differences in the loss landscapes across many linear relaxation-based methods, and that the current state-of-the-arts method often has a landscape with favorable optimization properties. Moreover, to test the claim, we design a new certifiable training method with the desired properties. With the tightness and the smoothness, the proposed method achieves a decent performance under a wide range of perturbations, while others with only one of the two factors can perform well only for a specific range of perturbations. Our code is available at \\url{https://github.com/sungyoon-lee/LossLandscapeMatters}.",
    "authors": [
      "Lee, Sungyoon",
      "Lee, Woojin",
      "Park, Jinseong",
      "Lee, Jaewook"
    ]
  },
  {
    "id": "07d5938693cc3903b261e1a3844590ed",
    "title": "Mitigating Covariate Shift in Imitation Learning via Offline Data With Partial Coverage",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/07d5938693cc3903b261e1a3844590ed-Paper.pdf",
    "abstract": "This paper studies offline Imitation Learning (IL) where an agent learns to imitate an expert demonstrator without additional online environment interactions. Instead, the learner is presented with a static offline dataset of state-action-next state triples from a potentially less proficient behavior policy. We introduce Model-based IL from Offline data (MILO): an algorithmic framework that utilizes the static dataset to solve the offline IL problem efficiently both in theory and in practice. In theory, even if the behavior policy is highly sub-optimal compared to the expert, we show that as long as the data from the behavior policy provides sufficient coverage on the expert state-action traces (and with no necessity for a global coverage over the entire state-action space), MILO can provably combat the covariate shift issue in IL. Complementing our theory results, we also demonstrate that a practical implementation of our approach mitigates covariate shift on benchmark MuJoCo continuous control tasks. We demonstrate that with behavior policies whose performances are less than half of that of the expert, MILO still successfully imitates with an extremely low number of expert state-action pairs while traditional offline IL methods such as behavior cloning (BC) fail completely. Source code is provided at https://github.com/jdchang1/milo.",
    "authors": [
      "Chang, Jonathan",
      "Uehara, Masatoshi",
      "Sreenivas, Dhruv",
      "Kidambi, Rahul",
      "Sun, Wen"
    ]
  },
  {
    "id": "07e87c2f4fc7f7c96116d8e2a92790f5",
    "title": "Global Filter Networks for Image Classification",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/07e87c2f4fc7f7c96116d8e2a92790f5-Paper.pdf",
    "abstract": "Recent advances in self-attention and pure multi-layer perceptrons (MLP) models for vision have shown great potential in achieving promising performance with fewer inductive biases. These models are generally based on learning interaction among spatial locations from raw data. The complexity of self-attention and MLP grows quadratically as the image size increases, which makes these models hard to scale up when high-resolution features are required. In this paper, we present the Global Filter Network (GFNet), a conceptually simple yet computationally efficient architecture, that learns long-term spatial dependencies in the frequency domain with log-linear complexity. Our architecture replaces the self-attention layer in vision transformers with three key operations: a 2D discrete Fourier transform, an element-wise multiplication between frequency-domain features and learnable global filters, and a 2D inverse Fourier transform. We exhibit favorable accuracy/complexity trade-offs of our models on both ImageNet and downstream tasks. Our results demonstrate that GFNet can be a very competitive alternative to transformer-style models and CNNs in efficiency, generalization ability and robustness. Code is available at https://github.com/raoyongming/GFNet",
    "authors": [
      "Rao, Yongming",
      "Zhao, Wenliang",
      "Zhu, Zheng",
      "Lu, Jiwen",
      "Zhou, Jie"
    ]
  },
  {
    "id": "08040837089cdf46631a10aca5258e16",
    "title": "CAFE: Catastrophic Data Leakage in Vertical Federated Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/08040837089cdf46631a10aca5258e16-Paper.pdf",
    "abstract": "Recent studies show that private training data can be leaked through the gradients sharing mechanism deployed in distributed machine learning systems, such as federated learning (FL). Increasing batch size to complicate data recovery is often viewed as a promising defense strategy against data leakage. In this paper, we revisit this defense premise and propose an advanced data leakage attack with theoretical justification to efficiently recover batch data from the shared aggregated gradients. We name our proposed method as catastrophic data leakage in vertical federated learning (CAFE). Comparing to existing data leakage attacks, our extensive experimental results on vertical FL settings demonstrate the effectiveness of CAFE to perform large-batch data leakage attack with improved data recovery quality. We also propose a practical countermeasure to mitigate CAFE. Our results suggest that private data participated in standard FL, especially the vertical case, have a high risk of being leaked from the training gradients. Our analysis implies unprecedented and practical data leakage risks in those learning settings. The code of our work is available at https://github.com/DeRafael/CAFE.",
    "authors": [
      "Jin, Xiao",
      "Chen, Pin-Yu",
      "Hsu, Chia-Yi",
      "Yu, Chia-Mu",
      "Chen, Tianyi"
    ]
  },
  {
    "id": "080acdcce72c06873a773c4311c2e464",
    "title": "Fault-Tolerant Federated Reinforcement Learning with Theoretical Guarantee",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/080acdcce72c06873a773c4311c2e464-Paper.pdf",
    "abstract": "The growing literature of Federated Learning (FL) has recently inspired Federated Reinforcement Learning (FRL) to encourage multiple agents to federatively build a better decision-making policy without sharing raw trajectories. Despite its promising applications, existing works on FRL fail to I) provide theoretical analysis on its convergence, and II) account for random system failures and adversarial attacks. Towards this end, we propose the first FRL framework the convergence of which is guaranteed and tolerant to less than half of the participating agents being random system failures or adversarial attackers. We prove that the sample efficiency of the proposed framework is guaranteed to improve with the number of agents and is able to account for such potential failures or attacks. All theoretical results are empirically verified on various RL benchmark tasks.",
    "authors": [
      "Fan, Xiaofeng",
      "Ma, Yining",
      "Dai, Zhongxiang",
      "Jing, Wei",
      "Tan, Cheston",
      "Low, Bryan Kian Hsiang"
    ]
  },
  {
    "id": "081be9fdff07f3bc808f935906ef70c0",
    "title": "Compacter: Efficient Low-Rank Hypercomplex Adapter Layers",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/081be9fdff07f3bc808f935906ef70c0-Paper.pdf",
    "abstract": "Adapting large-scale pretrained language models to downstream tasks via fine-tuning is the standard method for achieving state-of-the-art performance on NLP benchmarks. However, fine-tuning all weights of models with millions or billions of parameters is sample-inefficient, unstable in low-resource settings, and wasteful as it requires storing a separate copy of the model for each task. Recent work has developed parameter-efficient fine-tuning methods,  but these approaches either still require a relatively large number of parameters or underperform standard fine-tuning. In this work, we propose Compacter, a method for fine-tuning large-scale language models with a better trade-off between task performance and the number of trainable parameters than prior work. Compacter accomplishes this by building on top of ideas from adapters, low-rank optimization, and parameterized hypercomplex multiplication layers.Specifically, Compacter inserts task-specific weight matrices into a pretrained model's weights, which are computed efficiently as a sum of Kronecker products between shared slow'' weights andfast'' rank-one matrices defined per Compacter layer. By only training 0.047% of a pretrained model's parameters, Compacter performs on par with standard fine-tuning on GLUE and outperforms standard fine-tuning on SuperGLUE and low-resource settings. Our code is publicly available at https://github.com/rabeehk/compacter. ",
    "authors": [
      "Karimi Mahabadi, Rabeeh",
      "Henderson, James",
      "Ruder, Sebastian"
    ]
  },
  {
    "id": "082a8bbf2c357c09f26675f9cf5bcba3",
    "title": "Distilling Image Classifiers in Object Detectors",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/082a8bbf2c357c09f26675f9cf5bcba3-Paper.pdf",
    "abstract": "Knowledge distillation constitutes a simple yet effective way to improve the performance of a compact student network by exploiting the knowledge of a more powerful teacher. Nevertheless, the knowledge distillation literature remains limited to the scenario where the student and the teacher tackle the same task. Here, we investigate the problem of transferring knowledge not only across architectures but also across tasks. To this end, we study the case of object detection and, instead of following the standard detector-to-detector distillation approach, introduce a classifier-to-detector knowledge transfer framework. In particular, we propose strategies to exploit the classification teacher to improve both the detector's recognition accuracy and localization performance. Our experiments on several detectors with different backbones demonstrate the effectiveness of our approach, allowing us to outperform the state-of-the-art detector-to-detector distillation methods.",
    "authors": [
      "Guo, Shuxuan",
      "Alvarez, Jose M.",
      "Salzmann, Mathieu"
    ]
  },
  {
    "id": "08425b881bcde94a383cd258cea331be",
    "title": "Subgroup Generalization and Fairness of Graph Neural Networks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/08425b881bcde94a383cd258cea331be-Paper.pdf",
    "abstract": "Despite enormous successful applications of graph neural networks (GNNs), theoretical understanding of their generalization ability, especially for node-level tasks where data are not independent and identically-distributed (IID), has been sparse. The theoretical investigation of the generalization performance is beneficial for understanding fundamental issues (such as fairness) of GNN models and designing better learning methods. In this paper, we present a novel PAC-Bayesian analysis for GNNs under a non-IID semi-supervised learning setup. Moreover, we analyze the generalization performances on different subgroups of unlabeled nodes, which allows us to further study an accuracy-(dis)parity-style (un)fairness of GNNs from a theoretical perspective. Under reasonable assumptions, we demonstrate that the distance between a test subgroup and the training set can be a key factor affecting the GNN performance on that subgroup, which calls special attention to the training node selection for fair learning. Experiments across multiple GNN models and datasets support our theoretical results.",
    "authors": [
      "Ma, Jiaqi",
      "Deng, Junwei",
      "Mei, Qiaozhu"
    ]
  },
  {
    "id": "08ae6a26b7cb089ea588e94aed36bd15",
    "title": "Scaling Neural Tangent Kernels via Sketching and Random Features",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/08ae6a26b7cb089ea588e94aed36bd15-Paper.pdf",
    "abstract": "The Neural Tangent Kernel (NTK) characterizes the behavior of infinitely-wide neural networks trained under least squares loss by gradient descent. Recent works also report that NTK regression can outperform finitely-wide neural networks trained on small-scale datasets. However, the computational complexity of kernel methods has limited its use in large-scale learning tasks. To accelerate learning with NTK, we design a near input-sparsity time approximation algorithm for NTK, by sketching the polynomial expansions of arc-cosine kernels: our sketch for the convolutional counterpart of NTK (CNTK) can transform any image using a linear runtime in the number of pixels. Furthermore, we prove a spectral approximation guarantee for the NTK matrix, by combining random features (based on leverage score sampling) of the arc-cosine kernels with a sketching algorithm. We benchmark our methods on various large-scale regression and classification tasks and show that a linear regressor trained on our CNTK features matches the accuracy of exact CNTK on CIFAR-10 dataset while achieving 150x speedup.",
    "authors": [
      "Zandieh, Amir",
      "Han, Insu",
      "Avron, Haim",
      "Shoham, Neta",
      "Kim, Chaewon",
      "Shin, Jinwoo"
    ]
  },
  {
    "id": "08aee6276db142f4b8ac98fb8ee0ed1b",
    "title": "BatchQuant: Quantized-for-all Architecture Search with Robust Quantizer",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/08aee6276db142f4b8ac98fb8ee0ed1b-Paper.pdf",
    "abstract": "As the applications of deep learning models on edge devices increase at an accelerating pace, fast adaptation to various scenarios with varying resource constraints has become a crucial aspect of model deployment. As a result, model optimization strategies with adaptive configuration are becoming increasingly popular. While single-shot quantized neural architecture search enjoys flexibility in both model architecture and quantization policy, the combined search space comes with many challenges, including instability when training the weight-sharing supernet and difficulty in navigating the exponentially growing search space. Existing methods tend to either limit the architecture search space to a small set of options or limit the quantization policy search space to fixed precision policies. To this end, we propose BatchQuant, a robust quantizer formulation that allows fast and stable training of a compact, single-shot, mixed-precision, weight-sharing supernet. We employ BatchQuant to train a compact supernet (offering over $10^{76}$ quantized subnets) within substantially fewer GPU hours than previous methods. Our approach, Quantized-for-all (QFA), is the first to seamlessly extend one-shot weight-sharing NAS supernet to support subnets with arbitrary ultra-low bitwidth mixed-precision quantization policies without retraining. QFA opens up new possibilities in joint hardware-aware neural architecture search and quantization. We demonstrate the effectiveness of our method on ImageNet and achieve SOTA Top-1 accuracy under a low complexity constraint (<20 MFLOPs).",
    "authors": [
      "Bai, Haoping",
      "Cao, Meng",
      "Huang, Ping",
      "Shan, Jiulong"
    ]
  },
  {
    "id": "08b255a5d42b89b0585260b6f2360bdd",
    "title": "Long Short-Term Transformer for Online Action Detection",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/08b255a5d42b89b0585260b6f2360bdd-Paper.pdf",
    "abstract": "We present Long Short-term TRansformer (LSTR), a temporal modeling algorithm for online action detection, which employs a long- and short-term memory mechanism to model prolonged sequence data. It consists of an LSTR encoder that dynamically leverages coarse-scale historical information from an extended temporal window (e.g., 2048 frames spanning of up to 8 minutes), together with an LSTR decoder that focuses on a short time window (e.g., 32 frames spanning 8 seconds) to model the fine-scale characteristics of the data. Compared to prior work, LSTR provides an effective and efficient method to model long videos with fewer heuristics, which is validated by extensive empirical analysis. LSTR achieves state-of-the-art performance on three standard online action detection benchmarks, THUMOS'14, TVSeries, and HACS Segment. Code has been made available at: https://xumingze0308.github.io/projects/lstr.",
    "authors": [
      "Xu, Mingze",
      "Xiong, Yuanjun",
      "Chen, Hao",
      "Li, Xinyu",
      "Xia, Wei",
      "Tu, Zhuowen",
      "Soatto, Stefano"
    ]
  },
  {
    "id": "08d562c1eedd30b15b51e35d8486d14c",
    "title": "Near Optimal Policy Optimization via REPS",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/08d562c1eedd30b15b51e35d8486d14c-Paper.pdf",
    "abstract": "Since its introduction a decade ago, relative entropy policy search (REPS) has demonstrated successful policy learning on a number of simulated and real-world robotic domains, not to mention providing algorithmic components used by many recently proposed reinforcement learning (RL) algorithms. While REPS is commonly known in the community, there exist no guarantees on its performance when using stochastic and gradient-based solvers. In this paper we aim to fill this gap by providing guarantees and convergence rates for the sub-optimality of a policy learned using first-order optimization methods applied to the REPS objective. We first consider the setting in which we are given access to exact gradients and demonstrate how near-optimality of the objective translates to near-optimality of the policy. We then consider the practical setting of stochastic gradients, and introduce a technique that uses generative access to the underlying Markov decision process to compute parameter updates that maintain favorable convergence to the optimal regularized policy.",
    "authors": [
      "Pacchiano, Aldo",
      "Lee, Jonathan N",
      "Bartlett, Peter",
      "Nachum, Ofir"
    ]
  },
  {
    "id": "08f0efebb1c51aada9430a089a2050cc",
    "title": "Self-Consistent Models and Values",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/08f0efebb1c51aada9430a089a2050cc-Paper.pdf",
    "abstract": "Learned models of the environment provide reinforcement learning (RL) agents with flexible ways of making predictions about the environment.Models enable planning, i.e. using more computation to improve value functions or policies, without requiring additional environment interactions.In this work, we investigate a way of augmenting model-based RL, by additionally encouraging a learned model and value function to be jointly \\emph{self-consistent}.This lies in contrast to classic planning methods like Dyna, which only update the value function to be consistent with the model.We propose a number of possible self-consistency updates, study them empirically in both the tabular and function approximation settings, and find that with appropriate choices self-consistency can be useful both for policy evaluation and control.",
    "authors": [
      "Farquhar, Greg",
      "Baumli, Kate",
      "Marinho, Zita",
      "Filos, Angelos",
      "Hessel, Matteo",
      "van Hasselt, Hado P.",
      "Silver, David"
    ]
  },
  {
    "id": "08f36fcf88c0a84c19a6ed437b9cbcc9",
    "title": "Learning on Random Balls is Sufficient for Estimating (Some) Graph Parameters",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/08f36fcf88c0a84c19a6ed437b9cbcc9-Paper.pdf",
    "abstract": "Theoretical analyses for graph learning methods often assume a complete observation of the input graph. Such an assumption might not be useful for handling any-size graphs due to the scalability issues in practice. In this work, we develop a theoretical framework for graph classification problems in the partial observation setting (i.e., subgraph samplings). Equipped with insights from graph limit theory, we propose a new graph classification model that works on a randomly sampled subgraph and a novel topology to characterize the representability of the model. Our theoretical framework contributes a theoretical validation of mini-batch learning on graphs and leads to new learning-theoretic results on generalization bounds as well as size-generalizability without assumptions on the input.",
    "authors": [
      "Maehara, Takanori",
      "NT, Hoang"
    ]
  },
  {
    "id": "08f90c1a417155361a5c4b8d297e0d78",
    "title": "Risk-Averse Bayes-Adaptive Reinforcement Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/08f90c1a417155361a5c4b8d297e0d78-Paper.pdf",
    "abstract": "In this work, we address risk-averse Bayes-adaptive reinforcement learning. We pose the problem of optimising the conditional value at risk (CVaR) of the total return in Bayes-adaptive Markov decision processes (MDPs).  We show that a policy optimising CVaR in this setting is risk-averse to both the epistemic uncertainty due to the prior distribution over MDPs, and the aleatoric uncertainty due to the inherent stochasticity of MDPs. We reformulate the problem as a two-player stochastic game and propose an approximate algorithm based on Monte Carlo tree search and Bayesian optimisation. Our experiments demonstrate that our approach significantly outperforms baseline approaches for this problem.",
    "authors": [
      "Rigter, Marc",
      "Lacerda, Bruno",
      "Hawes, Nick"
    ]
  },
  {
    "id": "0919b5c38396c3f0c41f1112d538e42c",
    "title": "Iterative Connecting Probability Estimation for Networks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0919b5c38396c3f0c41f1112d538e42c-Paper.pdf",
    "abstract": "Estimating the probabilities of connections between vertices in a random network using an observed adjacency matrix is an important task for network data analysis. Many existing estimation methods are based on certain assumptions on network structure, which limit their applicability in practice. Without making strong assumptions, we develop an iterative connecting probability estimation method based on neighborhood averaging. Starting at a random initial point or an existing estimate, our method iteratively updates the pairwise vertex distances, the sets of similar vertices, and connecting probabilities to improve the precision of the estimate. We propose a two-stage neighborhood selection procedure to achieve the trade-off between smoothness of the estimate and the ability to discover local structure. The tuning parameters can be selected by cross-validation. We establish desirable theoretical properties for our method, and further justify its superior performance by comparing with existing methods in simulation and real data analysis.",
    "authors": [
      "Qin, Yichen",
      "Yu, Linhan",
      "Li, Yang"
    ]
  },
  {
    "id": "092cb13c22d51c22b9035a2b4fe76b00",
    "title": "Learning to Adapt via Latent Domains for Adaptive Semantic Segmentation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/092cb13c22d51c22b9035a2b4fe76b00-Paper.pdf",
    "abstract": "Domain adaptive semantic segmentation aims to transfer knowledge learned from labeled source domain to unlabeled target domain. To narrow down the domain gap and ease adaptation difficulty, some recent methods translate source images to target-like images (latent domains), which are used as supplement or substitute to the original source data. Nevertheless, these methods neglect to explicitly model the relationship of knowledge transferring across different domains. Alternatively, in this work we break through the standard \u201csource-target\u201d one pair adaptation framework and construct multiple adaptation pairs (e.g. \u201csource-latent\u201d and \u201clatent-target\u201d). The purpose is to use the meta-knowledge (how to adapt) learned from one pair as guidance to assist the adaptation of another pair under a meta-learning framework. Furthermore, we extend our method to a more practical setting of open compound domain adaptation (a.k.a multiple-target domain adaptation), where the target is a compound of multiple domains without domain labels. In this setting, we embed an additional pair of \u201clatent-latent\u201d to reduce the domain gap between the source and different latent domains, allowing the model to adapt well on multiple target domains simultaneously. When evaluated on standard benchmarks, our method is superior to the state-of-the-art methods in both the single target and multiple-target domain adaptation settings.",
    "authors": [
      "Liu, Yunan",
      "Zhang, Shanshan",
      "Li, Yang",
      "Yang, Jian"
    ]
  },
  {
    "id": "093b60fd0557804c8ba0cbf1453da22f",
    "title": "Single Layer Predictive Normalized Maximum Likelihood for Out-of-Distribution Detection",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/093b60fd0557804c8ba0cbf1453da22f-Paper.pdf",
    "abstract": "Detecting out-of-distribution (OOD) samples is vital for developing machine learning based models for critical safety systems. Common approaches for OOD detection assume access to some OOD samples during training which may not be available in a real-life scenario. Instead, we utilize the {\\em predictive normalized maximum likelihood} (pNML) learner, in which no assumptions are made on the tested input. We derive an explicit expression of the pNML and its generalization error, denoted as the regret, for a single layer neural network (NN). We show that this learner generalizes well when (i) the test vector resides in a subspace spanned by the eigenvectors associated with the large eigenvalues of the empirical correlation matrix of the training data, or (ii) the test sample is far from the decision boundary. Furthermore, we describe how to efficiently apply the derived pNML regret to any pretrained deep NN, by employing the explicit pNML for the last layer, followed by the softmax function. Applying the derived regret to deep NN requires neither additional tunable parameters nor extra data. We extensively evaluate our approach on 74 OOD detection benchmarks using DenseNet-100, ResNet-34, and WideResNet-40 models trained with CIFAR-100, CIFAR-10, SVHN, and ImageNet-30 showing a significant improvement of up to 15.6% over recent leading methods.",
    "authors": [
      "Bibas, Koby",
      "Feder, Meir",
      "Hassner, Tal"
    ]
  },
  {
    "id": "093f65e080a295f8076b1c5722a46aa2",
    "title": "Prototypical Cross-Attention Networks for Multiple Object Tracking and Segmentation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/093f65e080a295f8076b1c5722a46aa2-Paper.pdf",
    "abstract": "Multiple object tracking and segmentation requires detecting, tracking, and segmenting objects belonging to a set of given classes. Most approaches only exploit the temporal dimension to address the association problem, while relying on single frame predictions for the segmentation mask itself. We propose Prototypical Cross-Attention Network (PCAN), capable of leveraging rich spatio-temporal information for online multiple object tracking and segmentation. PCAN first distills a space-time memory into a set of prototypes and then employs cross-attention to retrieve rich information from the past frames. To segment each object, PCAN adopts a prototypical appearance module to learn a set of contrastive foreground and background prototypes, which are then propagated over time. Extensive experiments demonstrate that PCAN outperforms current video instance tracking and segmentation competition winners on both Youtube-VIS and BDD100K datasets, and shows efficacy to both one-stage and two-stage segmentation frameworks. Code and video resources are available at http://vis.xyz/pub/pcan.",
    "authors": [
      "Ke, Lei",
      "Li, Xia",
      "Danelljan, Martin",
      "Tai, Yu-Wing",
      "Tang, Chi-Keung",
      "Yu, Fisher"
    ]
  },
  {
    "id": "094bb65ef46d3eb4be0a87877ec333eb",
    "title": "Algorithmic Instabilities of Accelerated Gradient Descent",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/094bb65ef46d3eb4be0a87877ec333eb-Paper.pdf",
    "abstract": "We study the algorithmic stability of Nesterov's accelerated gradient method. For convex quadratic objectives, Chen et al. (2018) proved that the uniform stability of the method grows quadratically with the number of optimization steps, and conjectured that the same is true for the general convex and smooth case. We disprove this conjecture and show, for two notions of algorithmic stability (including uniform stability), that the stability of Nesterov's accelerated method in fact deteriorates exponentially fast with the number of gradient steps. This stands in sharp contrast to the bounds in the quadratic case, but also to known results for non-accelerated gradient methods where stability typically grows linearly with the number of steps.",
    "authors": [
      "Attia, Amit",
      "Koren, Tomer"
    ]
  },
  {
    "id": "09676fac73eda6cac726c43e43e86c58",
    "title": "Learning Optimal Predictive Checklists",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/09676fac73eda6cac726c43e43e86c58-Paper.pdf",
    "abstract": "Checklists are simple decision aids that are often used to promote safety and reliability in clinical applications. In this paper, we present a method to learn checklists for clinical decision support. We represent predictive checklists as discrete linear classifiers with binary features and unit weights. We then learn globally optimal predictive checklists from data by solving an integer programming problem. Our method allows users to customize checklists to obey complex constraints, including constraints to enforce group fairness and to binarize real-valued features at training time. In addition, it pairs models with an optimality gap that can inform model development and determine the feasibility of learning sufficiently accurate checklists on a given dataset. We pair our method with specialized techniques that speed up its ability to train a predictive checklist that performs well and has a small optimality gap. We benchmark the performance of our method on seven clinical classification problems, and demonstrate its practical benefits by training a short-form checklist for PTSD screening. Our results show that our method can fit simple predictive checklists that perform well and that can easily be customized to obey a rich class of custom constraints.",
    "authors": [
      "Zhang, Haoran",
      "Morris, Quaid",
      "Ustun, Berk",
      "Ghassemi, Marzyeh"
    ]
  },
  {
    "id": "096ffc299200f51751b08da6d865ae95",
    "title": "Finite Sample Analysis of Average-Reward TD Learning and $Q$-Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/096ffc299200f51751b08da6d865ae95-Paper.pdf",
    "abstract": "The focus of this paper is on sample complexity guarantees of average-reward reinforcement learning algorithms, which are known to be more challenging to study than their discounted-reward counterparts. To the best of our knowledge, we provide the first known finite sample guarantees using both constant and diminishing step sizes of (i) average-reward TD($\\lambda$) with linear function approximation for policy evaluation and (ii) average-reward $Q$-learning in the tabular setting to find the optimal policy. A major challenge is that since the value functions are agnostic to an additive constant, the corresponding Bellman operators are no longer contraction mappings under any norm. We obtain the results for TD($\\lambda$) by working in an appropriately defined subspace that ensures uniqueness of the solution. For $Q$-learning, we exploit the span seminorm contractive property of the Bellman operator, and construct a novel Lyapunov function obtained by infimal convolution of a generalized Moreau envelope and the indicator function of a set.",
    "authors": [
      "Zhang, Sheng",
      "Zhang, Zhe",
      "Maguluri, Siva Theja"
    ]
  },
  {
    "id": "09779bb7930c8a0a44360e12b538ae3c",
    "title": "Generalization Bounds for Graph Embedding Using Negative Sampling: Linear vs Hyperbolic",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/09779bb7930c8a0a44360e12b538ae3c-Paper.pdf",
    "abstract": "Graph embedding, which represents real-world entities in a mathematical space, has enabled numerous applications such as analyzing natural languages, social networks, biochemical networks, and knowledge bases.It has been experimentally shown that graph embedding in hyperbolic space can represent hierarchical tree-like data more effectively than embedding in linear space, owing to hyperbolic space's exponential growth property. However, since the theoretical comparison has been limited to ideal noiseless settings, the potential for the hyperbolic space's property to worsen the generalization error for practical data has not been analyzed.In this paper, we provide a generalization error bound applicable for graph embedding both in linear and hyperbolic spaces under various negative sampling settings that appear in graph embedding. Our bound states that error is polynomial and exponential with respect to the embedding space's radius in linear and hyperbolic spaces, respectively, which implies that hyperbolic space's exponential growth property worsens the error.Using our bound, we clarify the data size condition on which graph embedding in hyperbolic space can represent a tree better than in Euclidean space by discussing the bias-variance trade-off.Our bound also shows that imbalanced data distribution, which often appears in graph embedding, can worsen the error.",
    "authors": [
      "Suzuki, Atsushi",
      "Nitanda, Atsushi",
      "wang, jing",
      "Xu, Linchuan",
      "Yamanishi, Kenji",
      "Cavazza, Marc"
    ]
  },
  {
    "id": "0987b8b338d6c90bbedd8631bc499221",
    "title": "Gradient Starvation: A Learning Proclivity in Neural Networks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0987b8b338d6c90bbedd8631bc499221-Paper.pdf",
    "abstract": "We identify and formalize a fundamental gradient descent phenomenon resulting in a learning proclivity in over-parameterized neural networks. Gradient Starvation arises when cross-entropy loss is minimized by capturing only a subset of features relevant for the task, despite the presence of other predictive features that fail to be discovered. This work provides a theoretical explanation for the emergence of such feature imbalance in neural networks. Using tools from Dynamical Systems theory, we identify simple properties of learning dynamics during gradient descent that lead to this imbalance, and prove that such a situation can be expected given certain statistical structure in training data. Based on our proposed formalism, we develop guarantees for a novel regularization method aimed at decoupling feature learning dynamics, improving accuracy and robustness in cases hindered by gradient starvation. We illustrate our findings with simple and real-world out-of-distribution (OOD) generalization experiments.",
    "authors": [
      "Pezeshki, Mohammad",
      "Kaba, Oumar",
      "Bengio, Yoshua",
      "Courville, Aaron C.",
      "Precup, Doina",
      "Lajoie, Guillaume"
    ]
  },
  {
    "id": "099fe6b0b444c23836c4a5d07346082b",
    "title": "Offline Reinforcement Learning as One Big Sequence Modeling Problem",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/099fe6b0b444c23836c4a5d07346082b-Paper.pdf",
    "abstract": "Reinforcement learning (RL) is typically viewed as the problem of estimating single-step policies (for model-free RL) or single-step models (for model-based RL), leveraging the Markov property to factorize the problem in time. However, we can also view RL as a sequence modeling problem: predict a sequence of actions that leads to a sequence of high rewards. Viewed in this way, it is tempting to consider whether powerful, high-capacity sequence prediction models that work well in other supervised learning domains, such as natural-language processing, can also provide simple and effective solutions to the RL problem. To this end, we explore how RL can be reframed as \"one big sequence modeling\" problem, using state-of-the-art Transformer architectures to model distributions over sequences of states, actions, and rewards. Addressing RL as a sequence modeling problem significantly simplifies a range of design decisions: we no longer require separate behavior policy constraints, as is common in prior work on offline model-free RL, and we no longer require ensembles or other epistemic uncertainty estimators, as is common in prior work on model-based RL. All of these roles are filled by the same Transformer sequence model. In our experiments, we demonstrate the flexibility of this approach across imitation learning, goal-conditioned RL, and offline RL.",
    "authors": [
      "Janner, Michael",
      "Li, Qiyang",
      "Levine, Sergey"
    ]
  },
  {
    "id": "09a5e2a11bea20817477e0b1dfe2cc21",
    "title": "Optimality and Stability in Federated Learning: A Game-theoretic Approach",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/09a5e2a11bea20817477e0b1dfe2cc21-Paper.pdf",
    "abstract": "Federated learning is a distributed learning paradigm where multiple agents, each only with access to local data, jointly learn a global model. There has recently been an explosion of research aiming not only to improve the accuracy rates of federated learning, but also provide certain guarantees around social good properties such as total error. One branch of this research has taken a game-theoretic approach, and in particular, prior work has viewed federated learning as a hedonic game, where error-minimizing players arrange themselves into federating coalitions. This past work proves the existence of stable coalition partitions, but leaves open a wide range of questions, including how far from optimal these stable solutions are. In this work, we motivate and define a notion of optimality given by the average error rates among federating agents (players). First, we provide and prove the correctness of an efficient algorithm to calculate an optimal (error minimizing) arrangement of players. Next, we analyze the relationship between the stability and optimality of an arrangement. First, we show that for some regions of parameter space, all stable arrangements are optimal (Price of Anarchy equal to 1). However, we show this is not true for all settings: there exist examples of stable arrangements with higher cost than optimal (Price of Anarchy greater than 1). Finally, we give the first constant-factor bound on the performance gap between stability and optimality, proving that the total error of the worst stable solution can be no higher than 9 times the total error of an optimal solution (Price of Anarchy bound of 9).",
    "authors": [
      "Donahue, Kate",
      "Kleinberg, Jon"
    ]
  },
  {
    "id": "09a630e07af043e4cae879dd60db1cac",
    "title": "Understanding Deflation Process in Over-parametrized Tensor Decomposition",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/09a630e07af043e4cae879dd60db1cac-Paper.pdf",
    "abstract": "In this paper we study the training dynamics for gradient flow on over-parametrized tensor decomposition problems. Empirically, such training process often first fits larger components and then discovers smaller components, which is similar to a tensor deflation process that is commonly used in tensor decomposition algorithms. We prove that for orthogonally decomposable tensor, a slightly modified version of gradient flow would follow a tensor deflation process and recover all the tensor components. Our proof suggests that for orthogonal tensors, gradient flow dynamics works similarly as greedy low-rank learning in the matrix setting, which is a first step towards understanding the implicit regularization effect of over-parametrized models for low-rank tensors.",
    "authors": [
      "Ge, Rong",
      "Ren, Yunwei",
      "Wang, Xiang",
      "Zhou, Mo"
    ]
  },
  {
    "id": "09b69adcd7cbae914c6204984097d2da",
    "title": "Privately Learning Subspaces",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/09b69adcd7cbae914c6204984097d2da-Paper.pdf",
    "abstract": "Private data analysis suffers a costly curse of dimensionality. However, the data often has an underlying low-dimensional structure. For example, when optimizing via gradient descent, the gradients often lie in or near a low-dimensional subspace. If that low-dimensional structure can be identified, then we can avoid paying (in terms of privacy or accuracy) for the high ambient dimension. We present differentially private algorithms that take input data sampled from a low-dimensional linear subspace (possibly with a small amount of error) and output that subspace (or an approximation to it). These algorithms can serve as a pre-processing step for other procedures.",
    "authors": [
      "Singhal, Vikrant",
      "Steinke, Thomas"
    ]
  },
  {
    "id": "09dbc1177211571ef3e1ca961cc39363",
    "title": "On the Value of Interaction and Function Approximation in Imitation Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/09dbc1177211571ef3e1ca961cc39363-Paper.pdf",
    "abstract": "We study the statistical guarantees for the Imitation Learning (IL) problem in episodic MDPs.Rajaraman et al. (2020) show an information theoretic lower bound that in the worst case, a learner which can even actively query the expert policy suffers from a suboptimality growing quadratically in the length of the horizon, $H$. We study imitation learning under the $\\mu$-recoverability assumption of Ross et al. (2011) which assumes that the difference in the $Q$-value under the expert policy across different actions in a state do not deviate beyond $\\mu$ from the maximum. We show that the reduction proposed by Ross et al. (2010) is statistically optimal: the resulting algorithm upon interacting with the MDP for $N$ episodes results in a suboptimality bound of $\\widetilde{\\mathcal{O}} \\left( \\mu |\\mathcal{S}| H / N \\right)$ which we show is optimal up to log-factors. In contrast, we show that any algorithm which does not interact with the MDP and uses an offline dataset of $N$ expert trajectories must incur suboptimality growing as $\\gtrsim |\\mathcal{S}| H^2/N$ even under the $\\mu$-recoverability assumption. This establishes a clear and provable separation of the minimax rates between the active setting and the no-interaction setting. We also study IL with linear function approximation. When the expert plays actions according to a linear classifier of known state-action features, we use the reduction to multi-class classification to show that with high probability, the suboptimality of behavior cloning is  $\\widetilde{O}(dH^2/N)$ given $N$ rollouts from the optimal policy. This is optimal up to log-factors but can be improved to $\\widetilde{O}(dH/N)$ if we have a linear expert with parameter-sharing across time steps. In contrast, when the MDP transition structure is known to the learner such as in the case of simulators, we demonstrate fundamental differences compared to the tabular setting in terms of the performance of an optimal algorithm, Mimic-MD (Rajaraman et al. (2020)) when extended to the function approximation setting. Here, we introduce a new problem called confidence set linear classification, that can be used to construct sample-efficient IL algorithms.",
    "authors": [
      "Rajaraman, Nived",
      "Han, Yanjun",
      "Yang, Lin",
      "Liu, Jingbo",
      "Jiao, Jiantao",
      "Ramchandran, Kannan"
    ]
  },
  {
    "id": "09def3ebbc44ff3426b28fcd88c83554",
    "title": "Shapeshifter: a Parameter-efficient Transformer using Factorized Reshaped Matrices",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/09def3ebbc44ff3426b28fcd88c83554-Paper.pdf",
    "abstract": "Language models employ a very large number of trainable parameters. Despite being highly overparameterized, these networks often achieve good out-of-sample test performance on the original task and easily fine-tune to related tasks. Recent observations involving, for example, intrinsic dimension of the objective landscape and  the lottery ticket hypothesis, indicate that often training actively involves only a small fraction of the parameter space. Thus, a question remains how large a parameter space needs to be in the first place \u2013- the evidence from recent work on model compression, parameter sharing, factorized representations, and knowledge distillation increasingly shows that models can be made much smaller and still perform well. Here, we focus on factorized representations of matrices that underpin dense, embedding, and self-attention layers. We use low-rank factorized representation of a reshaped and rearranged original matrix to achieve space efficient and expressive linear layers. We prove that stacking such low-rank layers increases their expressiveness, providing theoretical understanding for their effectiveness in deep networks. In Transformer models, our approach leads to more than ten-fold reduction in the number of total trainable parameters, including embedding, attention, and feed-forward layers, with little degradation in on-task performance. The approach operates out-of-the-box,  replacing each parameter matrix with its compact equivalent while maintaining the architecture of the network.",
    "authors": [
      "Panahi, Aliakbar",
      "Saeedi, Seyran",
      "Arodz, Tom"
    ]
  },
  {
    "id": "09e7655fc1dc8fa7c9d6c4478313d5e6",
    "title": "The Adaptive Doubly Robust Estimator and a Paradox Concerning Logging Policy",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/09e7655fc1dc8fa7c9d6c4478313d5e6-Paper.pdf",
    "abstract": "The doubly robust (DR) estimator, which consists of two nuisance parameters, the conditional mean outcome and the logging policy (the probability of choosing an action), is crucial in causal inference. This paper proposes a DR estimator for dependent samples obtained from adaptive experiments. To obtain an asymptotically normal semiparametric estimator from dependent samples without non-Donsker nuisance estimators, we propose adaptive-fitting as a variant of sample-splitting. We also report an empirical paradox that our proposed DR estimator tends to show better performances compared to other estimators utilizing the true logging policy. While a similar phenomenon is known for estimators with i.i.d. samples, traditional explanations based on asymptotic efficiency cannot elucidate our case with dependent samples. We confirm this hypothesis through simulation studies.",
    "authors": [
      "Kato, Masahiro",
      "McAlinn, Kenichiro",
      "Yasui, Shota"
    ]
  },
  {
    "id": "0a113ef6b61820daa5611c870ed8d5ee",
    "title": "Regularized Softmax Deep Multi-Agent Q-Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0a113ef6b61820daa5611c870ed8d5ee-Paper.pdf",
    "abstract": "Tackling overestimation in $Q$-learning is an important problem that has been extensively studied in single-agent reinforcement learning, but has received comparatively little attention in the multi-agent setting. In this work, we empirically demonstrate that QMIX, a popular $Q$-learning algorithm for cooperative multi-agent reinforcement learning (MARL), suffers from a more severe overestimation in practice than previously acknowledged, and is not mitigated by existing approaches. We rectify this with a novel regularization-based update scheme that penalizes large joint action-values that deviate from a baseline and demonstrate its effectiveness in stabilizing learning. Furthermore, we propose to employ a softmax operator, which we efficiently approximate in a novel way in the multi-agent setting, to further reduce the potential overestimation bias. Our approach, Regularized Softmax (RES) Deep Multi-Agent $Q$-Learning, is general and can be applied to any $Q$-learning based MARL algorithm. We demonstrate that, when applied to QMIX, RES avoids severe overestimation and significantly improves performance, yielding state-of-the-art results in a variety of cooperative multi-agent tasks, including the challenging StarCraft II micromanagement benchmarks.",
    "authors": [
      "Pan, Ling",
      "Rashid, Tabish",
      "Peng, Bei",
      "Huang, Longbo",
      "Whiteson, Shimon"
    ]
  },
  {
    "id": "0a3b5a7a477d359746061d41c3a04fd6",
    "title": "Physics-Aware Downsampling with Deep Learning for Scalable Flood Modeling",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0a3b5a7a477d359746061d41c3a04fd6-Paper.pdf",
    "abstract": "Background. Floods are the most common natural disaster in the world, affecting the lives of hundreds of millions. Flood forecasting is therefore a vitally important endeavor, typically achieved using physical water flow simulations, which rely on accurate terrain elevation maps. However, such simulations, based on solving partial differential equations, are computationally prohibitive on a large scale. This scalability issue is commonly alleviated using a coarse grid representation of the elevation map, though this representation may distort crucial terrain details, leading to significant inaccuracies in the simulation.\\Contributions. We train a deep neural network to perform physics-informed downsampling of the terrain map: we optimize the coarse grid representation of the terrain maps, so that the flood prediction will match the fine grid solution. For the learning process to succeed, we configure a dataset specifically for this task. We demonstrate that with this method, it is possible to achieve a significant reduction in computational cost, while maintaining an accurate solution. A reference implementation accompanies the paper as well as documentation and code for dataset reproduction.",
    "authors": [
      "Giladi, Niv",
      "Ben-Haim, Zvika",
      "Nevo, Sella",
      "Matias, Yossi",
      "Soudry, Daniel"
    ]
  },
  {
    "id": "0a4dc6dae338c9cb08947c07581f77a2",
    "title": "Systematic Generalization with Edge Transformers",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0a4dc6dae338c9cb08947c07581f77a2-Paper.pdf",
    "abstract": "Recent research suggests that systematic generalization in natural language understanding remains a challenge for state-of-the-art neural models such as Transformers and Graph Neural Networks. To tackle this challenge, we propose Edge Transformer, a new model that combines inspiration from Transformers and rule-based symbolic AI. The first key idea in Edge Transformers is to associate vector states with every edge, that is, with every pair of input nodes---as opposed to just every node, as it is done in the Transformer model. The second major innovation is a triangular attention mechanism that updates edge representations in a way that is inspired by unification from logic programming. We evaluate Edge Transformer on compositional generalization benchmarks in relational reasoning, semantic parsing, and dependency parsing. In all three settings, the Edge Transformer outperforms Relation-aware, Universal and classical Transformer baselines.",
    "authors": [
      "Bergen, Leon",
      "O'Donnell, Timothy",
      "Bahdanau, Dzmitry"
    ]
  },
  {
    "id": "0a87257e5308197df43230edf4ad1dae",
    "title": "TransformerFusion: Monocular RGB Scene Reconstruction using Transformers",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0a87257e5308197df43230edf4ad1dae-Paper.pdf",
    "abstract": "We introduce TransformerFusion, a transformer-based 3D scene reconstruction approach. From an input monocular RGB video, the video frames are processed by a transformer network that fuses the observations into a volumetric feature grid representing the scene; this feature grid is then decoded into an implicit 3D scene representation. Key to our approach is the transformer architecture that enables the network to learn to attend to the most relevant image frames for each 3D location in the scene, supervised only by the scene reconstruction task. Features are fused in a coarse-to-fine fashion, storing fine-level features only where needed, requiring lower memory storage and enabling fusion at interactive rates. The feature grid is then decoded to a higher-resolution scene reconstruction, using an MLP-based surface occupancy prediction from interpolated coarse-to-fine 3D features. Our approach results in an accurate surface reconstruction, outperforming state-of-the-art multi-view stereo depth estimation methods, fully-convolutional 3D reconstruction approaches, and approaches using LSTM- or GRU-based recurrent networks for video sequence fusion.",
    "authors": [
      "Bozic, Aljaz",
      "Palafox, Pablo",
      "Thies, Justus",
      "Dai, Angela",
      "Niessner, Matthias"
    ]
  },
  {
    "id": "0a9fdbb17feb6ccb7ec405cfb85222c4",
    "title": "Maximum Likelihood Training of Score-Based Diffusion Models",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0a9fdbb17feb6ccb7ec405cfb85222c4-Paper.pdf",
    "abstract": "Score-based diffusion models synthesize samples by reversing a stochastic process that diffuses data to noise, and are trained by minimizing a weighted combination of score matching losses. The log-likelihood of score-based diffusion models can be tractably computed through a connection to continuous normalizing flows, but log-likelihood is not directly optimized by the weighted combination of score matching losses. We show that for a specific weighting scheme, the objective upper bounds the negative log-likelihood, thus enabling approximate maximum likelihood training of score-based diffusion models. We empirically observe that maximum likelihood training consistently improves the likelihood of score-based diffusion models across multiple datasets, stochastic processes, and model architectures. Our best models achieve negative log-likelihoods of 2.83 and 3.76 bits/dim on CIFAR-10 and ImageNet $32\\times 32$ without any data augmentation, on a par with state-of-the-art autoregressive models on these tasks. ",
    "authors": [
      "Song, Yang",
      "Durkan, Conor",
      "Murray, Iain",
      "Ermon, Stefano"
    ]
  },
  {
    "id": "0af854284f4ab0cfea8fcfd889cbb41a",
    "title": "Global Convergence of Gradient Descent for Asymmetric Low-Rank Matrix Factorization",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0af854284f4ab0cfea8fcfd889cbb41a-Paper.pdf",
    "abstract": "We study the asymmetric low-rank factorization problem:\\[\\min_{\\mathbf{U} \\in \\mathbb{R}^{m \\times d}, \\mathbf{V} \\in \\mathbb{R}^{n \\times d}} \\frac{1}{2}\\|\\mathbf{U}\\mathbf{V}^\\top -\\mathbf{\\Sigma}\\|_F^2\\]where $\\mathbf{\\Sigma}$ is a given matrix of size $m \\times n$ and rank $d$. This is a canonical problem that admits two difficulties in optimization: 1) non-convexity and 2) non-smoothness (due to unbalancedness of $\\mathbf{U}$ and $\\mathbf{V}$). This is also a prototype for more complex problems such as asymmetric matrix sensing and matrix completion. Despite being non-convex and non-smooth, it has been observed empirically that the randomly initialized gradient descent algorithm can solve this problem in polynomial time. Existing theories to explain this phenomenon all require artificial modifications of the algorithm, such as adding noise in each iteration and adding a balancing regularizer to balance the $\\mathbf{U}$ and $\\mathbf{V}$.This paper presents the first proof that shows randomly initialized gradient descent converges to a global minimum of the asymmetric low-rank factorization problem with a polynomial rate. For the proof, we develop 1) a new symmetrization technique to capture the magnitudes of the symmetry and asymmetry, and 2) a quantitative perturbation analysis to approximate matrix derivatives.  We believe both are useful for other related non-convex problems.",
    "authors": [
      "Ye, Tian",
      "Du, Simon S."
    ]
  },
  {
    "id": "0b0b0994d12ad343511adfbfc364256e",
    "title": "Adaptive Data Augmentation on Temporal Graphs",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0b0b0994d12ad343511adfbfc364256e-Paper.pdf",
    "abstract": "Temporal Graph Networks (TGNs) are powerful on modeling temporal graph data based on their increased complexity. Higher complexity carries with it a higher risk of overfitting, which makes TGNs capture random noise instead of essential semantic information. To address this issue, our idea is to transform the temporal graphs using data augmentation (DA) with adaptive magnitudes, so as to effectively augment the input features and preserve the essential semantic information. Based on this idea, we present the MeTA (Memory Tower Augmentation) module: a multi-level module that processes the augmented graphs of different magnitudes on separate levels, and performs message passing across levels to provide adaptively augmented inputs for every prediction. MeTA can be flexibly applied to the training of popular TGNs to improve their effectiveness without increasing their time complexity. To complement MeTA, we propose three DA strategies to realistically model noise by modifying both the temporal and topological features. Empirical results on standard datasets show that MeTA yields significant gains for the popular TGN models on edge prediction and node classification in an efficient manner.",
    "authors": [
      "Wang, Yiwei",
      "Cai, Yujun",
      "Liang, Yuxuan",
      "Ding, Henghui",
      "Wang, Changhu",
      "Bhatia, Siddharth",
      "Hooi, Bryan"
    ]
  },
  {
    "id": "0b0d29e5d5c8a7a25dced6405bd022a9",
    "title": "Regularized Frank-Wolfe for Dense CRFs: Generalizing Mean Field and Beyond",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0b0d29e5d5c8a7a25dced6405bd022a9-Paper.pdf",
    "abstract": "We introduce regularized Frank-Wolfe, a general and effective algorithm for inference and learning of dense conditional random fields (CRFs). The algorithm optimizes a nonconvex continuous relaxation of the CRF inference problem using vanilla Frank-Wolfe with approximate updates, which are equivalent to minimizing a regularized energy function. Our proposed method is a generalization of existing algorithms such as mean field or concave-convex procedure. This perspective not only offers a unified analysis of these algorithms, but also allows an easy way of exploring different variants that potentially yield better performance. We illustrate this in our empirical results on standard semantic segmentation datasets, where several instantiations of our regularized Frank-Wolfe outperform mean field inference, both as a standalone component and as an end-to-end trainable layer in a neural network. We also show that dense CRFs, coupled with our new algorithms, produce significant improvements over strong CNN baselines.",
    "authors": [
      "L\u00ea-Huu, \u0110.Khu\u00ea",
      "Alahari, Karteek"
    ]
  },
  {
    "id": "0b32f1a9efe5edf3dd2f38b0c0052bfe",
    "title": "Terra: Imperative-Symbolic Co-Execution of Imperative Deep Learning Programs",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0b32f1a9efe5edf3dd2f38b0c0052bfe-Paper.pdf",
    "abstract": "Imperative programming allows users to implement their deep neural networks (DNNs) easily and has become an essential part of recent deep learning (DL) frameworks. Recently, several systems have been proposed to combine the usability of imperative programming with the optimized performance of symbolic graph execution. Such systems convert imperative Python DL programs to optimized symbolic graphs and execute them. However, they cannot fully support the usability of imperative programming. For example, if an imperative DL program contains a Python feature with no corresponding symbolic representation (e.g., third-party library calls or unsupported dynamic control flows) they fail to execute the program. To overcome this limitation, we propose Terra, an imperative-symbolic co-execution system that can handle any imperative DL programs while achieving the optimized performance of symbolic graph execution. To achieve this, Terra builds a symbolic graph by decoupling DL operations from Python features. Then, Terra conducts the imperative execution to support all Python features, while delegating the decoupled operations to the symbolic execution. We evaluated Terra\u2019s performance improvement and coverage with ten imperative DL programs for several DNN architectures. The results show that Terra can speed up the execution of all ten imperative DL programs, whereas AutoGraph, one of the state-of-the-art systems, fails to execute five of them.",
    "authors": [
      "Kim, Taebum",
      "Jeong, Eunji",
      "Kim, Geon-Woo",
      "Koo, Yunmo",
      "Kim, Sehoon",
      "Yu, Gyeongin",
      "Chun, Byung-Gon"
    ]
  },
  {
    "id": "0b3f44d9054402de39441e165a4bdfe0",
    "title": "Uniform Sampling over Episode Difficulty",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0b3f44d9054402de39441e165a4bdfe0-Paper.pdf",
    "abstract": "Episodic training is a core ingredient of few-shot learning to train models on tasks with limited labelled data. Despite its success, episodic training remains largely understudied, prompting us to ask the question: what is the best way to sample episodes? In this paper, we first propose a method to approximate episode sampling distributions based on their difficulty. Building on this method, we perform an extensive analysis and find that sampling uniformly over episode difficulty outperforms other sampling schemes, including curriculum and easy-/hard-mining. As the proposed sampling method is algorithm agnostic, we can leverage these insights to improve few-shot learning accuracies across many episodic training algorithms. We demonstrate the efficacy of our method across popular few-shot learning datasets, algorithms, network architectures, and protocols.",
    "authors": [
      "Arnold, S\u00e9bastien",
      "Dhillon, Guneet",
      "Ravichandran, Avinash",
      "Soatto, Stefano"
    ]
  },
  {
    "id": "0b94ce08688c6389ce7b68c52ce3f8c7",
    "title": "Scalable Intervention Target Estimation in Linear Models",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0b94ce08688c6389ce7b68c52ce3f8c7-Paper.pdf",
    "abstract": "This paper considers the problem of estimating the unknown intervention targets in a causal directed acyclic graph from observational and interventional data. The focus is on soft interventions in linear structural equation models (SEMs). Current approaches to causal structure learning either work with known intervention targets or use hypothesis testing to discover the unknown intervention targets even for linear SEMs. This severely limits their scalability and sample complexity. This paper proposes a scalable and efficient algorithm that consistently identifies all intervention targets. The pivotal idea is to estimate the intervention sites from the difference between the precision matrices associated with the observational and interventional datasets. It involves repeatedly estimating such sites in different subsets of variables. The proposed algorithm can be used to also update a given observational Markov equivalence class into the interventional Markov equivalence class. Consistency, Markov equivalency, and sample complexity are established analytically. Finally, simulation results on both real and synthetic data demonstrate the gains of the proposed approach for scalable causal structure recovery. Implementation of the algorithm and the code to reproduce the simulation results are available at \\url{https://github.com/bvarici/intervention-estimation}.",
    "authors": [
      "Varici, Burak",
      "Shanmugam, Karthikeyan",
      "Sattigeri, Prasanna",
      "Tajer, Ali"
    ]
  },
  {
    "id": "0b9b6d6d154e98ce34b3f2e4ef76eae9",
    "title": "Play to Grade: Testing Coding Games as Classifying Markov Decision Process",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0b9b6d6d154e98ce34b3f2e4ef76eae9-Paper.pdf",
    "abstract": "Contemporary coding education often presents students with the task of developing programs that have user interaction and complex dynamic systems, such as mouse based games. While pedagogically compelling, there are no contemporary autonomous methods for providing feedback. Notably, interactive programs are impossible to grade by traditional unit tests. In this paper we formalize the challenge of providing feedback to interactive programs as a task of classifying Markov Decision Processes (MDPs). Each student's program fully specifies an MDP where the agent needs to operate and decide, under reasonable generalization, if the dynamics and reward model of the input MDP should be categorized as correct or broken. We demonstrate that by designing a cooperative objective between an agent and an autoregressive model, we can use the agent to sample differential trajectories from the input MDP that allows a classifier to determine membership: Play to Grade. Our method enables an automatic feedback system for interactive code assignments. We release a dataset of 711,274 anonymized student submissions to a single assignment with hand-coded bug labels to support future research.",
    "authors": [
      "Nie, Allen",
      "Brunskill, Emma",
      "Piech, Chris"
    ]
  },
  {
    "id": "0b9e57c46de934cee33b0e8d1839bfc2",
    "title": "Distributional Reinforcement Learning for Multi-Dimensional Reward Functions",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0b9e57c46de934cee33b0e8d1839bfc2-Paper.pdf",
    "abstract": "A growing trend for value-based reinforcement learning (RL) algorithms is to capture more information than scalar value functions in the value network. One of the most well-known methods in this branch is distributional RL, which models return distribution instead of scalar value. In another line of work, hybrid reward architectures (HRA) in RL have studied to model source-specific value functions for each source of reward, which is also shown to be beneficial in performance. To fully inherit the benefits of distributional RL and hybrid reward architectures, we introduce Multi-Dimensional Distributional DQN (MD3QN), which extends distributional RL to model the joint return distribution from multiple reward sources. As a by-product of joint distribution modeling, MD3QN can capture not only the randomness in returns for each source of reward, but also the rich reward correlation between the randomness of different sources. We prove the convergence for the joint distributional Bellman operator and build our empirical algorithm by minimizing the Maximum Mean Discrepancy between joint return distribution and its Bellman target. In experiments, our method accurately models the joint return distribution in environments with richly correlated reward functions, and outperforms previous RL methods utilizing multi-dimensional reward functions in the control setting.",
    "authors": [
      "Zhang, Pushi",
      "Chen, Xiaoyu",
      "Zhao, Li",
      "Xiong, Wei",
      "Qin, Tao",
      "Liu, Tie-Yan"
    ]
  },
  {
    "id": "0bc10d8a74dbafbf242e30433e83aa56",
    "title": "Differentiable Unsupervised Feature Selection based on a Gated Laplacian",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0bc10d8a74dbafbf242e30433e83aa56-Paper.pdf",
    "abstract": "Scientific observations may consist of a large number of variables (features). Selecting a subset of meaningful features is often crucial for identifying patterns hidden in the ambient space. In this paper, we present a method for unsupervised feature selection, and we demonstrate its advantage in clustering, a common unsupervised task. We propose a differentiable loss that combines a graph Laplacian-based score that favors low-frequency features with a gating mechanism for removing nuisance features. Our method improves upon the naive graph Laplacian score by replacing it with a gated variant computed on a subset of low-frequency features. We identify this subset by learning the parameters of continuously relaxed Bernoulli variables, which gate the entire feature space. We mathematically motivate the proposed approach and demonstrate that it is crucial to compute the graph Laplacian on the gated inputs rather than on the full feature set in the high noise regime. Using several real-world examples, we demonstrate the efficacy and advantage of the proposed approach over leading baselines.",
    "authors": [
      "Lindenbaum, Ofir",
      "Shaham, Uri",
      "Peterfreund, Erez",
      "Svirsky, Jonathan",
      "Casey, Nicolas",
      "Kluger, Yuval"
    ]
  },
  {
    "id": "0bed45bd5774ffddc95ffe500024f628",
    "title": "Smooth Bilevel Programming for Sparse Regularization",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0bed45bd5774ffddc95ffe500024f628-Paper.pdf",
    "abstract": "Iteratively reweighted least square (IRLS) is a popular approach to solve sparsity-enforcing regression problems in machine learning. State of the art approaches are more efficient but typically rely on specific coordinate pruning schemes. In this work, we show how a surprisingly simple re-parametrization of IRLS, coupled with a bilevel resolution (instead of an alternating scheme) is able to achieve top performances on a wide range of sparsity (such as Lasso, group Lasso and trace norm regularizations), regularization strength (including hard constraints), and design matrices (ranging from correlated designs to differential operators). Similarly to IRLS, our method only involves linear systems resolutions, but in sharp contrast, corresponds to the minimization of a smooth function. Despite being non-convex, we show that there is no spurious minima and that saddle points are \"ridable'', so that there always exists a descent direction.  We thus advocate for the use of a BFGS quasi-Newton solver, which makes our approach  simple, robust and efficient. We perform a numerical benchmark of the convergence speed of our algorithm against state of the art solvers for Lasso, group Lasso, trace norm and linearly constrained problems. These results highlight the versatility of our approach, removing the need to use different solvers depending on the specificity of the ML problem under study.",
    "authors": [
      "Poon, Clarice",
      "Peyr\u00e9, Gabriel"
    ]
  },
  {
    "id": "0c0bf917c7942b5a08df71f9da626f97",
    "title": "Grounding Representation Similarity Through Statistical Testing",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0c0bf917c7942b5a08df71f9da626f97-Paper.pdf",
    "abstract": "To understand neural network behavior, recent works quantitatively compare different networks' learned representations using canonical correlation analysis (CCA), centered kernel alignment (CKA), and other dissimilarity measures. Unfortunately, these widely used measures often disagree on fundamental observations, such as whether deep networks differing only in random initialization learn similar representations. These disagreements raise the question: which, if any, of these dissimilarity measures should we believe? We provide a framework to ground this question through a concrete test: measures should have \\emph{sensitivity} to changes that affect functional behavior, and \\emph{specificity} against changes that do not. We quantify this through a variety of functional behaviors including probing accuracy and robustness to distribution shift, and examine changes such as varying random initialization and deleting principal components. We find that current metrics exhibit different weaknesses, note that a classical baseline performs surprisingly well, and highlight settings where all metrics appear to fail, thus providing a challenge set for further improvement.",
    "authors": [
      "Ding, Frances",
      "Denain, Jean-Stanislas",
      "Steinhardt, Jacob"
    ]
  },
  {
    "id": "0c215f194276000be6a6df6528067151",
    "title": "A Consciousness-Inspired Planning Agent for Model-Based Reinforcement Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0c215f194276000be6a6df6528067151-Paper.pdf",
    "abstract": "We present an end-to-end, model-based deep reinforcement learning agent which dynamically attends to relevant parts of its state during planning. The agent uses a bottleneck mechanism over a set-based representation to force the number of entities to which the agent attends at each planning step to be small. In experiments, we investigate the bottleneck mechanism with several sets of customized environments featuring different challenges. We consistently observe that the design allows the planning agents to generalize their learned task-solving abilities in compatible unseen environments by attending to the relevant objects, leading to better out-of-distribution generalization performance.",
    "authors": [
      "Zhao, Mingde",
      "Liu, Zhen",
      "Luan, Sitao",
      "Zhang, Shuyuan",
      "Precup, Doina",
      "Bengio, Yoshua"
    ]
  },
  {
    "id": "0cb929eae7a499e50248a3a78f7acfc7",
    "title": "Reward-Free Model-Based Reinforcement Learning with Linear Function Approximation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0cb929eae7a499e50248a3a78f7acfc7-Paper.pdf",
    "abstract": "We study the model-based reward-free reinforcement learning with linear function approximation for episodic Markov decision processes (MDPs). In this setting, the agent works in two phases. In the exploration phase, the agent interacts with the environment and collects samples without the reward. In the planning phase, the agent is given a specific reward function and uses samples collected from the exploration phase to learn a good policy. We propose a new provably efficient algorithm, called UCRL-RFE under the Linear Mixture MDP assumption, where the transition probability kernel of the MDP can be parameterized by a linear function over certain feature mappings defined on the triplet of state, action, and next state. We show that to obtain an $\\epsilon$-optimal policy for arbitrary reward function, UCRL-RFE needs to sample at most $\\tilde O(H^5d^2\\epsilon^{-2})$ episodes during the exploration phase. Here, $H$ is the length of the episode, $d$ is the dimension of the feature mapping. We also propose a variant of UCRL-RFE using Bernstein-type bonus and show that it needs to sample at most $\\tilde O(H^4d(H + d)\\epsilon^{-2})$ to achieve an $\\epsilon$-optimal policy. By constructing a special class of linear Mixture MDPs, we also prove that for any reward-free algorithm, it needs to sample at least $\\tilde \\Omega(H^2d\\epsilon^{-2})$ episodes to obtain an $\\epsilon$-optimal policy. Our upper bound matches the lower bound in terms of the dependence on $\\epsilon$ and the dependence on $d$ if $H \\ge d$. ",
    "authors": [
      "ZHANG, Weitong",
      "Zhou, Dongruo",
      "Gu, Quanquan"
    ]
  },
  {
    "id": "0cbed40c0d920b94126eaf5e707be1f5",
    "title": "Beltrami Flow and Neural Diffusion on Graphs",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0cbed40c0d920b94126eaf5e707be1f5-Paper.pdf",
    "abstract": "We propose a novel class of graph neural networks based on the discretized Beltrami flow, a non-Euclidean diffusion PDE. In our model, node features are supplemented with  positional encodings derived from the graph topology and jointly evolved by the Beltrami flow,  producing simultaneously continuous feature learning, topology evolution. The resulting model generalizes many popular graph neural networks and achieves state-of-the-art results on several benchmarks. ",
    "authors": [
      "Chamberlain, Benjamin",
      "Rowbottom, James",
      "Eynard, Davide",
      "Di Giovanni, Francesco",
      "Dong, Xiaowen",
      "Bronstein, Michael"
    ]
  },
  {
    "id": "0cd6a652ed1f7811192db1f700c8f0e7",
    "title": "Think Big, Teach Small: Do Language Models Distil Occam\u2019s Razor?",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0cd6a652ed1f7811192db1f700c8f0e7-Paper.pdf",
    "abstract": "Large language models have recently shown a remarkable ability for few-shot learning, including patterns of algorithmic nature. However, it is still an open question to determine what kind of patterns these models can capture and how many examples they need in their prompts. We frame this question as a teaching problem with strong priors, and study whether language models can identify simple algorithmic concepts from small witness sets. In particular, we explore how several GPT architectures, program induction systems and humans perform in terms of the complexity of the concept and the number of additional examples, and how much their behaviour differs. This first joint analysis of language models and machine teaching can address key questions for artificial intelligence and machine learning, such as whether some strong priors, and Occam\u2019s razor in particular, can be distilled from data, making learning from a few examples possible.",
    "authors": [
      "Jaimovitch-Lopez, Gonzalo",
      "Castellano Falc\u00f3n, David",
      "Ferri, Cesar",
      "Hern\u00e1ndez-Orallo, Jos\u00e9"
    ]
  },
  {
    "id": "0cdbb4e65815fbaf79689b15482e7575",
    "title": "Disentangling Identifiable Features from Noisy Data with Structured Nonlinear ICA",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0cdbb4e65815fbaf79689b15482e7575-Paper.pdf",
    "abstract": "We introduce a new general identifiable framework for principled disentanglement referred to as Structured Nonlinear Independent Component Analysis (SNICA). Our contribution is to extend the identifiability theory of deep generative models for a very broad class of structured models. While previous works have shown identifiability for specific classes of time-series models, our theorems extend this to more general temporal structures as well as to models with more complex  structures such as spatial dependencies. In particular, we establish the major result that identifiability for this framework holds even in the presence of noise of unknown distribution. Finally, as an example of our framework's flexibility, we introduce the first nonlinear ICA model for time-series that combines the following very useful properties: it accounts for both nonstationarity and autocorrelation in a fully unsupervised setting;  performs dimensionality reduction;  models hidden states; and  enables principled estimation and inference by variational maximum-likelihood.",
    "authors": [
      "H\u00e4lv\u00e4, Hermanni",
      "Le Corff, Sylvain",
      "Leh\u00e9ricy, Luc",
      "So, Jonathan",
      "Zhu, Yongjie",
      "Gassiat, Elisabeth",
      "Hyvarinen, Aapo"
    ]
  },
  {
    "id": "0cddb7c06f1cd518e1efdc0e20b70c31",
    "title": "Conditionally Parameterized, Discretization-Aware Neural Networks for Mesh-Based Modeling of Physical Systems",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0cddb7c06f1cd518e1efdc0e20b70c31-Paper.pdf",
    "abstract": "Simulations of complex physical systems are typically realized by discretizing partial differential equations (PDEs) on unstructured meshes. While neural networks have recently been explored for the surrogate and reduced order modeling of PDE solutions, they often ignore interactions or hierarchical relations between input features, and process them as concatenated mixtures. We generalize the idea of conditional parameterization -- using trainable functions of input parameters to generate the weights of a neural network, and extend them in a flexible way to encode critical information. Inspired by discretized numerical methods, choices of the parameters include physical quantities and mesh topology features. The functional relation between the modeled features and the parameters is built into the network architecture. The method is implemented on different networks and applied to frontier scientific machine learning tasks including the discovery of unmodeled physics, super-resolution of coarse fields, and the simulation of unsteady flows with chemical reactions. The results show that the conditionally-parameterized networks provide superior performance compared to their traditional counterparts. The CP-GNet - an architecture that can be trained on very few data snapshots - is proposed as the first deep learning model capable of standalone prediction of reacting flows on irregular meshes.",
    "authors": [
      "Xu, Jiayang",
      "Pradhan, Aniruddhe",
      "Duraisamy, Karthikeyan"
    ]
  },
  {
    "id": "0d3180d672e08b4c5312dcdafdf6ef36",
    "title": "USCO-Solver: Solving Undetermined Stochastic Combinatorial Optimization Problems",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0d3180d672e08b4c5312dcdafdf6ef36-Paper.pdf",
    "abstract": "Real-world decision-making systems are often subject to uncertainties that have to be resolved through observational data. Therefore, we are frequently confronted with combinatorial optimization problems of which the objective function is unknown and thus has to be debunked using empirical evidence. In contrast to the common practice that relies on a learning-and-optimization strategy, we consider the regression between combinatorial spaces, aiming to infer high-quality optimization solutions from samples of input-solution pairs -- without the need to learn the objective function. Our main deliverable is a universal solver that is able to handle abstract undetermined stochastic combinatorial optimization problems. For learning foundations, we present learning-error analysis under the PAC-Bayesian framework using a new margin-based analysis. In empirical studies, we demonstrate our design using proof-of-concept experiments, and compare it with other methods that are potentially applicable. Overall, we obtain highly encouraging experimental results for several classic combinatorial problems on both synthetic and real-world datasets.",
    "authors": [
      "Tong, Guangmo"
    ]
  },
  {
    "id": "0d441de75945e5acbc865406fc9a2559",
    "title": "Adaptive Conformal Inference Under Distribution Shift",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0d441de75945e5acbc865406fc9a2559-Paper.pdf",
    "abstract": "We develop methods for forming prediction sets in an online setting where the data generating distribution is allowed to vary over time in an unknown fashion. Our framework builds on ideas from conformal inference to provide a general wrapper that can be combined with any black box method that produces point predictions of the unseen label or estimated quantiles of its distribution. While previous conformal inference methods rely on the assumption that the data are exchangeable, our adaptive approach provably achieves the desired coverage frequency over long-time intervals irrespective of the true data generating process. We accomplish this by modelling the distribution shift as a learning problem in a single parameter whose optimal value is varying over time and must be continuously re-estimated. We test our method, adaptive conformal inference, on two real world datasets and find that its predictions are robust to visible and significant distribution shifts.",
    "authors": [
      "Gibbs, Isaac",
      "Candes, Emmanuel"
    ]
  },
  {
    "id": "0d5a4a5a748611231b945d28436b8ece",
    "title": "Periodic Activation Functions Induce Stationarity",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0d5a4a5a748611231b945d28436b8ece-Paper.pdf",
    "abstract": "Neural network models are known to reinforce hidden data biases, making them unreliable and difficult to interpret. We seek to build models that `know what they do not know' by introducing inductive biases in the function space. We show that periodic activation functions in Bayesian neural networks establish a connection between the prior on the network weights and translation-invariant, stationary Gaussian process priors. Furthermore, we show that this link goes beyond sinusoidal (Fourier) activations by also covering triangular wave and periodic ReLU activation functions. In a series of experiments, we show that periodic activation functions obtain comparable performance for in-domain data and capture sensitivity to perturbed inputs in deep neural networks for out-of-domain detection.",
    "authors": [
      "Meronen, Lassi",
      "Trapp, Martin",
      "Solin, Arno"
    ]
  },
  {
    "id": "0d5bd023a3ee11c7abca5b42a93c4866",
    "title": "Towards Optimal Strategies for Training Self-Driving Perception Models in Simulation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0d5bd023a3ee11c7abca5b42a93c4866-Paper.pdf",
    "abstract": "Autonomous driving relies on a huge volume of real-world data to be labeled to high precision.  Alternative solutions seek to exploit driving simulators that can generate large amounts of labeled data with a plethora of content variations. However, the domain gap between the synthetic and real data remains,  raising the following important question: What are the best way to utilize a self-driving simulator for perception tasks?. In this work, we build on top of recent advances in domain-adaptation theory, and from this perspective, propose ways to minimize the reality gap. We primarily focus on the use of labels in the synthetic domain alone. Our approach introduces both a principled way to learn neural-invariant representations and a  theoretically inspired view on how to sample the data from the simulator. Our method is easy to implement in practice as it is agnostic of the network architecture and the choice of the simulator.   We showcase our approach on the bird's-eye-view vehicle segmentation task with multi-sensor data (cameras, lidar) using an open-source simulator (CARLA), and evaluate the entire framework on a real-world dataset (nuScenes). Last but not least, we show what types of variations (e.g. weather conditions, number of assets, map design and color diversity) matter to perception networks when trained with driving simulators, and which ones can be compensated for with our domain adaptation technique. ",
    "authors": [
      "Acuna, David",
      "Philion, Jonah",
      "Fidler, Sanja"
    ]
  },
  {
    "id": "0d7363894acdee742caf7fe4e97c4d49",
    "title": "KS-GNN: Keywords Search over Incomplete Graphs via Graphs Neural Network",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0d7363894acdee742caf7fe4e97c4d49-Paper.pdf",
    "abstract": "Keyword search is a fundamental task to retrieve information that is the most relevant to the query keywords. Keyword search over graphs aims to find subtrees or subgraphs containing all query keywords ranked according to some criteria. Existing studies all assume that the graphs have complete information. However, real-world graphs may contain some missing information (such as edges or keywords), thus making the problem much more challenging. To solve the problem of keyword search over incomplete graphs, we propose a novel model named KS-GNN based on the graph neural network and the auto-encoder. By considering the latent relationships and the frequency of different keywords, the proposed KS-GNN aims to alleviate the effect of missing information and is able to learn low-dimensional representative node embeddings that preserve both graph structure and keyword features. Our model can effectively answer keyword search queries with linear time complexity over incomplete graphs. The experiments on four real-world datasets show that our model consistently achieves better performance than state-of-the-art baseline methods in graphs having missing information.",
    "authors": [
      "HAO, YU",
      "Cao, Xin",
      "Sheng, Yufan",
      "Fang, Yixiang",
      "Wang, Wei"
    ]
  },
  {
    "id": "0d8080853a54f8985276b0130266a657",
    "title": "Reconstruction for Powerful Graph Representations",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0d8080853a54f8985276b0130266a657-Paper.pdf",
    "abstract": "Graph neural networks (GNNs) have limited expressive power, failing to represent many graph classes correctly. While more expressive graph representation learning (GRL) alternatives can distinguish some of these classes, they are significantly harder to implement, may not scale well, and have not been shown to outperform well-tuned GNNs in real-world tasks. Thus, devising simple, scalable, and expressive GRL architectures that also achieve real-world improvements remains an open challenge. In this work, we show the extent to which graph reconstruction---reconstructing a graph from its subgraphs---can mitigate the theoretical and practical problems currently faced by GRL architectures. First, we leverage graph reconstruction to build two new classes of expressive graph representations. Secondly, we show how graph reconstruction boosts the expressive power of any GNN architecture while being a (provably) powerful inductive bias for invariances to vertex removals. Empirically,  we show how reconstruction can boost GNN's expressive power---while maintaining its invariance to permutations of the vertices---by solving seven graph property tasks not solvable by the original GNN. Further, we demonstrate how it boosts state-of-the-art GNN's performance across nine real-world benchmark datasets.",
    "authors": [
      "Cotta, Leonardo",
      "Morris, Christopher",
      "Ribeiro, Bruno"
    ]
  },
  {
    "id": "0d924f0e6b3fd0d91074c22727a53966",
    "title": "Revealing and Protecting Labels in Distributed Training",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0d924f0e6b3fd0d91074c22727a53966-Paper.pdf",
    "abstract": "Distributed learning paradigms such as federated learning often involve transmission of model updates, or gradients, over a network, thereby avoiding transmission of private data. However, it is possible for sensitive information about the training data to be revealed from such gradients. Prior works have demonstrated that labels can be revealed analytically from the last layer of certain models (e.g., ResNet), or they can be reconstructed jointly with model inputs by using Gradients Matching [Zhu et al.] with additional knowledge about the current state of the model. In this work, we propose a method to discover the set of labels of training samples from only the gradient of the last layer and the id to label mapping. Our method is applicable to a wide variety of model architectures across multiple domains. We demonstrate the effectiveness of our method for model training in two domains - image classification, and automatic speech recognition. Furthermore, we show that existing reconstruction techniques improve their efficacy when used in conjunction with our method. Conversely, we demonstrate that gradient quantization and sparsification can significantly reduce the success of the attack.",
    "authors": [
      "Dang, Trung",
      "Thakkar, Om",
      "Ramaswamy, Swaroop",
      "Mathews, Rajiv",
      "Chin, Peter",
      "Beaufays, Fran\u00e7oise"
    ]
  },
  {
    "id": "0db2e204010400f5c506620adcd1ae68",
    "title": "Solving Graph-based Public Goods Games with Tree Search and Imitation Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0db2e204010400f5c506620adcd1ae68-Paper.pdf",
    "abstract": "Public goods games represent insightful settings for studying incentives for individual agents to make contributions that, while costly for each of them, benefit the wider society. In this work, we adopt the perspective of a central planner with a global view of a network of self-interested agents and the goal of maximizing some desired property in the context of a best-shot public goods game. Existing algorithms for this known NP-complete problem find solutions that are sub-optimal and cannot optimize for criteria other than social welfare.In order to efficiently solve public goods games, our proposed method directly exploits the correspondence between equilibria and the Maximal Independent Set (mIS) structural property of graphs. In particular, we define a Markov Decision Process which incrementally generates an mIS, and adopt a planning method to search for equilibria, outperforming existing methods. Furthermore, we devise a graph imitation learning technique that uses demonstrations of the search to obtain a graph neural network parametrized policy which quickly generalizes to unseen game instances. Our evaluation results show that this policy is able to reach 99.5\\% of the performance of the planning method while being three orders of magnitude faster to evaluate on the largest graphs tested. The methods presented in this work can be applied to a large class of public goods games of potentially high societal impact and more broadly to other graph combinatorial optimization problems.",
    "authors": [
      "Darvariu, Victor-Alexandru",
      "Hailes, Stephen",
      "Musolesi, Mirco"
    ]
  },
  {
    "id": "0dd1bc593a91620daecf7723d2235624",
    "title": "Stochastic Optimization of Areas Under Precision-Recall Curves with Provable Convergence",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0dd1bc593a91620daecf7723d2235624-Paper.pdf",
    "abstract": "Areas under ROC (AUROC) and precision-recall curves (AUPRC) are common metrics for evaluating classification performance for imbalanced problems. Compared with AUROC, AUPRC is a more appropriate metric for highly imbalanced datasets. While stochastic optimization of AUROC has been studied extensively, principled stochastic optimization of AUPRC has been rarely explored. In this work, we propose a principled technical method to optimize AUPRC for deep learning. Our approach is based on maximizing the averaged precision (AP), which is an unbiased point estimator of AUPRC. We cast the objective into a sum of dependent compositional functions with inner functions dependent on random variables of the outer level. We propose efficient adaptive and non-adaptive stochastic algorithms named SOAP with provable convergence guarantee under mild conditions by leveraging recent advances in stochastic compositional optimization. Extensive experimental results on image and graph datasets demonstrate that our proposed method outperforms prior methods on imbalanced problems in terms of AUPRC. To the best of our knowledge, our work represents the first attempt to optimize AUPRC with provable convergence. The SOAP has been implemented in the libAUC library at https://libauc.org/.",
    "authors": [
      "Qi, Qi",
      "Luo, Youzhi",
      "Xu, Zhao",
      "Ji, Shuiwang",
      "Yang, Tianbao"
    ]
  },
  {
    "id": "0dd6049f5fa537d41753be6d37859430",
    "title": "Transfer Learning of Graph Neural Networks with Ego-graph Information Maximization",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0dd6049f5fa537d41753be6d37859430-Paper.pdf",
    "abstract": "Graph neural networks (GNNs) have achieved superior performance in various applications, but training dedicated GNNs can be costly for large-scale graphs. Some recent work started to study the pre-training of GNNs. However, none of them provide theoretical insights into the design of their frameworks, or clear requirements and guarantees towards their transferability. In this work, we establish a theoretically grounded and practically useful framework for the transfer learning of GNNs. Firstly, we propose a novel view towards the essential graph information and advocate the capturing of it as the goal of transferable GNN training, which motivates the design of EGI (Ego-Graph Information maximization) to analytically achieve this goal. Secondly,when node features are structure-relevant, we conduct an analysis of EGI transferability regarding the difference between the local graph Laplacians of the source and target graphs. We conduct controlled synthetic experiments to directly justify our theoretical conclusions. Comprehensive experiments on two real-world network datasets show consistent results in the analyzed setting of direct-transfering, while those on large-scale knowledge graphs show promising results in the more practical setting of transfering with fine-tuning.",
    "authors": [
      "Zhu, Qi",
      "Yang, Carl",
      "Xu, Yidan",
      "Wang, Haonan",
      "Zhang, Chao",
      "Han, Jiawei"
    ]
  },
  {
    "id": "0dfd8a39e2a5dd536c185e19a804a73b",
    "title": "You are caught stealing my winning lottery ticket! Making a lottery ticket claim its ownership",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0dfd8a39e2a5dd536c185e19a804a73b-Paper.pdf",
    "abstract": "Despite tremendous success in many application scenarios, the training and inference costs of using deep learning are also rapidly increasing over time. The lottery ticket hypothesis (LTH) emerges as a promising framework to leverage a special sparse subnetwork (i.e., $\\textit{winning ticket}$) instead of a full model for both training and inference, that can lower both costs without sacrificing the performance. The main resource bottleneck of LTH is however the extraordinary cost to find the sparse mask of the winning ticket. That makes the found winning ticket become a valuable asset to the owners, highlighting the necessity of protecting its copyright. Our setting adds a new dimension to the recently soaring interest in protecting against the intellectual property (IP) infringement of deep models and verifying their ownerships, since they take owners' massive/unique resources to develop or train. While existing methods explored encrypted weights or predictions, we investigate a unique way to leverage sparse topological information to perform $\\textit{lottery verification}$, by developing several graph-based signatures that can be embedded as credentials. By further combining trigger set-based methods, our proposal can work in both white-box and black-box verification scenarios. Through extensive experiments, we demonstrate the effectiveness of lottery verification in diverse models (ResNet-20, ResNet-18, ResNet-50) on CIFAR-10 and CIFAR-100. Specifically, our verification is shown to be robust to removal attacks such as model fine-tuning and pruning, as well as several ambiguity attacks. Our codes are available at https://github.com/VITA-Group/NO-stealing-LTH.",
    "authors": [
      "Chen, Xuxi",
      "Chen, Tianlong",
      "Zhang, Zhenyu",
      "Wang, Zhangyang"
    ]
  },
  {
    "id": "0e105949d99a32ca1751703e94ece601",
    "title": "Complexity Lower Bounds for Nonconvex-Strongly-Concave Min-Max Optimization",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0e105949d99a32ca1751703e94ece601-Paper.pdf",
    "abstract": "We provide a first-order oracle complexity lower bound for finding stationary points of min-max optimization problems where the objective function is smooth, nonconvex in the minimization variable, and strongly concave in the maximization variable. We establish a lower bound of $\\Omega\\left(\\sqrt{\\kappa}\\epsilon^{-2}\\right)$ for deterministic oracles, where $\\epsilon$ defines the level of approximate stationarity and $\\kappa$ is the condition number. Our lower bound matches the best existing upper bound in the $\\epsilon$ and $\\kappa$ dependence up to logarithmic factors. For stochastic oracles, we provide a lower bound of $\\Omega\\left(\\sqrt{\\kappa}\\epsilon^{-2} + \\kappa^{1/3}\\epsilon^{-4}\\right)$. It suggests that there is a gap between the best existing upper bound $\\mathcal{O}(\\kappa^3 \\epsilon^{-4})$ and our lower bound in the condition number dependence. ",
    "authors": [
      "Li, Haochuan",
      "Tian, Yi",
      "Zhang, Jingzhao",
      "Jadbabaie, Ali"
    ]
  },
  {
    "id": "0e1ebad68af7f0ae4830b7ac92bc3c6f",
    "title": "Early-stopped neural networks are consistent",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0e1ebad68af7f0ae4830b7ac92bc3c6f-Paper.pdf",
    "abstract": "This work studies the behavior of shallow ReLU networks trained with the logistic loss via gradient descent on binary classification data where the underlying data distribution is general, and the (optimal) Bayes risk is not necessarily zero.  In this setting, it is shown that gradient descent with early stopping achieves population risk arbitrarily close to optimal in terms of not just logistic and misclassification losses, but also in terms of calibration, meaning the sigmoid mapping of its outputs approximates the true underlying conditional distribution arbitrarily finely.  Moreover, the necessary iteration, sample, and architectural complexities of this analysis all scale naturally with a certain complexity measure of the true conditional model.  Lastly, while it is not shown that early stopping is necessary, it is shown that any classifier satisfying a basic local interpolation property is inconsistent.",
    "authors": [
      "Ji, Ziwei",
      "Li, Justin",
      "Telgarsky, Matus"
    ]
  },
  {
    "id": "0e4f5cc9f4f3f7f1651a6b9f9214e5b1",
    "title": "NxMTransformer: Semi-Structured Sparsification for Natural Language Understanding via ADMM",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0e4f5cc9f4f3f7f1651a6b9f9214e5b1-Paper.pdf",
    "abstract": "Natural Language Processing (NLP) has recently achieved great success by using huge pre-trained Transformer networks. However, these models often contain hundreds of millions or even billions of parameters, bringing challenges to online deployment due to latency constraints. Recently, hardware manufacturers have introduced dedicated hardware for NxM sparsity to provide the flexibility of unstructured pruning with the runtime efficiency of structured approaches. NxM sparsity permits arbitrarily selecting M parameters to retain from a contiguous group of N in the dense representation. However, due to the extremely high complexity of pre-trained models, the standard sparse fine-tuning techniques often fail to generalize well on downstream tasks, which have limited data resources. To address such an issue in a principled manner, we introduce a new learning framework, called NxMTransformer, to induce NxM semi-structured sparsity on pretrained language models for natural language understanding to obtain better performance. In particular, we propose to formulate the NxM sparsity as a constrained optimization problem and use Alternating Direction Method of Multipliers (ADMM) to optimize the downstream tasks while taking the underlying hardware constraints into consideration. ADMM decomposes the NxM sparsification problem into two sub-problems that can be solved sequentially, generating sparsified Transformer networks that achieve high accuracy while being able to effectively execute on newly released hardware. We apply our approach to a wide range of NLP tasks, and our proposed method is able to achieve 1.7 points higher accuracy in GLUE score than current best practices. Moreover, we perform detailed analysis on our approach and shed light on how ADMM affects fine-tuning accuracy for downstream tasks. Finally, we illustrate how NxMTransformer achieves additional performance improvement with knowledge distillation based methods.",
    "authors": [
      "Holmes, Connor",
      "Zhang, Minjia",
      "He, Yuxiong",
      "Wu, Bo"
    ]
  },
  {
    "id": "0e65972dce68dad4d52d063967f0a705",
    "title": "Reliable Decisions with Threshold Calibration",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0e65972dce68dad4d52d063967f0a705-Paper.pdf",
    "abstract": "Decision makers rely on probabilistic forecasts to predict the loss of different decision rules before deployment. When the forecasted probabilities match the true frequencies, predicted losses will be accurate. Although perfect forecasts are typically impossible, probabilities can be calibrated to match the true frequencies on average. However, we find that this \\textit{average} notion of calibration, which is typically used in practice, does not necessarily guarantee accurate decision loss prediction. Specifically in the regression setting, the loss of threshold decisions, which are decisions based on whether the forecasted outcome falls above or below a cutoff, might not be predicted accurately. We propose a stronger notion of calibration called threshold calibration, which is exactly the condition required to ensure that decision loss is predicted accurately for threshold decisions. We provide an efficient algorithm which takes an uncalibrated forecaster as input and provably outputs a threshold-calibrated forecaster. Our procedure allows downstream decision makers to confidently estimate the loss of any threshold decision under any threshold loss function. Empirically, threshold calibration improves decision loss prediction without compromising on the quality of the decisions in two real-world settings: hospital scheduling decisions and resource allocation decisions.",
    "authors": [
      "Sahoo, Roshni",
      "Zhao, Shengjia",
      "Chen, Alyssa",
      "Ermon, Stefano"
    ]
  },
  {
    "id": "0e674a918ebca3f78bfe02e2f387689d",
    "title": "End-to-End Weak Supervision",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0e674a918ebca3f78bfe02e2f387689d-Paper.pdf",
    "abstract": "Aggregating multiple sources of weak supervision (WS) can ease the data-labeling bottleneck prevalent in many machine learning applications, by replacing the tedious manual collection of ground truth labels. Current state of the art approaches that do not use any labeled training data, however, require two separate modeling steps: Learning a probabilistic latent variable model based on the WS sources -- making assumptions that rarely hold in practice -- followed by downstream model training. Importantly, the first step of modeling does not consider the performance of the downstream model.To address these caveats we propose an end-to-end approach for directly learning the downstream model by maximizing its agreement with probabilistic labels generated by reparameterizing previous probabilistic posteriors with a neural network. Our results show improved performance over prior work in terms of end model performance on downstream test sets, as well as in terms of improved robustness to dependencies among weak supervision sources. ",
    "authors": [
      "R\u00fchling Cachay, Salva",
      "Boecking, Benedikt",
      "Dubrawski, Artur"
    ]
  },
  {
    "id": "0e7c7d6c41c76b9ee6445ae01cc0181d",
    "title": "Shift Invariance Can Reduce Adversarial Robustness",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0e7c7d6c41c76b9ee6445ae01cc0181d-Paper.pdf",
    "abstract": "Shift invariance is a critical property of CNNs that improves performance on classification.  However, we show that invariance to circular shifts can also lead to greater sensitivity to adversarial attacks.  We first characterize the margin between classes when a shift-invariant {\\em linear} classifier is used. We show that the margin can only depend on the DC component of the signals.  Then, using results about infinitely wide networks, we show that in some simple cases, fully connected and shift-invariant neural networks produce linear decision boundaries.  Using this, we prove that shift invariance in neural networks produces adversarial examples for the simple case of two classes, each consisting of a single image with a black or white dot on a gray background.  This is more than a curiosity; we show empirically that with real datasets and realistic architectures, shift invariance reduces adversarial robustness.  Finally, we describe initial experiments using synthetic data to probe the source of this connection.",
    "authors": [
      "Singla, Vasu",
      "Ge, Songwei",
      "Ronen, Basri",
      "Jacobs, David"
    ]
  },
  {
    "id": "0e900ad84f63618452210ab8baae0218",
    "title": "Wisdom of the Crowd Voting: Truthful Aggregation of Voter Information and Preferences",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0e900ad84f63618452210ab8baae0218-Paper.pdf",
    "abstract": "We consider two-alternative elections where voters' preferences depend on a state variable that is not directly observable. Each voter receives a private signal that is correlated to the state variable. As a special case, our model captures the common scenario where voters can be categorized into three types: those who always prefer one alternative, those who always prefer the other, and those contingent voters whose preferences depends on the state.  In this setting, even if every voter is a contingent voter, agents voting according to their private information need not result in the adoption of the universally preferred alternative, because the signals can be systematically biased.We present a mechanism that elicits and aggregates the private signals from the voters, and outputs the alternative that is favored by the majority.  In particular, voters truthfully reporting their signals forms a strong Bayes Nash equilibrium (where no coalition of voters can deviate and receive a better outcome).",
    "authors": [
      "Schoenebeck, Grant",
      "Tao, Biaoshuai"
    ]
  },
  {
    "id": "0e915db6326b6fb6a3c56546980a8c93",
    "title": "Replay-Guided Adversarial Environment Design",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0e915db6326b6fb6a3c56546980a8c93-Paper.pdf",
    "abstract": "Deep reinforcement learning (RL) agents may successfully generalize to new settings if trained on an appropriately diverse set of environment and task configurations. Unsupervised Environment Design (UED) is a promising self-supervised RL paradigm, wherein the free parameters of an underspecified environment are automatically adapted during training to the agent's capabilities, leading to the emergence of diverse training environments. Here, we cast Prioritized Level Replay (PLR), an empirically successful but theoretically unmotivated method that selectively samples randomly-generated training levels, as UED. We argue that by curating completely random levels, PLR, too, can generate novel and complex levels for effective training. This insight reveals a natural class of UED methods we call Dual Curriculum Design (DCD). Crucially, DCD includes both PLR and a popular UED algorithm, PAIRED, as special cases and inherits similar theoretical guarantees. This connection allows us to develop novel theory for PLR, providing a version with a robustness guarantee at Nash equilibria. Furthermore, our theory suggests a highly counterintuitive improvement to PLR: by stopping the agent from updating its policy on uncurated levels (training on less data), we can improve the convergence to Nash equilibria. Indeed, our experiments confirm that our new method, PLR$^{\\perp}$, obtains better results on a suite of out-of-distribution, zero-shot transfer tasks, in addition to demonstrating that PLR$^{\\perp}$ improves the performance of PAIRED, from which it inherited its theoretical framework.",
    "authors": [
      "Jiang, Minqi",
      "Dennis, Michael",
      "Parker-Holder, Jack",
      "Foerster, Jakob",
      "Grefenstette, Edward",
      "Rockt\u00e4schel, Tim"
    ]
  },
  {
    "id": "0e98aeeb54acf612b9eb4e48a269814c",
    "title": "There Is No Turning Back: A Self-Supervised Approach for Reversibility-Aware Reinforcement Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0e98aeeb54acf612b9eb4e48a269814c-Paper.pdf",
    "abstract": "We propose to learn to distinguish reversible from irreversible actions for better informed decision-making in Reinforcement Learning (RL). From theoretical considerations, we show that approximate reversibility can be learned through a simple surrogate task: ranking randomly sampled trajectory events in chronological order. Intuitively, pairs of events that are always observed in the same order are likely to be separated by an irreversible sequence of actions. Conveniently, learning the temporal order of events can be done in a fully self-supervised way, which we use to estimate the reversibility of actions from experience, without any priors.We propose two different strategies that incorporate reversibility in RL agents, one strategy for exploration (RAE) and one strategy for control (RAC). We demonstrate the potential of reversibility-aware agents in several environments, including the challenging Sokoban game. In synthetic tasks, we show that we can learn control policies that never fail and reduce to zero the side-effects of interactions, even without access to the reward function.",
    "authors": [
      "Grinsztajn, Nathan",
      "Ferret, Johan",
      "Pietquin, Olivier",
      "preux, philippe",
      "Geist, Matthieu"
    ]
  },
  {
    "id": "0e9b734aa25ca8096cb7b56dc0dd8929",
    "title": "Learning to Execute: Efficient Learning of Universal Plan-Conditioned Policies in Robotics",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0e9b734aa25ca8096cb7b56dc0dd8929-Paper.pdf",
    "abstract": "Applications of Reinforcement Learning (RL) in robotics are often limited by high data demand. On the other hand, approximate models are readily available in many robotics scenarios, making model-based approaches like planning a data-efficient alternative. Still, the performance of these methods suffers if the model is imprecise or wrong. In this sense, the respective strengths and weaknesses of RL and model-based planners are complementary. In the present work, we investigate how both approaches can be integrated into one framework that combines their strengths. We introduce Learning to Execute (L2E), which leverages information contained in approximate plans to learn universal policies that are conditioned on plans. In our robotic manipulation experiments, L2E exhibits increased performance when compared to pure RL, pure planning, or baseline methods combining learning and planning.",
    "authors": [
      "Schubert, Ingmar",
      "Driess, Danny",
      "Oguz, Ozgur S.",
      "Toussaint, Marc"
    ]
  },
  {
    "id": "0ebcc77dc72360d0eb8e9504c78d38bd",
    "title": "Self-Diagnosing GAN: Diagnosing Underrepresented Samples in Generative Adversarial Networks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0ebcc77dc72360d0eb8e9504c78d38bd-Paper.pdf",
    "abstract": "Despite remarkable performance in producing realistic samples, Generative Adversarial Networks (GANs) often produce low-quality samples near low-density regions of the data manifold, e.g., samples of minor groups. Many techniques have been developed to improve the quality of generated samples, either by post-processing generated samples or by pre-processing the empirical data distribution, but at the cost of reduced diversity. To promote diversity in sample generation without degrading the overall quality, we propose a simple yet effective method to diagnose and emphasize underrepresented samples during training of a GAN. The main idea is to use the statistics of the discrepancy between the data distribution and the model distribution at each data instance. Based on the observation that the underrepresented samples have a high average discrepancy or high variability in discrepancy, we propose a method to emphasize those samples during training of a GAN. Our experimental results demonstrate that the proposed method improves GAN performance on various datasets, and it is especially effective in improving the quality and diversity of sample generation for minor groups.",
    "authors": [
      "Lee, Jinhee",
      "Kim, Haeri",
      "Hong, Youngkyu",
      "Chung, Hye Won"
    ]
  },
  {
    "id": "0ec04cb3912c4f08874dd03716f80df1",
    "title": "Online Multi-Armed Bandits with Adaptive Inference",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0ec04cb3912c4f08874dd03716f80df1-Paper.pdf",
    "abstract": "During online decision making in Multi-Armed Bandits (MAB), one needs to conduct inference on the true mean reward of each arm based on data collected so far at each step. However, since the arms are adaptively selected--thereby yielding non-iid data--conducting inference accurately is not straightforward. In particular, sample averaging, which is used in the family of UCB and Thompson sampling (TS) algorithms, does not provide a good choice as it suffers from bias and a lack of good statistical properties (e.g.  asymptotic normality). Our thesis in this paper is that more sophisticated inference schemes that take into account the adaptive nature of the sequentially collected data can unlock further performance gains, even though both UCB and TS type algorithms are optimal in the worst case. In particular, we propose a variant of TS-style algorithms--which we call doubly adaptive TS--that leverages recent advances in causal inference and adaptively reweights the terms of a doubly robust estimator on the true mean reward of each arm. Through 20 synthetic domain experiments and a semi-synthetic experiment based on data from an A/B test of a web service, we demonstrate that using an adaptive inferential scheme (while still retaining the exploration efficacy of TS) provides clear benefits in online decision making: the proposed DATS algorithm has superior empirical performance to existing baselines (UCB and TS) in terms of regret and sample complexity in identifying the best arm. In addition, we also provide a finite-time regret bound of doubly adaptive TS that matches (up to log factors) those of UCB and TS algorithms, thereby establishing that its improved practical benefits do not come at the expense of worst-case suboptimality. ",
    "authors": [
      "Dimakopoulou, Maria",
      "Ren, Zhimei",
      "Zhou, Zhengyuan"
    ]
  },
  {
    "id": "0ed8861dc36bee580d100f91283d0559",
    "title": "Efficient Truncated Linear Regression with Unknown Noise Variance",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0ed8861dc36bee580d100f91283d0559-Paper.pdf",
    "abstract": "Truncated linear regression is a classical challenge in Statistics, wherein a label, $y = w^T x + \\varepsilon$, and its corresponding feature vector, $x \\in \\mathbb{R}^k$, are only observed if the label falls in some subset $S \\subseteq \\mathbb{R}$; otherwise the existence of the pair $(x, y)$ is hidden from observation. Linear regression with truncated observations has remained a challenge, in its general form, since the early works of [Tobin'58, Amemiya '73]. When the distribution of the error is normal with known variance, recent work of [Daskalakis et al. '19] provides computationally and statistically efficient estimators of the linear model, $w$. In this paper, we provide the first computationally and statistically efficient estimators for truncated linear regression when the noise variance is unknown, estimating both the linear model and the variance of the noise. Our estimator is based on an efficient implementation of Projected Stochastic Gradient Descent on the negative log-likelihood of the truncated sample. Importantly, we show that the error of our estimates is asymptotically normal, and we use this to provide explicit confidence regions for our estimates.",
    "authors": [
      "Daskalakis, Constantinos",
      "Stefanou, Patroklos",
      "Yao, Rui",
      "Zampetakis, Emmanouil"
    ]
  },
  {
    "id": "0f2818101a7ac4b96ceeba38de4b934c",
    "title": "Breaking the Dilemma of Medical Image-to-image Translation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0f2818101a7ac4b96ceeba38de4b934c-Paper.pdf",
    "abstract": "Supervised Pix2Pix and unsupervised Cycle-consistency are two modes that dominate the field of medical image-to-image translation. However, neither modes are ideal. The Pix2Pix mode has excellent performance. But it requires paired and well pixel-wise aligned images, which may not always be achievable due to respiratory motion or anatomy change between times that paired images are acquired. The Cycle-consistency mode is less stringent with training data and works well on unpaired or misaligned images. But its performance may not be optimal. In order to break the dilemma of the existing modes, we propose a new unsupervised mode called RegGAN for medical image-to-image translation. It is based on the theory of \"loss-correction\". In RegGAN, the misaligned\u00a0target images are considered as noisy labels\u00a0and the generator is trained with an additional registration network to fit the misaligned noise distribution adaptively. The goal is to search for the common optimal solution to both image-to-image translation and registration tasks. We incorporated RegGAN into a few state-of-the-art image-to-image translation methods and demonstrated that RegGAN could be easily combined with these methods to improve their performances. Such as a simple CycleGAN in our mode surpasses latest NICEGAN even though using less network parameters. Based on our results, RegGAN outperformed both Pix2Pix on aligned data and Cycle-consistency on misaligned or unpaired data. RegGAN is insensitive to noises which makes it a better choice for a wide range of scenarios, especially for medical image-to-image translation tasks in which well pixel-wise aligned data are not available. Code and dataset are available at https://github.com/Kid-Liet/Reg-GAN.",
    "authors": [
      "Kong, Lingke",
      "Lian, Chenyu",
      "Huang, Detian",
      "li, zhenjiang",
      "Hu, Yanle",
      "Zhou, Qichao"
    ]
  },
  {
    "id": "0f3d014eead934bbdbacb62a01dc4831",
    "title": "Temporally Abstract Partial Models",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0f3d014eead934bbdbacb62a01dc4831-Paper.pdf",
    "abstract": "Humans and animals have the ability to reason and make predictions about different courses of action at many time scales. In reinforcement learning, option models (Sutton, Precup \\& Singh, 1999; Precup, 2000) provide the framework for this kind of temporally abstract prediction and reasoning. Natural intelligent agents are also able to focus their attention on courses of action that are relevant or feasible in a given situation, sometimes termed affordable actions. In this paper, we define a notion of affordances for options, and develop temporally abstract partial option models, that take into account the fact that an option might be affordable only in certain situations. We analyze the trade-offs between estimation and approximation error in planning and learning when using such models, and identify some interesting special cases. Additionally, we empirically demonstrate the ability to learn both affordances and partial option models online resulting in improved sample efficiency and planning time in the Taxi domain.",
    "authors": [
      "Khetarpal, Khimya",
      "Ahmed, Zafarali",
      "Comanici, Gheorghe",
      "Precup, Doina"
    ]
  },
  {
    "id": "0f49c89d1e7298bb9930789c8ed59d48",
    "title": "TransMatcher: Deep Image Matching Through Transformers for Generalizable Person Re-identification",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0f49c89d1e7298bb9930789c8ed59d48-Paper.pdf",
    "abstract": "Transformers have recently gained increasing attention in computer vision. However, existing studies mostly use Transformers for feature representation learning, e.g. for image classification and dense predictions, and the generalizability of Transformers is unknown. In this work, we further investigate the possibility of applying Transformers for image matching and metric learning given pairs of images. We find that the Vision Transformer (ViT) and the vanilla Transformer with decoders are not adequate for image matching due to their lack of image-to-image attention. Thus, we further design two naive solutions, i.e. query-gallery concatenation in ViT, and query-gallery cross-attention in the vanilla Transformer. The latter improves the performance, but it is still limited. This implies that the attention mechanism in Transformers is primarily designed for global feature aggregation, which is not naturally suitable for image matching. Accordingly, we propose a new simplified decoder, which drops the full attention implementation with the softmax weighting, keeping only the query-key similarity computation. Additionally, global max pooling and a multilayer perceptron (MLP) head are applied to decode the matching result. This way, the simplified decoder is computationally more efficient, while at the same time more effective for image matching. The proposed method, called TransMatcher, achieves state-of-the-art performance in generalizable person re-identification, with up to 6.1% and 5.7% performance gains in Rank-1 and mAP, respectively, on several popular datasets. Code is available at https://github.com/ShengcaiLiao/QAConv.",
    "authors": [
      "Liao, Shengcai",
      "Shao, Ling"
    ]
  },
  {
    "id": "0f65caf0a7d00afd2b87c028e88fe931",
    "title": "Multi-Objective SPIBB: Seldonian Offline Policy Improvement with Safety Constraints in Finite MDPs",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0f65caf0a7d00afd2b87c028e88fe931-Paper.pdf",
    "abstract": "We study the problem of Safe Policy Improvement (SPI) under constraints in the offline Reinforcement Learning (RL) setting. We consider the scenario where: (i) we have a dataset collected under a known baseline policy, (ii) multiple reward signals are received from the environment inducing as many objectives to optimize. We present an SPI formulation for this RL setting that takes into account the preferences of the algorithm\u2019s user for handling the trade-offs for different reward signals while ensuring that the new policy performs at least as well as the baseline policy along each individual objective. We build on traditional SPI algorithms and propose a novel method based on Safe Policy Iteration with Baseline Bootstrapping (SPIBB, Laroche et al., 2019) that provides high probability guarantees on the performance of the agent in the true environment. We show the effectiveness of our method on a synthetic grid-world safety task as well as in a real-world critical care context to learn a policy for the administration of IV fluids and vasopressors to treat sepsis.",
    "authors": [
      "satija, harsh",
      "Thomas, Philip S.",
      "Pineau, Joelle",
      "Laroche, Romain"
    ]
  },
  {
    "id": "0f83556a305d789b1d71815e8ea4f4b0",
    "title": "Is Automated Topic Model Evaluation Broken? The Incoherence of Coherence",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0f83556a305d789b1d71815e8ea4f4b0-Paper.pdf",
    "abstract": "Topic model evaluation, like evaluation of other unsupervised methods, can be contentious. However, the field has coalesced around automated estimates of topic coherence, which rely on the frequency of word co-occurrences in a reference corpus. Contemporary neural topic models surpass classical ones according to these metrics. At the same time, topic model evaluation suffers from a validation gap: automated coherence, developed for classical models, has not been validated using human experimentation for neural models. In addition, a meta-analysis of topic modeling literature reveals a substantial standardization gap in automated topic modeling benchmarks. To address the validation gap, we compare automated coherence with the two most widely accepted human judgment tasks: topic rating and word intrusion. To address the standardization gap, we systematically evaluate a dominant classical model and two state-of-the-art neural models on two commonly used datasets. Automated evaluations declare a winning model when corresponding human evaluations do not, calling into question the validity of fully automatic evaluations independent of human judgments.",
    "authors": [
      "Hoyle, Alexander",
      "Goel, Pranav",
      "Hian-Cheong, Andrew",
      "Peskov, Denis",
      "Boyd-Graber, Jordan",
      "Resnik, Philip"
    ]
  },
  {
    "id": "0fd600c953cde8121262e322ef09f70e",
    "title": "INDIGO: GNN-Based Inductive Knowledge Graph Completion Using Pair-Wise Encoding",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0fd600c953cde8121262e322ef09f70e-Paper.pdf",
    "abstract": "The aim of knowledge graph (KG) completion is to extend an incomplete KG with missing triples. Popular approaches based on graph embeddings typically work by first representing the KG in a vector space, and then applying a predefined scoring function to the resulting vectors to complete the KG. These approaches work well in transductive settings, where predicted triples involve only constants seen during training; however, they are not applicable in inductive settings, where the KG on which the model was trained is extended with new constants or merged with other KGs. The use of Graph Neural Networks (GNNs) has recently been proposed as a way to overcome these limitations; however, existing approaches do not fully exploit the capabilities of GNNs and still rely on heuristics and ad-hoc scoring functions. In this paper, we propose a novel approach, where the KG is fully encoded into a GNN in a transparent way, and where the predicted triples can be read out directly from the last layer of the GNN without the need for additional components or scoring functions. Our experiments show that our model outperforms state-of-the-art approaches on inductive KG completion benchmarks.",
    "authors": [
      "Liu, Shuwen",
      "Grau, Bernardo",
      "Horrocks, Ian",
      "Kostylev, Egor"
    ]
  },
  {
    "id": "0fe6a94848e5c68a54010b61b3e94b0e",
    "title": "Do Input Gradients Highlight Discriminative Features? ",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/0fe6a94848e5c68a54010b61b3e94b0e-Paper.pdf",
    "abstract": "Post-hoc gradient-based interpretability methods [Simonyan et al., 2013, Smilkov et al., 2017] that provide instance-specific explanations of model predictions are often based on assumption (A): magnitude of input gradients\u2014gradients of logits with respect to input\u2014noisily highlight discriminative task-relevant features. In this work, we test the validity of assumption (A) using a three-pronged approach:1. We develop an evaluation framework, DiffROAR, to test assumption (A) on four image classification benchmarks. Our results suggest that (i) input gradients of standard models (i.e., trained on original data) may grossly violate (A), whereas (ii) input gradients of adversarially robust models satisfy (A).2. We then introduce BlockMNIST, an MNIST-based semi-real dataset, that by design encodes a priori knowledge of discriminative features. Our analysis on BlockMNIST leverages this information to validate as well as characterize differences between input gradient attributions of standard and robust models.3. Finally, we theoretically prove that our empirical findings hold on a simplified version of the BlockMNIST dataset. Specifically, we prove that input gradients of standard one-hidden-layer MLPs trained on this dataset do not highlight instance-specific signal coordinates, thus grossly violating assumption (A).Our findings motivate the need to formalize and test common assumptions in interpretability in a falsifiable manner [Leavitt and Morcos, 2020]. We believe that the DiffROAR evaluation framework and BlockMNIST-based datasets can serve as sanity checks to audit instance-specific interpretability methods; code and data available at https://github.com/harshays/inputgradients.",
    "authors": [
      "Shah, Harshay",
      "Jain, Prateek",
      "Netrapalli, Praneeth"
    ]
  },
  {
    "id": "1006ff12c465532f8c574aeaa4461b16",
    "title": "Improving Conditional Coverage via Orthogonal Quantile Regression",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1006ff12c465532f8c574aeaa4461b16-Paper.pdf",
    "abstract": "We develop a method to generate prediction intervals that have a user-specified coverage level across all regions of feature-space, a property called conditional coverage. A typical approach to this task is to estimate the conditional quantiles with quantile regression---it is well-known that this leads to correct coverage in the large-sample limit, although it may not be accurate in finite samples. We find in experiments that traditional quantile regression can have poor conditional coverage. To remedy this, we modify the loss function to promote independence between the size of the intervals and the indicator of a miscoverage event. For the true conditional quantiles, these two quantities are independent (orthogonal), so the modified loss function continues to be valid. Moreover, we empirically show that the modified loss function leads to improved conditional coverage, as evaluated by several metrics. We also introduce two new metrics that check conditional coverage by looking at the strength of the dependence between the interval size and the indicator of miscoverage. ",
    "authors": [
      "Feldman, Shai",
      "Bates, Stephen",
      "Romano, Yaniv"
    ]
  },
  {
    "id": "101951fe7ebe7bd8c77d14f75746b4bc",
    "title": "Minimizing Polarization and Disagreement in Social Networks via Link Recommendation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/101951fe7ebe7bd8c77d14f75746b4bc-Paper.pdf",
    "abstract": "Individual's opinions are fundamentally shaped  and evolved by their interactions with other people, and social phenomena such as disagreement and polarization are now tightly woven into daily life. The quantification and optimization of these concepts have been the subject of much recent research behind a wealth of high-impact data mining applications. In particular, researchers have addressed the question of how such concepts can be optimized by influencing the opinion of a small number of individuals or by designing the network from scratch.Here, rather than  a \u201cdesign-from-scratch\u201d approach or altering the initial opinion, we study the optimization problem of recommending $k$ new links to minimize the sum of polarization and disagreement in a social network with $n$ nodes and $m$ edges.  We show that our objective function of this combinatorial optimization problem is not submodular, although it is monotone. We propose a simple greedy algorithm with a constant-factor approximation that  solves the problem in cubic running time, and we provide  theoretical analysis of the approximation guarantee for the algorithm. To overcome the computation challenge for large networks, we also provide a fast algorithm with computation complexity $\\Otil (mk\\eps^{-2})$ for any $\\eps>0$,  where the $\\Otil (\\cdot)$ notation suppresses the ${\\rm poly} (\\log n)$ factors. Extensive experiments on real datasets demonstrate both the efficiency and effectiveness of our algorithms.",
    "authors": [
      "Zhu, Liwang",
      "Bao, Qi",
      "Zhang, Zhongzhi"
    ]
  },
  {
    "id": "103303dd56a731e377d01f6a37badae3",
    "title": "Adversarial Attacks on Black Box Video Classifiers: Leveraging the Power of Geometric Transformations",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/103303dd56a731e377d01f6a37badae3-Paper.pdf",
    "abstract": "When compared to the image classification models, black-box adversarial attacks against video classification models have been largely understudied. This could be possible because, with video, the temporal dimension poses significant additional challenges in gradient estimation. Query-efficient black-box attacks rely on effectively estimated gradients towards maximizing the probability of misclassifying the target video. In this work, we demonstrate that such effective gradients can be searched for by parameterizing the temporal structure of the search space with geometric transformations. Specifically, we design a novel iterative algorithm GEOmetric TRAnsformed Perturbations (GEO-TRAP), for attacking video classification models. GEO-TRAP employs standard geometric transformation operations to reduce the search space for effective gradients into searching for a small group of parameters that define these operations. This group of parameters describes the geometric progression of gradients, resulting in a reduced and structured search space. Our algorithm inherently leads to successful perturbations with surprisingly few queries. For example, adversarial examples generated from GEO-TRAP have better attack success rates with ~73.55% fewer queries compared to the state-of-the-art method for video adversarial attacks on the widely used Jester dataset. Overall, our algorithm exposes vulnerabilities of diverse video classification models and achieves new state-of-the-art results under black-box settings on two large datasets.",
    "authors": [
      "Li, Shasha",
      "Aich, Abhishek",
      "Zhu, Shitong",
      "Asif, Salman",
      "Song, Chengyu",
      "Roy-Chowdhury, Amit",
      "Krishnamurthy, Srikanth"
    ]
  },
  {
    "id": "107030ca685076c0ed5e054e2c3ed940",
    "title": "Optimal Rates for Random Order Online Optimization",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/107030ca685076c0ed5e054e2c3ed940-Paper.pdf",
    "abstract": "We study online convex optimization in the random order model, recently proposed by Garber et al. (2020), where the loss functions may be chosen by an adversary, but are then presented to the online algorithm in a uniformly random order. Focusing on the scenario where the cumulative loss function is (strongly) convex, yet individual loss functions are smooth but might be non-convex, we give algorithms that achieve the optimal bounds and significantly outperform the results of Garber et al. (2020), completely removing the dimension dependence and improve their scaling with respect to the strong convexity parameter. Our analysis relies on novel connections between algorithmic stability and generalization for sampling without-replacement analogous to those studied in the with-replacement i.i.d. setting, as well as on a refined average stability analysis of stochastic gradient descent.",
    "authors": [
      "Sherman, Uri",
      "Koren, Tomer",
      "Mansour, Yishay"
    ]
  },
  {
    "id": "10907813b97e249163587e6246612e21",
    "title": "Discrete-Valued Neural Communication",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/10907813b97e249163587e6246612e21-Paper.pdf",
    "abstract": "Deep learning has advanced from fully connected architectures to structured models organized into components, e.g., the transformer composed of positional elements, modular architectures divided into slots, and graph neural nets made up of nodes. The nature of structured models is that communication among the components has a bottleneck, typically achieved by restricted connectivity and attention. In this work, we further tighten the bottleneck via discreteness of the representations transmitted between components. We hypothesize that this constraint serves as a useful form of inductive bias. Our hypothesis is motivated by past empirical work showing the benefits of discretization in non-structured architectures as well as our own theoretical results showing that discretization increases noise robustness and reduces the underlying dimensionality of the model. Building on an existing technique for discretization from the VQ-VAE, we consider multi-headed discretization with shared codebooks as the output of each architectural component. One motivating intuition is human language in which communication occurs through multiple discrete symbols. This form of communication is hypothesized to facilitate transmission of information between functional components of the brain by providing a common interlingua, just as it does for human-to-human communication. Our experiments show that discrete-valued neural communication (DVNC) substantially improves systematic generalization in a variety of architectures\u2014transformers, modular architectures, and graph neural networks. We also show that the DVNC is robust to the choice of hyperparameters, making the method useful in practice.",
    "authors": [
      "Liu, Dianbo",
      "Lamb, Alex M.",
      "Kawaguchi, Kenji",
      "ALIAS PARTH GOYAL, Anirudh Goyal",
      "Sun, Chen",
      "Mozer, Michael C.",
      "Bengio, Yoshua"
    ]
  },
  {
    "id": "10a7cdd970fe135cf4f7bb55c0e3b59f",
    "title": "Skyformer: Remodel Self-Attention with Gaussian Kernel and Nystr\\\"om Method",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/10a7cdd970fe135cf4f7bb55c0e3b59f-Paper.pdf",
    "abstract": "Transformers are expensive to train due to the quadratic time and space complexity in the self-attention mechanism. On the other hand, although kernel machines suffer from the same computation bottleneck in pairwise dot products, several approximation schemes have been successfully incorporated to considerably reduce their computational cost without sacrificing too much accuracy. In this work, we leverage the computation methods for kernel machines to alleviate the high computational cost and introduce Skyformer, which replaces the softmax structure with a Gaussian kernel to stabilize the model training and adapts the Nystr\u00f6m method to a non-positive semidefinite matrix to accelerate the computation. We further conduct theoretical analysis by showing that the matrix approximation error of our proposed method is small in the spectral norm. Experiments on Long Range Arena benchmark show that the proposed method is sufficient in getting comparable or even better performance than the full self-attention while requiring fewer computation resources.",
    "authors": [
      "Chen, Yifan",
      "Zeng, Qi",
      "Ji, Heng",
      "Yang, Yun"
    ]
  },
  {
    "id": "10c272d06794d3e5785d5e7c5356e9ff",
    "title": "TransMIL: Transformer based Correlated Multiple Instance Learning for Whole Slide Image Classification",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/10c272d06794d3e5785d5e7c5356e9ff-Paper.pdf",
    "abstract": "Multiple instance learning (MIL) is a powerful tool to solve the weakly supervised classification in whole slide image (WSI) based pathology diagnosis. However, the current MIL methods are usually based on independent and identical distribution hypothesis, thus neglect the correlation among different instances. To address this problem, we proposed a new framework, called correlated MIL, and provided a proof for convergence. Based on this framework, we devised a Transformer based MIL (TransMIL), which explored both morphological and spatial information. The proposed TransMIL can effectively deal with unbalanced/balanced and binary/multiple classification with great visualization and interpretability. We conducted various experiments for three different computational pathology problems and achieved better performance and faster convergence compared with state-of-the-art methods. The test AUC for the binary tumor classification can be up to 93.09% over CAMELYON16 dataset. And the AUC over the cancer subtypes classification can be up to 96.03% and 98.82% over TCGA-NSCLC dataset and TCGA-RCC dataset, respectively. Implementation is available at: https://github.com/szc19990412/TransMIL.",
    "authors": [
      "Shao, Zhuchen",
      "Bian, Hao",
      "Chen, Yang",
      "Wang, Yifeng",
      "Zhang, Jian",
      "Ji, Xiangyang",
      "zhang, yongbing"
    ]
  },
  {
    "id": "10c66082c124f8afe3df4886f5e516e0",
    "title": "Multi-view Contrastive Graph Clustering",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/10c66082c124f8afe3df4886f5e516e0-Paper.pdf",
    "abstract": "With the explosive growth of information technology, multi-view graph data have become increasingly prevalent and valuable. Most existing multi-view clustering techniques either focus on the scenario of multiple graphs or multi-view attributes. In this paper, we propose a generic framework to cluster multi-view attributed graph data. Specifically, inspired by the success of contrastive learning, we propose multi-view contrastive graph clustering (MCGC) method to learn a consensus graph since the original graph could be noisy or incomplete and is not directly applicable. Our method composes of two key steps: we first filter out the undesirable high-frequency noise while preserving the graph geometric features via graph filtering and obtain a smooth representation of nodes; we then learn a consensus graph regularized by graph contrastive loss. Results on several benchmark datasets show the superiority of our method with respect to state-of-the-art approaches. In particular, our simple approach outperforms existing deep learning-based methods.",
    "authors": [
      "Pan, ErLin",
      "Kang, Zhao"
    ]
  },
  {
    "id": "10fb6cfa4c990d2bad5ddef4f70e8ba2",
    "title": "Inverse-Weighted Survival Games",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/10fb6cfa4c990d2bad5ddef4f70e8ba2-Paper.pdf",
    "abstract": "Deep models trained through maximum likelihood have achieved state-of-the-art results for survival analysis. Despite this training scheme, practitioners evaluate models under other criteria, such as binary classification losses at a chosen set of time horizons, e.g. Brier score (BS) and Bernoulli log likelihood (BLL). Models trained with maximum likelihood may have poor BS or BLL since maximum likelihood does not directly optimize these criteria. Directly optimizing criteria like BS requires inverse-weighting by the censoring distribution. However, estimating the censoring model under these metrics requires inverse-weighting by the failure distribution. The objective for each model requires the other, but neither are known. To resolve this dilemma, we introduce Inverse-Weighted Survival Games. In these games, objectives for each model are built from re-weighted estimates featuring the other model, where the latter is held fixed during training. When the loss is proper, we show that the games always have the true failure and censoring distributions as a stationary point. This means models in the game do not leave the correct distributions once reached. We construct one case where this stationary point is unique. We show that these games optimize BS on simulations and then apply these principles on real world cancer and critically-ill patient data.",
    "authors": [
      "Han, Xintian",
      "Goldstein, Mark",
      "Puli, Aahlad",
      "Wies, Thomas",
      "Perotte, Adler",
      "Ranganath, Rajesh"
    ]
  },
  {
    "id": "1102a326d5f7c9e04fc3c89d0ede88c9",
    "title": "Generalization Bounds for Meta-Learning via PAC-Bayes and Uniform Stability",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1102a326d5f7c9e04fc3c89d0ede88c9-Paper.pdf",
    "abstract": "We are motivated by the problem of providing strong generalization guarantees in the context of meta-learning. Existing generalization bounds are either challenging to evaluate or provide vacuous guarantees in even relatively simple settings. We derive a probably approximately correct (PAC) bound for gradient-based meta-learning using two different generalization frameworks in order to deal with the qualitatively different challenges of generalization at the \"base\" and \"meta\" levels. We employ bounds for uniformly stable algorithms at the base level and bounds from the PAC-Bayes framework at the meta level. The result of this approach is a novel PAC bound that is tighter when the base learner adapts quickly, which is precisely the goal of meta-learning. We show that our bound provides a tighter guarantee than other bounds on a toy non-convex problem on the unit sphere and a text-based classification example. We also present a practical regularization scheme motivated by the bound in settings where the bound is loose and demonstrate improved performance over baseline techniques.",
    "authors": [
      "Farid, Alec",
      "Majumdar, Anirudha"
    ]
  },
  {
    "id": "11704817e347269b7254e744b5e22dac",
    "title": "Parallel Bayesian Optimization of Multiple Noisy Objectives with Expected Hypervolume Improvement",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/11704817e347269b7254e744b5e22dac-Paper.pdf",
    "abstract": "Optimizing multiple competing black-box objectives is a challenging problem in many fields, including science, engineering, and machine learning. Multi-objective Bayesian optimization (MOBO) is a sample-efficient approach for identifying the optimal trade-offs between the objectives. However, many existing methods perform poorly when the observations are corrupted by noise. We propose a novel acquisition function, NEHVI, that overcomes this important practical limitation by applying a Bayesian treatment to the popular expected hypervolume improvement (EHVI) criterion and integrating over this uncertainty in the Pareto frontier. We argue that, even in the noiseless setting, generating multiple candidates in parallel is an incarnation of EHVI with uncertainty in the Pareto frontier and therefore can be addressed using the same underlying technique. Through this lens, we derive a natural parallel variant, qNEHVI, that reduces computational complexity of parallel EHVI from exponential to polynomial with respect to the batch size. qNEHVI is one-step Bayes-optimal for hypervolume maximization in both noisy and noiseless environments, and we show that it can be optimized effectively with gradient-based methods via sample average approximation. Empirically, we demonstrate not only that qNEHVI is substantially more robust to observation noise than existing MOBO approaches, but also that it achieves state-of-the-art optimization performance and competitive wall-times in large-batch environments.",
    "authors": [
      "Daulton, Samuel",
      "Balandat, Maximilian",
      "Bakshy, Eytan"
    ]
  },
  {
    "id": "118921efba23fc329e6560b27861f0c2",
    "title": "Evolution Gym: A Large-Scale Benchmark for Evolving Soft Robots",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/118921efba23fc329e6560b27861f0c2-Paper.pdf",
    "abstract": "Both the design and control of a robot play equally important roles in its task performance. However, while optimal control is well studied in the machine learning and robotics community, less attention is placed on finding the optimal robot design. This is mainly because co-optimizing design and control in robotics is characterized as a challenging problem, and more importantly, a comprehensive evaluation benchmark for co-optimization does not exist. In this paper, we propose Evolution Gym, the first large-scale benchmark for co-optimizing the design and control of soft robots. In our benchmark, each robot is composed of different types of voxels (e.g., soft, rigid, actuators), resulting in a modular and expressive robot design space. Our benchmark environments span a wide range of tasks, including locomotion on various types of terrains and manipulation. Furthermore, we develop several robot co-evolution algorithms by combining state-of-the-art design optimization methods and deep reinforcement learning techniques. Evaluating the algorithms on our benchmark platform, we observe robots exhibiting increasingly complex behaviors as evolution progresses, with the best evolved designs solving many of our proposed tasks. Additionally, even though robot designs are evolved autonomously from scratch without prior knowledge, they often grow to resemble existing natural creatures while outperforming hand-designed robots. Nevertheless, all tested algorithms fail to find robots that succeed in our hardest environments. This suggests that more advanced algorithms are required to explore the high-dimensional design space and evolve increasingly intelligent robots -- an area of research in which we hope Evolution Gym will accelerate progress. Our website with code, environments, documentation, and tutorials is available at http://evogym.csail.mit.edu/.",
    "authors": [
      "Bhatia, Jagdeep",
      "Jackson, Holly",
      "Tian, Yunsheng",
      "Xu, Jie",
      "Matusik, Wojciech"
    ]
  },
  {
    "id": "118bd558033a1016fcc82560c65cca5f",
    "title": "On Calibration and Out-of-Domain Generalization",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/118bd558033a1016fcc82560c65cca5f-Paper.pdf",
    "abstract": "Out-of-domain (OOD) generalization is a significant challenge for machine learning models. Many techniques have been proposed to overcome this challenge, often focused on learning models with certain invariance properties. In this work, we draw a link between OOD performance and model calibration, arguing that calibration across multiple domains can be viewed as a special case of an invariant representation leading to better OOD generalization. Specifically, we show that under certain conditions, models which achieve \\emph{multi-domain calibration} are provably free of spurious correlations. This leads us to propose multi-domain calibration as a measurable and trainable surrogate for the OOD performance of a classifier. We therefore introduce methods that are easy to apply and allow practitioners to improve multi-domain calibration by training or modifying an existing model, leading to better performance on unseen domains. Using four datasets from the recently proposed WILDS OOD benchmark, as well as the Colored MNIST, we demonstrate that training or tuning models so they are calibrated across multiple domains leads to significantly improved performance on unseen test domains. We believe this intriguing connection between calibration and OOD generalization is promising from both a practical and theoretical point of view.",
    "authors": [
      "Wald, Yoav",
      "Feder, Amir",
      "Greenfeld, Daniel",
      "Shalit, Uri"
    ]
  },
  {
    "id": "11c484ea9305ea4c7bb6b2e6d570d466",
    "title": "On the Convergence and Sample Efficiency of Variance-Reduced Policy Gradient Method",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/11c484ea9305ea4c7bb6b2e6d570d466-Paper.pdf",
    "abstract": "Policy gradient (PG) gives rise to a rich class of reinforcement learning (RL) methods. Recently, there has been an emerging trend to augment the existing PG methods such as REINFORCE by the \\emph{variance reduction} techniques.  However, all existing variance-reduced PG methods heavily rely on an uncheckable importance weight assumption made for every single iteration of the algorithms. In this paper, a simple gradient truncation mechanism is proposed to address this issue. Moreover, we design a Truncated Stochastic Incremental Variance-Reduced Policy Gradient (TSIVR-PG) method, which is able to maximize not only a cumulative sum of rewards but also a general utility function over a policy's long-term visiting distribution.  We show an $\\tilde{\\mathcal{O}}(\\epsilon^{-3})$ sample complexity for TSIVR-PG to find an $\\epsilon$-stationary policy. By assuming the \\emph{overparameterization} of policy and exploiting the \\emph{hidden convexity} of the problem, we further show that TSIVR-PG converges to global $\\epsilon$-optimal policy with $\\tilde{\\mathcal{O}}(\\epsilon^{-2})$ samples. ",
    "authors": [
      "Zhang, Junyu",
      "Ni, Chengzhuo",
      "Yu, zheng",
      "Szepesvari, Csaba",
      "Wang, Mengdi"
    ]
  },
  {
    "id": "11eba2991cc62daa4a85be5c0cfdae97",
    "title": "Circa: Stochastic ReLUs for Private Deep Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/11eba2991cc62daa4a85be5c0cfdae97-Paper.pdf",
    "abstract": "The simultaneous rise of machine learning as a service and concerns over user privacy have increasingly motivated the need for private inference (PI). While recent work demonstrates PI is possible using cryptographic primitives, the computational overheads render it impractical. State-of-art deep networks are inadequate in this context because the source of slowdown in PI stems from the ReLU operations whereas optimizations for plaintext inference focus on reducing FLOPs. In this paper we re-think ReLU computations and propose optimizations for PI tailored to properties of neural networks. Specifically, we reformulate ReLU as an approximate sign test and introduce a novel truncation method for the sign test that significantly reduces the cost per ReLU. These optimizations result in a specific type of stochastic ReLU. The key observation is that the stochastic fault behavior is well suited for the fault-tolerant properties of neural network inference. Thus, we provide significant savings without impacting accuracy. We collectively call the optimizations Circa and demonstrate improvements of up to 4.7$\\times$ storage and 3$\\times$ runtime over baseline implementations; we further show that Circa can be used on top of recent PI optimizations to obtain 1.8$\\times$ additional speedup.",
    "authors": [
      "Ghodsi, Zahra",
      "Jha, Nandan Kumar",
      "Reagen, Brandon",
      "Garg, Siddharth"
    ]
  },
  {
    "id": "11f9e78e4899a78dedd439fc583b6693",
    "title": "Reinforcement Learning in Reward-Mixing MDPs",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/11f9e78e4899a78dedd439fc583b6693-Paper.pdf",
    "abstract": "Learning a near optimal policy in a partially observable system remains an elusive challenge in contemporary reinforcement learning. In this work, we consider episodic reinforcement learning in a reward-mixing Markov decision process (MDP). There, a reward function is drawn from one of $M$ possible reward models at the beginning of every episode, but the identity of the chosen reward model is not revealed to the agent. Hence, the latent state space, for which the dynamics are Markovian, is not given to the agent. We study the problem of learning a near optimal policy for two reward-mixing MDPs. Unlike existing approaches that rely on strong assumptions on the dynamics, we make no assumptions and study the problem in full generality. Indeed, with no further assumptions, even for two switching reward-models, the problem requires several new ideas beyond existing algorithmic and analysis techniques for efficient exploration. We provide the first polynomial-time algorithm that finds an $\\epsilon$-optimal policy after exploring $\\tilde{O}(poly(H,\\epsilon^{-1}) \\cdot S^2 A^2)$ episodes, where $H$ is time-horizon and $S, A$ are the number of states and actions respectively. This is the first efficient algorithm that does not require any assumptions in partially observed environments where the observation space is smaller than the latent state space. ",
    "authors": [
      "Kwon, Jeongyeol",
      "Efroni, Yonathan",
      "Caramanis, Constantine",
      "Mannor, Shie"
    ]
  },
  {
    "id": "124461dcd3571e6674ec4e0e140cc298",
    "title": "A Gang of Adversarial Bandits",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/124461dcd3571e6674ec4e0e140cc298-Paper.pdf",
    "abstract": "We consider running multiple instances of multi-armed bandit (MAB) problems in parallel. A main motivation for this study are online recommendation systems, in which each of $N$ users is associated with a MAB problem and the goal is to exploit users' similarity in order to learn users' preferences to $K$ items more efficiently. We consider the adversarial MAB setting, whereby an adversary is free to choose which user and which loss to present to the learner during the learning process. Users are in a social network and the learner is aided by a-priori knowledge of the strengths of the social links between all pairs of users. It is assumed that if the social link between two users is strong then they tend to share the same action. The regret is measured relative to an arbitrary function which maps users to actions. The smoothness of the function is captured by a resistance-based dispersion measure $\\Psi$. We present two learning algorithms, GABA-I and GABA-II, which exploit the network structure to bias towards functions of low $\\Psi$  values. We show that GABA-I has an expected regret bound of $\\mathcal{O}(\\sqrt{\\ln(NK/\\Psi)\\Psi KT})$ and per-trial time complexity of $\\mathcal{O}(K\\ln(N))$, whilst GABA-II has a weaker $\\mathcal{O}(\\sqrt{\\ln(N/\\Psi)\\ln(NK/\\Psi)\\Psi KT})$ regret, but a better $\\mathcal{O}(\\ln(K)\\ln(N))$ per-trial time complexity. We highlight improvements of both algorithms over running independent standard MABs across users.",
    "authors": [
      "Herbster, Mark",
      "Pasteris, Stephen",
      "Vitale, Fabio",
      "Pontil, Massimiliano"
    ]
  },
  {
    "id": "12ced2db6f0193dda91ba86224ea1cd8",
    "title": "Explaining Hyperparameter Optimization via Partial Dependence Plots",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/12ced2db6f0193dda91ba86224ea1cd8-Paper.pdf",
    "abstract": "Automated hyperparameter optimization (HPO) can support practitioners to obtain peak performance in machine learning models.However, there is often a lack of valuable insights into the effects of different hyperparameters on the final model performance.This lack of explainability makes it difficult to trust and understand the automated HPO process and its results.We suggest using interpretable machine learning (IML) to gain insights from the experimental data obtained during HPO with Bayesian optimization (BO).BO tends to focus on promising regions with potential high-performance configurations and thus induces a sampling bias.Hence, many IML techniques, such as the partial dependence plot (PDP), carry the risk of generating biased interpretations.By leveraging the posterior uncertainty of the BO surrogate model, we introduce a variant of the PDP with estimated confidence bands.We propose to partition the hyperparameter space to obtain more confident and reliable PDPs in relevant sub-regions.In an experimental study, we provide quantitative evidence for the increased quality of the PDPs within sub-regions.",
    "authors": [
      "Moosbauer, Julia",
      "Herbinger, Julia",
      "Casalicchio, Giuseppe",
      "Lindauer, Marius",
      "Bischl, Bernd"
    ]
  },
  {
    "id": "12e086066892a311b752673a28583d3f",
    "title": "Robustifying Algorithms of Learning Latent Trees with Vector Variables",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/12e086066892a311b752673a28583d3f-Paper.pdf",
    "abstract": "We consider learning the structures of Gaussian latent tree models with vector observations when a subset of them are arbitrarily corrupted. First, we present the sample complexities of Recursive Grouping (RG) and Chow-Liu Recursive Grouping (CLRG) without the assumption that the effective depth is bounded in the number of observed nodes, significantly generalizing the results in Choi et al. (2011). We show that Chow-Liu initialization in CLRG greatly reduces the sample complexity of RG from being exponential in the diameter of the tree to only logarithmic in the diameter for the hidden Markov model (HMM). Second, we robustify RG, CLRG, Neighbor Joining (NJ) and Spectral NJ (SNJ) by using the truncated inner product. These robustified algorithms can tolerate a number of corruptions up to the square root of the number of clean samples. Finally, we derive the first known instance-dependent impossibility result for structure learning of latent trees. The optimalities of the robust version of CLRG and NJ are verified by comparing their sample complexities and the impossibility result.",
    "authors": [
      "Zhang, Fengzhuo",
      "Tan, Vincent"
    ]
  },
  {
    "id": "12e35d9186dd72fe62fd039385890b9c",
    "title": "Representation Learning on Spatial Networks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/12e35d9186dd72fe62fd039385890b9c-Paper.pdf",
    "abstract": "Spatial networks are networks for which the nodes and edges are constrained by geometry and embedded in real space, which has crucial effects on their topological properties. Although tremendous success has been achieved in spatial and network representation separately in recent years, there exist very little works on the representation of spatial networks. Extracting powerful representations from spatial networks requires the development of appropriate tools to uncover the pairing of both spatial and network information in the appearance of node permutation invariant, and rotation and translation invariant. Hence it can not be modeled merely with either spatial or network models individually. To address these challenges, this paper proposes a generic framework for spatial network representation learning. Specifically, a provably information-lossless and roto-translation invariant representation of spatial information on networks is presented. Then a higher-order spatial network convolution operation that adapts to our proposed representation is introduced. To ensure efficiency, we also propose a new approach that relied on sampling random spanning trees to reduce the time and memory complexity from $O(N^3)$ to $O(N)$. We demonstrate the strength of our proposed framework through extensive experiments on both synthetic and real-world datasets. The code for the proposed model is available at \\url{https://github.com/rollingstonezz/SGMP_code}. ",
    "authors": [
      "Zhang, Zheng",
      "Zhao, Liang"
    ]
  },
  {
    "id": "1301962d8b7bd03fffaa27119aa7fc2b",
    "title": "Continuous-time edge modelling using non-parametric point processes",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1301962d8b7bd03fffaa27119aa7fc2b-Paper.pdf",
    "abstract": "The mutually-exciting Hawkes process (ME-HP) is a natural choice to model reciprocity, which is an important attribute of continuous-time edge (dyadic) data. However, existing ways of implementing the ME-HP for such data are either inflexible, as the exogenous (background) rate functions are typically constant and the endogenous (excitation) rate functions are specified parametrically, or inefficient, as inference usually relies on Markov chain Monte Carlo methods with high computational costs. To address these limitations, we discuss various approaches to model design, and develop three variants of non-parametric point processes for continuous-time edge modelling (CTEM). The resulting models are highly adaptable as they generate intensity functions through sigmoidal Gaussian processes, and so provide greater modelling flexibility than parametric forms. The models are implemented via a fast variational inference method enabled by a novel edge modelling construction. The superior performance of the proposed CTEM models is demonstrated through extensive experimental evaluations on four real-world continuous-time edge data sets.",
    "authors": [
      "Fan, Xuhui",
      "Li, Bin",
      "Zhou, Feng",
      "SIsson, Scott"
    ]
  },
  {
    "id": "1325cdae3b6f0f91a1b629307bf2d498",
    "title": "Deep inference of latent dynamics with spatio-temporal super-resolution using selective backpropagation through time",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1325cdae3b6f0f91a1b629307bf2d498-Paper.pdf",
    "abstract": "Modern neural interfaces allow access to the activity of up to a million neurons within brain circuits. However, bandwidth limits often create a trade-off between greater spatial sampling (more channels or pixels) and the temporal frequency of sampling. Here we demonstrate that it is possible to obtain spatio-temporal super-resolution in neuronal time series by exploiting relationships among neurons, embedded in latent low-dimensional population dynamics. Our novel neural network training strategy, selective backpropagation through time (SBTT), enables learning of deep generative models of latent dynamics from data in which the set of observed variables changes at each time step. The resulting models are able to infer activity for missing samples by combining observations with learned latent dynamics. We test SBTT applied to sequential autoencoders and demonstrate more efficient and higher-fidelity characterization of neural population dynamics in electrophysiological and calcium imaging data. In electrophysiology, SBTT enables accurate inference of neuronal population dynamics with lower interface bandwidths, providing an avenue to significant power savings for implanted neuroelectronic interfaces. In applications to two-photon calcium imaging, SBTT accurately uncovers high-frequency temporal structure underlying neural population activity, substantially outperforming the current state-of-the-art. Finally, we demonstrate that performance could be further improved by using limited, high-bandwidth sampling to pretrain dynamics models, and then using SBTT to adapt these models for sparsely-sampled data.",
    "authors": [
      "Zhu, Feng",
      "Sedler, Andrew",
      "Grier, Harrison A",
      "Ahad, Nauman",
      "Davenport, Mark",
      "Kaufman, Matthew",
      "Giovannucci, Andrea",
      "Pandarinath, Chethan"
    ]
  },
  {
    "id": "1371bccec2447b5aa6d96d2a540fb401",
    "title": "Memory-efficient Patch-based Inference for Tiny Deep Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1371bccec2447b5aa6d96d2a540fb401-Paper.pdf",
    "abstract": "Tiny deep learning on microcontroller units (MCUs) is challenging due to the limited memory size. We find that the memory bottleneck is due to the imbalanced memory distribution in convolutional neural network (CNN) designs:  the first several blocks have an order of magnitude larger memory usage than the rest of the network. To alleviate this issue, we propose a generic patch-by-patch inference scheduling, which operates only on a small spatial region of the feature map and significantly cuts down the peak memory. However, naive implementation brings overlapping patches and computation overhead. We further propose receptive field redistribution to shift the receptive field and FLOPs to the later stage and reduce the computation overhead. Manually redistributing the receptive field is difficult. We automate the process with neural architecture search to jointly optimize the neural architecture and inference scheduling, leading to MCUNetV2. Patch-based inference effectively reduces the peak memory usage of existing networks by4-8\u00d7.  Co-designed with neural networks, MCUNetV2 sets a record ImageNetaccuracy on MCU (71.8%) and achieves >90% accuracy on the visual wake words dataset under only 32kB SRAM. MCUNetV2 also unblocks object detection on tiny devices, achieving 16.9% higher mAP on Pascal VOC compared to the state-of-the-art result. Our study largely addressed the memory bottleneck in tinyML and paved the way for various vision applications beyond image classification.",
    "authors": [
      "Lin, Ji",
      "Chen, Wei-Ming",
      "Cai, Han",
      "Gan, Chuang",
      "Han, Song"
    ]
  },
  {
    "id": "1387a00f03b4b423e63127b08c261bdc",
    "title": "Self-Interpretable Model with Transformation Equivariant Interpretation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1387a00f03b4b423e63127b08c261bdc-Paper.pdf",
    "abstract": "With the proliferation of machine learning applications in the real world, the demand for explaining machine learning predictions continues to grow especially in high-stakes fields. Recent studies have found that interpretation methods can be sensitive and unreliable, where the interpretations can be disturbed by perturbations or transformations of input data. To address this issue, we propose to learn robust interpretation through transformation equivariant regularization in a self-interpretable model. The resulting model is capable of capturing valid interpretation that is equivariant to geometric transformations. Moreover, since our model is self-interpretable, it enables faithful interpretations that reflect the true predictive mechanism. Unlike existing self-interpretable models, which usually sacrifice expressive power for the sake of interpretation quality, our model preserves the high expressive capability comparable to the state-of-the-art deep learning models in complex tasks, while providing visualizable and faithful high-quality interpretation. We compare with various related methods and validate the interpretation quality and consistency of our model.",
    "authors": [
      "Wang, Yipei",
      "Wang, Xiaoqian"
    ]
  },
  {
    "id": "13bf4a96378f3854bcd9792d132eff9f",
    "title": "Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/13bf4a96378f3854bcd9792d132eff9f-Paper.pdf",
    "abstract": "Many recent AI architectures are inspired by zero-sum games, however, the behavior of their dynamics is still not well understood. Inspired by this, we study standard gradient descent ascent (GDA) dynamics in a specific class of non-convex non-concave zero-sum games, that we call hidden zero-sum games. In this class, players control the inputs of smooth but possibly non-linear functions whose outputs are being applied as inputs to a convex-concave game. Unlike general zero-sum games, these games have a well-defined notion of solution; outcomes that implement the von-Neumann equilibrium of the ``hidden\" convex-concave game. We provide conditions under which vanilla GDA provably converges not merely to local Nash, but the actual von-Neumann solution. If the hidden game lacks strict convexity properties, GDA may fail to converge to any equilibrium, however, by applying standard regularization techniques we can prove convergence to a von-Neumann solution of a slightly perturbed zero-sum game. Our convergence results are non-local despite working in the setting of non-convex non-concave games. Critically, under proper assumptions we combine the Center-Stable Manifold Theorem along with novel type of initialization dependent Lyapunov functions to prove that almost all initial conditions converge to the solution. Finally, we discuss diverse applications of our framework ranging from generative adversarial networks to evolutionary biology.",
    "authors": [
      "Vlatakis-Gkaragkounis, Emmanouil-Vasileios",
      "Flokas, Lampros",
      "Piliouras, Georgios"
    ]
  },
  {
    "id": "13d63838ef1fb6f34ca2dc6821c60e49",
    "title": "Preserved central model for faster bidirectional compression in distributed settings",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/13d63838ef1fb6f34ca2dc6821c60e49-Paper.pdf",
    "abstract": "We develop a new approach to tackle communication constraints in a distributed learning problem with a central server. We propose and analyze a new algorithm that performs bidirectional compression and achieves the same convergence rate as algorithms using only uplink (from the local workers to the central server) compression. To obtain this improvement, we design MCM, an algorithm such that the downlink compression only impacts local models, while the global model is preserved. As a result, and contrary to previous works, the gradients on local servers are computed on perturbed models. Consequently, convergence proofs are more challenging and require a precise control of this perturbation. To ensure it, MCM additionally combines model compression with a memory mechanism. This analysis opens new doors, e.g. incorporating worker dependent randomized-models and partial participation.",
    "authors": [
      "Philippenko, Constantin",
      "Dieuleveut, Aymeric"
    ]
  },
  {
    "id": "13d7dc096493e1f77fb4ccf3eaf79df1",
    "title": "Understanding Instance-based Interpretability of Variational Auto-Encoders",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/13d7dc096493e1f77fb4ccf3eaf79df1-Paper.pdf",
    "abstract": "Instance-based interpretation methods have been widely studied for supervised learning methods as they help explain how black box neural networks predict. However, instance-based interpretations remain ill-understood in the context of unsupervised learning. In this paper, we investigate influence functions [Koh and Liang, 2017], a popular instance-based interpretation method, for a class of deep generative models called variational auto-encoders (VAE). We formally frame the counter-factual question answered by influence functions in this setting, and through theoretical analysis, examine what they reveal about the impact of training samples on classical unsupervised learning methods. We then introduce VAE- TracIn, a computationally efficient and theoretically sound solution based on Pruthi et al. [2020], for VAEs. Finally, we evaluate VAE-TracIn on several real world datasets with extensive quantitative and qualitative analysis.",
    "authors": [
      "Kong, Zhifeng",
      "Chaudhuri, Kamalika"
    ]
  },
  {
    "id": "1415db70fe9ddb119e23e9b2808cde38",
    "title": "Voxel-based 3D Detection and Reconstruction of Multiple Objects from a Single Image",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1415db70fe9ddb119e23e9b2808cde38-Paper.pdf",
    "abstract": "Inferring 3D locations and shapes of multiple objects from a single 2D image is a long-standing objective of computer vision. Most of the existing works either predict one of these 3D properties or focus on solving both for a single object. One fundamental challenge lies in how to learn an effective representation of the image that is well-suited for 3D detection and reconstruction. In this work, we propose to learn a regular grid of 3D voxel features from the input image which is aligned with 3D scene space via a 3D feature lifting operator. Based on the 3D voxel features, our novel CenterNet-3D detection head formulates the 3D detection as keypoint detection in the 3D space. Moreover, we devise an efficient coarse-to-fine reconstruction module, including coarse-level voxelization and a novel local PCA-SDF shape representation, which enables fine detail reconstruction and two orders of magnitude faster inference than prior methods. With complementary supervision from both 3D detection and reconstruction, one enables the 3D voxel features to be geometry and context preserving, benefiting both tasks. The effectiveness of our approach is demonstrated through 3D detection and reconstruction on single-object and multiple-object scenarios. ",
    "authors": [
      "Liu, Feng",
      "Liu, Xiaoming"
    ]
  },
  {
    "id": "1415fe9fea0fa1e45dddcff5682239a0",
    "title": "Test-Time Classifier Adjustment Module for Model-Agnostic Domain Generalization",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1415fe9fea0fa1e45dddcff5682239a0-Paper.pdf",
    "abstract": "This paper presents a new algorithm for domain generalization (DG), \\textit{test-time template adjuster (T3A)}, aiming to robustify a model to unknown distribution shift. Unlike existing methods that focus on \\textit{training phase}, our method focuses \\textit{test phase}, i.e., correcting its prediction by itself during test time. Specifically, T3A adjusts a trained linear classifier (the last layer of deep neural networks) with the following procedure:  (1) compute a pseudo-prototype representation for each class using online unlabeled data augmented by the base classifier trained in the source domains, (2) and then classify each sample based on its distance to the pseudo-prototypes. T3A is back-propagation-free and modifies only the linear layer; therefore, the increase in computational cost during inference is negligible and avoids the catastrophic failure might caused by stochastic optimization. Despite its simplicity, T3A can leverage knowledge about the target domain by using off-the-shelf test-time data and improve performance. We tested our method on four domain generalization benchmarks, namely PACS, VLCS, OfficeHome, and TerraIncognita, along with various backbone networks including ResNet18, ResNet50, Big Transfer (BiT), Vision Transformers (ViT), and MLP-Mixer. The results show T3A stably improves performance on unseen domains across choices of backbone networks, and outperforms existing domain generalization methods. ",
    "authors": [
      "Iwasawa, Yusuke",
      "Matsuo, Yutaka"
    ]
  },
  {
    "id": "14319d9cfc6123106878dc20b94fbaf3",
    "title": "Luna: Linear Unified Nested Attention",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/14319d9cfc6123106878dc20b94fbaf3-Paper.pdf",
    "abstract": "The quadratic computational and memory complexities of the Transformer's attention mechanism have limited its scalability for modeling long sequences.  In this paper, we propose Luna, a linear unified nested attention mechanism that approximates softmax attention with two nested linear attention functions, yielding only linear (as opposed to quadratic) time and space complexity. Specifically, with the first attention function, Luna packs the input sequence into a sequence of fixed length. Then, the packed sequence is unpacked using the second attention function. As compared to a more traditional attention mechanism, Luna introduces an additional sequence with a fixed length as input and an additional corresponding output, which allows Luna to perform attention operation linearly, while also storing adequate contextual information. We perform extensive evaluations on three benchmarks of sequence modeling tasks: long-context sequence modelling, neural machine translation and masked language modeling for large-scale pretraining. Competitive or even better experimental results demonstrate both the effectiveness and efficiency of Luna compared to a variety of strong baseline methods including the full-rank attention and other efficient sparse and dense attention methods. ",
    "authors": [
      "Ma, Xuezhe",
      "Kong, Xiang",
      "Wang, Sinong",
      "Zhou, Chunting",
      "May, Jonathan",
      "Ma, Hao",
      "Zettlemoyer, Luke"
    ]
  },
  {
    "id": "144a3f71a03ab7c4f46f9656608efdb2",
    "title": "Iterative Causal Discovery in the Possible Presence of Latent Confounders and Selection Bias",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/144a3f71a03ab7c4f46f9656608efdb2-Paper.pdf",
    "abstract": "We present a sound and complete algorithm, called iterative causal discovery (ICD), for recovering causal graphs in the presence of latent confounders and selection bias. ICD relies on the causal Markov and faithfulness assumptions and recovers the equivalence class of the underlying causal graph. It starts with a complete graph, and consists of a single iterative stage that gradually refines this graph by identifying conditional independence (CI) between connected nodes. Independence and causal relations entailed after any iteration are correct, rendering ICD anytime. Essentially, we tie the size of the CI conditioning set to its distance on the graph from the tested nodes, and increase this value in the successive iteration. Thus, each iteration refines a graph that was recovered by previous iterations having smaller conditioning sets---a higher statistical power---which contributes to stability. We demonstrate empirically that ICD requires significantly fewer CI tests and learns more accurate causal graphs compared to FCI, FCI+, and RFCI algorithms.",
    "authors": [
      "Rohekar, Raanan Y.",
      "Nisimov, Shami",
      "Gurwicz, Yaniv",
      "Novik, Gal"
    ]
  },
  {
    "id": "1454ca2270599546dfcd2a3700e4d2f1",
    "title": "Hindsight Task Relabelling: Experience Replay for Sparse Reward Meta-RL",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1454ca2270599546dfcd2a3700e4d2f1-Paper.pdf",
    "abstract": "Meta-reinforcement learning (meta-RL) has proven to be a successful framework for leveraging experience from prior tasks to rapidly learn new related tasks, however, current meta-RL approaches struggle to learn in sparse reward environments. Although existing meta-RL algorithms can learn strategies for adapting to new sparse reward tasks, the actual adaptation strategies are learned using hand-shaped reward functions, or require simple environments where random exploration is sufficient to encounter sparse reward. In this paper we present a formulation of hindsight relabelling for meta-RL, which relabels experience during meta-training to enable learning to learn entirely using sparse reward. We demonstrate the effectiveness of our approach on a suite of challenging sparse reward environments that previously required dense reward during meta-training to solve. Our approach solves these environments using the true sparse reward function, with performance comparable to training with a proxy dense reward function.",
    "authors": [
      "Packer, Charles",
      "Abbeel, Pieter",
      "Gonzalez, Joseph E."
    ]
  },
  {
    "id": "147540e129e096fa91700e9db6588354",
    "title": "A Bayesian-Symbolic Approach to Reasoning and Learning in Intuitive Physics",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/147540e129e096fa91700e9db6588354-Paper.pdf",
    "abstract": "Humans can reason about intuitive physics in fully or partially observed environments even after being exposed to a very limited set of observations. This sample-efficient intuitive physical reasoning is considered a core domain of human common sense knowledge. One hypothesis to explain this remarkable capacity, posits that humans quickly learn approximations to the laws of physics that govern the dynamics of the environment. In this paper, we propose a Bayesian-symbolic framework (BSP) for physical reasoning and learning that is close to human-level sample-efficiency and accuracy. In BSP, the environment is represented by a top-down generative model of entities, which are assumed to interact with each other under unknown force laws over their latent and observed properties. BSP models each of these entities as random variables, and uses Bayesian inference to estimate their unknown properties. For learning the unknown forces, BSP leverages symbolic regression on a novel grammar of Newtonian physics in a bilevel optimization setup. These inference and regression steps are performed in an iterative manner using expectation-maximization, allowing BSP to simultaneously learn force laws while maintaining uncertainty over entity properties. We show that BSP is more sample-efficient compared to neural alternatives on controlled synthetic datasets, demonstrate BSP's applicability to real-world common sense scenes and study BSP's performance on tasks previously used to study human physical reasoning.",
    "authors": [
      "Xu, Kai",
      "Srivastava, Akash",
      "Gutfreund, Dan",
      "Sosa, Felix",
      "Ullman, Tomer",
      "Tenenbaum, Josh",
      "Sutton, Charles"
    ]
  },
  {
    "id": "147702db07145348245dc5a2f2fe5683",
    "title": "Associating Objects with Transformers for Video Object Segmentation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/147702db07145348245dc5a2f2fe5683-Paper.pdf",
    "abstract": "This paper investigates how to realize better and more efficient embedding learning to tackle the semi-supervised video object segmentation under challenging multi-object scenarios. The state-of-the-art methods learn to decode features with a single positive object and thus have to match and segment each target separately under multi-object scenarios, consuming multiple times computing resources. To solve the problem, we propose an Associating Objects with Transformers (AOT) approach to match and decode multiple objects uniformly. In detail, AOT employs an identification mechanism to associate multiple targets into the same high-dimensional embedding space. Thus, we can simultaneously process multiple objects' matching and segmentation decoding as efficiently as processing a single object. For sufficiently modeling multi-object association, a Long Short-Term Transformer is designed for constructing hierarchical matching and propagation. We conduct extensive experiments on both multi-object and single-object benchmarks to examine AOT variant networks with different complexities. Particularly, our R50-AOT-L outperforms all the state-of-the-art competitors on three popular benchmarks, i.e., YouTube-VOS (84.1% J&F), DAVIS 2017 (84.9%), and DAVIS 2016 (91.1%), while keeping more than 3X faster multi-object run-time. Meanwhile, our AOT-T can maintain real-time multi-object speed on the above benchmarks. Based on AOT, we ranked 1st in the 3rd Large-scale VOS Challenge.",
    "authors": [
      "Yang, Zongxin",
      "Wei, Yunchao",
      "Yang, Yi"
    ]
  },
  {
    "id": "148148d62be67e0916a833931bd32b26",
    "title": "Automatic Symmetry Discovery with Lie Algebra Convolutional Network",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/148148d62be67e0916a833931bd32b26-Paper.pdf",
    "abstract": "Existing equivariant neural networks require prior knowledge of the symmetry group and discretization for continuous groups. We propose to work with Lie algebras (infinitesimal generators) instead of Lie groups. Our model, the Lie algebra convolutional network (L-conv) can automatically discover symmetries and does not require discretization of the group. We show that L-conv can serve as a building block to construct any group equivariant feedforward architecture. Both CNNs and Graph Convolutional Networks can be expressed as L-conv with appropriate groups. We discover direct connections between L-conv and physics: (1) group invariant loss generalizes field theory (2) Euler-Lagrange equation measures the robustness, and (3) equivariance leads to conservation laws and Noether current. These connections open up new avenues for designing more general equivariant networks and applying them to important problems in physical sciences.",
    "authors": [
      "Dehmamy, Nima",
      "Walters, Robin",
      "Liu, Yanchen",
      "Wang, Dashun",
      "Yu, Rose"
    ]
  },
  {
    "id": "149ef6419512be56a93169cd5e6fa8fd",
    "title": "Zero Time Waste: Recycling Predictions in Early Exit Neural Networks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/149ef6419512be56a93169cd5e6fa8fd-Paper.pdf",
    "abstract": "The problem of reducing processing time of large deep learning models is a fundamental challenge in many real-world applications. Early exit methods strive towards this goal by attaching additional Internal Classifiers (ICs) to intermediate layers of a neural network. ICs can quickly return predictions for easy examples and, as a result, reduce the average inference time of the whole model. However, if a particular IC does not decide to return an answer early, its predictions are discarded, with its computations effectively being wasted. To solve this issue, we introduce Zero Time Waste (ZTW), a novel approach in which each IC reuses predictions returned by its predecessors by (1) adding direct connections between ICs and (2) combining previous outputs in an ensemble-like manner. We conduct extensive experiments across various datasets and architectures to demonstrate that ZTW achieves a significantly better accuracy vs. inference time trade-off than other recently proposed early exit methods.",
    "authors": [
      "Wo\u0142czyk, Maciej",
      "W\u00f3jcik, Bartosz",
      "Ba\u0142azy, Klaudia",
      "Podolak, Igor T",
      "Tabor, Jacek",
      "\u015amieja, Marek",
      "Trzcinski, Tomasz"
    ]
  },
  {
    "id": "14ad095ecc1c3e1b87f3c522836e9158",
    "title": "On Model Calibration for Long-Tailed Object Detection and Instance Segmentation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/14ad095ecc1c3e1b87f3c522836e9158-Paper.pdf",
    "abstract": "Vanilla models for object detection and instance segmentation suffer from the heavy bias toward detecting frequent objects in the long-tailed setting. Existing methods address this issue mostly during training, e.g., by re-sampling or re-weighting. In this paper, we investigate a largely overlooked approach --- post-processing calibration of confidence scores. We propose NorCal, Normalized Calibration for long-tailed object detection and instance segmentation, a simple and straightforward recipe that reweighs the predicted scores of each class by its training sample size. We show that separately handling the background class and normalizing the scores over classes for each proposal are keys to achieving superior performance. On the LVIS dataset, NorCal can effectively improve nearly all the baseline models not only on rare classes but also on common and frequent classes.  Finally, we conduct extensive analysis and ablation studies to offer insights into various modeling choices and mechanisms of our approach. Our code is publicly available at https://github.com/tydpan/NorCal.",
    "authors": [
      "Pan, Tai-Yu",
      "Zhang, Cheng",
      "Li, Yandong",
      "Hu, Hexiang",
      "Xuan, Dong",
      "Changpinyo, Soravit",
      "Gong, Boqing",
      "Chao, Wei-Lun"
    ]
  },
  {
    "id": "14c4f36143b4b09cbc320d7c95a50ee7",
    "title": "ReSSL: Relational Self-Supervised Learning with Weak Augmentation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/14c4f36143b4b09cbc320d7c95a50ee7-Paper.pdf",
    "abstract": "Self-supervised Learning (SSL) including the mainstream contrastive learning has achieved great success in learning visual representations without data annotations. However, most of methods mainly focus on the instance level information (\\ie, the different augmented images of the same instance should have the same feature or cluster into the same class), but there is a lack of attention on the relationships between different instances. In this paper, we introduced a novel SSL paradigm, which we term as relational self-supervised learning  (ReSSL) framework that learns representations by modeling the relationship between different instances. Specifically, our proposed method employs sharpened distribution of pairwise similarities among different instances as \\textit{relation} metric, which is thus utilized to match the feature embeddings of different augmentations. Moreover, to boost the performance, we argue that weak augmentations matter to represent a more reliable relation, and leverage momentum strategy for practical efficiency. Experimental results show that our proposed ReSSL significantly outperforms the previous state-of-the-art algorithms in terms of both performance and training efficiency.",
    "authors": [
      "Zheng, Mingkai",
      "You, Shan",
      "Wang, Fei",
      "Qian, Chen",
      "Zhang, Changshui",
      "Wang, Xiaogang",
      "Xu, Chang"
    ]
  },
  {
    "id": "14f2ebeab937ca128186e7ba876faef9",
    "title": "Learning to See by Looking at Noise",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/14f2ebeab937ca128186e7ba876faef9-Paper.pdf",
    "abstract": "Current vision systems are trained on huge datasets, and these datasets come with costs: curation is expensive, they inherit human biases, and there are concerns over privacy and usage rights. To counter these costs, interest has surged in learning from cheaper data sources, such as unlabeled images. In this paper we go a step further and ask if we can do away with real image datasets entirely, instead learning from procedural noise processes. We investigate a suite of image generation models that produce images from simple random processes. These are then used as training data for a visual representation learner with a contrastive loss. In particular, we study statistical image models, randomly initialized deep generative models, and procedural graphics models.Our findings show that it is important for the noise to capture certain structural properties of real data but that good performance can be achieved even with processes that are far from realistic. We also find that diversity is a key property to learn good representations.",
    "authors": [
      "Baradad Jurjo, Manel",
      "Wulff, Jonas",
      "Wang, Tongzhou",
      "Isola, Phillip",
      "Torralba, Antonio"
    ]
  },
  {
    "id": "14faf969228fc18fcd4fcf59437b0c97",
    "title": "Explicit loss asymptotics in the gradient descent training of neural networks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/14faf969228fc18fcd4fcf59437b0c97-Paper.pdf",
    "abstract": "Current theoretical results on optimization trajectories of neural networks trained by gradient descent typically have the form of rigorous but potentially loose bounds on the loss values. In the present work we take a different approach and show that the learning trajectory of a wide network in a lazy training regime can be characterized by an explicit asymptotic at large training times. Specifically, the leading term in the asymptotic expansion of the loss behaves as a power law $L(t) \\sim C t^{-\\xi}$ with exponent $\\xi$ expressed only through the data dimension, the smoothness of the activation function, and the class of function being approximated. Our results are based on spectral analysis of the integral operator representing the linearized evolution of a large network trained on the expected loss. Importantly, the techniques we employ do not require a specific form of the data distribution, for example Gaussian, thus making our findings sufficiently universal.   ",
    "authors": [
      "Velikanov, Maksim",
      "Yarotsky, Dmitry"
    ]
  },
  {
    "id": "1517c8664be296f0d87d9e5fc54fdd60",
    "title": "Test-Time Personalization with a Transformer for Human Pose Estimation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1517c8664be296f0d87d9e5fc54fdd60-Paper.pdf",
    "abstract": "We propose to personalize a 2D human pose estimator given a set of test images of a person without using any manual annotations. While there is a significant advancement in human pose estimation, it is still very challenging for a model to generalize to different unknown environments and unseen persons. Instead of using a fixed model for every test case, we adapt our pose estimator during test time to exploit person-specific information. We first train our model on diverse data with both a supervised and a self-supervised pose estimation objectives jointly. We use a Transformer model to build a transformation between the self-supervised keypoints and the supervised keypoints. During test time, we personalize and adapt our model by fine-tuning with the self-supervised objective. The pose is then improved by transforming the updated self-supervised keypoints. We experiment with multiple datasets and show significant improvements on pose estimations with our self-supervised personalization. Project page with code is available at https://liyz15.github.io/TTP/.",
    "authors": [
      "Li, Yizhuo",
      "Hao, Miao",
      "Di, Zonglin",
      "Gundavarapu, Nitesh Bharadwaj",
      "Wang, Xiaolong"
    ]
  },
  {
    "id": "151de84cca69258b17375e2f44239191",
    "title": "Towards Scalable Unpaired Virtual Try-On via Patch-Routed Spatially-Adaptive GAN",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/151de84cca69258b17375e2f44239191-Paper.pdf",
    "abstract": "Image-based virtual try-on is one of the most promising applications of human-centric image generation due to its tremendous real-world potential. Yet, as most try-on approaches fit in-shop garments onto a target person, they require the laborious and restrictive construction of a paired training dataset, severely limiting their scalability. While a few recent works attempt to transfer garments directly from one person to another, alleviating the need to collect paired datasets, their performance is impacted by the lack of paired (supervised) information.  In particular, disentangling style and spatial information of the garment becomes a challenge, which existing methods either address by requiring auxiliary data or extensive online optimization procedures, thereby still inhibiting their scalability. To achieve a scalable virtual try-on system that can transfer arbitrary garments between a source and a target person in an unsupervised manner, we thus propose a texture-preserving end-to-end network, the PAtch-routed SpaTially-Adaptive GAN (PASTA-GAN), that facilitates real-world unpaired virtual try-on. Specifically, to disentangle the style and spatial information of each garment, PASTA-GAN consists of an innovative patch-routed disentanglement module for successfully retaining garment texture and shape characteristics.  Guided by the source person's keypoints, the patch-routed disentanglement module first decouples garments into normalized patches, thus eliminating the inherent spatial information of the garment, and then reconstructs the normalized patches to the warped garment complying with the target person pose. Given the warped garment, PASTA-GAN further introduces novel spatially-adaptive residual blocks that guide the generator to synthesize more realistic garment details. Extensive comparisons with paired and unpaired approaches demonstrate the superiority of PASTA-GAN, highlighting its ability to generate high-quality try-on images when faced with a large variety of garments(e.g. vests, shirts, pants), taking a crucial step towards real-world scalable try-on.",
    "authors": [
      "Xie, Zhenyu",
      "Huang, Zaiyu",
      "Zhao, Fuwei",
      "Dong, Haoye",
      "Kampffmeyer, Michael",
      "Liang, Xiaodan"
    ]
  },
  {
    "id": "1531beb762df4029513ebf9295e0d34f",
    "title": "Bias Out-of-the-Box: An Empirical Analysis of Intersectional Occupational Biases in Popular Generative Language Models",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1531beb762df4029513ebf9295e0d34f-Paper.pdf",
    "abstract": "The capabilities of natural language models trained on large-scale data have increased immensely over the past few years. Open source libraries such as HuggingFace have made these models easily available and accessible. While prior research has identified biases in large language models, this paper considers biases contained in the most popular versions of these models when applied `out-of-the-box' for downstream tasks. We focus on generative language models as they are well-suited for extracting biases inherited from training data. Specifically, we conduct an in-depth analysis of GPT-2, which is the most downloaded text generation model on HuggingFace, with over half a million downloads per month. We assess biases related to occupational associations for different protected categories by intersecting gender with religion, sexuality, ethnicity, political affiliation, and continental name origin. Using a template-based data collection pipeline, we collect 396K sentence completions made by GPT-2 and find: (i) The machine-predicted jobs are less diverse and more stereotypical for women than for men, especially for intersections; (ii) Intersectional interactions are highly relevant for occupational associations, which we quantify by fitting 262 logistic models; (iii) For most occupations, GPT-2 reflects the skewed gender and ethnicity distribution found in US Labor Bureau data, and even pulls the societally-skewed distribution towards gender parity in cases where its predictions deviate from real labor market observations. This raises the normative question of what language models \\textit{should} learn - whether they should reflect or correct for existing inequalities.",
    "authors": [
      "Kirk, Hannah Rose",
      "Jun, Yennie",
      "Volpin, Filippo",
      "Iqbal, Haider",
      "Benussi, Elias",
      "Dreyer, Frederic",
      "Shtedritski, Aleksandar",
      "Asano, Yuki"
    ]
  },
  {
    "id": "157792e4abb490f99dbd738483e0d2d4",
    "title": "Weisfeiler and Lehman Go Cellular: CW Networks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/157792e4abb490f99dbd738483e0d2d4-Paper.pdf",
    "abstract": "Graph Neural Networks (GNNs) are limited in their expressive power, struggle with long-range interactions and lack a principled way to model higher-order structures. These problems can be attributed to the strong coupling between the computational graph and the input graph structure. The recently proposed Message Passing Simplicial Networks naturally decouple these elements by performing message passing on the clique complex of the graph. Nevertheless, these models can be severely constrained by the rigid combinatorial structure of Simplicial Complexes (SCs). In this work, we extend recent theoretical results on SCs to regular Cell Complexes, topological objects that flexibly subsume SCs and graphs. We show that this generalisation provides a powerful set of graph \"lifting\" transformations, each leading to a unique hierarchical message passing procedure. The resulting methods, which we collectively call CW Networks (CWNs), are strictly more powerful than the WL test and not less powerful than the 3-WL test. In particular, we demonstrate the effectiveness of one such scheme, based on rings, when applied to molecular graph problems. The proposed architecture benefits from provably larger expressivity than commonly used GNNs, principled modelling of higher-order signals and from compressing the distances between nodes. We demonstrate that our model achieves state-of-the-art results on a variety of molecular datasets.",
    "authors": [
      "Bodnar, Cristian",
      "Frasca, Fabrizio",
      "Otter, Nina",
      "Wang, Yuguang",
      "Li\u00f2, Pietro",
      "Montufar, Guido F.",
      "Bronstein, Michael"
    ]
  },
  {
    "id": "1587965fb4d4b5afe8428a4a024feb0d",
    "title": "Learning Conjoint Attentions for Graph Neural Nets",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1587965fb4d4b5afe8428a4a024feb0d-Paper.pdf",
    "abstract": "In this paper, we present Conjoint Attentions (CAs), a class of novel learning-to-attend strategies for graph neural networks (GNNs). Besides considering the layer-wise node features propagated within the GNN, CAs can additionally incorporate various structural interventions, such as node cluster embedding, and higher-order structural correlations that can be learned outside of GNN, when computing attention scores. The node features that are regarded as significant by the conjoint criteria are therefore more likely to be propagated in the GNN. Given the novel Conjoint Attention strategies, we then propose Graph conjoint attention networks (CATs) that can learn representations embedded with significant latent features deemed by the Conjoint Attentions. Besides, we theoretically validate the discriminative capacity of CATs.  CATs utilizing the proposed Conjoint Attention strategies have been extensively tested in well-established benchmarking datasets and comprehensively compared with state-of-the-art baselines. The obtained notable performance demonstrates the effectiveness of the proposed Conjoint Attentions.",
    "authors": [
      "He, Tiantian",
      "Ong, Yew Soon",
      "Bai, L"
    ]
  },
  {
    "id": "15a50c8ba6a0002a2fa7e5d8c0a40bd9",
    "title": "Hybrid Regret Bounds for Combinatorial Semi-Bandits and Adversarial Linear Bandits",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/15a50c8ba6a0002a2fa7e5d8c0a40bd9-Paper.pdf",
    "abstract": "This study aims to develop bandit algorithms that automatically exploit tendencies of certain environments to improve performance, without any prior knowledge regarding the environments. We first propose an algorithm for combinatorial semi-bandits with a hybrid regret bound that includes two main features: a best-of-three-worlds guarantee and multiple data-dependent regret bounds. The former means that the algorithm will work nearly optimally in all environments in an adversarial setting, a stochastic setting, or a stochastic setting with adversarial corruptions. The latter implies that, even if the environment is far from exhibiting stochastic behavior, the algorithm will perform better as long as the environment is \"easy\" in terms of certain metrics. The metrics w.r.t. the easiness referred to in this paper include cumulative loss for optimal actions, total quadratic variation of losses, and path-length of a loss sequence. We also show hybrid data-dependent regret bounds for adversarial linear bandits, which include a first path-length regret bound that is tight up to logarithmic factors.",
    "authors": [
      "Ito, Shinji"
    ]
  },
  {
    "id": "15c00b5250ddedaabc203b67f8b034fd",
    "title": "Pay Better Attention to Attention: Head Selection in Multilingual and Multi-Domain Sequence Modeling",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/15c00b5250ddedaabc203b67f8b034fd-Paper.pdf",
    "abstract": "Multi-head attention has each of the attention heads collect salient information from different parts of an input sequence, making it a powerful mechanism for sequence modeling. Multilingual and multi-domain learning are common scenarios for sequence modeling, where the key challenge is to maximize positive transfer and mitigate negative interference across languages and domains. In this paper, we find that non-selective attention sharing is sub-optimal for achieving good generalization across all languages and domains. We further propose attention sharing strategies to facilitate parameter sharing and specialization in multilingual and multi-domain sequence modeling. Our approach automatically learns shared and specialized attention heads for different languages and domains. Evaluated in various tasks including speech recognition, text-to-text and speech-to-text translation, the proposed attention sharing strategies consistently bring gains to sequence models built upon multi-head attention. For speech-to-text translation, our approach yields an average of $+2.0$ BLEU over $13$ language directions in multilingual setting and $+2.0$ BLEU over $3$ domains in multi-domain setting.",
    "authors": [
      "Gong, Hongyu",
      "Tang, Yun",
      "Pino, Juan",
      "Li, Xian"
    ]
  },
  {
    "id": "15cf76466b97264765356fcc56d801d1",
    "title": "Cardinality-Regularized Hawkes-Granger Model",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/15cf76466b97264765356fcc56d801d1-Paper.pdf",
    "abstract": "We propose a new sparse Granger-causal learning framework for temporal event data. We focus on a specific class of point processes called the Hawkes process. We begin by pointing out that most of the existing sparse causal learning algorithms for the Hawkes process suffer from a singularity in maximum likelihood estimation. As a result, their sparse solutions can appear only as numerical artifacts. In this paper, we propose a mathematically well-defined sparse causal learning framework based on a cardinality-regularized Hawkes process, which remedies the pathological issues of existing approaches. We leverage the proposed algorithm for the task of instance-wise causal event analysis, where sparsity plays a critical role. We validate the proposed framework with two real use-cases, one from the power grid and the other from the cloud data center management domain. ",
    "authors": [
      "Ide, Tsuyoshi",
      "Kollias, Georgios",
      "Phan, Dzung",
      "Abe, Naoki"
    ]
  },
  {
    "id": "15de21c670ae7c3f6f3f1f37029303c9",
    "title": "Aligned Structured Sparsity Learning for Efficient Image Super-Resolution",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/15de21c670ae7c3f6f3f1f37029303c9-Paper.pdf",
    "abstract": "Lightweight image super-resolution (SR) networks have obtained promising results with moderate model size. Many SR methods have focused on designing lightweight architectures, which neglect to further reduce the redundancy of network parameters. On the other hand, model compression techniques, like neural architecture search and knowledge distillation, typically consume considerable memory and computation resources. In contrast, network pruning is a cheap and effective model compression technique. However, it is hard to be applied to SR networks directly, because filter pruning for residual blocks is well-known tricky. To address the above issues, we propose aligned structured sparsity learning (ASSL), which introduces a weight normalization layer and applies $L_2$ regularization to the scale parameters for sparsity. To align the pruned locations across different layers, we propose a \\emph{sparsity structure alignment} penalty term, which minimizes the norm of soft mask gram matrix. We apply aligned structured sparsity learning strategy to train efficient image SR network, named as ASSLN, with smaller model size and lower computation than state-of-the-art methods. We conduct extensive comparisons with lightweight SR networks. Our ASSLN achieves superior performance gains over recent methods quantitatively and visually.",
    "authors": [
      "Zhang, Yulun",
      "Wang, Huan",
      "Qin, Can",
      "Fu, Yun"
    ]
  },
  {
    "id": "15f99f2165aa8c86c9dface16fefd281",
    "title": "Why Lottery Ticket Wins? A Theoretical Perspective of Sample Complexity on Sparse Neural Networks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/15f99f2165aa8c86c9dface16fefd281-Paper.pdf",
    "abstract": "The lottery ticket hypothesis (LTH) states that learning on a properly pruned network (the winning ticket) has improved test accuracy over the original unpruned network. Although LTH has been justified empirically in a broad range of deep neural network (DNN) involved applications like computer vision and natural language processing, the theoretical validation of the improved generalization of a winning ticket remains elusive. To the best of our knowledge, our work, for the first time, characterizes the performance of training a pruned neural network by analyzing the geometric structure of the objective function and the sample complexity to achieve zero generalization error. We show that the convex region near a desirable model with guaranteed generalization enlarges as the neural network model is pruned, indicating the structural importance of a winning ticket. Moreover, as the algorithm for training a pruned neural network is specified as an (accelerated) stochastic gradient descent algorithm, we theoretically show that the number of samples required for achieving zero generalization error is proportional to the number of the non-pruned weights in the hidden layer. With a fixed number of samples, training a pruned neural network enjoys a faster convergence rate to the desired model than training the original unpruned one, providing a formal justification of the improved generalization of the winning ticket. Our theoretical results are acquired from learning a pruned neural network of one hidden layer, while experimental results are further provided to justify the implications in pruning multi-layer neural networks.",
    "authors": [
      "Zhang, Shuai",
      "Wang, Meng",
      "Liu, Sijia",
      "Chen, Pin-Yu",
      "Xiong, Jinjun"
    ]
  },
  {
    "id": "161882dd2d19c716819081aee2c08b98",
    "title": "Constrained Robust Submodular Partitioning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/161882dd2d19c716819081aee2c08b98-Paper.pdf",
    "abstract": "In the robust submodular partitioning problem, we aim to allocate a set of items into $m$ blocks, so that the evaluation of the minimum block according to a submodular function is maximized. Robust submodular partitioning promotes the diversity of every block in the partition. It has many applications in machine learning, e.g., partitioning data for distributed training so that the gradients computed on every block are consistent. We study an extension of the robust submodular partition problem with additional constraints (e.g., cardinality, multiple matroids, and/or knapsack) on every block. For example, when partitioning data for distributed training, we can add a constraint that the number of samples of each class is the same in each partition block, ensuring data balance. We present two classes of algorithms, i.e., Min-Block Greedy based algorithms (with an $\\Omega(1/m)$ bound), and Round-Robin Greedy based algorithms (with a constant bound) and show that under various constraints, they still have good approximation guarantees. Interestingly, while normally the latter runs in only weakly polynomial time, we show that using the two together yields strongly polynomial running time while preserving the approximation guarantee. Lastly, we apply the algorithms on a real-world machine learning data partitioning problem showing good results.",
    "authors": [
      "Wang, Shengjie",
      "Zhou, Tianyi",
      "Lavania, Chandrashekhar",
      "Bilmes, Jeff A"
    ]
  },
  {
    "id": "161c5c5ad51fcc884157890511b3c8b0",
    "title": "Online Knapsack with Frequency Predictions",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/161c5c5ad51fcc884157890511b3c8b0-Paper.pdf",
    "abstract": "There has been recent interest in using machine-learned predictions to improve the worst-case guarantees of online algorithms.  In this paper we continue this line of work by studying the online knapsack problem, but with very weak predictions: in the form of knowing an upper and lower bound for the number of items of each value.  We systematically derive online algorithms that attain the best possible competitive ratio for any fixed prediction; we also extend the results to more general settings such as generalized one-way trading and two-stage online knapsack. Our work shows that even seemingly weak predictions can be utilized effectively to provably improve the performance of online algorithms.",
    "authors": [
      "Im, Sungjin",
      "Kumar, Ravi",
      "Montazer Qaem, Mahshid",
      "Purohit, Manish"
    ]
  },
  {
    "id": "162d18156abe38a3b32851b72b1d44f5",
    "title": "On Component Interactions in Two-Stage Recommender Systems",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/162d18156abe38a3b32851b72b1d44f5-Paper.pdf",
    "abstract": "Thanks to their scalability, two-stage recommenders are used by many of today's largest online platforms, including YouTube, LinkedIn, and Pinterest. These systems produce recommendations in two steps: (i) multiple nominators\u2014tuned for low prediction latency\u2014preselect a small subset of candidates from the whole item pool; (ii) a slower but more accurate ranker further narrows down the nominated items, and serves to the user. Despite their popularity, the literature on two-stage recommenders is relatively scarce, and the algorithms are often treated as mere sums of their parts. Such treatment presupposes that the two-stage performance is explained by the behavior of the individual components in isolation. This is not the case: using synthetic and real-world data, we demonstrate that interactions between the ranker and the nominators substantially affect the overall performance. Motivated by these findings, we derive a generalization lower bound which shows that independent nominator training can lead to performance on par with uniformly random recommendations. We find that careful design of item pools, each assigned to a different nominator, alleviates these issues. As manual search for a good pool allocation is difficult, we propose to learn one instead using a Mixture-of-Experts based approach. This significantly improves both precision and recall at $K$.",
    "authors": [
      "Hron, Jiri",
      "Krauth, Karl",
      "Jordan, Michael",
      "Kilbertus, Niki"
    ]
  },
  {
    "id": "16437d40c29a1a7b1e78143c9c38f289",
    "title": "Lip to Speech Synthesis with Visual Context Attentional GAN",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/16437d40c29a1a7b1e78143c9c38f289-Paper.pdf",
    "abstract": "In this paper, we propose a novel lip-to-speech generative adversarial network, Visual Context Attentional GAN (VCA-GAN), which can jointly model local and global lip movements during speech synthesis. Specifically, the proposed VCA-GAN synthesizes the speech from local lip visual features by finding a mapping function of viseme-to-phoneme, while global visual context is embedded into the intermediate layers of the generator to clarify the ambiguity in the mapping induced by homophene. To achieve this, a visual context attention module is proposed where it encodes global representations from the local visual features, and provides the desired global visual context corresponding to the given coarse speech representation to the generator through audio-visual attention. In addition to the explicit modelling of local and global visual representations, synchronization learning is introduced as a form of contrastive learning that guides the generator to synthesize a speech in sync with the given input lip movements. Extensive experiments demonstrate that the proposed VCA-GAN outperforms existing state-of-the-art and is able to effectively synthesize the speech from multi-speaker that has been barely handled in the previous works.",
    "authors": [
      "Kim, Minsu",
      "Hong, Joanna",
      "Ro, Yong Man"
    ]
  },
  {
    "id": "164bf317ea19ccfd9e97853edc2389f4",
    "title": "Non-convex Distributionally Robust Optimization: Non-asymptotic Analysis",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/164bf317ea19ccfd9e97853edc2389f4-Paper.pdf",
    "abstract": "Distributionally robust optimization (DRO) is a widely-used approach to learn models that are robust against distribution shift. Compared with the standard optimization setting, the objective function in DRO is more difficult to optimize, and most of the existing theoretical results make strong assumptions on the loss function. In this work we bridge the gap by studying DRO algorithms for general smooth non-convex losses. By carefully exploiting the specific form of the DRO objective, we are able to provide non-asymptotic convergence guarantees even though the objective function is possibly non-convex, non-smooth and has unbounded gradient noise. In particular, we prove that a special algorithm called the mini-batch normalized gradient descent with momentum, can find an $\\epsilon$-first-order stationary point within $\\mathcal O(\\epsilon^{-4})$ gradient complexity. We also discuss the conditional value-at-risk (CVaR) setting, where we propose a penalized DRO objective based on a smoothed version of the CVaR that allows us to obtain a similar convergence guarantee. We finally verify our theoretical results in a number of tasks and find that the proposed algorithm can consistently achieve prominent acceleration.",
    "authors": [
      "Jin, Jikai",
      "Zhang, Bohang",
      "Wang, Haiyang",
      "Wang, Liwei"
    ]
  },
  {
    "id": "165a59f7cf3b5c4396ba65953d679f17",
    "title": "Goal-Aware Cross-Entropy for Multi-Target Reinforcement Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/165a59f7cf3b5c4396ba65953d679f17-Paper.pdf",
    "abstract": "Learning in a multi-target environment without prior knowledge about the targets requires a large amount of samples and makes generalization difficult. To solve this problem, it is important to be able to discriminate targets through semantic understanding. In this paper, we propose goal-aware cross-entropy (GACE) loss, that can be utilized in a self-supervised way using auto-labeled goal states alongside reinforcement learning. Based on the loss, we then devise goal-discriminative attention networks (GDAN) which utilize the goal-relevant information to focus on the given instruction. We evaluate the proposed methods on visual navigation and robot arm manipulation tasks with multi-target environments and show that GDAN outperforms the state-of-the-art methods in terms of task success ratio, sample efficiency, and generalization. Additionally, qualitative analyses demonstrate that our proposed method can help the agent become aware of and focus on the given instruction clearly, promoting goal-directed behavior.",
    "authors": [
      "Kim, Kibeom",
      "Lee, Min Whoo",
      "Kim, Yoonsung",
      "Ryu, JeHwan",
      "Lee, Minsu",
      "Zhang, Byoung-Tak"
    ]
  },
  {
    "id": "167434fa6219316417cd4160c0c5e7d2",
    "title": "Smooth Normalizing Flows",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/167434fa6219316417cd4160c0c5e7d2-Paper.pdf",
    "abstract": "Normalizing flows are a promising tool for modeling probability distributions in physical systems. While state-of-the-art flows accurately approximate distributions and energies, applications in physics additionally require smooth energies to compute forces and higher-order derivatives. Furthermore, such densities are often defined on non-trivial topologies. A recent example are Boltzmann Generators for generating 3D-structures of peptides and small proteins. These generative models leverage the space of internal coordinates (dihedrals, angles, and bonds), which is a product of hypertori and compact intervals. In this work, we introduce a class of smooth mixture transformations working on both compact intervals and hypertori.Mixture transformations employ root-finding methods to invert them in practice, which has so far prevented bi-directional flow training. To this end, we show that parameter gradients and forces of such inverses can be computed from forward evaluations via the inverse function theorem.We demonstrate two advantages of such smooth flows: they allow training by force matching to simulation data and can be used as potentials in molecular dynamics simulations. ",
    "authors": [
      "K\u00f6hler, Jonas",
      "Kr\u00e4mer, Andreas",
      "Noe, Frank"
    ]
  },
  {
    "id": "1680829293f2a8541efa2647a0290f88",
    "title": "MetaAvatar: Learning Animatable Clothed Human Models from Few Depth Images",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1680829293f2a8541efa2647a0290f88-Paper.pdf",
    "abstract": "In this paper, we aim to create generalizable and controllable neural signed distance fields (SDFs) that represent clothed humans from monocular depth observations. Recent advances in deep learning, especially neural implicit representations, have enabled human shape reconstruction and controllable avatar generation from different sensor inputs. However, to generate realistic cloth deformations from novel input poses, watertight meshes or dense full-body scans are usually needed as inputs. Furthermore, due to the difficulty of effectively modeling pose-dependent cloth deformations for diverse body shapes and cloth types, existing approaches resort to per-subject/cloth-type optimization from scratch, which is computationally expensive. In contrast, we propose an approach that can quickly generate realistic clothed human avatars, represented as controllable neural SDFs, given only monocular depth images. We achieve this by using meta-learning to learn an initialization of a hypernetwork that predicts the parameters of neural SDFs. The hypernetwork is conditioned on human poses and represents a clothed neural avatar that deforms non-rigidly according to the input poses. Meanwhile, it is meta-learned to effectively incorporate priors of diverse body shapes and cloth types and thus can be much faster to fine-tune, compared to models trained from scratch. We qualitatively and quantitatively show that our approach outperforms state-of-the-art approaches that require complete meshes as inputs while our approach requires only depth frames as inputs and runs orders of magnitudes faster. Furthermore, we demonstrate that our meta-learned hypernetwork is very robust, being the first to generate avatars with realistic dynamic cloth deformations given as few as 8 monocular depth frames.",
    "authors": [
      "Wang, Shaofei",
      "Mihajlovic, Marko",
      "Ma, Qianli",
      "Geiger, Andreas",
      "Tang, Siyu"
    ]
  },
  {
    "id": "1680e9fa7b4dd5d62ece800239bb53bd",
    "title": "Distributed Principal Component Analysis with Limited Communication",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1680e9fa7b4dd5d62ece800239bb53bd-Paper.pdf",
    "abstract": "We study efficient distributed algorithms for the fundamental problem of principal component analysis and leading eigenvector computation on the sphere, when the data are randomly distributed among a set of computational nodes. We propose a new quantized variant of Riemannian gradient descent to solve this problem, and prove that the algorithm converges with high probability under a set of necessary spherical-convexity properties. We give bounds on the number of bits transmitted by the algorithm under common initialization schemes, and investigate the dependency on the problem dimension in each case.",
    "authors": [
      "Alimisis, Foivos",
      "Davies, Peter",
      "Vandereycken, Bart",
      "Alistarh, Dan"
    ]
  },
  {
    "id": "16837163fee34175358a47e0b51485ff",
    "title": "Newton-LESS: Sparsification without Trade-offs for the Sketched Newton Update",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/16837163fee34175358a47e0b51485ff-Paper.pdf",
    "abstract": "In second-order optimization, a potential bottleneck can be computing the Hessian matrix of the optimized function at every iteration. Randomized sketching has emerged as a powerful technique for constructing estimates of the Hessian which can be used to perform approximate Newton steps. This involves multiplication by a random sketching matrix, which introduces a trade-off between the computational cost of sketching and the convergence rate of the optimization. A theoretically desirable but practically much too expensive choice is to use a dense Gaussian sketching matrix, which produces unbiased estimates of the exact Newton step and offers strong problem-independent convergence guarantees. We show that the Gaussian matrix can be drastically sparsified, substantially reducing the computational cost, without affecting its convergence properties in any way. This approach, called Newton-LESS, is based on a recently introduced sketching technique: LEverage Score Sparsified (LESS) embeddings. We prove that Newton-LESS enjoys nearly the same problem-independent local convergence rate as Gaussian embeddings for a large class of functions. In particular, this leads to a new state-of-the-art convergence result for an iterative least squares solver. Finally, we substantially extend LESS embeddings to include uniformly sparsified random sign matrices which can be implemented efficiently and perform well in numerical experiments. ",
    "authors": [
      "Derezinski, Michal",
      "Lacotte, Jonathan",
      "Pilanci, Mert",
      "Mahoney, Michael W."
    ]
  },
  {
    "id": "168908dd3227b8358eababa07fcaf091",
    "title": "Confident Anchor-Induced Multi-Source Free Domain Adaptation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/168908dd3227b8358eababa07fcaf091-Paper.pdf",
    "abstract": "Unsupervised domain adaptation has attracted appealing academic attentions by transferring knowledge from labeled source domain to unlabeled target domain. However, most existing methods assume the source data are drawn from a single domain, which cannot be successfully applied to explore complementarily transferable knowledge from multiple source domains with large distribution discrepancies. Moreover, they require access to source data during training, which are inefficient and unpractical due to privacy preservation and memory storage. To address these challenges, we develop a novel Confident-Anchor-induced multi-source-free Domain Adaptation (CAiDA) model, which is a pioneer exploration of knowledge adaptation from multiple source domains to the unlabeled target domain without any source data, but with only pre-trained source models. Specifically, a source-specific transferable perception module is proposed to automatically quantify the contributions of the complementary knowledge transferred from multi-source domains to the target domain. To generate pseudo labels for the target domain without access to the source data, we develop a confident-anchor-induced pseudo label generator by constructing a confident anchor group and assigning each unconfident target sample with a semantic-nearest confident anchor. Furthermore, a class-relationship-aware consistency loss is proposed to preserve consistent inter-class relationships by aligning soft confusion matrices across domains. Theoretical analysis answers why multi-source domains are better than a single source domain, and establishes a novel learning bound to show the effectiveness of exploiting multi-source domains. Experiments on several representative datasets illustrate the superiority of our proposed CAiDA model. The code is available at https://github.com/Learning-group123/CAiDA.",
    "authors": [
      "Dong, Jiahua",
      "Fang, Zhen",
      "Liu, Anjin",
      "Sun, Gan",
      "Liu, Tongliang"
    ]
  },
  {
    "id": "16a5cdae362b8d27a1d8f8c7b78b4330",
    "title": "Word2Fun: Modelling Words as Functions for Diachronic Word Representation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/16a5cdae362b8d27a1d8f8c7b78b4330-Paper.pdf",
    "abstract": "Word meaning may change over time as a reflection of changes in human society. Therefore, modeling time in word representation is necessary for some diachronic tasks. Most existing diachronic word representation approaches train the embeddings separately for each pre-grouped time-stamped corpus and align these embeddings, e.g., by orthogonal projections, vector initialization, temporal referencing, and compass. However, not only does word meaning change in a short time, word meaning may also be subject to evolution over long timespans, thus resulting in a unified continuous process. A recent approach called `DiffTime'  models semantic evolution as functions parameterized by multiple-layer nonlinear neural networks over time. In this paper, we will carry on this line of work by learning explicit functions over time  for each word. Our approach, called `Word2Fun', reduces the space complexity from $\\mathcal{O}(TVD)$ to $\\mathcal{O}(kVD)$ where $k$  is a small constant ($k \\ll T $). In particular, a specific instance based on polynomial functions could provably approximate any function modeling word evolution with a given negligible error thanks to the Weierstrass Approximation Theorem. The effectiveness of the proposed approach is evaluated in diverse tasks including time-aware word clustering, temporal analogy, and semantic change detection. Code at: {\\url{https://github.com/wabyking/Word2Fun.git}}. ",
    "authors": [
      "Wang, Benyou",
      "Di Buccio, Emanuele",
      "Melucci, Massimo"
    ]
  },
  {
    "id": "16bda725ae44af3bb9316f416bd13b1b",
    "title": "Iteratively Reweighted Least Squares for Basis Pursuit with Global Linear Convergence Rate",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/16bda725ae44af3bb9316f416bd13b1b-Paper.pdf",
    "abstract": "The recovery of sparse data is at the core of many applications in machine learning and signal processing. While such problems can be tackled using $\\ell_1$-regularization as in the LASSO estimator and in the Basis Pursuit approach, specialized algorithms are typically required to solve the corresponding high-dimensional non-smooth optimization for large instances.Iteratively Reweighted Least Squares (IRLS) is a widely used algorithm for this purpose due to its excellent numerical performance. However, while existing theory is able to guarantee convergence of this algorithm to the minimizer, it does not provide a global convergence rate. In this paper, we prove that a variant of IRLS converges \\emph{with a global linear rate} to a sparse solution, i.e., with a linear error decrease occurring immediately from any initialization if the measurements fulfill the usual null space property assumption. We support our theory by numerical experiments showing that our linear rate captures the correct dimension dependence. We anticipate that our theoretical findings will lead to new insights for many other use cases of the IRLS algorithm, such as in low-rank matrix recovery.",
    "authors": [
      "K\u00fcmmerle, Christian",
      "Mayrink Verdun, Claudio",
      "St\u00f6ger, Dominik"
    ]
  },
  {
    "id": "16c0d78ef6a76b5c247113a4c9514059",
    "title": "Low-Rank Constraints for Fast Inference in Structured Models",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/16c0d78ef6a76b5c247113a4c9514059-Paper.pdf",
    "abstract": "Structured distributions, i.e. distributions over combinatorial spaces, are commonly used to learn latent probabilistic representations from observed data. However, scaling these models is bottlenecked by the high computational and memory complexity with respect to the size of the latent representations. Common models such as Hidden Markov Models (HMMs) and Probabilistic Context-Free Grammars (PCFGs) require time and space quadratic and cubic in the number of hidden states respectively. This work demonstrates a simple approach to reduce the computational and memory complexity of a large class of structured models. We show that by viewing the central inference step as a matrix-vector product and using a low-rank constraint, we can trade off model expressivity and speed via the rank.  Experiments with neural parameterized structured models for language modeling, polyphonic music modeling, unsupervised grammar induction, and video modeling show that our approach matches the accuracy of standard models at large state spaces while providing practical speedups.",
    "authors": [
      "Chiu, Justin",
      "Deng, Yuntian",
      "Rush, Alexander"
    ]
  },
  {
    "id": "16d11e9595188dbad0418a85f0351aba",
    "title": "Accumulative Poisoning Attacks on Real-time Data",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/16d11e9595188dbad0418a85f0351aba-Paper.pdf",
    "abstract": "Collecting training data from untrusted sources exposes machine learning services to poisoning adversaries, who maliciously manipulate training data to degrade the model accuracy. When trained on offline datasets, poisoning adversaries have to inject the poisoned data in advance before training, and the order of feeding these poisoned batches into the model is stochastic. In contrast, practical systems are more usually trained/fine-tuned on sequentially captured real-time data, in which case poisoning adversaries could dynamically poison each data batch according to the current model state. In this paper, we focus on the real-time settings and propose a new attacking strategy, which affiliates an accumulative phase with poisoning attacks to secretly (i.e., without affecting accuracy) magnify the destructive effect of a (poisoned) trigger batch. By mimicking online learning and federated learning on MNIST and CIFAR-10, we show that model accuracy significantly drops by a single update step on the trigger batch after the accumulative phase. Our work validates that a well-designed but straightforward attacking strategy can dramatically amplify the poisoning effects, with no need to explore complex techniques.",
    "authors": [
      "Pang, Tianyu",
      "Yang, Xiao",
      "Dong, Yinpeng",
      "Su, Hang",
      "Zhu, Jun"
    ]
  },
  {
    "id": "16f852a6d01b6065c8ff5cc11caae9c6",
    "title": "UCB-based Algorithms for Multinomial Logistic Regression Bandits",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/16f852a6d01b6065c8ff5cc11caae9c6-Paper.pdf",
    "abstract": "Out of the rich family of generalized linear bandits, perhaps the most well studied ones are logistic bandits that are used in problems with binary rewards: for instance, when the learner aims to maximize the profit over a user that can select one of two possible outcomes (e.g., `click' vs `no-click'). Despite remarkable recent progress and improved algorithms for logistic bandits, existing works do not address practical situations where the number of outcomes that can be selected by the user is larger than two (e.g., `click', `show me later', `never show again', `no click'). In this paper, we study such an extension. We use multinomial logit (MNL) to model the probability of each one of $K+1\\geq 2$ possible outcomes (+1 stands for the `not click' outcome): we assume that for a learner's action $\\mathbf{x}_t$, the user selects one of $K+1\\geq 2$ outcomes, say outcome $i$, with a MNL probabilistic model with corresponding unknown parameter $\\bar{\\boldsymbol{\\theta}}_{\\ast i}$. Each outcome $i$ is also associated with a revenue parameter $\\rho_i$ and the goal is to maximize the expected revenue. For this problem, we present MNL-UCB, an upper confidence bound (UCB)-based algorithm, that achieves regret $\\tilde{\\mathcal{O}}(dK\\sqrt{T})$ with small dependency on problem-dependent constants that can otherwise be arbitrarily large and lead to loose regret bounds. We present numerical simulations that corroborate our theoretical results.",
    "authors": [
      "Amani, Sanae",
      "Thrampoulidis, Christos"
    ]
  },
  {
    "id": "16fa2b0294e410b2551c3bf6965c0853",
    "title": "Estimating the Long-Term Effects of Novel Treatments",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/16fa2b0294e410b2551c3bf6965c0853-Paper.pdf",
    "abstract": "Policy makers often need to estimate the long-term effects of novel treatments, while only having historical data of older treatment options. We propose a surrogate-based approach using a long-term dataset where only past treatments were administered and a short-term dataset where novel treatments have been administered. Our approach generalizes previous surrogate-style methods, allowing for continuous treatments and serially-correlated treatment policies while maintaining consistency and root-n asymptotically normal estimates under a Markovian assumption on the data and the observational policy. Using a semi-synthetic dataset on customer incentives from a major corporation, we evaluate the performance of our method and discuss solutions to practical challenges when deploying our methodology.",
    "authors": [
      "Battocchi, Keith",
      "Dillon, Eleanor",
      "Hei, Maggie",
      "Lewis, Greg",
      "Oprescu, Miruna",
      "Syrgkanis, Vasilis"
    ]
  },
  {
    "id": "1700002963a49da13542e0726b7bb758",
    "title": "Dual Progressive Prototype Network for Generalized Zero-Shot Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1700002963a49da13542e0726b7bb758-Paper.pdf",
    "abstract": "Generalized Zero-Shot Learning (GZSL) aims to recognize new categories with auxiliary semantic information, e.g., category attributes. In this paper, we handle the critical issue of domain shift problem, i.e., confusion between seen and unseen categories, by progressively improving cross-domain transferability and category discriminability of visual representations. Our approach, named Dual Progressive Prototype Network (DPPN), constructs two types of prototypes that record prototypical visual patterns for attributes and categories, respectively. With attribute prototypes, DPPN alternately searches attribute-related local regions and updates corresponding attribute prototypes to progressively explore accurate attribute-region correspondence. This enables DPPN to produce visual representations with accurate attribute localization ability, which benefits the semantic-visual alignment and representation transferability. Besides, along with progressive attribute localization, DPPN further projects category prototypes into multiple spaces to progressively repel visual representations from different categories, which boosts category discriminability. Both attribute and category prototypes are collaboratively learned in a unified framework, which makes visual representations of DPPN transferable and distinctive.Experiments on four benchmarks prove that DPPN effectively alleviates the domain shift problem in GZSL.",
    "authors": [
      "Wang, Chaoqun",
      "Min, Shaobo",
      "Chen, Xuejin",
      "Sun, Xiaoyan",
      "Li, Houqiang"
    ]
  },
  {
    "id": "1714726c817af50457d810aae9d27a2e",
    "title": "Derivative-Free Policy Optimization for Linear Risk-Sensitive and Robust Control Design: Implicit Regularization and Sample Complexity",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1714726c817af50457d810aae9d27a2e-Paper.pdf",
    "abstract": "Direct policy search serves as one of the workhorses in modern reinforcement learning (RL), and its applications in continuous control tasks have recently attracted increasing attention. In this work, we investigate the convergence theory of policy gradient (PG) methods for learning the linear risk-sensitive and robust controller. In particular, we develop PG methods that can be implemented in a derivative-free fashion by sampling system trajectories, and establish both global convergence and sample complexity results in the solutions of two fundamental settings in risk-sensitive and robust control: the finite-horizon linear exponential quadratic Gaussian, and the finite-horizon linear-quadratic disturbance attenuation problems. As a by-product, our results also provide the first sample complexity for the global convergence of PG methods on solving zero-sum linear-quadratic dynamic games, a nonconvex-nonconcave minimax optimization problem that serves as a baseline setting in multi-agent reinforcement learning (MARL) with continuous spaces. One feature of our algorithms is that during the learning phase, a certain level of robustness/risk-sensitivity of the controller is preserved, which we termed as the implicit regularization property, and is an essential requirement in safety-critical control systems. ",
    "authors": [
      "Zhang, Kaiqing",
      "Zhang, Xiangyuan",
      "Hu, Bin",
      "Basar, Tamer"
    ]
  },
  {
    "id": "171ae1bbb81475eb96287dd78565b38b",
    "title": "G-PATE: Scalable Differentially Private Data Generator via Private Aggregation of Teacher Discriminators",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/171ae1bbb81475eb96287dd78565b38b-Paper.pdf",
    "abstract": "Recent advances in machine learning have largely benefited from the massive accessible training data. However, large-scale data sharing has raised great privacy concerns. In this work, we propose a novel privacy-preserving data Generative model based on the PATE framework (G-PATE), aiming to train a scalable differentially private data generator that preserves high generated data utility. Our approach leverages generative adversarial nets to generate data, combined with private aggregation among different discriminators to ensure strong privacy guarantees. Compared to existing approaches, G-PATE significantly improves the use of privacy budgets. In particular, we train a student data generator with an ensemble of teacher discriminators and propose a novel private gradient aggregation mechanism to ensure differential privacy on all information that flows from teacher discriminators to the student generator. In addition, with random projection and gradient discretization, the proposed gradient aggregation mechanism is able to effectively deal with high-dimensional gradient vectors. Theoretically, we prove that G-PATE ensures differential privacy for the data generator.  Empirically, we demonstrate the superiority of G-PATE over prior work through extensive experiments. We show that G-PATE is the first work being able to generate high-dimensional image data with high data utility under limited privacy budgets ($\\varepsilon \\le 1$). Our code is available at https://github.com/AI-secure/G-PATE.",
    "authors": [
      "Long, Yunhui",
      "Wang, Boxin",
      "Yang, Zhuolin",
      "Kailkhura, Bhavya",
      "Zhang, Aston",
      "Gunter, Carl",
      "Li, Bo"
    ]
  },
  {
    "id": "172ef5a94b4dd0aa120c6878fc29f70c",
    "title": "On the Existence of The Adversarial Bayes Classifier",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/172ef5a94b4dd0aa120c6878fc29f70c-Paper.pdf",
    "abstract": "Adversarial robustness is a critical property in a variety of modern machine learning applications. While it has been the subject of several recent theoretical studies, many important questions related to adversarial robustness are still open.  In this work, we study a fundamental question regarding Bayes optimality for adversarial robustness. We provide general sufficient conditions under which the existence of a Bayes optimal classifier can be guaranteed for adversarial robustness. Our results can provide a useful tool for a subsequent study of surrogate losses in adversarial robustness and their consistency properties.",
    "authors": [
      "Awasthi, Pranjal",
      "Frank, Natalie",
      "Mohri, Mehryar"
    ]
  },
  {
    "id": "174a61b0b3eab8c94e0a9e78b912307f",
    "title": "Convex-Concave Min-Max Stackelberg Games",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/174a61b0b3eab8c94e0a9e78b912307f-Paper.pdf",
    "abstract": "Min-max optimization problems (i.e., min-max games) have been attracting a great deal of attention because of their applicability to a wide range of machine learning problems. Although significant progress has been made recently, the literature to date has focused on games with independent strategy sets; little is known about solving games with dependent strategy sets, which can be characterized as min-max Stackelberg games. We introduce two first-order methods that solve a large class of convex-concave min-max Stackelberg games, and show that our methods converge in polynomial time. Min-max Stackelberg games were first studied by Wald, under the posthumous name of Wald\u2019s maximin model, a variant of which is the main paradigm used in robust optimization, which means that our methods can likewise solve many convex robust optimization problems. We observe that the computation of competitive equilibria in Fisher markets also comprises a min-max Stackelberg game.  Further, we demonstrate the efficacy and efficiency of our algorithms in practice by computing competitive equilibria in Fisher markets with varying utility structures. Our experiments suggest potential ways to extend our theoretical results, by demonstrating how different smoothness properties can affect the convergence rate of our algorithms.",
    "authors": [
      "Goktas, Denizalp",
      "Greenwald, Amy"
    ]
  },
  {
    "id": "177db6acfe388526a4c7bff88e1feb15",
    "title": "Misspecified Gaussian Process Bandit Optimization",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/177db6acfe388526a4c7bff88e1feb15-Paper.pdf",
    "abstract": "We consider the problem of optimizing a black-box function based on noisy bandit feedback. Kernelized bandit algorithms have shown strong empirical and theoretical performance for this problem. They heavily rely on the assumption that the model is well-specified, however, and can fail without it. Instead, we introduce and address a \\emph{misspecified} kernelized bandit setting where the unknown function can be $\\epsilon$--uniformly approximated by a function with a bounded norm in some Reproducing Kernel Hilbert Space (RKHS). We design efficient and practical algorithms whose performance degrades minimally in the presence of model misspecification. Specifically, we present two algorithms based on Gaussian process (GP) methods: an optimistic EC-GP-UCB algorithm that requires knowing the misspecification error, and Phased GP Uncertainty Sampling, an elimination-type algorithm that can adapt to unknown model misspecification. We provide upper bounds on their cumulative regret in terms of $\\epsilon$, the time horizon, and the underlying kernel, and we show that our algorithm achieves optimal dependence on $\\epsilon$ with no prior knowledge of misspecification. In addition, in a stochastic contextual setting, we show that EC-GP-UCB can be effectively combined with the regret bound balancing strategy and attain similar regret bounds despite not knowing $\\epsilon$.",
    "authors": [
      "Bogunovic, Ilija",
      "Krause, Andreas"
    ]
  },
  {
    "id": "1796a48fa1968edd5c5d10d42c7b1813",
    "title": "Visual Adversarial Imitation Learning using Variational Models",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1796a48fa1968edd5c5d10d42c7b1813-Paper.pdf",
    "abstract": "Reward function specification, which requires considerable human effort and iteration, remains a major impediment for learning behaviors through deep reinforcement learning. In contrast, providing visual demonstrations of desired behaviors presents an easier and more natural way to teach agents. We consider a setting where an agent is provided a fixed dataset of visual demonstrations illustrating how to perform a task, and must learn to solve the task using the provided demonstrations and unsupervised environment interactions. This setting presents a number of challenges including representation learning for visual observations, sample complexity due to high dimensional spaces, and learning instability due to the lack of a fixed reward or learning signal. Towards addressing these challenges, we develop a variational model-based adversarial imitation learning (V-MAIL) algorithm. The model-based approach provides a strong signal for representation learning, enables sample efficiency, and improves the stability of adversarial training by enabling on-policy learning. Through experiments involving several vision-based locomotion and manipulation tasks, we find that V-MAIL learns successful visuomotor policies in a sample-efficient manner, has better stability compared to prior work, and also achieves higher asymptotic performance. We further find that by transferring the learned models, V-MAIL can learn new tasks from visual demonstrations without any additional environment interactions. All results including videos can be found online at https://sites.google.com/view/variational-mail",
    "authors": [
      "Rafailov, Rafael",
      "Yu, Tianhe",
      "Rajeswaran, Aravind",
      "Finn, Chelsea"
    ]
  },
  {
    "id": "17a3120e4e5fbdc3cb5b5f946809b06a",
    "title": "Object-Aware Regularization for Addressing Causal Confusion in Imitation Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/17a3120e4e5fbdc3cb5b5f946809b06a-Paper.pdf",
    "abstract": "Behavioral cloning has proven to be effective for learning sequential decision-making policies from expert demonstrations. However, behavioral cloning often suffers from the causal confusion problem where a policy relies on the noticeable effect of expert actions due to the strong correlation but not the cause we desire. This paper presents Object-aware REgularizatiOn (OREO), a simple technique that regularizes an imitation policy in an object-aware manner. Our main idea is to encourage a policy to uniformly attend to all semantic objects, in order to prevent the policy from exploiting nuisance variables strongly correlated with expert actions. To this end, we introduce a two-stage approach: (a) we extract semantic objects from images by utilizing discrete codes from a vector-quantized variational autoencoder, and (b) we randomly drop the units that share the same discrete code together, i.e., masking out semantic objects. Our experiments demonstrate that OREO significantly improves the performance of behavioral cloning, outperforming various other regularization and causality-based methods on a variety of Atari environments and a self-driving CARLA environment. We also show that our method even outperforms inverse reinforcement learning methods trained with a considerable amount of environment interaction.",
    "authors": [
      "Park, Jongjin",
      "Seo, Younggyo",
      "Liu, Chang",
      "Zhao, Li",
      "Qin, Tao",
      "Shin, Jinwoo",
      "Liu, Tie-Yan"
    ]
  },
  {
    "id": "17e23e50bedc63b4095e3d8204ce063b",
    "title": "Reliable and Trustworthy Machine Learning for Health Using Dataset Shift Detection",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/17e23e50bedc63b4095e3d8204ce063b-Paper.pdf",
    "abstract": "Unpredictable ML model behavior on unseen data, especially in the health domain, raises serious concerns about its safety as repercussions for mistakes can be fatal. In this paper, we explore the feasibility of using state-of-the-art out-of-distribution detectors for reliable and trustworthy diagnostic predictions. We select publicly available deep learning models relating to various health conditions (e.g., skin cancer, lung sound, and Parkinson's disease) using various input data types (e.g., image, audio, and motion data). We demonstrate that these models show unreasonable predictions on out-of-distribution datasets. We show that Mahalanobis distance- and Gram matrices-based out-of-distribution detection methods are able to detect out-of-distribution data with high accuracy for the health models that operate on different modalities. We then translate the out-of-distribution score into a human interpretable \\textsc{confidence score} to investigate its effect on the users' interaction with health ML applications. Our user study shows that the \\textsc{confidence score} helped the participants only trust the results with a high score to make a medical decision and disregard results with a low score. Through this work, we demonstrate that dataset shift is a critical piece of information for high-stake ML applications, such as medical diagnosis and healthcare, to provide reliable and trustworthy predictions to the users. ",
    "authors": [
      "Park, Chunjong",
      "Awadalla, Anas",
      "Kohno, Tadayoshi",
      "Patel, Shwetak"
    ]
  },
  {
    "id": "17f5e6db87929fb55cebeb7fd58c1d41",
    "title": "Multiclass Boosting and the Cost of Weak Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/17f5e6db87929fb55cebeb7fd58c1d41-Paper.pdf",
    "abstract": "Boosting is an algorithmic approach which is based on the idea     of combining weak and moderately inaccurate hypotheses to a strong and accurate one.     In this work we study multiclass boosting with a possibly large number of classes or categories.    Multiclass boosting can be formulated in various ways.    Here, we focus on an especially natural formulation in which the weak hypotheses    are assumed to belong to an ''easy-to-learn'' base class, and    the weak learner is an agnostic PAC learner for that class    with respect to the standard classification loss.    This is in contrast with other, more complicated losses as have often been considered in the past.    The goal of the overall boosting algorithm    is then to learn a combination of weak hypotheses    by repeatedly calling the weak learner.We study the resources required for boosting, especially how theydepend on the number of classes $k$, for both the booster and weak learner.We find that the boosting algorithm itself only requires $O(\\log k)$samples, as we show by analyzing a variant of AdaBoost for oursetting. In stark contrast, assuming typical limits on the number of weak-learner calls,we prove that the number of samples required by a weak learner is at least polynomial in $k$, exponentially more than thenumber of samples needed by the booster.Alternatively, we prove that the weak learner's accuracy parametermust be smaller  than an inverse polynomial in $k$, showing that the returned weakhypotheses must be nearly the best in their class when $k$ is large.We also prove a trade-off between number of oracle calls and theresources required of the weak learner, meaning that the fewer calls to theweak learner the more that is demanded on each call.",
    "authors": [
      "Brukhim, Nataly",
      "Hazan, Elad",
      "Moran, Shay",
      "Mukherjee, Indraneel",
      "Schapire, Robert E."
    ]
  },
  {
    "id": "17f98ddf040204eda0af36a108cbdea4",
    "title": "Partition-Based Formulations for Mixed-Integer Optimization of Trained ReLU Neural Networks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/17f98ddf040204eda0af36a108cbdea4-Paper.pdf",
    "abstract": "This paper introduces a class of mixed-integer formulations for trained ReLU neural networks. The approach balances model size and tightness by partitioning node inputs into a number of groups and forming the convex hull over the partitions via disjunctive programming. At one extreme, one partition per input recovers the convex hull of a node, i.e., the tightest possible formulation for each node. For fewer partitions, we develop smaller relaxations that approximate the convex hull, and show that they outperform existing formulations. Specifically, we propose strategies for partitioning variables based on theoretical motivations and validate these strategies using extensive computational experiments. Furthermore, the proposed scheme complements known algorithmic approaches, e.g., optimization-based bound tightening captures dependencies within a partition.",
    "authors": [
      "Tsay, Calvin",
      "Kronqvist, Jan",
      "Thebelt, Alexander",
      "Misener, Ruth"
    ]
  },
  {
    "id": "17fafe5f6ce2f1904eb09d2e80a4cbf6",
    "title": "Hyperparameter Optimization Is Deceiving Us, and How to Stop It",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/17fafe5f6ce2f1904eb09d2e80a4cbf6-Paper.pdf",
    "abstract": "Recent empirical work shows that inconsistent results based on choice of hyperparameter optimization (HPO) configuration are a widespread problem in ML research. When comparing two algorithms J and K searching one subspace can yield the conclusion that J outperforms K, whereas searching another can entail the opposite. In short, the way we choose hyperparameters can deceive us. We provide a theoretical complement to this prior work, arguing that, to avoid such deception, the process of drawing conclusions from HPO should be made more rigorous. We call this process epistemic hyperparameter optimization (EHPO), and put forth a logical framework to capture its semantics and how it can lead to inconsistent conclusions about performance. Our framework enables us to prove EHPO methods that are guaranteed to be defended against deception, given bounded compute time budget t. We demonstrate our framework's utility by proving and empirically validating a defended variant of random search. ",
    "authors": [
      "Cooper, A. Feder",
      "Lu, Yucheng",
      "Forde, Jessica",
      "De Sa, Christopher M."
    ]
  },
  {
    "id": "18085327b86002fc604c323b9a07f997",
    "title": "On the Convergence Theory of Debiased Model-Agnostic Meta-Reinforcement Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/18085327b86002fc604c323b9a07f997-Paper.pdf",
    "abstract": "We consider Model-Agnostic Meta-Learning (MAML) methods for Reinforcement Learning (RL) problems, where the goal is to find a policy using data from several tasks represented by Markov Decision Processes (MDPs) that can be updated by one step of \\textit{stochastic} policy gradient for the realized MDP. In particular, using stochastic gradients in MAML update steps is crucial for RL problems since computation of exact gradients requires access to a large number of possible trajectories. For this formulation, we propose a variant of the MAML method, named Stochastic Gradient Meta-Reinforcement Learning (SG-MRL), and study its convergence properties. We derive the iteration and sample complexity of SG-MRL to find an $\\epsilon$-first-order stationary point, which, to the best of our knowledge, provides the first convergence guarantee for model-agnostic meta-reinforcement learning algorithms. We further show how our results extend to the case where more than one step of stochastic policy gradient method is used at test time. Finally, we empirically compare SG-MRL and MAML in several deep RL environments.",
    "authors": [
      "Fallah, Alireza",
      "Georgiev, Kristian",
      "Mokhtari, Aryan",
      "Ozdaglar, Asuman"
    ]
  },
  {
    "id": "18a411989b47ed75a60ac69d9da05aa5",
    "title": "3D Pose Transfer with Correspondence Learning and Mesh Refinement",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/18a411989b47ed75a60ac69d9da05aa5-Paper.pdf",
    "abstract": "3D pose transfer is one of the most challenging 3D generation tasks. It aims to transfer the pose of a source mesh to a target mesh and keep the identity (e.g., body shape) of the target mesh. Some previous works require key point annotations to build reliable correspondence between the source and target meshes, while other methods do not consider any shape correspondence between sources and targets, which leads to limited generation quality. In this work, we propose a correspondence-refinement network to achieve the 3D pose transfer for both human and animal meshes. The correspondence between source and target meshes is first established by solving an optimal transport problem. Then, we warp the source mesh according to the dense correspondence and obtain a coarse warped mesh. The warped mesh will be better refined with our proposed Elastic Instance Normalization, which is a conditional normalization layer and can help to generate high-quality meshes. Extensive experimental results show that the proposed architecture can effectively transfer the poses from source to target meshes and produce better results with satisfied visual performance than state-of-the-art methods.",
    "authors": [
      "Song, Chaoyue",
      "Wei, Jiacheng",
      "Li, Ruibo",
      "Liu, Fayao",
      "Lin, Guosheng"
    ]
  },
  {
    "id": "18a9042b3fc5b02fe3d57fea87d6992f",
    "title": "Framing RNN as a kernel method: A neural ODE approach",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/18a9042b3fc5b02fe3d57fea87d6992f-Paper.pdf",
    "abstract": "Building on the interpretation of a recurrent neural network (RNN) as a continuous-time neural differential equation, we show, under appropriate conditions, that the solution of a RNN can be viewed as a linear function of a specific feature set of the input sequence, known as the signature. This connection allows us to frame a RNN as a kernel method in a suitable reproducing kernel Hilbert space. As a consequence, we obtain theoretical guarantees on generalization and stability for a large class of recurrent networks. Our results are illustrated on simulated datasets.",
    "authors": [
      "Fermanian, Adeline",
      "Marion, Pierre",
      "Vert, Jean-Philippe",
      "Biau, G\u00e9rard"
    ]
  },
  {
    "id": "18d10dc6e666eab6de9215ae5b3d54df",
    "title": "Contextual Similarity Aggregation with Self-attention for Visual Re-ranking",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/18d10dc6e666eab6de9215ae5b3d54df-Paper.pdf",
    "abstract": "In content-based image retrieval, the first-round retrieval result by simple visual feature comparison may be unsatisfactory, which can be refined by visual re-ranking techniques. In image retrieval, it is observed that the contextual similarity among the top-ranked images is an important clue to distinguish the semantic relevance. Inspired by this observation, in this paper, we propose a visual re-ranking method by contextual similarity aggregation with self-attention.  In our approach, for each image in the top-K ranking list, we represent it into an affinity feature vector by comparing it with a set of anchor images. Then, the affinity features of the top-K images are refined by aggregating the contextual information with a transformer encoder. Finally, the affinity features are used to recalculate the similarity scores between the query and the top-K images for re-ranking of the latter. To further improve the robustness of our re-ranking model and enhance the performance of our method, a new data augmentation scheme is designed. Since our re-ranking model is not directly involved with the visual feature used in the initial retrieval, it is ready to be applied to retrieval result lists obtained from various retrieval algorithms. We conduct comprehensive experiments on four benchmark datasets to demonstrate the generality and effectiveness of our proposed visual re-ranking method.",
    "authors": [
      "Ouyang, Jianbo",
      "Wu, Hui",
      "Wang, Min",
      "Zhou, Wengang",
      "Li, Houqiang"
    ]
  },
  {
    "id": "18de4beb01f6a17b6e1dfb9813ba6045",
    "title": "Can Information Flows Suggest Targets for Interventions in Neural Circuits?",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/18de4beb01f6a17b6e1dfb9813ba6045-Paper.pdf",
    "abstract": "Motivated by neuroscientific and clinical applications, we empirically examine whether observational measures of information flow can suggest interventions. We do so by performing experiments on artificial neural networks in the context of fairness in machine learning, where the goal is to induce fairness in the system through interventions. Using our recently developed M-information flow framework, we measure the flow of information about the true label (responsible for accuracy, and hence desirable), and separately, the flow of information about a protected attribute (responsible for bias, and hence undesirable) on the edges of a trained neural network. We then compare the flow magnitudes against the effect of intervening on those edges by pruning. We show that pruning edges that carry larger information flows about the protected attribute reduces bias at the output to a greater extent. This demonstrates that M-information flow can meaningfully suggest targets for interventions, answering the title's question in the affirmative. We also evaluate bias-accuracy tradeoffs for different intervention strategies, to analyze how one might use estimates of desirable and undesirable information flows (here, accuracy and bias flows) to inform interventions that preserve the former while reducing the latter.",
    "authors": [
      "Venkatesh, Praveen",
      "Dutta, Sanghamitra",
      "Mehta, Neil",
      "Grover, Pulkit"
    ]
  },
  {
    "id": "191f8f858acda435ae0daf994e2a72c2",
    "title": "AutoBalance: Optimized Loss Functions for Imbalanced Data",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/191f8f858acda435ae0daf994e2a72c2-Paper.pdf",
    "abstract": "Imbalanced datasets are commonplace in modern machine learning problems. The presence of under-represented classes or groups with sensitive attributes results in concerns about generalization and fairness. Such concerns are further exacerbated by the fact that large capacity deep nets can perfectly fit the training data and appear to achieve perfect accuracy and fairness during training, but perform poorly during test. To address these challenges, we propose AutoBalance, a bi-level optimization framework that automatically designs a training loss function to optimize a blend of accuracy and fairness-seeking objectives. Specifically, a lower-level problem trains the model weights, and an upper-level problem tunes the loss function by monitoring and optimizing the desired objective over the validation data. Our loss design enables personalized treatment for classes/groups by employing a parametric cross-entropy loss and individualized data augmentation schemes. We evaluate the benefits and performance of our approach for the application scenarios of imbalanced and group-sensitive classification. Extensive empirical evaluations demonstrate the benefits of AutoBalance over state-of-the-art approaches. Our experimental findings are complemented with theoretical insights on loss function design and the benefits of the train-validation split. All code is available open-source.",
    "authors": [
      "Li, Mingchen",
      "Zhang, Xuechen",
      "Thrampoulidis, Christos",
      "Chen, Jiasi",
      "Oymak, Samet"
    ]
  },
  {
    "id": "19485224d128528da1602ca47383f078",
    "title": "SyncTwin: Treatment Effect Estimation with Longitudinal Outcomes",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/19485224d128528da1602ca47383f078-Paper.pdf",
    "abstract": "Most of the medical observational studies estimate the causal treatment effects using electronic health records (EHR), where a patient's covariates and outcomes are both observed longitudinally. However, previous methods focus only on adjusting for the covariates while neglecting the temporal structure in the outcomes. To bridge the gap, this paper develops a new method, SyncTwin, that learns a patient-specific time-constant representation from the pre-treatment observations. SyncTwin issues counterfactual prediction of a target patient by constructing a synthetic twin that closely matches the target in representation. The reliability of the estimated treatment effect can be assessed by comparing the observed and synthetic pre-treatment outcomes. The medical experts can interpret the estimate by examining the most important contributing individuals to the synthetic twin. In the real-data experiment, SyncTwin successfully reproduced the findings of a randomized controlled clinical trial using observational data, which demonstrates its usability in the complex real-world EHR.",
    "authors": [
      "Qian, Zhaozhi",
      "Zhang, Yao",
      "Bica, Ioana",
      "Wood, Angela",
      "van der Schaar, Mihaela"
    ]
  },
  {
    "id": "19b1b73d63d4c9ea79f8ca57e9d67095",
    "title": "Statistical Query Lower Bounds for List-Decodable Linear Regression",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/19b1b73d63d4c9ea79f8ca57e9d67095-Paper.pdf",
    "abstract": "We study the problem of list-decodable linear regression, where an adversary can corrupt a majority of the examples. Specifically, we are given a set $T$ of labeled examples $(x, y) \\in \\mathbb{R}^d \\times \\mathbb{R}$ and a parameter $0< \\alpha <1/2$ such that an $\\alpha$-fraction of the points in $T$ are i.i.d. samples from a linear regression model with Gaussian covariates, and the remaining $(1-\\alpha)$-fraction of the points are drawn from an arbitrary noise distribution. The goal is to output a small list of hypothesis vectors such that at least one of them is close to the target regression vector. Our main result is a Statistical Query (SQ) lower bound of $d^{\\mathrm{poly}(1/\\alpha)}$ for this problem. Our SQ lower bound qualitatively matches the performance of previously developed algorithms, providing evidence that current upper bounds for this task are nearly best possible.",
    "authors": [
      "Diakonikolas, Ilias",
      "Kane, Daniel",
      "Pensia, Ankit",
      "Pittas, Thanasis",
      "Stewart, Alistair"
    ]
  },
  {
    "id": "19ca14e7ea6328a42e0eb13d585e4c22",
    "title": "Unsupervised Motion Representation Learning with Capsule Autoencoders",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/19ca14e7ea6328a42e0eb13d585e4c22-Paper.pdf",
    "abstract": "We propose the Motion Capsule Autoencoder (MCAE), which addresses a key challenge in the unsupervised learning of motion representations: transformation invariance. MCAE models motion in a two-level hierarchy. In the lower level, a spatio-temporal motion signal is divided into short, local, and semantic-agnostic snippets. In the higher level, the snippets are aggregated to form full-length semantic-aware segments. For both levels, we represent motion with a set of learned transformation invariant templates and the corresponding geometric transformations by using capsule autoencoders of a novel design. This leads to a robust and efficient encoding of viewpoint changes. MCAE is evaluated on a novel Trajectory20 motion dataset and various real-world skeleton-based human action datasets. Notably, it achieves better results than baselines on Trajectory20 with considerably fewer parameters and state-of-the-art performance on the unsupervised skeleton-based action recognition task.",
    "authors": [
      "Xu, Ziwei",
      "Shen, Xudong",
      "Wong, Yongkang",
      "Kankanhalli, Mohan S."
    ]
  },
  {
    "id": "1a344877f11195aaf947ccfe48ee9c89",
    "title": "VigDet: Knowledge Informed Neural Temporal Point Process for Coordination Detection on Social Media",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1a344877f11195aaf947ccfe48ee9c89-Paper.pdf",
    "abstract": "Recent years have witnessed an increasing use of coordinated accounts on social media, operated by misinformation campaigns to influence public opinion and manipulate social outcomes. Consequently, there is an urgent need to develop an effective methodology for coordinated group detection to combat the misinformation on social media. However, existing works suffer from various drawbacks, such as, either limited performance due to extreme reliance on predefined signatures of coordination, or instead an inability to address the natural sparsity of account activities on social media with useful prior domain knowledge. Therefore, in this paper, we propose a coordination detection framework incorporating neural temporal point process with prior knowledge such as temporal logic or pre-defined filtering functions. Specifically, when modeling the observed data from social media with neural temporal point process, we jointly learn a Gibbs-like distribution of group assignment based on how consistent an assignment is to (1) the account embedding space and (2) the prior knowledge. To address the challenge that the distribution is hard to be efficiently computed and sampled from, we design a theoretically guaranteed variational inference approach to learn a mean-field approximation for it. Experimental results on a real-world dataset show the effectiveness of our proposed method compared to the SOTA model in both unsupervised and semi-supervised settings. We further apply our model on a COVID-19 Vaccine Tweets dataset. The detection result suggests the presence of suspicious coordinated efforts on spreading misinformation about COVID-19 vaccines.",
    "authors": [
      "Zhang, Yizhou",
      "Sharma, Karishma",
      "Liu, Yan"
    ]
  },
  {
    "id": "1a3650aedfdd3a21444047ed2d89458f",
    "title": "An Improved Analysis and Rates for Variance Reduction under Without-replacement Sampling Orders ",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1a3650aedfdd3a21444047ed2d89458f-Paper.pdf",
    "abstract": "When applying a stochastic algorithm, one must choose an order to draw samples. The practical choices are without-replacement sampling orders, which are empirically faster and more cache-friendly than uniform-iid-sampling but often have inferior theoretical guarantees. Without-replacement sampling is well understood only for SGD without variance reduction. In this paper, we will improve the convergence analysis and rates of variance reduction under without-replacement sampling orders for composite finite-sum minimization.Our results are in two-folds. First, we develop a damped variant of Finito called Prox-DFinito and  establish its convergence rates with random reshuffling, cyclic sampling, and shuffling-once, under both generally and strongly convex scenarios. These rates match full-batch gradient descent and are state-of-the-art compared to the existing results for without-replacement sampling with variance-reduction. Second, our analysis can gauge how the cyclic order will influence the rate of cyclic sampling and, thus, allows us to derive the optimal fixed ordering. In the highly data-heterogeneous scenario, Prox-DFinito with optimal cyclic sampling can attain a sample-size-independent convergence rate, which, to our knowledge, is the first result that can match with uniform-iid-sampling with variance reduction. We also propose a practical method to discover the optimal cyclic ordering numerically.",
    "authors": [
      "Huang, Xinmeng",
      "Yuan, Kun",
      "Mao, Xianghui",
      "Yin, Wotao"
    ]
  },
  {
    "id": "1a423f7c07a179ec243e82b0c017a034",
    "title": "Exploring Forensic Dental Identification with Deep Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1a423f7c07a179ec243e82b0c017a034-Paper.pdf",
    "abstract": "Dental forensic identification targets to identify persons with dental traces.The task is vital for the investigation of criminal scenes and mass disasters because of the resistance of dental structures and the wide-existence of dental imaging. However, no widely accepted automated solution is available for this labour-costly task. In this work, we pioneer to study deep learning for dental forensic identification based on panoramic radiographs. We construct a comprehensive benchmark with various dental variations that can adequately reflect the difficulties of the task. By considering the task's unique challenges, we propose FoID, a deep learning method featured by: (\\textit{i}) clinical-inspired attention localization, (\\textit{ii}) domain-specific augmentations that enable instance discriminative learning, and (\\textit{iii}) transformer-based self-attention mechanism that dynamically reasons the relative importance of attentions. We show that FoID can outperform traditional approaches by at least \\textbf{22.98\\%} in terms of Rank-1 accuracy, and outperform strong CNN baselines by at least \\textbf{10.50\\%} in terms of mean Average Precision (mAP). Moreover, extensive ablation studies verify the effectiveness of each building blocks of FoID. Our work can be a first step towards the automated system for forensic identification among large-scale multi-site databases. Also, the proposed techniques, \\textit{e.g.}, self-attention mechanism, can also be meaningful for other identification tasks, \\textit{e.g.}, pedestrian re-identification.Related data and codes can be found at \\href{https://github.com/liangyuandg/FoID}{https://github.com/liangyuandg/FoID}. ",
    "authors": [
      "Liang, Yuan",
      "Han, Weikun",
      "Qiu, Liang",
      "Wu, Chen",
      "Shao, Yiting",
      "Wang, Kun",
      "He, Lei"
    ]
  },
  {
    "id": "1a5b1e4daae265b790965a275b53ae50",
    "title": "Learning to Generate Realistic Noisy Images via Pixel-level Noise-aware  Adversarial Training",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1a5b1e4daae265b790965a275b53ae50-Paper.pdf",
    "abstract": "Existing deep learning real denoising methods require a large amount of noisy-clean image pairs for supervision. Nonetheless, capturing a  real noisy-clean dataset is an unacceptable expensive and cumbersome procedure. To alleviate this problem, this work investigates how to generate realistic noisy images. Firstly, we formulate a simple yet reasonable noise model that treats each real noisy pixel as a random variable. This model splits the noisy image generation problem into two sub-problems: image domain alignment and noise domain alignment. Subsequently, we propose a novel framework, namely Pixel-level Noise-aware Generative Adversarial Network (PNGAN). PNGAN employs a pre-trained real denoiser to map the fake and real noisy images into a nearly noise-free solution space to perform image domain alignment. Simultaneously, PNGAN establishes a pixel-level adversarial training to conduct noise domain alignment. Additionally, for better noise fitting, we present an efficient architecture Simple Multi-scale Network (SMNet) as the generator. Qualitative validation shows that noise generated by PNGAN is highly similar to real noise in terms of intensity and distribution. Quantitative experiments demonstrate that a series of denoisers trained with the generated noisy images achieve state-of-the-art (SOTA) results on four real denoising benchmarks.",
    "authors": [
      "Cai, Yuanhao",
      "Hu, Xiaowan",
      "Wang, Haoqian",
      "Zhang, Yulun",
      "Pfister, Hanspeter",
      "Wei, Donglai"
    ]
  },
  {
    "id": "1a6727711b84fd1efbb87fc565199d13",
    "title": "Multi-Agent Reinforcement Learning for Active Voltage Control on Power Distribution Networks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1a6727711b84fd1efbb87fc565199d13-Paper.pdf",
    "abstract": "This paper presents a problem in power networks that creates an exciting and yet challenging real-world scenario for application of multi-agent reinforcement learning (MARL). The emerging trend of decarbonisation is placing excessive stress on power distribution networks. Active voltage control is seen as a promising solution to relieve power congestion and improve voltage quality without extra hardware investment, taking advantage of the controllable apparatuses in the network, such as roof-top photovoltaics (PVs) and static var compensators (SVCs). These controllable apparatuses appear in a vast number and are distributed in a wide geographic area, making MARL a natural candidate. This paper formulates the active voltage control problem in the framework of Dec-POMDP and establishes an open-source environment. It aims to bridge the gap between the power community and the MARL community and be a drive force towards real-world applications of MARL algorithms. Finally, we analyse the special characteristics of the active voltage control problems that cause challenges (e.g. interpretability) for state-of-the-art MARL approaches, and summarise the potential directions.",
    "authors": [
      "Wang, Jianhong",
      "Xu, Wangkun",
      "Gu, Yunjie",
      "Song, Wenbin",
      "Green, Tim C"
    ]
  },
  {
    "id": "1a68e5f4ade56ed1d4bf273e55510750",
    "title": "Looking Beyond Single Images for Contrastive Semantic Segmentation Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1a68e5f4ade56ed1d4bf273e55510750-Paper.pdf",
    "abstract": "We present an approach to contrastive representation learning for semantic segmentation. Our approach leverages the representational power of existing feature extractors to find corresponding regions across images. These cross-image correspondences are used as auxiliary labels to guide the pixel-level selection of  positive and negative samples for more effective contrastive learning in semantic segmentation. We show that auxiliary labels can be generated from a variety of feature extractors, ranging from image classification networks that have been trained using unsupervised contrastive learning to segmentation models that have been trained on a small amount of labeled data. We additionally introduce a novel metric for rapidly judging the quality of a given auxiliary-labeling strategy, and empirically analyze various factors that influence the performance of contrastive learning for semantic segmentation. We demonstrate the effectiveness of our method both in the low-data as well as the high-data regime on various datasets. Our experiments show that contrastive learning with our auxiliary-labeling approach consistently boosts semantic segmentation accuracy when compared to standard ImageNet pretraining and outperforms existing approaches of contrastive and semi-supervised semantic segmentation.",
    "authors": [
      "ZHANG, FEIHU",
      "Torr, Philip",
      "Ranftl, Rene",
      "Richter, Stephan"
    ]
  },
  {
    "id": "1aa057313c28fa4a40c5bc084b11d276",
    "title": "A Constant Approximation Algorithm for Sequential Random-Order No-Substitution k-Median Clustering",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1aa057313c28fa4a40c5bc084b11d276-Paper.pdf",
    "abstract": "We study k-median clustering under the sequential no-substitution setting. In this setting, a data stream is sequentially observed, and some of the points are selected by the algorithm as cluster centers. However, a point can be selected as a center only immediately after it is observed, before observing the next point. In addition, a selected center cannot be substituted later. We give the first algorithm for this setting that obtains a constant approximation factor on the optimal cost under a random arrival order, an exponential improvement over previous work. This is also the first constant approximation guarantee that holds without any structural assumptions on the input data. Moreover, the number of selected centers is only quasi-linear in k.  Our algorithm and analysis are based on a careful cost estimation that avoids outliers, a new concept of a linear bin division, and a multi-scale approach to center selection. ",
    "authors": [
      "Hess, Tom",
      "Moshkovitz, Michal",
      "Sabato, Sivan"
    ]
  },
  {
    "id": "1ab60b5e8bd4eac8a7537abb5936aadc",
    "title": "Dangers of Bayesian Model Averaging under Covariate Shift",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1ab60b5e8bd4eac8a7537abb5936aadc-Paper.pdf",
    "abstract": "Approximate Bayesian inference for neural networks is considered a robust alternative to standard training, often providing good performance on out-of-distribution data. However, Bayesian neural networks (BNNs) with high-fidelity approximate inference via full-batch Hamiltonian Monte Carlo achieve poor generalization under covariate shift, even underperforming classical estimation. We explain this surprising result, showing how a Bayesian model average can in fact be problematic under covariate shift, particularly in cases where linear dependencies in the input features cause a lack of posterior contraction. We additionally show why the same issue does not affect many approximate inference procedures, or classical maximum a-posteriori (MAP) training. Finally, we propose novel priors that improve the robustness of BNNs to many sources of covariate shift.",
    "authors": [
      "Izmailov, Pavel",
      "Nicholson, Patrick",
      "Lotfi, Sanae",
      "Wilson, Andrew G."
    ]
  },
  {
    "id": "1b89a2e980724cb8997459fadb907712",
    "title": "Learning Equilibria in Matching Markets from Bandit Feedback",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1b89a2e980724cb8997459fadb907712-Paper.pdf",
    "abstract": "Large-scale, two-sided matching platforms must find market outcomes that align with user preferences while simultaneously learning these preferences from data. But since preferences are inherently uncertain during learning, the classical notion of stability (Gale and Shapley, 1962; Shapley and Shubik, 1971) is unattainable in these settings. To bridge this gap, we develop a framework and algorithms for learning stable market outcomes under uncertainty. Our primary setting is matching with transferable utilities, where the platform both matches agents and sets monetary transfers between them. We design an incentive-aware learning objective that captures the distance of a market outcome from equilibrium. Using this objective, we analyze the complexity of learning as a function of preference structure, casting learning as a stochastic multi-armed bandit problem. Algorithmically, we show that \"optimism in the face of uncertainty,\" the principle underlying many bandit algorithms, applies to a primal-dual formulation of matching with transfers and leads to near-optimal regret bounds. Our work takes a first step toward elucidating when and how stable matchings arise in large, data-driven marketplaces.",
    "authors": [
      "Jagadeesan, Meena",
      "Wei, Alexander",
      "Wang, Yixin",
      "Jordan, Michael",
      "Steinhardt, Jacob"
    ]
  },
  {
    "id": "1b9812b99fe2672af746cefda86be5f9",
    "title": "Towards Lower Bounds on the Depth of ReLU Neural Networks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1b9812b99fe2672af746cefda86be5f9-Paper.pdf",
    "abstract": "We contribute to a better understanding of the class of functions that is represented by a neural network with ReLU activations and a given architecture. Using techniques from mixed-integer optimization, polyhedral theory, and tropical geometry, we provide a mathematical counterbalance to the universal approximation theorems which suggest that a single hidden layer is sufficient for learning tasks. In particular, we investigate whether the class of exactly representable functions strictly increases by adding more layers (with no restrictions on size). This problem has potential impact on algorithmic and statistical aspects because of the insight it provides into the class of functions represented by neural hypothesis classes. However, to the best of our knowledge, this question has not been investigated in the neural network literature. We also present upper bounds on the sizes of neural networks required to represent functions in these neural hypothesis classes.",
    "authors": [
      "Hertrich, Christoph",
      "Basu, Amitabh",
      "Di Summa, Marco",
      "Skutella, Martin"
    ]
  },
  {
    "id": "1b9f38268c50805669fd8caf8f3cc84a",
    "title": "The Limitations of Large Width in Neural Networks: A Deep Gaussian Process Perspective",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1b9f38268c50805669fd8caf8f3cc84a-Paper.pdf",
    "abstract": "Large width limits have been a recent focus of deep learning research: modulo computational practicalities, do wider networks outperform narrower ones? Answering this question has been challenging, as conventional networks gain representational power with width, potentially masking any negative effects. Our analysis in this paper decouples capacity and width via the generalization of neural networks to Deep Gaussian Processes (Deep GP), a class of nonparametric hierarchical models that subsume neural nets. In doing so, we aim to understand how width affects (standard) neural networks once they have sufficient capacity for a given modeling task. Our theoretical and empirical results on Deep GP suggest that large width can be detrimental to hierarchical models. Surprisingly, we prove that even nonparametric Deep GP converge to Gaussian processes, effectively becoming shallower without any increase in representational power. The posterior, which corresponds to a mixture of data-adaptable basis functions, becomes less data-dependent with width. Our tail analysis demonstrates that width and depth have opposite effects: depth accentuates a model\u2019s non-Gaussianity, while width makes models increasingly Gaussian. We find there is a \u201csweet spot\u201d that maximizes test performance before the limiting GP behavior prevents adaptability, occurring at width = 1 or width = 2 for nonparametric Deep GP. These results make strong predictions about the same phenomenon in conventional neural networks trained with L2 regularization (analogous to a Gaussian prior on parameters): we show that such neural networks may need up to 500 \u2212 1000 hidden units for sufficient capacity - depending on the dataset - but further width degrades performance.",
    "authors": [
      "Pleiss, Geoff",
      "Cunningham, John P."
    ]
  },
  {
    "id": "1baff70e2669e8376347efd3a874a341",
    "title": "Exact marginal prior distributions of finite Bayesian neural networks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1baff70e2669e8376347efd3a874a341-Paper.pdf",
    "abstract": "Bayesian neural networks are theoretically well-understood only in the infinite-width limit, where Gaussian priors over network weights yield Gaussian priors over network outputs. Recent work has suggested that finite Bayesian networks may outperform their infinite counterparts, but their non-Gaussian output priors have been characterized only though perturbative approaches. Here, we derive exact solutions for the function space priors for individual input examples of a class of finite fully-connected feedforward Bayesian neural networks. For deep linear networks, the prior has a simple expression in terms of the Meijer $G$-function. The prior of a finite ReLU network is a mixture of the priors of linear networks of smaller widths, corresponding to different numbers of active units in each layer. Our results unify previous descriptions of finite network priors in terms of their tail decay and large-width behavior. ",
    "authors": [
      "Zavatone-Veth, Jacob",
      "Pehlevan, Cengiz"
    ]
  },
  {
    "id": "1bb91f73e9d31ea2830a5e73ce3ed328",
    "title": "Spatiotemporal Joint Filter Decomposition in 3D Convolutional Neural Networks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1bb91f73e9d31ea2830a5e73ce3ed328-Paper.pdf",
    "abstract": "In this paper, we introduce spatiotemporal joint filter decomposition to decouple spatial and temporal learning, while preserving spatiotemporal dependency in a video. A 3D convolutional filter is now jointly decomposed over a set of spatial and temporal filter atoms respectively. In this way, a 3D convolutional layer becomes three: a temporal atom layer, a spatial atom layer, and a joint coefficient layer, all three remaining convolutional. One obvious arithmetic manipulation allowed in our joint decomposition is to swap spatial or temporal atoms with a set of atoms that have the same number but different sizes, while keeping the remaining unchanged. For example, as shown later, we can now achieve tempo-invariance by simply dilating temporal atoms only. To illustrate this useful atom-swapping property, we further demonstrate how such a decomposition permits the direct learning of 3D CNNs with full-size videos through iterations of two consecutive sub-stages of learning: In the temporal stage, full-temporal downsampled-spatial data are used to learn temporal atoms and joint coefficients while fixing spatial atoms. In the spatial stage, full-spatial downsampled-temporal data are used for spatial atoms and joint coefficients while fixing temporal atoms. We show empirically on multiple action recognition datasets that, the decoupled spatiotemporal learning significantly reduces the model memory footprints, and allows deep 3D CNNs to model high-spatial long-temporal dependency with limited computational resources while delivering comparable performance.",
    "authors": [
      "Miao, Zichen",
      "Wang, Ze",
      "Cheng, Xiuyuan",
      "Qiu, Qiang"
    ]
  },
  {
    "id": "1bc2029a8851ad344a8d503930dfd7f7",
    "title": "Pooling by Sliced-Wasserstein Embedding",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1bc2029a8851ad344a8d503930dfd7f7-Paper.pdf",
    "abstract": "Learning representations from sets has become increasingly important with many applications in point cloud processing, graph learning, image/video recognition, and object detection. We introduce a geometrically-interpretable and generic pooling mechanism for aggregating a set of features into a fixed-dimensional representation. In particular, we treat elements of a set as samples from a probability distribution and propose an end-to-end trainable Euclidean embedding for sliced-Wasserstein distance to learn from set-structured data effectively. We evaluate our proposed pooling method on a wide variety of set-structured data, including point-cloud, graph, and image classification tasks, and demonstrate that our proposed method provides superior performance over existing set representation learning approaches. Our code is available at https://github.com/navid-naderi/PSWE.",
    "authors": [
      "Naderializadeh, Navid",
      "Comer, Joseph F",
      "Andrews, Reed",
      "Hoffmann, Heiko",
      "Kolouri, Soheil"
    ]
  },
  {
    "id": "1bf2efbbe0c49b9f567c2e40f645279a",
    "title": "On the Theory of Reinforcement Learning with Once-per-Episode Feedback",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1bf2efbbe0c49b9f567c2e40f645279a-Paper.pdf",
    "abstract": "We study a theory of reinforcement learning (RL) in which the learner receives binary feedback only once at the end of an episode. While this is an extreme test case for theory, it is also arguably more representative of real-world applications than the traditional requirement in RL practice that the learner receive feedback at every time step. Indeed, in many real-world applications of reinforcement learning, such as self-driving cars and robotics, it is easier to evaluate whether a learner's complete trajectory was either good'' orbad,'' but harder to provide a reward signal at each step. To show that learning is possible in this more challenging setting, we study the case where trajectory labels are generated by an unknown parametric model, and provide a statistically and computationally efficient algorithm that achieves sublinear regret.",
    "authors": [
      "Chatterji, Niladri",
      "Pacchiano, Aldo",
      "Bartlett, Peter",
      "Jordan, Michael"
    ]
  },
  {
    "id": "1bf50aaf147b3b0ddd26a820d2ed394d",
    "title": "ResNEsts and DenseNEsts: Block-based DNN Models with Improved Representation Guarantees",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1bf50aaf147b3b0ddd26a820d2ed394d-Paper.pdf",
    "abstract": "Models recently used in the literature proving residual networks (ResNets) are better than linear predictors are actually different from standard ResNets that have been widely used in computer vision. In addition to the assumptions such as scalar-valued output or single residual block, the models fundamentally considered in the literature have no nonlinearities at the final residual representation that feeds into the final affine layer. To codify such a difference in nonlinearities and reveal a linear estimation property, we define ResNEsts, i.e., Residual Nonlinear Estimators, by simply dropping nonlinearities at the last residual representation from standard ResNets. We show that wide ResNEsts with bottleneck blocks can always guarantee a very desirable training property that standard ResNets aim to achieve, i.e., adding more blocks does not decrease performance given the same set of basis elements. To prove that, we first recognize ResNEsts are basis function models that are limited by a coupling problem in basis learning and linear prediction. Then, to decouple prediction weights from basis learning, we construct a special architecture termed augmented ResNEst (A-ResNEst) that always guarantees no worse performance with the addition of a block. As a result, such an A-ResNEst establishes empirical risk lower bounds for a ResNEst using corresponding bases. Our results demonstrate ResNEsts indeed have a problem of diminishing feature reuse; however, it can be avoided by sufficiently expanding or widening the input space, leading to the above-mentioned desirable property. Inspired by the densely connected networks (DenseNets) that have been shown to outperform ResNets, we also propose a corresponding new model called Densely connected Nonlinear Estimator (DenseNEst). We show that any DenseNEst can be represented as a wide ResNEst with bottleneck blocks. Unlike ResNEsts, DenseNEsts exhibit the desirable property without any special architectural re-design.",
    "authors": [
      "Chen, Kuan-Lin",
      "Lee, Ching-Hua",
      "Garudadri, Harinath",
      "Rao, Bhaskar D"
    ]
  },
  {
    "id": "1c1d4df596d01da60385f0bb17a4a9e0",
    "title": "Locally private online change point detection",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1c1d4df596d01da60385f0bb17a4a9e0-Paper.pdf",
    "abstract": "We study online change point detection problems under the constraint of local differential privacy (LDP) where, in particular, the statistician does not have access to the raw data.  As a concrete problem, we study a multivariate nonparametric regression problem.  At each time point $t$, the raw data are assumed to be of the form $(X_t, Y_t)$, where $X_t$ is a $d$-dimensional feature vector and $Y_t$ is a response variable. Our primary aim is to detect changes in the regression function $m_t(x)=\\mathbb{E}(Y_t |X_t=x)$ as soon as the change occurs.  We provide algorithms which respect the LDP constraint, which control the false alarm probability, and which detect changes with a minimal (minimax rate-optimal) delay.  To quantify the cost of privacy, we also present the optimal rate in the benchmark, non-private setting.  These non-private results are also new to the literature and thus are interesting \\emph{per se}.  In addition, we study the univariate mean online change point detection problem, under privacy constraints.  This serves as the blueprint of studying more complicated private change point detection problems.",
    "authors": [
      "Berrett, Tom",
      "Yu, Yi"
    ]
  },
  {
    "id": "1c336b8080f82bcc2cd2499b4c57261d",
    "title": "Invariance Principle Meets Information Bottleneck for Out-of-Distribution Generalization",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1c336b8080f82bcc2cd2499b4c57261d-Paper.pdf",
    "abstract": "The invariance principle from causality is at the heart of notable approaches such as invariant risk minimization (IRM) that seek to address out-of-distribution (OOD) generalization failures. Despite the promising theory, invariance principle-based approaches fail in common classification tasks, where invariant (causal) features capture all the information about the label.  Are these failures due to the methods failing to capture the invariance? Or is the invariance principle itself insufficient? To answer these questions, we revisit the fundamental assumptions in linear regression tasks, where invariance-based approaches were shown to provably generalize OOD. In contrast to the linear regression tasks, we show that for linear classification tasks we need much stronger restrictions on the distribution shifts, or otherwise OOD generalization is impossible.  Furthermore, even with appropriate restrictions on distribution shifts in place, we show that the invariance principle alone is insufficient. We prove that a form of the information bottleneck constraint along with invariance helps address the key failures when invariant features capture all the information about the label and also retains the existing success when they do not. We propose an approach that incorporates both of these principles and demonstrate its effectiveness in several experiments.",
    "authors": [
      "Ahuja, Kartik",
      "Caballero, Ethan",
      "Zhang, Dinghuai",
      "Gagnon-Audet, Jean-Christophe",
      "Bengio, Yoshua",
      "Mitliagkas, Ioannis",
      "Rish, Irina"
    ]
  },
  {
    "id": "1c63926ebcabda26b5cdb31b5cc91efb",
    "title": "Repulsive Deep Ensembles are Bayesian",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1c63926ebcabda26b5cdb31b5cc91efb-Paper.pdf",
    "abstract": "Deep ensembles have recently gained popularity in the deep learning community for their conceptual simplicity and efficiency. However, maintaining functional diversity between ensemble members that are independently trained with gradient descent is challenging. This can lead to pathologies when adding more ensemble members, such as a saturation of the ensemble performance, which converges to the performance of a single model. Moreover, this does not only affect the quality of its predictions, but even more so the uncertainty estimates of the ensemble, and thus its performance on out-of-distribution data. We hypothesize that this limitation can be overcome by discouraging different ensemble members from collapsing to the same function. To this end, we introduce a kernelized repulsive term in the update rule of the deep ensembles. We show that this simple modification not only enforces and maintains diversity among the members but, even more importantly, transforms the maximum a posteriori inference into proper Bayesian inference. Namely, we show that the training dynamics of our proposed repulsive ensembles follow a Wasserstein gradient flow of the KL divergence with the true posterior. We study repulsive terms in weight and function space and empirically compare their performance to standard ensembles and Bayesian baselines on synthetic and real-world prediction tasks.",
    "authors": [
      "D'Angelo, Francesco",
      "Fortuin, Vincent"
    ]
  },
  {
    "id": "1ca5c750a30312d1919ae6a4d636dcc4",
    "title": "BayesIMP: Uncertainty Quantification for Causal Data Fusion",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1ca5c750a30312d1919ae6a4d636dcc4-Paper.pdf",
    "abstract": "While causal models are becoming one of the mainstays of machine learning, the problem of uncertainty quantification in causal inference remains challenging. In this paper, we study the causal data fusion problem, where data arising from multiple causal graphs are combined to estimate the average treatment effect of a target variable. As data arises from multiple sources and can vary in quality and sample size, principled uncertainty quantification becomes essential. To that end, we introduce \\emph{Bayesian Causal Mean Processes}, the framework which combines ideas from probabilistic integration and kernel mean embeddings to represent interventional distributions in the reproducing kernel Hilbert space, while taking into account the uncertainty within each causal graph. To demonstrate the informativeness of our uncertainty estimation, we apply our method to the Causal Bayesian Optimisation task and show improvements over state-of-the-art methods.",
    "authors": [
      "Chau, Siu Lun",
      "Ton, Jean-Francois",
      "Gonz\u00e1lez, Javier",
      "Teh, Yee",
      "Sejdinovic, Dino"
    ]
  },
  {
    "id": "1cbcaa5abbb6b70f378a3a03d0c26386",
    "title": "RMM: Reinforced Memory Management for Class-Incremental Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1cbcaa5abbb6b70f378a3a03d0c26386-Paper.pdf",
    "abstract": "Class-Incremental Learning (CIL) [38] trains classifiers under a strict memory budget: in each incremental phase, learning is done for new data, most of which is abandoned to free space for the next phase. The preserved data are exemplars used for replaying. However, existing methods use a static and ad hoc strategy for memory allocation, which is often sub-optimal. In this work, we propose a dynamic memory management strategy that is optimized for the incremental phases and different object classes. We call our method reinforced memory management (RMM), leveraging reinforcement learning. RMM training is not naturally compatible with CIL as the past, and future data are strictly non-accessible during the incremental phases. We solve this by training the policy function of RMM on pseudo CIL tasks, e.g., the tasks built on the data of the zeroth phase, and then applying it to target tasks. RMM propagates two levels of actions: Level-1 determines how to split the memory between old and new classes, and Level-2 allocates memory for each specific class. In essence, it is an optimizable and general method for memory management that can be used in any replaying-based CIL method. For evaluation, we plug RMM into two top-performing baselines (LUCIR+AANets and POD+AANets [28]) and conduct experiments on three benchmarks (CIFAR-100, ImageNet-Subset, and ImageNet-Full). Our results show clear improvements, e.g., boosting POD+AANets by 3.6%, 4.4%, and 1.9% in the 25-Phase settings of the above benchmarks, respectively.  The code is available at https://class-il.mpi-inf.mpg.de/rmm/.",
    "authors": [
      "Liu, Yaoyao",
      "Schiele, Bernt",
      "Sun, Qianru"
    ]
  },
  {
    "id": "1cc8a8ea51cd0adddf5dab504a285915",
    "title": "Learning Compact Representations of Neural Networks using DiscriminAtive Masking (DAM)",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1cc8a8ea51cd0adddf5dab504a285915-Paper.pdf",
    "abstract": "A central goal in deep learning is to learn  compact representations of features at every layer of a neural network, which is useful for both unsupervised representation learning  and structured network pruning. While there is a growing body of work in structured pruning, current state-of-the-art methods  suffer from two key limitations: (i) instability during training, and (ii) need for an additional step of fine-tuning, which is resource-intensive. At the core of these limitations is the lack of a systematic approach that jointly prunes and refines weights during training in a single stage, and does not require any fine-tuning upon convergence to achieve state-of-the-art performance. We present a novel single-stage structured pruning method termed DiscriminAtive Masking (DAM). The key intuition behind DAM is to discriminatively prefer some of the neurons to be refined during the training process, while gradually masking out other neurons. We show that our proposed DAM approach has remarkably good performance over a diverse range of applications in representation learning and structured pruning, including dimensionality reduction, recommendation system, graph representation learning, and structured pruning for image classification. We also theoretically show that the learning objective of DAM is directly related to minimizing the L_0 norm of the masking layer. All of our codes and datasets are available https://github.com/jayroxis/dam-pytorch.",
    "authors": [
      "Bu, Jie",
      "Daw, Arka",
      "Maruf, M.",
      "Karpatne, Anuj"
    ]
  },
  {
    "id": "1cd73be1e256a7405516501e94e892ac",
    "title": "Neural Auto-Curricula in Two-Player Zero-Sum Games",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1cd73be1e256a7405516501e94e892ac-Paper.pdf",
    "abstract": "When solving two-player zero-sum games, multi-agent reinforcement learning (MARL) algorithms often create populations of agents where, at each iteration, a new agent is discovered as the best response to a mixture over the opponent population. Within such a process, the update rules of \"who to compete with\" (i.e., the opponent mixture) and \"how to beat them\" (i.e., finding best responses) are underpinned by manually developed game theoretical principles such as fictitious play and Double Oracle. In this paper, we introduce a novel framework\u2014Neural Auto-Curricula (NAC)\u2014that leverages meta-gradient descent to automate the discovery of the learning update rule without explicit human design. Specifically, we parameterise the opponent selection module by neural networks and the best-response module by optimisation subroutines, and update their parameters solely via interaction with the game engine, where both players aim to minimise their exploitability. Surprisingly, even without human design, the discovered MARL algorithms achieve competitive or even better performance with the state-of-the-art population-based game solvers (e.g., PSRO) on Games of Skill, differentiable Lotto, non-transitive Mixture Games, Iterated Matching Pennies, and Kuhn Poker. Additionally, we show that NAC is able to generalise from small games to large games, for example training on Kuhn Poker and outperforming PSRO on Leduc Poker. Our work inspires a promising future direction to discover general MARL algorithms solely from data.",
    "authors": [
      "Feng, Xidong",
      "Slumbers, Oliver",
      "Wan, Ziyu",
      "Liu, Bo",
      "McAleer, Stephen",
      "Wen, Ying",
      "Wang, Jun",
      "Yang, Yaodong"
    ]
  },
  {
    "id": "1cdf14d1e3699d61d237cf76ce1c2dca",
    "title": "ImageBART: Bidirectional Context with Multinomial Diffusion for Autoregressive Image Synthesis",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1cdf14d1e3699d61d237cf76ce1c2dca-Paper.pdf",
    "abstract": "Autoregressive models and their sequential factorization of the data likelihood have recently demonstrated great potential for image representation and synthesis. Nevertheless, they incorporate image context in a linear 1D order by attending only to previously synthesized image patches above or to the left. Not only is this unidirectional, sequential bias of attention unnatural for images as it disregards large parts of a scene until synthesis is almost complete. It also processes the entire image on a single scale, thus ignoring more global contextual information up to the gist of the entire scene. As a remedy we incorporate a coarse-to-fine hierarchy of context by combining the autoregressive formulation with a multinomial diffusion process: Whereas a multistage diffusion process successively compresses and removes information to coarsen an image, we train a Markov chain to invert this process. In each stage, the resulting autoregressive ImageBART model progressively incorporates context from previous stages in a coarse-to-fine manner. Experiments demonstrate the gain over current autoregressive models, continuous diffusion probabilistic models, and latent variable models. Moreover, the approach enables to control the synthesis process and to trade compression rate against reconstruction accuracy, while still guaranteeing visually plausible results.",
    "authors": [
      "Esser, Patrick",
      "Rombach, Robin",
      "Blattmann, Andreas",
      "Ommer, Bjorn"
    ]
  },
  {
    "id": "1cfa81af29c6f2d8cacb44921722e753",
    "title": "From global to local MDI variable importances for random forests and when they are Shapley values",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1cfa81af29c6f2d8cacb44921722e753-Paper.pdf",
    "abstract": "Random forests have been widely used for their ability to provide so-called importance measures, which give insight at a global (per dataset) level on the relevance of input variables to predict a certain output. On the other hand, methods based on Shapley values have been introduced to refine the analysis of feature relevance in tree-based models to a local (per instance) level. In this context, we first show that the global Mean Decrease of Impurity (MDI) variable importance scores correspond to Shapley values under some conditions. Then, we derive a local MDI importance measure of variable relevance, which has a very natural connection with the global MDI measure and can be related to a new notion of local feature relevance. We further link local MDI importances with Shapley values and discuss them in the light of related measures from the literature. The measures are illustrated through experiments on several classification and regression problems.",
    "authors": [
      "Sutera, Antonio",
      "Louppe, Gilles",
      "Huynh-Thu, Van Anh",
      "Wehenkel, Louis",
      "Geurts, Pierre"
    ]
  },
  {
    "id": "1d01bd2e16f57892f0954902899f0692",
    "title": "Adversarial Robustness of Streaming Algorithms through Importance Sampling",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1d01bd2e16f57892f0954902899f0692-Paper.pdf",
    "abstract": "Robustness against adversarial attacks has recently been at the forefront of algorithmic design for machine learning tasks. In the adversarial streaming model, an adversary gives an algorithm a sequence of adaptively chosen updates $u_1,\\ldots,u_n$ as a data stream. The goal of the algorithm is to compute or approximate some predetermined function for every prefix of the adversarial stream, but the adversary may generate future updates based on previous outputs of the algorithm. In particular, the adversary may gradually learn the random bits internally used by an algorithm to manipulate dependencies in the input. This is especially problematic as many important problems in the streaming model require randomized algorithms, as they are known to not admit any deterministic algorithms that use sublinear space. In this paper, we introduce adversarially robust streaming algorithms for central machine learning and algorithmic tasks, such as regression and clustering, as well as their more general counterparts, subspace embedding, low-rank approximation, and coreset construction. For regression and other numerical linear algebra related tasks, we consider the row arrival streaming model. Our results are based on a simple, but powerful, observation that many importance sampling-based algorithms give rise to adversarial robustness which is in contrast to sketching based algorithms, which are very prevalent in the streaming literature but suffer from adversarial attacks. In addition, we show that the well-known merge and reduce paradigm in streaming is adversarially robust. Since the merge and reduce paradigm allows coreset constructions in the streaming setting, we thus obtain robust algorithms for $k$-means, $k$-median, $k$-center, Bregman clustering, projective clustering, principal component analysis (PCA) and non-negative matrix factorization. To the best of our knowledge, these are the first adversarially robust results for these problems yet require no new algorithmic implementations. Finally, we empirically confirm the robustness of our algorithms on various adversarial attacks and demonstrate that by contrast, some common existing algorithms are not robust. ",
    "authors": [
      "Braverman, Vladimir",
      "Hassidim, Avinatan",
      "Matias, Yossi",
      "Schain, Mariano",
      "Silwal, Sandeep",
      "Zhou, Samson"
    ]
  },
  {
    "id": "1d0832c4969f6a4cc8e8a8fffe083efb",
    "title": "Tractable Regularization of Probabilistic Circuits",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1d0832c4969f6a4cc8e8a8fffe083efb-Paper.pdf",
    "abstract": "Probabilistic Circuits (PCs) are a promising avenue for probabilistic modeling. They combine advantages of probabilistic graphical models (PGMs) with those of neural networks (NNs). Crucially, however, they are tractable probabilistic models, supporting efficient and exact computation of many probabilistic inference queries, such as marginals and MAP. Further, since PCs are structured computation graphs, they can take advantage of deep-learning-style parameter updates, which greatly improves their scalability. However, this innovation also makes PCs prone to overfitting, which has been observed in many standard benchmarks. Despite the existence of abundant regularization techniques for both PGMs and NNs, they are not effective enough when applied to PCs. Instead, we re-think regularization for PCs and propose two intuitive techniques, data softening and entropy regularization, that both take advantage of PCs' tractability and still have an efficient implementation as a computation graph. Specifically, data softening provides a principled way to add uncertainty in datasets in closed form, which implicitly regularizes PC parameters. To learn parameters from a softened dataset, PCs only need linear time by virtue of their tractability. In entropy regularization, the exact entropy of the distribution encoded by a PC can be regularized directly, which is again infeasible for most other density estimation models. We show that both methods consistently improve the generalization performance of a wide variety of PCs. Moreover, when paired with a simple PC structure, we achieved state-of-the-art results on 10 out of 20 standard discrete density estimation benchmarks. Open-source code and experiments are available at https://github.com/UCLA-StarAI/Tractable-PC-Regularization.",
    "authors": [
      "Liu, Anji",
      "Van den Broeck, Guy"
    ]
  },
  {
    "id": "1d49780520898fe37f0cd6b41c5311bf",
    "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1d49780520898fe37f0cd6b41c5311bf-Paper.pdf",
    "abstract": "Invariance to a broad array of image corruptions, such as warping, noise, or color shifts, is an important aspect of building robust models in computer vision. Recently, several new data augmentations have been proposed that significantly improve performance on ImageNet-C, a benchmark of such corruptions. However, there is still a lack of basic understanding on the relationship between data augmentations and test-time corruptions. To this end, we develop a feature space for image transforms, and then use a new measure in this space between augmentations and corruptions called the Minimal Sample Distance to demonstrate there is a strong correlation between similarity and performance. We then investigate recent data augmentations and observe a significant degradation in corruption robustness when the test-time corruptions are sampled to be perceptually dissimilar from ImageNet-C in this feature space. Our results suggest that test error can be improved by training on perceptually similar augmentations, and data augmentations may not generalize well beyond the existing benchmark. We hope our results and tools will allow for more robust progress towards improving robustness to image corruptions. We provide code at https://github.com/facebookresearch/augmentation-corruption.",
    "authors": [
      "Mintun, Eric",
      "Kirillov, Alexander",
      "Xie, Saining"
    ]
  },
  {
    "id": "1d6408264d31d453d556c60fe7d0459e",
    "title": "Dynamic Distillation Network for Cross-Domain Few-Shot Recognition with Unlabeled Data",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1d6408264d31d453d556c60fe7d0459e-Paper.pdf",
    "abstract": "Most existing works in few-shot learning rely on meta-learning the network on a large base dataset which is typically from the same domain as the target dataset. We tackle the problem of cross-domain few-shot learning where there is a large shift between the base and target domain. The problem of cross-domain few-shot recognition with unlabeled target data is largely unaddressed in the literature. STARTUP was the first method that tackles this problem using self-training. However, it uses a fixed teacher pretrained on a labeled base dataset to create soft labels for the unlabeled target samples. As the base dataset and unlabeled dataset are from different domains, projecting the target images in the class-domain of the base dataset with a fixed pretrained model might be sub-optimal. We propose a simple dynamic distillation-based approach to facilitate unlabeled images from the novel/base dataset. We impose consistency regularization by calculating predictions from the weakly-augmented versions of the unlabeled images from a teacher network and matching it with the strongly augmented versions of the same images from a student network. The parameters of the teacher network are updated as exponential moving average of the parameters of the student network. We show that the proposed network learns representation that can be easily adapted to the target domain even though it has not been trained with target-specific classes during the pretraining phase. Our model outperforms the current state-of-the art method by 4.4% for 1-shot and 3.6% for 5-shot classification in the BSCD-FSL benchmark, and also shows competitive performance on traditional in-domain few-shot learning task.",
    "authors": [
      "Islam, Ashraful",
      "Chen, Chun-Fu (Richard)",
      "Panda, Rameswar",
      "Karlinsky, Leonid",
      "Feris, Rogerio",
      "Radke, Richard J."
    ]
  },
  {
    "id": "1da546f25222c1ee710cf7e2f7a3ff0c",
    "title": "Hypergraph Propagation and Community Selection for Objects Retrieval",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1da546f25222c1ee710cf7e2f7a3ff0c-Paper.pdf",
    "abstract": "Spatial verification is a crucial technique for particular object retrieval. It utilizes spatial information for the accurate detection of true positive images. However, existing query expansion and diffusion methods cannot efficiently propagate the spatial information in an ordinary graph with scalar edge weights, resulting in low recall or precision. To tackle these problems, we propose a novel hypergraph-based framework that efficiently propagates spatial information in query time and retrieves an object in the database accurately. Additionally, we propose using the image graph's structure information through community selection technique, to measure the accuracy of the initial search result and to provide correct starting points for hypergraph propagation without heavy spatial verification computations. Experiment results on ROxford and RParis show that our method  significantly outperforms the existing query expansion and diffusion methods.",
    "authors": [
      "An, Guoyuan",
      "Huo, Yuchi",
      "Yoon, Sung-eui"
    ]
  },
  {
    "id": "1dacb10f0623c67cb7dbb37587d8b38a",
    "title": "Deep learning is adaptive to intrinsic dimensionality of model smoothness in anisotropic Besov space",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1dacb10f0623c67cb7dbb37587d8b38a-Paper.pdf",
    "abstract": "Deep learning has exhibited superior performance for various tasks, especially for high-dimensional datasets, such as images. To understand this property, we investigate the approximation and estimation ability of deep learning on {\\it anisotropic Besov spaces}.The anisotropic Besov space is characterized by direction-dependent smoothness and includes several function classes that have been investigated thus far.We demonstrate that the approximation error and estimation error of deep learning only depend on the average value of the smoothness parameters in all directions. Consequently, the curse of dimensionality can be avoided if the smoothness of the target function is highly anisotropic.Unlike existing studies, our analysis does not require a low-dimensional structure of the input data.We also investigate the minimax optimality of deep learning and compare its performance with that of the kernel method (more generally, linear estimators).The results show that deep learning has better dependence on the input dimensionality if the target function possesses anisotropic smoothness, and it achieves an adaptive rate for functions with spatially inhomogeneous smoothness.",
    "authors": [
      "Suzuki, Taiji",
      "Nitanda, Atsushi"
    ]
  },
  {
    "id": "1dba3025b159cd9354da65e2d0436a31",
    "title": "QuPeD: Quantized Personalization via Distillation with Applications to Federated Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1dba3025b159cd9354da65e2d0436a31-Paper.pdf",
    "abstract": "Traditionally, federated learning (FL) aims to train a single global model while collaboratively using multiple clients and a server. Two natural challenges that FL algorithms face are heterogeneity in data across clients and collaboration of clients with diverse resources. In this work, we introduce a quantized and personalized FL algorithm QuPeD that facilitates collective (personalized model compression) training via knowledge distillation (KD)  among clients who have access to heterogeneous data and resources. For personalization, we allow clients to learn compressed personalized models with different quantization parameters and model dimensions/structures. Towards this, first we propose an algorithm for learning quantized models through a relaxed optimization problem, where quantization values are also optimized over. When each client participating in the (federated) learning process has different requirements of the compressed model (both in model dimension and precision), we formulate a compressed personalization framework by introducing knowledge distillation loss for local client objectives collaborating through a global model. We develop an alternating proximal gradient update for solving this compressed personalization problem, and analyze its convergence properties. Numerically, we validate that QuPeD outperforms competing personalized FL methods, FedAvg, and local training of clients in various heterogeneous settings.",
    "authors": [
      "Ozkara, Kaan",
      "Singh, Navjot",
      "Data, Deepesh",
      "Diggavi, Suhas"
    ]
  },
  {
    "id": "1dba5eed8838571e1c80af145184e515",
    "title": "Model Adaptation: Historical Contrastive Learning for Unsupervised Domain Adaptation without Source Data",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1dba5eed8838571e1c80af145184e515-Paper.pdf",
    "abstract": "Unsupervised domain adaptation aims to align a labeled source domain and an unlabeled target domain, but it requires to access the source data which often raises concerns in data privacy, data portability and data transmission efficiency. We study unsupervised model adaptation (UMA), or called Unsupervised Domain Adaptation without Source Data, an alternative setting that aims to adapt source-trained models towards target distributions without accessing source data. To this end, we design an innovative historical contrastive learning (HCL) technique that exploits historical source hypothesis to make up for the absence of source data in UMA. HCL addresses the UMA challenge from two perspectives. First, it introduces historical contrastive instance discrimination (HCID) that learns from target samples by contrasting their embeddings which are generated by the currently adapted model and the historical models. With the historical models, HCID encourages UMA to learn instance-discriminative target representations while preserving the source hypothesis. Second, it introduces historical contrastive category discrimination (HCCD) that pseudo-labels target samples to learn category-discriminative target representations. Specifically, HCCD re-weights pseudo labels according to their prediction consistency across the current and historical models. Extensive experiments show that HCL outperforms and state-of-the-art methods consistently across a variety of visual tasks and setups.",
    "authors": [
      "Huang, Jiaxing",
      "Guan, Dayan",
      "Xiao, Aoran",
      "Lu, Shijian"
    ]
  },
  {
    "id": "1def1713ebf17722cbe300cfc1c88558",
    "title": "The Out-of-Distribution Problem in Explainability and Search Methods for Feature Importance Explanations",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1def1713ebf17722cbe300cfc1c88558-Paper.pdf",
    "abstract": "Feature importance (FI) estimates are a popular form of explanation, and they are commonly created and evaluated by computing the change in model confidence caused by removing certain input features at test time. For example, in the standard Sufficiency metric, only the top-k most important tokens are kept. In this paper, we study several under-explored dimensions of FI explanations, providing conceptual and empirical improvements for this form of explanation. First, we advance a new argument for why it can be problematic to remove features from an input when creating or evaluating explanations: the fact that these counterfactual inputs are out-of-distribution (OOD) to models implies that the resulting explanations are socially misaligned. The crux of the problem is that the model prior and random weight initialization influence the explanations (and explanation metrics) in unintended ways. To resolve this issue, we propose a simple alteration to the model training process, which results in more socially aligned explanations and metrics. Second, we compare among five approaches for removing features from model inputs. We find that some methods produce more OOD counterfactuals than others, and we make recommendations for selecting a feature-replacement function. Finally, we introduce four search-based methods for identifying FI explanations and compare them to strong baselines, including LIME, Anchors, and Integrated Gradients. Through experiments with six diverse text classification datasets, we find that the only method that consistently outperforms random search is a Parallel Local Search (PLS) that we introduce. Improvements over the second best method are as large as 5.4 points for Sufficiency and 17 points for Comprehensiveness.",
    "authors": [
      "Hase, Peter",
      "Xie, Harry",
      "Bansal, Mohit"
    ]
  },
  {
    "id": "1e0b802d5c0e1e8434a771ba7ff2c301",
    "title": "Control Variates for Slate Off-Policy Evaluation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1e0b802d5c0e1e8434a771ba7ff2c301-Paper.pdf",
    "abstract": "We study the problem of off-policy evaluation from batched contextual bandit data with multidimensional actions, often termed slates. The problem is common to recommender systems and user-interface optimization, and it is particularly challenging because of the combinatorially-sized action space. Swaminathan et al. (2017) have proposed the pseudoinverse (PI) estimator under the assumption that the conditional mean rewards are additive in actions. Using control variates, we consider a large class of unbiased estimators that includes as specific cases the PI estimator and (asymptotically) its self-normalized variant. By optimizing over this class, we obtain new estimators with risk improvement guarantees over both the PI and the self-normalized PI estimators. Experiments with real-world recommender data as well as synthetic data validate these improvements in practice.",
    "authors": [
      "Vlassis, Nikos",
      "Chandrashekar, Ashok",
      "Amat, Fernando",
      "Kallus, Nathan"
    ]
  },
  {
    "id": "1e0f65eb20acbfb27ee05ddc000b50ec",
    "title": "Stabilizing Deep Q-Learning with ConvNets and Vision Transformers under Data Augmentation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1e0f65eb20acbfb27ee05ddc000b50ec-Paper.pdf",
    "abstract": "While agents trained by Reinforcement Learning (RL) can solve increasingly challenging tasks directly from visual observations, generalizing learned skills to novel environments remains very challenging. Extensive use of data augmentation is a promising technique for improving generalization in RL, but it is often found to decrease sample efficiency and can even lead to divergence. In this paper, we investigate causes of instability when using data augmentation in common off-policy RL algorithms. We identify two problems, both rooted in high-variance Q-targets. Based on our findings, we propose a simple yet effective technique for stabilizing this class of algorithms under augmentation. We perform extensive empirical evaluation of image-based RL using both ConvNets and Vision Transformers (ViT) on a family of benchmarks based on DeepMind Control Suite, as well as in robotic manipulation tasks. Our method greatly improves stability and sample efficiency of ConvNets under augmentation, and achieves generalization results competitive with state-of-the-art methods for image-based RL in environments with unseen visuals. We further show that our method scales to RL with ViT-based architectures, and that data augmentation may be especially important in this setting.",
    "authors": [
      "Hansen, Nicklas",
      "Su, Hao",
      "Wang, Xiaolong"
    ]
  },
  {
    "id": "1e4d36177d71bbb3558e43af9577d70e",
    "title": "On Effective Scheduling of Model-based Reinforcement Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1e4d36177d71bbb3558e43af9577d70e-Paper.pdf",
    "abstract": "Model-based reinforcement learning has attracted wide attention due to its superior sample efficiency. Despite its impressive success so far, it is still unclear how to appropriately schedule the important hyperparameters to achieve adequate performance, such as the real data ratio for policy optimization in Dyna-style model-based algorithms. In this paper, we first theoretically analyze the role of real data in policy training, which suggests that gradually increasing the ratio of real data yields better performance. Inspired by the analysis, we propose a framework named AutoMBPO to automatically schedule the real data ratio as well as other hyperparameters in training model-based policy optimization (MBPO) algorithm, a representative running case of model-based methods. On several continuous control tasks, the MBPO instance trained with hyperparameters scheduled by AutoMBPO can significantly surpass the original one, and the real data ratio schedule found by AutoMBPO shows consistency with our theoretical analysis.",
    "authors": [
      "Lai, Hang",
      "Shen, Jian",
      "Zhang, Weinan",
      "Huang, Yimin",
      "Zhang, Xing",
      "Tang, Ruiming",
      "Yu, Yong",
      "Li, Zhenguo"
    ]
  },
  {
    "id": "1e5eeb40a3fce716b244599862fd2200",
    "title": "Removing Inter-Experimental Variability from Functional Data in Systems Neuroscience",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1e5eeb40a3fce716b244599862fd2200-Paper.pdf",
    "abstract": "Integrating data from multiple experiments is common practice in systems neuroscience but it requires inter-experimental variability to be negligible compared to the biological signal of interest. This requirement is rarely fulfilled; systematic changes between experiments can drastically affect the outcome of complex analysis pipelines. Modern machine learning approaches designed to adapt models across multiple data domains offer flexible ways of removing inter-experimental variability where classical statistical methods often fail. While applications of these methods have been mostly limited to single-cell genomics, in this work, we develop a theoretical framework for domain adaptation in systems neuroscience. We implement this in an adversarial optimization scheme that removes inter-experimental variability while preserving the biological signal. We compare our method to previous approaches on a large-scale dataset of two-photon imaging recordings of retinal bipolar cell responses to visual stimuli. This dataset provides a unique benchmark as it contains biological signal from well-defined cell types that is obscured by large inter-experimental variability. In a supervised setting, we compare the generalization performance of cell type classifiers across experiments, which we validate with anatomical cell type distributions from electron microscopy data. In an unsupervised setting, we remove inter-experimental variability from the data which can then be fed into arbitrary downstream analyses. In both settings, we find that our method achieves the best trade-off between removing inter-experimental variability and preserving biological signal. Thus, we offer a flexible approach to remove inter-experimental variability and integrate datasets across experiments in systems neuroscience. Code available at https://github.com/eulerlab/rave.",
    "authors": [
      "Gonschorek, Dominic",
      "H\u00f6fling, Larissa",
      "Szatko, Klaudia P.",
      "Franke, Katrin",
      "Schubert, Timm",
      "Dunn, Benjamin",
      "Berens, Philipp",
      "Klindt, David",
      "Euler, Thomas"
    ]
  },
  {
    "id": "1e747ddbea997a1b933aaf58a7953c3c",
    "title": "Learning Knowledge Graph-based World Models of Textual Environments",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1e747ddbea997a1b933aaf58a7953c3c-Paper.pdf",
    "abstract": "World models improve a learning agent's ability to efficiently operate in interactive and situated environments. This work focuses on the task of building world models of text-based game environments. Text-based games, or interactive narratives, are reinforcement learning environments in which agents perceive and interact with the world using textual natural language. These environments contain long, multi-step puzzles or quests woven through a world that is filled with hundreds of characters, locations, and objects. Our world model learns to simultaneously: (1) predict changes in the world caused by an agent's actions when representing the world as a knowledge graph; and (2) generate the set of contextually relevant natural language actions required to operate in the world. We frame this task as a Set of Sequences generation problem by exploiting the inherent structure of knowledge graphs and actions and introduce both a transformer-based multi-task architecture and a loss function to train it. A zero-shot ablation study on never-before-seen textual worlds shows that our methodology significantly outperforms existing textual world modeling techniques as well as the importance of each of our contributions.",
    "authors": [
      "Ammanabrolu, Prithviraj",
      "Riedl, Mark"
    ]
  },
  {
    "id": "1e79596878b2320cac26dd792a6c51c9",
    "title": "Damped Anderson Mixing for Deep Reinforcement Learning: Acceleration, Convergence, and Stabilization",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1e79596878b2320cac26dd792a6c51c9-Paper.pdf",
    "abstract": "Anderson mixing has been heuristically applied to reinforcement learning (RL) algorithms for accelerating convergence and improving the sampling efficiency of deep RL. Despite its heuristic improvement of convergence, a rigorous mathematical justification for the benefits of Anderson mixing in RL has not yet been put forward. In this paper, we provide deeper insights into a class of acceleration schemes built on Anderson mixing that improve the convergence of deep RL algorithms. Our main results establish a connection between Anderson mixing and quasi-Newton methods and prove that Anderson mixing increases the convergence radius of policy iteration schemes by an extra contraction factor. The key focus of the analysis roots in the fixed-point iteration nature of RL. We further propose a stabilization strategy by introducing a stable regularization term in Anderson mixing and a differentiable, non-expansive MellowMax operator that can allow both faster convergence and more stable behavior. Extensive experiments demonstrate that our proposed method enhances the convergence, stability, and performance of RL algorithms.",
    "authors": [
      "Sun, Ke",
      "Wang, Yafei",
      "Liu, Yi",
      "zhao, yingnan",
      "Pan, Bo",
      "Jui, Shangling",
      "Jiang, Bei",
      "Kong, Linglong"
    ]
  },
  {
    "id": "1e8a19426224ca89e83cef47f1e7f53b",
    "title": "Approximate Decomposable Submodular Function Minimization for Cardinality-Based Components",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1e8a19426224ca89e83cef47f1e7f53b-Paper.pdf",
    "abstract": "Minimizing a sum of simple submodular functions of limited support is a special case of general submodular function minimization that has seen numerous applications in machine learning. We develop faster techniques for instances where components in the sum are cardinality-based, meaning they depend only on the size of the input set. This variant is one of the most widely applied in practice, encompassing, e.g., common energy functions arising in image segmentation and recent generalized hypergraph cut functions. We develop the first approximation algorithms for this problem, where the approximations can be quickly computed via reduction to a sparse graph cut problem, with graph sparsity controlled by the desired approximation factor. Our method relies on a new connection between sparse graph reduction techniques and piecewise linear approximations to concave functions. Our sparse reduction technique leads to significant improvements in theoretical runtimes, as well as substantial practical gains in problems ranging from benchmark image segmentation tasks to hypergraph clustering problems. ",
    "authors": [
      "Veldt, Nate",
      "Benson, Austin R.",
      "Kleinberg, Jon"
    ]
  },
  {
    "id": "1e8ca836c962598551882e689265c1c5",
    "title": "Episodic Multi-agent Reinforcement Learning with Curiosity-driven Exploration",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1e8ca836c962598551882e689265c1c5-Paper.pdf",
    "abstract": "Efficient exploration in deep cooperative multi-agent reinforcement learning (MARL) still remains challenging in complex coordination problems. In this paper, we introduce a novel Episodic Multi-agent reinforcement learning with Curiosity-driven exploration, called EMC. We leverage an insight of popular factorized MARL algorithms that the ``induced\" individual Q-values, i.e., the individual utility functions  used for local execution,  are the embeddings of local action-observation histories, and can capture the interaction between agents due to reward backpropagation during centralized training. Therefore, we use prediction errors of individual Q-values as intrinsic rewards for coordinated exploration and utilize episodic memory to exploit explored informative experience to boost policy training. As the dynamics of an agent's individual Q-value function captures the novelty of states and the influence from other agents, our intrinsic reward can induce coordinated exploration to new or promising states. We illustrate the advantages of our method by didactic examples, and demonstrate its significant outperformance over state-of-the-art MARL baselines on challenging tasks in the StarCraft II micromanagement benchmark.",
    "authors": [
      "Zheng, Lulu",
      "Chen, Jiarui",
      "Wang, Jianhao",
      "He, Jiamin",
      "Hu, Yujing",
      "Chen, Yingfeng",
      "Fan, Changjie",
      "Gao, Yang",
      "Zhang, Chongjie"
    ]
  },
  {
    "id": "1e932f24dc0aa4e7a6ac2beec387416d",
    "title": "Two Sides of Meta-Learning Evaluation: In vs. Out of Distribution",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1e932f24dc0aa4e7a6ac2beec387416d-Paper.pdf",
    "abstract": "We categorize meta-learning evaluation into two settings: $\\textit{in-distribution}$ [ID], in which the train and test tasks are sampled $\\textit{iid}$ from the same underlying task distribution, and $\\textit{out-of-distribution}$ [OOD], in which they are not. While most meta-learning theory and some FSL applications follow the ID setting, we identify that most existing few-shot classification benchmarks instead reflect OOD evaluation, as they use disjoint sets of train (base) and test (novel) classes for task generation. This discrepancy is problematic because -- as we show on numerous benchmarks -- meta-learning methods that perform better on existing OOD datasets may perform significantly worse in the ID setting. In addition, in the OOD setting, even though current FSL benchmarks seem befitting, our study highlights concerns in 1) reliably performing model selection for a given meta-learning method, and 2) consistently comparing the performance of different methods. To address these concerns, we provide suggestions on how to construct FSL benchmarks to allow for ID evaluation as well as more reliable OOD evaluation. Our work aims to inform the meta-learning community about the importance and distinction of ID vs. OOD evaluation, as well as the subtleties of OOD evaluation with current benchmarks.",
    "authors": [
      "Setlur, Amrith",
      "Li, Oscar",
      "Smith, Virginia"
    ]
  },
  {
    "id": "1f4477bad7af3616c1f933a02bfabe4e",
    "title": "Debiased Visual Question Answering from Feature and Sample Perspectives",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1f4477bad7af3616c1f933a02bfabe4e-Paper.pdf",
    "abstract": "Visual question answering (VQA) is designed to examine the visual-textual reasoning ability of an intelligent agent. However, recent observations show that many VQA models may only capture the biases between questions and answers in a dataset rather than showing real reasoning abilities. For example, given a question, some VQA models tend to output the answer that occurs frequently in the dataset and ignore the images. To reduce this tendency, existing methods focus on weakening the language bias. Meanwhile, only a few works also consider vision bias implicitly. However, these methods introduce additional annotations or show unsatisfactory performance. Moreover, not all biases are harmful to the models. Some \u201cbiases\u201d learnt from datasets represent natural rules of the world and can help limit the range of answers. Thus, how to filter and remove the true negative biases in language and vision modalities remain a major challenge. In this paper, we propose a method named D-VQA to alleviate the above challenges from the feature and sample perspectives. Specifically, from the feature perspective, we build a question-to-answer and vision-to-answer branch to capture the language and vision biases, respectively. Next, we apply two unimodal bias detection modules to explicitly recognise and remove the negative biases. From the sample perspective, we construct two types of negative samples to assist the training of the models, without introducing additional annotations. Extensive experiments on the VQA-CP v2 and VQA v2 datasets demonstrate the effectiveness of our D-VQA method.",
    "authors": [
      "Wen, Zhiquan",
      "Xu, Guanghui",
      "Tan, Mingkui",
      "Wu, Qingyao",
      "Wu, Qi"
    ]
  },
  {
    "id": "1f4fe6a4411edc2ff625888b4093e917",
    "title": "Towards a Unified Game-Theoretic View of Adversarial Perturbations and Robustness",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1f4fe6a4411edc2ff625888b4093e917-Paper.pdf",
    "abstract": "This paper provides a unified view to explain different adversarial attacks and defense methods, i.e. the view of multi-order interactions between input variables of DNNs. Based on the multi-order interaction, we discover that adversarial attacks mainly affect high-order interactions to fool the DNN. Furthermore, we find that the robustness of adversarially trained DNNs comes from category-specific low-order interactions. Our findings provide a potential method to unify adversarial perturbations and robustness, which can explain the existing robustness-boosting methods in a principle way. Besides, our findings also make a revision of previous inaccurate understanding of the shape bias of adversarially learned features. Our code is available online at https://github.com/Jie-Ren/A-Unified-Game-Theoretic-Interpretation-of-Adversarial-Robustness.",
    "authors": [
      "Ren, Jie",
      "Zhang, Die",
      "Wang, Yisen",
      "Chen, Lu",
      "Zhou, Zhanpeng",
      "Chen, Yiting",
      "Cheng, Xu",
      "Wang, Xin",
      "Zhou, Meng",
      "Shi, Jie",
      "Zhang, Quanshi"
    ]
  },
  {
    "id": "1f88c7c5d7d94ae08bd752aa3d82108b",
    "title": "On the Out-of-distribution Generalization of Probabilistic Image Modelling",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1f88c7c5d7d94ae08bd752aa3d82108b-Paper.pdf",
    "abstract": "Out-of-distribution (OOD) detection and lossless compression constitute two problems that can be solved by the training of probabilistic models on a first dataset with subsequent likelihood evaluation on a second dataset, where data distributions differ. By defining the generalization of probabilistic models in terms of likelihood we show that, in the case of image models, the OOD generalization ability is dominated by local features. This motivates our proposal of a Local Autoregressive model that exclusively models local image features towards improving OOD performance. We apply the proposed model to OOD detection tasks and achieve state-of-the-art unsupervised OOD detection performance without the introduction of additional data. Additionally, we employ our model to build a new lossless image compressor: NeLLoC (Neural Local Lossless Compressor) and report state-of-the-art compression rates and model size.",
    "authors": [
      "Zhang, Mingtian",
      "Zhang, Andi",
      "McDonagh, Steven"
    ]
  },
  {
    "id": "1f9b616faddedc02339603f3b37d196c",
    "title": "Exploiting Local Convergence of Quasi-Newton Methods Globally: Adaptive Sample Size Approach",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1f9b616faddedc02339603f3b37d196c-Paper.pdf",
    "abstract": "In this paper, we study the application of quasi-Newton methods for solving empirical risk minimization (ERM) problems defined over a large dataset. Traditional deterministic and stochastic quasi-Newton methods can be executed to solve such problems; however, it is known that their global convergence rate may not be better than first-order methods, and their local superlinear convergence only appears towards the end of the learning process. In this paper, we use an adaptive sample size scheme that exploits the superlinear convergence of quasi-Newton methods globally and throughout the entire learning process. The main idea of the proposed adaptive sample size algorithms is to start with a small subset of data points and solve their corresponding ERM problem within its statistical accuracy, and then enlarge the sample size geometrically and use the optimal solution of the problem corresponding to the smaller set as an initial point for solving the subsequent ERM problem with more samples. We show that if the initial sample size is sufficiently large and we use quasi-Newton methods to solve each subproblem, the subproblems can be solved superlinearly fast (after at most three iterations), as we guarantee that the iterates always stay within a neighborhood that quasi-Newton methods converge superlinearly. Numerical experiments on various datasets confirm our theoretical results and demonstrate the computational advantages of our method.",
    "authors": [
      "Jin, Qiujiang",
      "Mokhtari, Aryan"
    ]
  },
  {
    "id": "1f9f9d8ff75205aa73ec83e543d8b571",
    "title": "PDE-GCN: Novel Architectures for Graph Neural Networks Motivated by Partial Differential Equations",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1f9f9d8ff75205aa73ec83e543d8b571-Paper.pdf",
    "abstract": "Graph neural networks are increasingly becoming the go-to approach in various fields such as computer vision, computational biology and chemistry, where data are naturally explained by graphs. However, unlike traditional convolutional neural networks, deep graph networks do not necessarily yield better performance than shallow graph networks. This behavior usually stems from the over-smoothing phenomenon. In this work, we propose a family of architecturesto control this behavior by design. Our networks are motivated by numerical methods for solving Partial Differential Equations (PDEs) on manifolds, and as such, their behavior can be explained by similar analysis. Moreover, as we demonstrate using an extensive set of experiments, our PDE-motivated networks can generalize and be effective for various types of problems from different fields. Our architectures obtain better or on par with the current state-of-the-art results for problems that are typically approached using different architectures.",
    "authors": [
      "Eliasof, Moshe",
      "Haber, Eldad",
      "Treister, Eran"
    ]
  },
  {
    "id": "1fa6269f58898f0e809575c9a48747ef",
    "title": "Information Directed Reward Learning for Reinforcement Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1fa6269f58898f0e809575c9a48747ef-Paper.pdf",
    "abstract": "For many reinforcement learning (RL) applications, specifying a reward is difficult. In this paper, we consider an RL setting where the agent can obtain information about the reward only by querying an expert that can, for example, evaluate individual states or provide binary preferences over trajectories. From such expensive feedback, we aim to learn a model of the reward function that allows standard RL algorithms to achieve high expected return with as few expert queries as possible. For this purpose, we propose Information Directed Reward Learning (IDRL), which uses a Bayesian model of the reward function and selects queries that maximize the information gain about the difference in return between potentially optimal policies. In contrast to prior active reward learning methods designed for specific types of queries, IDRL naturally accommodates different query types. Moreover, by shifting the focus from reducing the reward approximation error to improving the policy induced by the reward model, it achieves similar or better performance with significantly fewer queries. We support our findings with extensive evaluations  in multiple environments and with different types of queries.",
    "authors": [
      "Lindner, David",
      "Turchetta, Matteo",
      "Tschiatschek, Sebastian",
      "Ciosek, Kamil",
      "Krause, Andreas"
    ]
  },
  {
    "id": "1fb2a1c37b18aa4611c3949d6148d0f8",
    "title": "SSMF: Shifting Seasonal Matrix Factorization",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1fb2a1c37b18aa4611c3949d6148d0f8-Paper.pdf",
    "abstract": "Given taxi-ride counts information between departure and destination locations, how can we forecast their future demands? In general, given a data stream of events with seasonal patterns that innovate over time, how can we effectively and efficiently forecast future events? In this paper, we propose Shifting Seasonal Matrix Factorization approach, namely SSMF, that can adaptively learn multiple seasonal patterns (called regimes), as well as switching between them. Our proposed method has the following properties: (a) it accurately forecasts future events by detecting regime shifts in seasonal patterns as the data stream evolves; (b) it works in an online setting, i.e., processes each observation in constant time and memory; (c) it effectively realizes regime shifts without human intervention by using a lossless data compression scheme. We demonstrate that our algorithm outperforms state-of-the-art baseline methods by accurately forecasting upcoming events on three real-world data streams.",
    "authors": [
      "Kawabata, Koki",
      "Bhatia, Siddharth",
      "Liu, Rui",
      "Wadhwa, Mohit",
      "Hooi, Bryan"
    ]
  },
  {
    "id": "1fb36c4ccf88f7e67ead155496f02338",
    "title": "Associative Memories via Predictive Coding",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1fb36c4ccf88f7e67ead155496f02338-Paper.pdf",
    "abstract": "Associative memories in the brain receive and store patterns of activity registered by the sensory neurons, and are able to retrieve them when necessary. Due to their importance in human intelligence, computational models of associative memories have been developed for several decades now. In this paper, we present a novel neural model for realizing associative memories, which is based on a hierarchical generative network that receives external stimuli via sensory neurons. It is trained using predictive coding, an error-based learning algorithm inspired by information processing in the cortex. To test the model's capabilities, we perform multiple retrieval experiments from both corrupted and incomplete data points. In an extensive comparison, we show that this new model outperforms in retrieval accuracy and robustness popular associative memory models, such as  autoencoders trained via backpropagation, and modern Hopfield networks. In particular, in completing partial data points, our model achieves remarkable results on natural image datasets, such as ImageNet, with a surprisingly high accuracy, even when only a tiny fraction of pixels of the original images is presented. Our model provides a plausible framework to study learning and retrieval of memories in the brain, as it closely mimics the behavior of the hippocampus as a memory index and generative model.",
    "authors": [
      "Salvatori, Tommaso",
      "Song, Yuhang",
      "Hong, Yujian",
      "Sha, Lei",
      "Frieder, Simon",
      "Xu, Zhenghua",
      "Bogacz, Rafal",
      "Lukasiewicz, Thomas"
    ]
  },
  {
    "id": "1fc5309ccc651bf6b5d22470f67561ea",
    "title": "Robust and differentially private mean estimation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1fc5309ccc651bf6b5d22470f67561ea-Paper.pdf",
    "abstract": "In statistical learning and analysis from shared data, which is increasingly widely adopted in platforms such as federated learning and meta-learning, there are two major concerns: privacy and robustness. Each participating individual should be able to contribute without the fear of leaking one's sensitive information.  At the same time, the system should be robust in the presence of malicious participants inserting corrupted data. Recent algorithmic advances in learning from shared data focus on either one of these threats, leaving the system vulnerable to the other. We bridge this gap for the canonical problem of estimating the mean from i.i.d.~samples. We introduce PRIME, which is the first efficient algorithm that achieves both privacy and robustness for a wide range of distributions. We further complement this result with a novel exponential time algorithm that improves the sample complexity of PRIME, achieving a near-optimal guarantee and matching that of a known lower bound for (non-robust) private mean estimation. This proves that there is no extra statistical cost to simultaneously guaranteeing privacy and robustness.  ",
    "authors": [
      "Liu, Xiyang",
      "Kong, Weihao",
      "Kakade, Sham",
      "Oh, Sewoong"
    ]
  },
  {
    "id": "1fc8c3d03b0021478a8c9ebdcd457c67",
    "title": "Adaptable Agent Populations via a Generative Model of Policies",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/1fc8c3d03b0021478a8c9ebdcd457c67-Paper.pdf",
    "abstract": "In the natural world, life has found innumerable ways to survive and often thrive. Between and even within species, each individual is in some manner unique, and this diversity lends adaptability and robustness to life. In this work, we aim to learn a space of diverse and high-reward policies in a given environment. To this end, we introduce a generative model of policies for reinforcement learning, which maps a low-dimensional latent space to an agent policy space. Our method enables learning an entire population of agent policies, without requiring the use of separate policy parameters. Just as real world populations can adapt and evolve via natural selection, our method is able to adapt to changes in our environment solely by selecting for policies in latent space. We test our generative model\u2019s capabilities in a variety of environments, including an open-ended grid-world and a two-player soccer environment. Code, visualizations, and additional experiments can be found at https://kennyderek.github.io/adap/.",
    "authors": [
      "Derek, Kenneth",
      "Isola, Phillip"
    ]
  },
  {
    "id": "201d546992726352471cfea6b0df0a48",
    "title": "A No-go Theorem for Robust Acceleration in the Hyperbolic Plane",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/201d546992726352471cfea6b0df0a48-Paper.pdf",
    "abstract": "In recent years there has been significant effort to adapt the key tools and ideas in convex optimization to the Riemannian setting. One key challenge has remained: Is there a Nesterov-like accelerated gradient method for geodesically convex functions on a Riemannian manifold? Recent work has given partial answers and the hope was that this ought to be possible. Here we prove that in a noisy setting, there is no analogue of accelerated gradient descent for geodesically convex functions on the hyperbolic plane. Our results apply even when the noise is exponentially small. The key intuition behind our proof is short and simple: In negatively curved spaces, the volume of a ball grows so fast that information about the past gradients is not useful in the future.",
    "authors": [
      "Hamilton, Linus",
      "Moitra, Ankur"
    ]
  },
  {
    "id": "201d7288b4c18a679e48b31c72c30ded",
    "title": "Privately Learning Mixtures of Axis-Aligned Gaussians",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/201d7288b4c18a679e48b31c72c30ded-Paper.pdf",
    "abstract": "We consider the problem of learning multivariate Gaussians under the constraint of approximate differential privacy. We prove that $\\widetilde{O}(k^2 d \\log^{3/2}(1/\\delta) / \\alpha^2 \\varepsilon)$ samples are sufficient to learn a mixture of $k$ axis-aligned Gaussians in $\\mathbb{R}^d$ to within total variation distance $\\alpha$ while satisfying $(\\varepsilon, \\delta)$-differential privacy. This is the first result for privately learning mixtures of unbounded axis-aligned (or even unbounded univariate) Gaussians. If the covariance matrices of each of the Gaussians is the identity matrix, we show that $\\widetilde{O}(kd/\\alpha^2 + kd \\log(1/\\delta) / \\alpha \\varepsilon)$ samples are sufficient.To prove our results, we design a new technique for privately learning mixture distributions.  A class of distributions $\\mathcal{F}$ is said to be list-decodable if there is an algorithm that, given \"heavily corrupted\" samples from $f \\in \\mathcal{F}$, outputs a list of distributions one of which approximates $f$. We show that if $\\mathcal{F}$ is privately list-decodable then we can learn mixtures of distributions in $\\mathcal{F}$. Finally, we show axis-aligned Gaussian distributions are privately list-decodable, thereby proving mixtures of such distributions are privately learnable.",
    "authors": [
      "Aden-Ali, Ishaq",
      "Ashtiani, Hassan",
      "Liaw, Christopher"
    ]
  },
  {
    "id": "20479c788fb27378c2c99eadcf207e7f",
    "title": "Deep Self-Dissimilarities as Powerful Visual Fingerprints",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/20479c788fb27378c2c99eadcf207e7f-Paper.pdf",
    "abstract": "Features extracted from deep layers of classification networks are widely used as image descriptors. Here, we exploit an unexplored property of these features: their internal dissimilarity. While small image patches are known to have similar statistics across image scales, it turns out that the internal distribution of deep features varies distinctively between scales. We show how this deep self dissimilarity (DSD) property can be used as a powerful visual fingerprint. Particularly, we illustrate that full-reference and no-reference image quality measures derived from DSD are highly correlated with human preference. In addition, incorporating DSD as a loss function in training of image restoration networks, leads to results that are at least as photo-realistic as those obtained by GAN based methods, while not requiring adversarial training.",
    "authors": [
      "Kligvasser, Idan",
      "Shaham, Tamar",
      "Bahat, Yuval",
      "Michaeli, Tomer"
    ]
  },
  {
    "id": "204904e461002b28511d5880e1c36a0f",
    "title": "Invariant Causal Imitation Learning for Generalizable Policies",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/204904e461002b28511d5880e1c36a0f-Paper.pdf",
    "abstract": "Consider learning an imitation policy on the basis of demonstrated behavior from multiple environments, with an eye towards deployment in an unseen environment. Since the observable features from each setting may be different, directly learning individual policies as mappings from features to actions is prone to spurious correlations---and may not generalize well. However, the expert\u2019s policy is often a function of a shared latent structure underlying those observable features that is invariant across settings. By leveraging data from multiple environments, we propose Invariant Causal Imitation Learning (ICIL), a novel technique in which we learn a feature representation that is invariant across domains, on the basis of which we learn an imitation policy that matches expert behavior. To cope with transition dynamics mismatch, ICIL learns a shared representation of causal features (for all training environments), that is disentangled from the specific representations of noise variables (for each of those environments). Moreover, to ensure that the learned policy matches the observation distribution of the expert's policy, ICIL estimates the energy of the expert's observations and uses a regularization term that minimizes the imitator policy's next state energy. Experimentally, we compare our methods against several benchmarks in control and healthcare tasks and show its effectiveness in learning imitation policies capable of generalizing to unseen environments.",
    "authors": [
      "Bica, Ioana",
      "Jarrett, Daniel",
      "van der Schaar, Mihaela"
    ]
  },
  {
    "id": "20568692db622456cc42a2e853ca21f8",
    "title": "CoAtNet: Marrying Convolution and Attention for All Data Sizes",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/20568692db622456cc42a2e853ca21f8-Paper.pdf",
    "abstract": "Transformers have attracted increasing interests in computer vision, but they still fall behind state-of-the-art convolutional networks. In this work, we show that while Transformers tend to have larger model capacity, their generalization can be worse than convolutional networks due to the lack of the right inductive bias. To effectively combine the strengths from both architectures, we present CoAtNets(pronounced \"coat\" nets), a family of hybrid models built from two key insights: (1) depthwise Convolution and self-Attention can be naturally unified via simple relative attention; (2) vertically stacking convolution layers and attention layers in a principled way is surprisingly effective in improving generalization, capacity and efficiency. Experiments show that our CoAtNets achieve state-of-the-art performance under different resource constraints across various datasets: Without extra data, CoAtNet achieves 86.0% ImageNet top-1 accuracy; When pre-trained with 13M images from ImageNet-21K, our CoAtNet achieves 88.56% top-1 accuracy, matching ViT-huge pre-trained with 300M images from JFT-300M while using 23x less data; Notably, when we further scale up CoAtNet with JFT-3B, it achieves 90.88% top-1 accuracy on ImageNet, establishing a new state-of-the-art result.",
    "authors": [
      "Dai, Zihang",
      "Liu, Hanxiao",
      "Le, Quoc V",
      "Tan, Mingxing"
    ]
  },
  {
    "id": "20885c72ca35d75619d6a378edea9f76",
    "title": "Mixed Supervised Object Detection by Transferring Mask Prior and Semantic Similarity",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/20885c72ca35d75619d6a378edea9f76-Paper.pdf",
    "abstract": "Object detection has achieved promising success, but requires large-scale fully-annotated data, which is time-consuming and labor-extensive. Therefore, we consider object detection with mixed supervision, which learns novel object categories using weak annotations with the help of full annotations of existing base object categories. Previous works using mixed supervision mainly learn the class-agnostic objectness from fully-annotated categories, which can be transferred to upgrade the weak annotations to pseudo full annotations for novel categories. In this paper, we further transfer mask prior and semantic similarity to bridge the gap between novel categories and base categories. Specifically, the ability of using mask prior to help detect objects is learned from base categories and transferred to novel categories. Moreover, the semantic similarity between objects learned from base categories is transferred to denoise the pseudo full annotations for novel categories. Experimental results on three benchmark datasets demonstrate the effectiveness of our method over existing methods. Codes are available at https://github.com/bcmi/TraMaS-Weak-Shot-Object-Detection.",
    "authors": [
      "Liu, Yan",
      "Zhang, Zhijie",
      "Niu, Li",
      "Chen, Junjie",
      "Zhang, Liqing"
    ]
  },
  {
    "id": "20aee3a5f4643755a79ee5f6a73050ac",
    "title": "Celebrating Diversity in Shared Multi-Agent Reinforcement Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/20aee3a5f4643755a79ee5f6a73050ac-Paper.pdf",
    "abstract": "Recently, deep multi-agent reinforcement learning (MARL) has shown the promise to solve complex cooperative tasks. Its success is partly because of parameter sharing among agents. However, such sharing may lead agents to behave similarly and limit their coordination capacity. In this paper, we aim to introduce diversity in both optimization and representation of shared multi-agent reinforcement learning. Specifically, we propose an information-theoretical regularization to maximize the mutual information between agents' identities and their trajectories, encouraging extensive exploration and diverse individualized behaviors. In representation, we incorporate agent-specific modules in the shared neural network architecture, which are regularized by L1-norm to promote learning sharing among agents while keeping necessary diversity. Empirical results show that our method achieves state-of-the-art performance on Google Research Football and super hard StarCraft II micromanagement tasks.",
    "authors": [
      "Li, Chenghao",
      "Wang, Tonghan",
      "Wu, Chengjie",
      "Zhao, Qianchuan",
      "Yang, Jun",
      "Zhang, Chongjie"
    ]
  },
  {
    "id": "2109737282d2c2de4fc5534be26c9bb6",
    "title": "Rebounding Bandits for Modeling Satiation Effects",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2109737282d2c2de4fc5534be26c9bb6-Paper.pdf",
    "abstract": "Psychological research shows that enjoyment of many goods is subject to satiation, with short-term satisfaction declining after repeated exposures to the same item. Nevertheless, proposed algorithms for powering recommender systems seldom model these dynamics, instead proceeding as though user preferences were fixed in time. In this work, we introduce rebounding bandits, a multi-armed bandit setup, where satiation dynamics are modeled as time-invariant linear dynamical systems. Expected rewards for each arm decline monotonically with consecutive exposures and rebound towards the initial reward whenever that arm is not pulled. Unlike classical bandit algorithms, methods for tackling rebounding bandits must plan ahead and model-based methods rely on estimating the parameters of the satiation dynamics. We characterize the planning problem, showing that the greedy policy is optimal when the arms exhibit identical deterministic dynamics. To address stochastic satiation dynamics with unknown parameters, we propose Explore-Estimate-Plan, an algorithm that pulls arms methodically, estimates the system dynamics, and then plans accordingly.",
    "authors": [
      "Leqi, Liu",
      "Kilinc Karzan, Fatma",
      "Lipton, Zachary",
      "Montgomery, Alan"
    ]
  },
  {
    "id": "210b7ec74fc9cec6fb8388dbbdaf23f7",
    "title": "Sample Complexity of Tree Search Configuration: Cutting Planes and Beyond",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/210b7ec74fc9cec6fb8388dbbdaf23f7-Paper.pdf",
    "abstract": "Cutting-plane methods have enabled remarkable successes in integer programming over the last few decades. State-of-the-art solvers integrate a myriad of cutting-plane techniques to speed up the underlying tree-search algorithm used to find optimal solutions. In this paper we provide sample complexity bounds for cut-selection in branch-and-cut (B&C). Given a training set of integer programs sampled from an application-specific input distribution and a family of cut selection policies, these guarantees bound the number of samples sufficient to ensure that using any policy in the family, the size of the tree B&C builds on average over the training set is close to the expected size of the tree B&C builds. We first bound the sample complexity of learning cutting planes from the canonical family of Chv\u00e1tal-Gomory cuts. Our bounds handle any number of waves of any number of cuts and are fine tuned to the magnitudes of the constraint coefficients. Next, we prove sample complexity bounds for more sophisticated cut selection policies that use a combination of scoring rules to choose from a family of cuts. Finally, beyond the realm of cutting planes for integer programming, we develop a general abstraction of tree search that captures key components such as node selection and variable selection. For this abstraction, we bound the sample complexity of learning a good policy for building the search tree.",
    "authors": [
      "Balcan, Maria-Florina F.",
      "Prasad, Siddharth",
      "Sandholm, Tuomas",
      "Vitercik, Ellen"
    ]
  },
  {
    "id": "210f760a89db30aa72ca258a3483cc7f",
    "title": "IQ-Learn: Inverse soft-Q Learning for Imitation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/210f760a89db30aa72ca258a3483cc7f-Paper.pdf",
    "abstract": "In many sequential decision-making problems (e.g., robotics control, game playing, sequential prediction), human or expert data is available containing useful information about the task. However, imitation learning (IL) from a small amount of expert data can be challenging in high-dimensional environments with complex dynamics. Behavioral cloning is a simple method that is widely used due to its simplicity of implementation and stable convergence but doesn't utilize any information involving the environment\u2019s dynamics. Many existing methods that exploit dynamics information are difficult to train in practice due to an adversarial optimization process over reward and policy approximators or biased, high variance gradient estimators. We introduce a method for dynamics-aware IL which avoids adversarial training by learning a single Q-function, implicitly representing both reward and policy. On standard benchmarks, the implicitly learned rewards show a high positive correlation with the ground-truth rewards, illustrating our method can also be used for inverse reinforcement learning (IRL). Our method, Inverse soft-Q learning (IQ-Learn) obtains state-of-the-art results in offline and online imitation learning settings, significantly outperforming existing methods both in the number of required environment interactions and scalability in high-dimensional spaces, often by more than 3x.",
    "authors": [
      "Garg, Divyansh",
      "Chakraborty, Shuvam",
      "Cundy, Chris",
      "Song, Jiaming",
      "Ermon, Stefano"
    ]
  },
  {
    "id": "21186d7b1482412ab14f0332b8aee119",
    "title": "Task-Agnostic Undesirable Feature Deactivation Using Out-of-Distribution Data",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/21186d7b1482412ab14f0332b8aee119-Paper.pdf",
    "abstract": "A deep neural network (DNN) has achieved great success in many machine learning tasks by virtue of its high expressive power. However, its prediction can be easily biased to undesirable features, which are not essential for solving the target task and are even imperceptible to a human, thereby resulting in poor generalization. Leveraging plenty of undesirable features in out-of-distribution (OOD) examples has emerged as a potential solution for de-biasing such features, and a recent study shows that softmax-level calibration of OOD examples can successfully remove the contribution of undesirable features to the last fully-connected layer of a classifier. However, its applicability is confined to the classification task, and its impact on a DNN feature extractor is not properly investigated. In this paper, we propose Taufe, a novel regularizer that deactivates many undesirable features using OOD examples in the feature extraction layer and thus removes the dependency on the task-specific softmax layer. To show the task-agnostic nature of Taufe, we rigorously validate its performance on three tasks, classification, regression, and a mix of them, on CIFAR-10, CIFAR-100, ImageNet, CUB200, and CAR datasets. The results demonstrate that Taufe consistently outperforms the state-of-the-art method as well as the baselines without regularization. ",
    "authors": [
      "Park, Dongmin",
      "Song, Hwanjun",
      "Kim, Minseok",
      "Lee, Jae-Gil"
    ]
  },
  {
    "id": "211c1e0b83b9c69fa9c4bdede203c1e3",
    "title": "Private Non-smooth ERM and SCO in Subquadratic Steps",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/211c1e0b83b9c69fa9c4bdede203c1e3-Paper.pdf",
    "abstract": "We study the differentially private Empirical Risk Minimization (ERM) and Stochastic Convex Optimization (SCO) problems for non-smooth convex functions. We get a (nearly) optimal bound on the excess empirical risk for ERM with $O(\\frac{N^{3/2}}{d^{1/8}}+ \\frac{N^2}{d})$ gradient queries, which is achieved with the help of subsampling and smoothing the function via convolution. Combining this result with the iterative localization technique of Feldman et al. \\cite{fkt20}, we achieve the optimal excess population loss for the SCO problem with $O(\\min\\{N^{5/4}d^{1/8},\\frac{ N^{3/2}}{d^{1/8}}\\})$ gradient queries.Our work makes progress towards resolving a question raised by Bassily et al. \\cite{bfgt20}, giving first algorithms for private SCO with subquadratic steps. In a concurrent work, Asi et al. \\cite{afkt21} gave other algorithms for private ERM and SCO with subquadratic steps.",
    "authors": [
      "Kulkarni, Janardhan",
      "Lee, Yin Tat",
      "Liu, Daogao"
    ]
  },
  {
    "id": "212ab20dbdf4191cbcdcf015511783f4",
    "title": "Towards Instance-Optimal Offline Reinforcement Learning with Pessimism",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/212ab20dbdf4191cbcdcf015511783f4-Paper.pdf",
    "abstract": "We study the \\emph{offline reinforcement learning}  (offline RL) problem, where the goal is to learn a reward-maximizing policy in an unknown \\emph{Markov Decision Process} (MDP) using the data coming from a policy $\\mu$. In particular, we consider the sample complexity problems of offline RL for the finite horizon MDPs. Prior works derive the information-theoretical lower bounds based on different data-coverage assumptions and their upper bounds are expressed by the covering coefficients which lack the explicit characterization of system quantities. In this work, we analyze the \\emph{Adaptive Pessimistic Value Iteration} (APVI) algorithm and derive the suboptimality upper bound that nearly matches\\[O\\left(\\sum_{h=1}^H\\sum_{s_h,a_h}d^{\\pi^\\star}_h(s_h,a_h)\\sqrt{\\frac{\\mathrm{Var}_{P_{s_h,a_h}}{(V^\\star_{h+1}+r_h)}}{d^\\mu_h(s_h,a_h)}}\\sqrt{\\frac{1}{n}}\\right).\\]We also prove an information-theoretical lower bound to show this quantity is required under the weak assumption that $d^\\mu_h(s_h,a_h)>0$ if $d^{\\pi^\\star}_h(s_h,a_h)>0$. Here $\\pi^\\star$ is a optimal policy, $\\mu$ is the behavior policy and $d(s_h,a_h)$ is the marginal state-action probability. We call this adaptive bound the \\emph{intrinsic offline reinforcement learning bound} since it directly implies all the existing optimal results: minimax rate under uniform data-coverage assumption, horizon-free setting, single policy concentrability, and the tight problem-dependent results. Later, we extend the result to the \\emph{assumption-free} regime (where we make no assumption on $\\mu$) and obtain the assumption-free intrinsic bound. Due to its generic form, we believe the intrinsic bound could help illuminate what makes a specific problem hard and reveal the fundamental challenges in offline RL.",
    "authors": [
      "Yin, Ming",
      "Wang, Yu-Xiang"
    ]
  },
  {
    "id": "2130eb640e0a272898a51da41363542d",
    "title": "Speedy Performance Estimation for Neural Architecture Search",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2130eb640e0a272898a51da41363542d-Paper.pdf",
    "abstract": "Reliable yet efficient evaluation of generalisation performance of a proposed architecture is crucial to the success of neural architecture search (NAS). Traditional approaches face a variety of limitations: training each architecture to completion is prohibitively expensive, early stopped validation accuracy may correlate poorly with fully trained performance, and model-based estimators require large training sets. We instead propose to estimate the final test performance based on a simple measure of training speed. Our estimator is theoretically motivated by the connection between generalisation and training speed, and is also inspired by the reformulation of a PAC-Bayes bound under the Bayesian setting. Our model-free estimator is simple, efficient, and cheap to implement, and does not require hyperparameter-tuning or surrogate training before deployment. We demonstrate on various NAS search spaces that our estimator consistently outperforms other alternatives in achieving better correlation with the true test performance rankings. We further show that our estimator can be easily incorporated into both query-based and one-shot NAS methods to improve the speed or quality of the search.",
    "authors": [
      "Ru, Robin",
      "Lyle, Clare",
      "Schut, Lisa",
      "Fil, Miroslav",
      "van der Wilk, Mark",
      "Gal, Yarin"
    ]
  },
  {
    "id": "214cfbe603b7f9f9bc005d5f53f7a1d3",
    "title": "How Tight Can PAC-Bayes be in the Small Data Regime?",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/214cfbe603b7f9f9bc005d5f53f7a1d3-Paper.pdf",
    "abstract": "In this paper, we investigate the question: _Given a small number of datapoints, for example $N = 30$, how tight can PAC-Bayes and test set bounds be made?_ For such small datasets, test set bounds adversely affect generalisation performance by withholding data from the training procedure. In this setting, PAC-Bayes bounds are especially attractive, due to their ability to use all the data to simultaneously learn a posterior and bound its generalisation risk. We focus on the case of i.i.d. data with a bounded loss and consider the generic PAC-Bayes theorem of Germain et al. While their theorem is known to recover many existing PAC-Bayes bounds, it is unclear what the tightest bound derivable from their framework is. For a fixed learning algorithm and dataset, we show that the tightest possible bound coincides with a bound considered by Catoni; and, in the more natural case of distributions over datasets, we establish a lower bound on the best bound achievable in expectation. Interestingly, this lower bound recovers the Chernoff test set bound if the posterior is equal to the prior. Moreover, to illustrate how tight these bounds can be, we study synthetic one-dimensional classification tasks in which it is feasible to meta-learn both the prior and the form of the bound to numerically optimise for the tightest bounds possible. We find that in this simple, controlled scenario, PAC-Bayes bounds are competitive with comparable, commonly used Chernoff test set bounds. However, the sharpest test set bounds still lead to better guarantees on the generalisation error than the PAC-Bayes bounds we consider.",
    "authors": [
      "Foong, Andrew",
      "Bruinsma, Wessel",
      "Burt, David",
      "Turner, Richard"
    ]
  },
  {
    "id": "215a71a12769b056c3c32e7299f1c5ed",
    "title": "Deep Synoptic Monte-Carlo Planning in Reconnaissance Blind Chess",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/215a71a12769b056c3c32e7299f1c5ed-Paper.pdf",
    "abstract": "This paper introduces deep synoptic Monte Carlo planning (DSMCP) for large imperfect information games. The algorithm constructs a belief state with an unweighted particle filter and plans via playouts that start at samples drawn from the belief state. The algorithm accounts for uncertainty by performing inference on \"synopses,\" a novel stochastic abstraction of information states. DSMCP is the basis of the program Penumbra, which won the official 2020 reconnaissance blind chess competition versus 33 other programs. This paper also evaluates algorithm variants that incorporate caution, paranoia, and a novel bandit algorithm. Furthermore, it audits the synopsis features used in Penumbra with per-bit saliency statistics.",
    "authors": [
      "Clark, Gregory"
    ]
  },
  {
    "id": "2172fde49301047270b2897085e4319d",
    "title": "Dynamic Analysis of Higher-Order Coordination in Neuronal Assemblies via De-Sparsified Orthogonal Matching Pursuit",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2172fde49301047270b2897085e4319d-Paper.pdf",
    "abstract": "Coordinated ensemble spiking activity is widely observable in neural recordings and central in the study of population codes, with hypothesized roles including robust stimulus representation, interareal communication of neural information, and learning and memory formation. Model-free measures of synchrony characterize the coherence of pairwise activity, but not higher-order interactions; this limitation is transcended by statistical models of ensemble spiking activity. However, existing model-based analyses often impose assumptions about the relevance of higher-order interactions and require multiple repeated trials in order to characterize dynamics in the correlational structure of ensemble activity. To address these shortcomings, we propose an adaptive greedy filtering algorithm based on a discretized mark point-process model of ensemble spiking and a corresponding precise statistical inference framework to identify significant coordinated higher-order spiking activity. In the course of developing the statistical inference procedures, we also show that confidence intervals can be constructed for greedily estimated parameters. We demonstrate the utility of our proposed methods on simulated neuronal assemblies. Applied to multi-electrode recordings of human cortical ensembles, our proposed methods provide new insights into the dynamics underlying localized population activity during transitions between brain states.",
    "authors": [
      "Mukherjee, Shoutik",
      "Babadi, Behtash"
    ]
  },
  {
    "id": "2175f8c5cd9604f6b1e576b252d4c86e",
    "title": "Efficient Training of Retrieval Models using Negative Cache",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2175f8c5cd9604f6b1e576b252d4c86e-Paper.pdf",
    "abstract": "Factorized models, such as two tower neural network models, are widely used for scoring (query, document) pairs in information retrieval tasks. These models are typically trained by optimizing the model parameters to score relevant positive\" pairs higher than the irrelevantnegative\" ones. While a large set of negatives typically improves the model performance, limited computation and memory budgets place constraints on the number of negatives used during training. In this paper, we develop a novel negative sampling technique for accelerating training with softmax cross-entropy loss. By using cached (possibly stale) item embeddings, our technique enables training with a large pool of negatives with reduced memory and computation. We also develop a streaming variant of our algorithm geared towards very large datasets. Furthermore, we establish a theoretical basis for our approach by showing that updating a very small fraction of the cache at each iteration can still ensure fast convergence. Finally, we experimentally validate our approach and show that it is efficient and compares favorably with more complex, state-of-the-art approaches.",
    "authors": [
      "Lindgren, Erik",
      "Reddi, Sashank",
      "Guo, Ruiqi",
      "Kumar, Sanjiv"
    ]
  },
  {
    "id": "217c0e01c1828e7279051f1b6675745d",
    "title": "Understanding Partial Multi-Label Learning via Mutual Information",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/217c0e01c1828e7279051f1b6675745d-Paper.pdf",
    "abstract": "To deal with ambiguities in partial multilabel learning (PML), state-of-the-art methods perform disambiguation by identifying ground-truth labels directly. However, there is an essential question:\u201cCan the ground-truth labels be identified precisely?\". If yes, \u201cHow can the ground-truth labels be found?\". This paper provides affirmative answers to these questions. Instead of adopting hand-made heuristic strategy, we propose a novel Mutual Information Label Identification for Partial Multilabel Learning (MILI-PML), which is derived from a clear probabilistic formulation and could be easily interpreted theoretically from the mutual information perspective, as well as naturally incorporates the feature/label relevancy considerations. Extensive experiments on synthetic and real-world datasets clearly demonstrate the superiorities of the proposed MILI-PML.",
    "authors": [
      "Gong, Xiuwen",
      "Yuan, Dong",
      "Bao, Wei"
    ]
  },
  {
    "id": "218344619d8fb95d504ccfa11804073f",
    "title": "Environment Generation for Zero-Shot Compositional Reinforcement Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/218344619d8fb95d504ccfa11804073f-Paper.pdf",
    "abstract": "Many real-world problems are compositional \u2013 solving them requires completing interdependent sub-tasks, either in series or in parallel, that can be represented as a dependency graph. Deep reinforcement learning (RL) agents often struggle to learn such complex tasks due to the long time horizons and sparse rewards. To address this problem, we present Compositional Design of Environments (CoDE), which trains a Generator agent to automatically build a series of compositional tasks tailored to the RL agent\u2019s current skill level. This automatic curriculum not only enables the agent to learn more complex tasks than it could have otherwise, but also selects tasks where the agent\u2019s performance is weak, enhancing its robustness and ability to generalize zero-shot to unseen tasks at test-time. We analyze why current environment generation techniques are insufficient for the problem of generating compositional tasks, and propose a new algorithm that addresses these issues. Our results assess learning and generalization across multiple compositional tasks, including the real-world problem of learning to navigate and interact with web pages. We learn to generate environments composed of multiple pages or rooms, and train RL agents capable of completing wide-range of complex tasks in those environments. We contribute two new benchmark frameworks for generating compositional tasks, compositional MiniGrid and gMiniWoB for web navigation. CoDE yields 4x higher success rate than the strongest baseline, and demonstrates strong performance of real websites learned on 3500 primitive tasks.",
    "authors": [
      "Gur, Izzeddin",
      "Jaques, Natasha",
      "Miao, Yingjie",
      "Choi, Jongwook",
      "Tiwari, Manoj ",
      "Lee, Honglak",
      "Faust, Aleksandra"
    ]
  },
  {
    "id": "219ece62fae865562d4510ea501cf349",
    "title": "Optimizing Conditional Value-At-Risk of Black-Box Functions",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/219ece62fae865562d4510ea501cf349-Paper.pdf",
    "abstract": "This paper presents two Bayesian optimization (BO) algorithms with theoretical performance guarantee to maximize the conditional value-at-risk (CVaR) of a black-box function: CV-UCB and CV-TS which are based on the well-established principle of optimism in the face of uncertainty and Thompson sampling, respectively. To achieve this, we develop an upper confidence bound of CVaR and prove the no-regret guarantee of CV-UCB by utilizing an interesting connection between CVaR and value-at-risk (VaR). For CV-TS, though it is straightforwardly performed with Thompson sampling, bounding its Bayesian regret is non-trivial because it requires a tail expectation bound for the distribution of CVaR of a black-box function, which has not been shown in the literature. The performances of both CV-UCB and CV-TS are empirically evaluated in optimizing CVaR of synthetic benchmark functions and simulated real-world optimization problems.",
    "authors": [
      "Nguyen, Quoc Phong",
      "Dai, Zhongxiang",
      "Low, Bryan Kian Hsiang",
      "Jaillet, Patrick"
    ]
  },
  {
    "id": "21b5680d80f75a616096f2e791affac6",
    "title": "E(n) Equivariant Normalizing Flows",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/21b5680d80f75a616096f2e791affac6-Paper.pdf",
    "abstract": "This paper introduces a generative model equivariant to Euclidean symmetries: E(n) Equivariant Normalizing Flows (E-NFs). To construct E-NFs, we take the discriminative E(n) graph neural networks and integrate them as a differential equation to obtain an invertible equivariant function: a continuous-time normalizing flow. We demonstrate that E-NFs considerably outperform baselines and existing methods from the literature on particle systems such as DW4 and LJ13, and on molecules from QM9 in terms of log-likelihood. To the best of our knowledge, this is the first flow that jointly generates molecule features and positions in 3D.",
    "authors": [
      "Garcia Satorras, Victor",
      "Hoogeboom, Emiel",
      "Fuchs, Fabian",
      "Posner, Ingmar",
      "Welling, Max"
    ]
  },
  {
    "id": "21be992eb8016e541a15953eee90760e",
    "title": "Revitalizing CNN Attention via Transformers in Self-Supervised Visual Representation Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/21be992eb8016e541a15953eee90760e-Paper.pdf",
    "abstract": "Studies on self-supervised visual representation learning (SSL) improve encoder backbones to discriminate training samples without labels. While CNN encoders via SSL achieve comparable recognition performance to those via supervised learning, their network attention is under-explored for further improvement. Motivated by the transformers that explore visual attention effectively in recognition scenarios, we propose a CNN Attention REvitalization (CARE) framework to train attentive CNN encoders guided by transformers in SSL. The proposed CARE framework consists of a CNN stream (C-stream) and a transformer stream (T-stream), where each stream contains two branches. C-stream follows an existing SSL framework with two CNN encoders, two projectors, and a predictor. T-stream contains two transformers, two projectors, and a predictor. T-stream connects to CNN encoders and is in parallel to the remaining C-Stream. During training, we perform SSL in both streams simultaneously and use the T-stream output to supervise C-stream. The features from CNN encoders are modulated in T-stream for visual attention enhancement and become suitable for the SSL scenario. We use these modulated features to supervise C-stream for learning attentive CNN encoders. To this end, we revitalize CNN attention by using transformers as guidance. Experiments on several standard visual recognition benchmarks, including image classification, object detection, and semantic segmentation, show that the proposed CARE framework improves CNN encoder backbones to the state-of-the-art performance. ",
    "authors": [
      "GE, Chongjian",
      "Liang, Youwei",
      "SONG, YIBING",
      "Jiao, Jianbo",
      "Wang, Jue",
      "Luo, Ping"
    ]
  },
  {
    "id": "21c5bba1dd6aed9ab48c2b34c1a0adde",
    "title": "A Critical Look at the Consistency of Causal Estimation with Deep Latent Variable Models",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/21c5bba1dd6aed9ab48c2b34c1a0adde-Paper.pdf",
    "abstract": "Using deep latent variable models in causal inference has attracted considerable interest recently, but an essential open question is their ability to yield consistent causal estimates. While they have demonstrated promising results and theory exists on some simple model formulations, we also know that causal effects are not even identifiable in general with latent variables. We investigate this gap between theory and empirical results with analytical considerations and extensive experiments under multiple synthetic and real-world data sets, using the causal effect variational autoencoder (CEVAE) as a case study. While CEVAE seems to work reliably under some simple scenarios, it does not estimate the causal effect correctly with a misspecified latent variable or a complex data distribution, as opposed to its original motivation. Hence, our results show that more attention should be paid to ensuring the correctness of causal estimates with deep latent variable models.",
    "authors": [
      "Rissanen, Severi",
      "Marttinen, Pekka"
    ]
  },
  {
    "id": "21ca6d0cf2f25c4dbb35d8dc0b679c3f",
    "title": "Improving Robustness using Generated Data",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/21ca6d0cf2f25c4dbb35d8dc0b679c3f-Paper.pdf",
    "abstract": "Recent work argues that robust training requires substantially larger datasets than those required for standard classification. On CIFAR-10 and CIFAR-100, this translates into a sizable robust-accuracy gap between models trained solely on data from the original training set and those trained with additional data extracted from the \"80 Million Tiny Images\" dataset (TI-80M). In this paper, we explore how generative models trained solely on the original training set can be leveraged to artificially increase the size of the original training set and improve adversarial robustness to $\\ell_p$ norm-bounded perturbations. We identify the sufficient conditions under which incorporating additional generated data can improve robustness, and demonstrate that it is possible to significantly reduce the robust-accuracy gap to models trained with additional real data. Surprisingly, we even show that even the addition of non-realistic random data (generated by Gaussian sampling) can improve robustness. We evaluate our approach on CIFAR-10, CIFAR-100, SVHN and TinyImageNet against $\\ell_\\infty$ and $\\ell_2$ norm-bounded perturbations of size $\\epsilon = 8/255$ and $\\epsilon = 128/255$, respectively. We show large absolute improvements in robust accuracy compared to previous state-of-the-art methods. Against $\\ell_\\infty$ norm-bounded perturbations of size $\\epsilon = 8/255$, our models achieve 66.10% and 33.49% robust accuracy on CIFAR-10 and CIFAR-100, respectively (improving upon the state-of-the-art by +8.96% and +3.29%). Against $\\ell_2$ norm-bounded perturbations of size $\\epsilon = 128/255$, our model achieves 78.31% on CIFAR-10 (+3.81%). These results beat most prior works that use external data.",
    "authors": [
      "Gowal, Sven",
      "Rebuffi, Sylvestre-Alvise",
      "Wiles, Olivia",
      "Stimberg, Florian",
      "Calian, Dan Andrei",
      "Mann, Timothy A"
    ]
  },
  {
    "id": "21ce689121e39821d07d04faab328370",
    "title": "An Analysis of Constant Step Size SGD in the Non-convex Regime: Asymptotic Normality and Bias",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/21ce689121e39821d07d04faab328370-Paper.pdf",
    "abstract": "Structured non-convex learning problems, for which critical points have favorable statistical properties, arise frequently in statistical machine learning. Algorithmic convergence and statistical estimation rates are well-understood for such problems. However, quantifying the uncertainty associated with the underlying training algorithm is not well-studied in the non-convex setting. In order to address this shortcoming, in this work, we establish an asymptotic normality result for the constant step size stochastic gradient descent (SGD)  algorithm---a widely used algorithm in practice. Specifically, based on the relationship between SGD and Markov Chains  [DDB19], we show that the average of SGD iterates is asymptotically normally distributed around the expected value of their unique invariant distribution, as long as the non-convex and non-smooth objective function satisfies a dissipativity property. We also characterize the bias between this expected value and the critical points of the objective function under various local regularity conditions. Together, the above two results could be leveraged to construct confidence intervals for non-convex problems that are trained using the SGD algorithm.",
    "authors": [
      "Yu, Lu",
      "Balasubramanian, Krishnakumar",
      "Volgushev, Stanislav",
      "Erdogdu, Murat A."
    ]
  },
  {
    "id": "21e4ef94f2a6b23597efabaec584b504",
    "title": "Learning to Learn Graph Topologies",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/21e4ef94f2a6b23597efabaec584b504-Paper.pdf",
    "abstract": "Learning a graph topology to reveal the underlying relationship between data entities plays an important role in various machine learning and data analysis tasks. Under the assumption that structured data vary smoothly over a graph, the problem can be formulated as a regularised convex optimisation over a positive semidefinite cone and solved by iterative algorithms. Classic methods require an explicit convex function to reflect generic topological priors, e.g. the $\\ell_1$ penalty for enforcing sparsity, which limits the flexibility and expressiveness in learning rich topological structures. We propose to learn a mapping from node data to the graph structure based on the idea of learning to optimise (L2O). Specifically, our model first unrolls an iterative primal-dual splitting algorithm into a neural network. The key structural proximal projection is replaced with a variational autoencoder that refines the estimated graph with enhanced topological properties. The model is trained in an end-to-end fashion with pairs of node data and graph samples. Experiments on both synthetic and real-world data demonstrate that our model is more efficient than classic iterative algorithms in learning a graph with specific topological properties. ",
    "authors": [
      "Pu, Xingyue",
      "Cao, Tianyue",
      "Zhang, Xiaoyun",
      "Dong, Xiaowen",
      "Chen, Siheng"
    ]
  },
  {
    "id": "22456f4b545572855c766df5eefc9832",
    "title": "Invertible Tabular GANs: Killing Two Birds with One Stone for Tabular Data Synthesis",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/22456f4b545572855c766df5eefc9832-Paper.pdf",
    "abstract": "Tabular data synthesis has received wide attention in the literature. This is because available data is often limited, incomplete, or cannot be obtained easily, and data privacy is becoming increasingly important. In this work, we present a generalized GAN framework for tabular synthesis, which combines the adversarial training of GANs and the negative log-density regularization of invertible neural networks. The proposed framework can be used for two distinctive objectives. First,  we can further improve the synthesis quality, by decreasing the negative log-density of real records in the process of adversarial training. On the other hand, by increasing the negative log-density of real records, realistic fake records can be synthesized in a way that they are not too much close to real records and reduce the chance of potential information leakage. We conduct experiments with real-world datasets for classification, regression, and privacy attacks. In general, the proposed method demonstrates the best synthesis quality (in terms of task-oriented evaluation metrics, e.g., F1) when decreasing the negative log-density during the adversarial training. If increasing the negative log-density, our experimental results show that the distance between real and fake records increases, enhancing robustness against privacy attacks.",
    "authors": [
      "LEE, JAEHOON",
      "Hyeong, Jihyeon",
      "Jeon, Jinsung",
      "Park, Noseong",
      "Cho, Jihoon"
    ]
  },
  {
    "id": "224e5e49814ca908e58c02e28a0462c1",
    "title": "Reducing Collision Checking for Sampling-Based Motion Planning Using Graph Neural Networks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/224e5e49814ca908e58c02e28a0462c1-Paper.pdf",
    "abstract": "Sampling-based motion planning is a popular approach in robotics for finding paths in continuous configuration spaces. Checking collision with obstacles is the major computational bottleneck in this process. We propose new learning-based methods for reducing collision checking to accelerate motion planning by training graph neural networks (GNNs) that perform path exploration and path smoothing. Given random geometric graphs (RGGs) generated from batch sampling, the path exploration component iteratively predicts collision-free edges to prioritize their exploration. The path smoothing component then optimizes paths obtained from the exploration stage. The methods benefit from the ability of GNNs of capturing geometric patterns from RGGs through batch sampling and generalize better to unseen environments. Experimental results show that the learned components can significantly reduce collision checking and improve overall planning efficiency in challenging high-dimensional motion planning tasks.",
    "authors": [
      "Yu, Chenning",
      "Gao, Sicun"
    ]
  },
  {
    "id": "22508552d3fc22f867e33e6c56b30b16",
    "title": "Sample Complexity Bounds for Active Ranking from Multi-wise Comparisons",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/22508552d3fc22f867e33e6c56b30b16-Paper.pdf",
    "abstract": "We study the sample complexity (i.e., the number of comparisons needed) bounds for actively ranking a set of $n$ items from multi-wise comparisons. Here, a multi-wise comparison takes $m$ items as input and returns a (noisy) result about the best item (the winner feedback) or the order of these items (the full-ranking feedback). We consider two basic ranking problems: top-$k$ items selection and full ranking. Unlike previous works that study ranking from multi-wise comparisons, in this paper, we do not require any parametric model or assumption and work on the fundamental setting where each comparison returns the correct result with probability $1$ or a certain probability larger than $\\frac{1}{2}$. This paper helps understand whether and to what degree utilizing multi-wise comparisons can reduce the sample complexity for the ranking problems compared to ranking from pairwise comparisons. Specifically, under the winner feedback setting, one can reduce the sample complexity for top-$k$ selection up to an $m$ factor and that for full ranking up to a $\\log{m}$ factor. Under the full-ranking feedback setting, one can reduce the sample complexity for top-$k$ selection up to an $m$ factor and that for full ranking up to an $m\\log{m}$ factor. We also conduct numerical simulations to confirm our theoretical results. ",
    "authors": [
      "Ren, Wenbo",
      "Liu, Jia",
      "Shroff, Ness"
    ]
  },
  {
    "id": "22722a343513ed45f14905eb07621686",
    "title": "Efficient Bayesian network structure learning via local Markov boundary search",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/22722a343513ed45f14905eb07621686-Paper.pdf",
    "abstract": "We analyze the complexity of learning directed acyclic graphical models from observational data in general settings without specific distributional assumptions. Our approach is information-theoretic and uses a local Markov boundary search procedure in order to recursively construct ancestral sets in the underlying graphical model. Perhaps surprisingly, we show that for certain graph ensembles, a simple forward greedy search algorithm (i.e. without a backward pruning phase) suffices to learn the Markov boundary of each node. This substantially improves the sample complexity, which we show is at most polynomial in the number of nodes. This is then applied to learn the entire graph under a novel identifiability condition that generalizes existing conditions from the literature. As a matter of independent interest, we establish finite-sample guarantees for the problem of recovering Markov boundaries from data. Moreover, we apply our results to the special case of polytrees, for which the assumptions simplify, and provide explicit conditions under which polytrees are identifiable and learnable in polynomial time. We further illustrate the performance of the algorithm, which is easy to implement, in a simulation study. Our approach is general, works for discrete or continuous distributions without distributional assumptions, and as such sheds light on the minimal assumptions required to efficiently learn the structure of directed graphical models from data.",
    "authors": [
      "Gao, Ming",
      "Aragam, Bryon"
    ]
  },
  {
    "id": "22785dd2577be2ce28ef79febe80db10",
    "title": "Learning Dynamic Graph Representation of Brain Connectome with Spatio-Temporal Attention",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/22785dd2577be2ce28ef79febe80db10-Paper.pdf",
    "abstract": "Functional connectivity (FC) between regions of the brain can be assessed by the degree of temporal correlation measured with functional neuroimaging modalities. Based on the fact that these connectivities build a network, graph-based approaches for analyzing the brain connectome have provided insights into the functions of the human brain. The development of graph neural networks (GNNs) capable of learning representation from graph structured data has led to increased interest in learning the graph representation of the brain connectome. Although recent attempts to apply GNN to the FC network have shown promising results, there is still a common limitation that they usually do not incorporate the dynamic characteristics of the FC network which fluctuates over time. In addition, a few studies that have attempted to use dynamic FC as an input for the GNN reported a reduction in performance compared to static FC methods, and did not provide temporal explainability. Here, we propose STAGIN, a method for learning dynamic graph representation of the brain connectome with spatio-temporal attention. Specifically, a temporal sequence of brain graphs is input to the STAGIN to obtain the dynamic graph representation, while novel READOUT functions and the Transformer encoder provide spatial and temporal explainability with attention, respectively. Experiments on the HCP-Rest and the HCP-Task datasets demonstrate exceptional performance of our proposed method. Analysis of the spatio-temporal attention also provide concurrent interpretation with the neuroscientific knowledge, which further validates our method. Code is available at https://github.com/egyptdj/stagin",
    "authors": [
      "Kim, Byung-Hoon",
      "Ye, Jong Chul",
      "Kim, Jae-Jin"
    ]
  },
  {
    "id": "2287c6b8641dd2d21ab050eb9ff795f3",
    "title": "Understanding the Generalization Benefit of Model Invariance from a Data Perspective",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2287c6b8641dd2d21ab050eb9ff795f3-Paper.pdf",
    "abstract": "Machine learning models that are developed to be invariant under certain types of data transformations have shown improved generalization in practice. However, a principled understanding of why invariance benefits generalization is limited. Given a dataset, there is often no principled way to select \"suitable\" data transformations under which model invariance guarantees better generalization. This paper studies the generalization benefit of model invariance by introducing the sample cover induced by transformations, i.e., a representative subset of a dataset that can approximately recover the whole dataset using transformations. For any data transformations, we provide refined generalization bounds for invariant models based on the sample cover. We also characterize the \"suitability\" of a set of data transformations by the sample covering number induced by transformations, i.e., the smallest size of its induced sample covers. We show that we may tighten the generalization bounds for \"suitable\" transformations that have a small sample covering number. In addition, our proposed sample covering number can be empirically evaluated and thus provides a guidance for selecting transformations to develop model invariance for better generalization. In experiments on multiple datasets, we evaluate sample covering numbers for some commonly used transformations and show that the smaller sample covering number for a set of transformations (e.g., the 3D-view transformation) indicates a smaller gap between the test and training error for invariant models, which verifies our propositions.",
    "authors": [
      "Zhu, Sicheng",
      "An, Bang",
      "Huang, Furong"
    ]
  },
  {
    "id": "228bbc2f87caeb21bb7f6949fddcb91d",
    "title": "Improved Variance-Aware Confidence Sets for Linear Bandits and Linear Mixture MDP",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/228bbc2f87caeb21bb7f6949fddcb91d-Paper.pdf",
    "abstract": "This paper presents new \\emph{variance-aware} confidence sets for linear bandits and linear mixture Markov Decision Processes (MDPs).With the new confidence sets, we obtain the follow regret bounds:For linear bandits, we obtain an $\\widetilde{O}(\\mathrm{poly}(d)\\sqrt{1 + \\sum_{k=1}^{K}\\sigma_k^2})$ data-dependent regret bound, where $d$ is the feature dimension, $K$ is the number of rounds, and $\\sigma_k^2$ is the \\emph{unknown} variance of the reward at the $k$-th round. This is the first regret bound that only scales with the variance and the dimension but \\emph{no explicit polynomial dependency on $K$}.When variances are small, this bound can be significantly smaller than the $\\widetilde{\\Theta}\\left(d\\sqrt{K}\\right)$ worst-case regret bound.For linear mixture MDPs, we obtain an $\\widetilde{O}(\\mathrm{poly}(d, \\log H)\\sqrt{K})$ regret bound, where $d$ is the number of base models, $K$ is the number of episodes, and $H$ is the planning horizon. This is the first regret bound that only scales \\emph{logarithmically} with $H$ in the reinforcement learning with linear function approximation setting, thus \\emph{exponentially improving} existing results, and resolving an open problem in \\citep{zhou2020nearly}.We develop three technical ideas that may be of independent interest:1) applications of the peeling technique to both the input norm and the variance magnitude, 2) a recursion-based estimator for the variance, and 3) a new convex potential lemma that generalizes the seminal elliptical potential lemma.",
    "authors": [
      "Zhang, Zihan",
      "Yang, Jiaqi",
      "Ji, Xiangyang",
      "Du, Simon S."
    ]
  },
  {
    "id": "22b1f2e0983160db6f7bb9f62f4dbb39",
    "title": "How Should Pre-Trained Language Models Be Fine-Tuned Towards Adversarial Robustness?",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/22b1f2e0983160db6f7bb9f62f4dbb39-Paper.pdf",
    "abstract": "The fine-tuning of pre-trained language models has a great success in many NLP fields. Yet, it is strikingly vulnerable to adversarial examples, e.g., word substitution attacks using only synonyms can easily fool a BERT-based sentiment analysis model. In this paper, we demonstrate that adversarial training, the prevalent defense technique, does not directly fit a conventional fine-tuning scenario, because it suffers severely from catastrophic forgetting: failing to retain the generic and robust linguistic features that have already been captured by the pre-trained model. In this light, we propose Robust Informative Fine-Tuning (RIFT), a novel adversarial fine-tuning method from an information-theoretical perspective. In particular, RIFT encourages an objective model to retain the features learned from the pre-trained model throughout the entire fine-tuning process, whereas a conventional one only uses the pre-trained weights for initialization. Experimental results show that RIFT consistently outperforms the state-of-the-arts on two popular NLP tasks: sentiment analysis and natural language inference, under different attacks across various pre-trained language models.",
    "authors": [
      "Dong, Xinshuai",
      "Luu, Anh Tuan",
      "Lin, Min",
      "Yan, Shuicheng",
      "Zhang, Hanwang"
    ]
  },
  {
    "id": "22fb0cee7e1f3bde58293de743871417",
    "title": "Recursive Bayesian Networks: Generalising and Unifying Probabilistic Context-Free Grammars and Dynamic Bayesian Networks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/22fb0cee7e1f3bde58293de743871417-Paper.pdf",
    "abstract": "Probabilistic context-free grammars (PCFGs) and dynamic Bayesian networks (DBNs) are widely used sequence models with complementary strengths and limitations. While PCFGs allow for nested hierarchical dependencies (tree structures), their latent variables (non-terminal symbols) have to be discrete. In contrast, DBNs allow for continuous latent variables, but the dependencies are strictly sequential (chain structure). Therefore, neither can be applied if the latent variables are assumed to be continuous and also to have a nested hierarchical dependency structure. In this paper, we present Recursive Bayesian Networks (RBNs), which generalise and unify PCFGs and DBNs, combining their strengths and containing both as special cases. RBNs define a joint distribution over tree-structured Bayesian networks with discrete or continuous latent variables. The main challenge lies in performing joint inference over the exponential number of possible structures and the continuous variables. We provide two solutions: 1) For arbitrary RBNs, we generalise inside and outside probabilities from PCFGs to the mixed discrete-continuous case, which allows for maximum posterior estimates of the continuous latent variables via gradient descent, while marginalising over network structures. 2) For Gaussian RBNs, we additionally derive an analytic approximation of the marginal data likelihood (evidence) and marginal posterior distribution, allowing for robust parameter optimisation and Bayesian inference. The capacity and diverse applications of RBNs are illustrated on two examples: In a quantitative evaluation on synthetic data, we demonstrate and discuss the advantage of RBNs for segmentation and tree induction from noisy sequences, compared to change point detection and hierarchical clustering. In an application to musical data, we approach the unsolved problem of hierarchical music analysis from the raw note level and compare our results to expert annotations.",
    "authors": [
      "Lieck, Robert",
      "Rohrmeier, Martin"
    ]
  },
  {
    "id": "231141b34c82aa95e48810a9d1b33a79",
    "title": "EF21: A New, Simpler, Theoretically Better, and Practically Faster Error Feedback",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/231141b34c82aa95e48810a9d1b33a79-Paper.pdf",
    "abstract": "Error feedback (EF), also known as error compensation, is an immensely popular convergence stabilization mechanism in the context of distributed training of supervised machine learning models enhanced by the use of contractive communication compression mechanisms, such as Top-$k$. First proposed by Seide et al [2014] as a heuristic, EF resisted any theoretical understanding until recently [Stich et al., 2018, Alistarh et al., 2018]. While these early breakthroughs were followed by a steady stream of works offering various improvements and generalizations, the current theoretical understanding of EF is still very limited. Indeed, to the best of our knowledge, all existing analyses either i) apply to the single node setting only, ii) rely on very strong and often unreasonable assumptions, such as global boundedness of the gradients, or iterate-dependent assumptions that cannot be checked a-priori and may not hold in practice, or iii) circumvent these issues via the introduction of additional unbiased compressors, which increase the communication cost. In this work we fix all these deficiencies by proposing and analyzing a new EF mechanism, which we call EF21, which consistently and substantially outperforms EF in practice. Moreover, our theoretical analysis relies on standard assumptions only, works in the distributed heterogeneous data setting, and leads to better and more meaningful rates. In particular, we prove that EF21 enjoys a fast $\\mathcal{O}(1/T)$  convergence rate for smooth nonconvex problems, beating the previous bound of $\\mathcal{O}(1/T^{2/3})$, which was shown under a strong bounded gradients assumption. We further improve this to a fast linear rate for Polyak-Lojasiewicz functions, which is the first linear convergence result for an error feedback method not relying on unbiased compressors. Since EF has a large number of applications where it reigns supreme, we believe that our 2021 variant, EF21, will have a large impact on the practice of communication efficient distributed learning.",
    "authors": [
      "Richtarik, Peter",
      "Sokolov, Igor",
      "Fatkhullin, Ilyas"
    ]
  },
  {
    "id": "233f1dd0f3f537bcb7a338ea74d63483",
    "title": "Mixture weights optimisation for Alpha-Divergence Variational Inference",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/233f1dd0f3f537bcb7a338ea74d63483-Paper.pdf",
    "abstract": "This paper focuses on $\\alpha$-divergence minimisation methods for Variational Inference. More precisely, we are interested in algorithms optimising the mixture weights of any given mixture model, without any information on the underlying distribution of its mixture components parameters. The Power Descent, defined for all $\\alpha \\neq 1$, is one such algorithm and we establish in our work the full proof of its convergence towards the optimal mixture weights when $\\alpha <1$. Since the $\\alpha$-divergence recovers the widely-used forward Kullback-Leibler when $\\alpha \\to 1$, we then extend the Power Descent to the case $\\alpha = 1$ and show that we obtain an Entropic Mirror Descent. This leads us to investigate the link between Power Descent and Entropic Mirror Descent: first-order approximations allow us to introduce the R\\'{e}nyi Descent, a novel algorithm for which we prove an $O(1/N)$ convergence rate. Lastly, we compare numerically the behavior of the unbiased Power Descent and of the biased R\\'{e}nyi Descent and we discuss the potential advantages of one algorithm over the other.",
    "authors": [
      "Daudel, Kam\u00e9lia",
      "douc, randal"
    ]
  },
  {
    "id": "23451391cd1399019fa0421129066bc6",
    "title": "Instance-dependent Label-noise Learning under a Structural Causal Model",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/23451391cd1399019fa0421129066bc6-Paper.pdf",
    "abstract": "Label noise generally degenerates the performance of deep learning algorithms because deep neural networks easily overfit label errors. Let $X$ and $Y$ denote the instance and clean label, respectively. When $Y$ is a cause of $X$, according to which many datasets have been constructed, e.g., \\textit{SVHN} and \\textit{CIFAR}, the distributions of $P(X)$ and $P(Y|X)$ are generally entangled. This means that the unsupervised instances are helpful to learn the classifier and thus reduce the side effect of label noise. However, it remains elusive on how to exploit the causal information to handle the label-noise problem. We propose to model and make use of the causal process in order to correct the label-noise effect.Empirically, the proposed method outperforms all state-of-the-art methods on both synthetic and real-world label-noise datasets.",
    "authors": [
      "Yao, Yu",
      "Liu, Tongliang",
      "Gong, Mingming",
      "Han, Bo",
      "Niu, Gang",
      "Zhang, Kun"
    ]
  },
  {
    "id": "234b941e88b755b7a72a1c1dd5022f30",
    "title": "Combining Human Predictions with Model Probabilities via Confusion Matrices and Calibration",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/234b941e88b755b7a72a1c1dd5022f30-Paper.pdf",
    "abstract": "An increasingly common use case for machine learning models is augmenting the abilities of human decision makers. For classification tasks where neither the human nor model are perfectly accurate, a key step in obtaining high performance is combining their individual predictions in a manner that leverages their relative strengths. In this work, we develop a set of algorithms that combine the probabilistic output of a model with the class-level output of a human. We show theoretically that the accuracy of our combination model is driven not only by the individual human and model accuracies, but also by the model's confidence.  Empirical results on image classification with CIFAR-10 and a subset of ImageNet demonstrate that such human-model combinations consistently have higher accuracies than the model or human alone, and that the parameters of the combination method can be estimated effectively with as few as ten labeled datapoints.",
    "authors": [
      "Kerrigan, Gavin",
      "Smyth, Padhraic",
      "Steyvers, Mark"
    ]
  },
  {
    "id": "2387337ba1e0b0249ba90f55b2ba2521",
    "title": "$\\texttt{LeadCache}$: Regret-Optimal Caching in Networks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2387337ba1e0b0249ba90f55b2ba2521-Paper.pdf",
    "abstract": "We consider an online prediction problem in the context of network caching. Assume that multiple users are connected to several caches via a bipartite network. At any time slot, each user may request an arbitrary file chosen from a large catalog. A user's request at a slot is met if the requested file is cached in at least one of the caches connected to the user. Our objective is to predict, prefetch, and optimally distribute the files on the caches at each slot to maximize the total number of cache hits. The problem is non-trivial due to the non-convex and non-smooth nature of the objective function. In this paper, we propose $\\texttt{LeadCache}$ - an efficient online caching policy based on the Follow-the-Perturbed-Leader paradigm. We show that $\\texttt{LeadCache}$ is regret-optimal up to a factor of $\\tilde{O}(n^{3/8}),$ where $n$ is the number of users. We design two efficient implementations of the $\\texttt{LeadCache}$ policy, one based on Pipage rounding and the other based on Madow's sampling, each of which makes precisely one call to an LP-solver per iteration. Furthermore, with a Strong-Law-type assumption, we show that the total number of file fetches under $\\texttt{LeadCache}$ remains almost surely finite over an infinite horizon. Finally, we derive an approximately tight regret lower bound using results from graph coloring. We conclude that the learning-based $\\texttt{LeadCache}$ policy decisively outperforms the state-of-the-art caching policies both theoretically and empirically.",
    "authors": [
      "Paria, Debjit",
      "Sinha, Abhishek"
    ]
  },
  {
    "id": "23937b42f9273974570fb5a56a6652ee",
    "title": "Probabilistic Attention for Interactive Segmentation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/23937b42f9273974570fb5a56a6652ee-Paper.pdf",
    "abstract": "We provide a probabilistic interpretation of attention and show that the standard dot-product attention in transformers is a special case of Maximum A Posteriori (MAP) inference. The proposed approach suggests the use of Expectation Maximization algorithms for on-line adaptation of key and value model parameters. This approach is  useful for cases in which external agents, e.g., annotators, provide inference-time information about the correct values of some tokens, e.g., the semantic category of some pixels,  and we need for this new information to propagate to other tokens in a principled manner. We illustrate the approach on an interactive semantic segmentation task in which annotators and models collaborate online to improve annotation efficiency. Using standard benchmarks, we observe that key adaptation boosts model performance ($\\sim10\\%$ mIoU) in the low feedback regime and value propagation improves model responsiveness in the high feedback regime. A PyTorch layer implementation of our probabilistic attention model is available here: https://github.com/apple/ml-probabilistic-attention.",
    "authors": [
      "Gabbur, Prasad",
      "Bilkhu, Manjot",
      "Movellan, Javier"
    ]
  },
  {
    "id": "239f914f30ea3c948fce2ea07a9efb33",
    "title": "Influence Patterns for Explaining Information Flow in BERT",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/239f914f30ea3c948fce2ea07a9efb33-Paper.pdf",
    "abstract": "While attention is all you need may be proving true, we do not know why: attention-based transformer models such as BERT are superior but how information flows from input tokens to output predictions are unclear.  We introduce influence patterns,  abstractions of sets of paths  through a transformer model. Patterns quantify and localize the flow of  information to paths passing through a sequence of model nodes. Experimentally, we find that significant portion of information flow in BERT goes through skip connections instead of attention heads. We further show that consistency of patterns across instances is an indicator of BERT\u2019s performance. Finally, we demonstrate that patterns account for far more model performance than previous attention-based and layer-based methods.",
    "authors": [
      "Lu, Kaiji",
      "Wang, Zifan",
      "Mardziel, Piotr",
      "Datta, Anupam"
    ]
  },
  {
    "id": "23b023b22d0bf47626029d5961328028",
    "title": "Robust Regression Revisited: Acceleration and Improved Estimation Rates",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/23b023b22d0bf47626029d5961328028-Paper.pdf",
    "abstract": "We study fast algorithms for statistical regression problems under the strong contamination model, where the goal is to approximately optimize a generalized linear model (GLM) given adversarially corrupted samples. Prior works in this line of research were based on the \\emph{robust gradient descent} framework of \\cite{PrasadSBR20}, a first-order method using biased gradient queries, or the \\emph{Sever} framework of \\cite{DiakonikolasKK019}, an iterative outlier-removal method calling a stationary point finder. We present nearly-linear time algorithms for robust regression problems with improved runtime or estimation guarantees compared to the state-of-the-art. For the general case of smooth GLMs (e.g.\\ logistic regression), we show that the robust gradient descent framework of \\cite{PrasadSBR20} can be \\emph{accelerated}, and show our algorithm extends to optimizing the Moreau envelopes of Lipschitz GLMs (e.g.\\ support vector machines), answering several open questions in the literature. For the well-studied case of robust linear regression, we present an alternative approach obtaining improved estimation rates over prior nearly-linear time algorithms. Interestingly, our algorithm starts with an identifiability proof introduced in the context of the sum-of-squares algorithm of \\cite{BakshiP21}, which achieved optimal error rates while requiring large polynomial runtime and sample complexity. We reinterpret their proof within the Sever framework and obtain a dramatically faster and more sample-efficient algorithm under fewer distributional assumptions.",
    "authors": [
      "Jambulapati, Arun",
      "Li, Jerry",
      "Schramm, Tselil",
      "Tian, Kevin"
    ]
  },
  {
    "id": "23c894276a2c5a16470e6a31f4618d73",
    "title": "Automatic Unsupervised Outlier Model Selection",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/23c894276a2c5a16470e6a31f4618d73-Paper.pdf",
    "abstract": "Given an unsupervised outlier detection task on a new dataset, how can we automatically select a good outlier detection algorithm and its hyperparameter(s) (collectively called a model)? In this work, we tackle the unsupervised outlier model selection (UOMS) problem, and propose MetaOD, a principled, data-driven approach to UOMS based on meta-learning. The UOMS problem is notoriously challenging, as compared to model selection for classification and clustering, since (i) model evaluation is infeasible due to the lack of hold-out data with labels, and (ii) model comparison is infeasible due to the lack of a universal objective function. MetaOD capitalizes on the performances of a large body of detection models on historical outlier detection benchmark datasets, and carries over this prior experience to automatically select an effective model to be employed on a new dataset without any labels, model evaluations or model comparisons. To capture task similarity within our meta-learning framework, we introduce specialized meta-features that quantify outlying characteristics of a dataset. Extensive experiments show that selecting a model by MetaOD significantly outperforms no model selection (e.g. always using the same popular model or the ensemble of many) as well as other meta-learning techniques that we tailored for UOMS. Moreover upon (meta-)training, MetaOD is extremely efficient at test time; selecting from a large pool of 300+ models takes less than 1 second for a new task. We open-source MetaOD and our meta-learning database for practical use and to foster further research on the UOMS problem. ",
    "authors": [
      "Zhao, Yue",
      "Rossi, Ryan",
      "Akoglu, Leman"
    ]
  },
  {
    "id": "23e582ad8087f2c03a5a31c125123f9a",
    "title": "Pruning Randomly Initialized Neural Networks with Iterative Randomization",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/23e582ad8087f2c03a5a31c125123f9a-Paper.pdf",
    "abstract": "Pruning the weights of randomly initialized neural networks plays an important role in the context of lottery ticket hypothesis. Ramanujan et al. (2020) empirically showed that only pruning the weights can achieve remarkable performance instead of optimizing the weight values. However, to achieve the same level of performance as the weight optimization, the pruning approach requires more parameters in the networks before pruning and thus more memory space. To overcome this parameter inefficiency, we introduce a novel framework to prune randomly initialized neural networks with iteratively randomizing weight values (IteRand). Theoretically, we prove an approximation theorem in our framework, which indicates that the randomizing operations are provably effective to reduce the required number of the parameters. We also empirically demonstrate the parameter efficiency in multiple experiments on CIFAR-10 and ImageNet.",
    "authors": [
      "Chijiwa, Daiki",
      "Yamaguchi, Shin'ya",
      "Ida, Yasutoshi",
      "Umakoshi, Kenji",
      "INOUE, Tomohiro"
    ]
  },
  {
    "id": "23fa71cc32babb7b91130824466d25a5",
    "title": "Probing Inter-modality: Visual Parsing with Self-Attention for Vision-and-Language Pre-training",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/23fa71cc32babb7b91130824466d25a5-Paper.pdf",
    "abstract": "Vision-Language Pre-training (VLP) aims to learn multi-modal representations from image-text pairs and serves for downstream vision-language tasks in a fine-tuning fashion. The dominant VLP models adopt a CNN-Transformer architecture, which embeds images with a CNN, and then aligns images and text with a Transformer.  Visual relationship between visual contents plays an important role in image understanding and is the basic for inter-modal alignment learning. However, CNNs have limitations in visual relation learning due to local receptive field's weakness in modeling long-range dependencies. Thus the two objectives of learning visual relation and inter-modal alignment are encapsulated in the same Transformer network. Such design might restrict the inter-modal alignment learning in the Transformer by ignoring the specialized characteristic of each objective. To tackle this, we propose a fully Transformer visual embedding for VLP to better learn visual relation and further promote inter-modal alignment. Specifically, we propose a metric named Inter-Modality Flow (IMF) to measure the interaction between vision and language modalities (i.e., inter-modality). We also design a novel masking optimization mechanism named Masked Feature Regression (MFR) in Transformer to further promote the inter-modality learning. To the best of our knowledge, this is the first study to explore the benefit of Transformer for visual feature learning in VLP.  We verify our method on a wide range of vision-language tasks, including Visual Question Answering (VQA), Visual Entailment and Visual Reasoning. Our approach not only outperforms the state-of-the-art VLP performance, but also shows benefits on the IMF metric.",
    "authors": [
      "Xue, Hongwei",
      "Huang, Yupan",
      "Liu, Bei",
      "Peng, Houwen",
      "Fu, Jianlong",
      "Li, Houqiang",
      "Luo, Jiebo"
    ]
  },
  {
    "id": "2406a0a94c80406914ff2f6c9fdd67d5",
    "title": "Stability and Generalization of Bilevel Programming in Hyperparameter Optimization",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2406a0a94c80406914ff2f6c9fdd67d5-Paper.pdf",
    "abstract": "The (gradient-based) bilevel programming framework is widely used in hyperparameter optimization and has achieved excellent performance empirically. Previous theoretical work mainly focuses on its optimization properties, while leaving the analysis on generalization largely open. This paper attempts to address the issue by presenting an expectation bound w.r.t. the validation set based on uniform stability. Our results can explain some mysterious behaviours of the bilevel programming in practice, for instance, overfitting to the validation set. We also present an expectation bound for the classical cross-validation algorithm. Our results suggest that gradient-based algorithms can be better than cross-validation under certain conditions in a theoretical perspective. Furthermore, we prove that regularization terms in both the outer and inner levels can relieve the overfitting problem in  gradient-based algorithms. In experiments on feature learning and data reweighting for noisy labels, we corroborate our theoretical findings.",
    "authors": [
      "Bao, Fan",
      "Wu, Guoqiang",
      "LI, Chongxuan",
      "Zhu, Jun",
      "Zhang, Bo"
    ]
  },
  {
    "id": "240ac9371ec2671ae99847c3ae2e6384",
    "title": "Regime Switching Bandits",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/240ac9371ec2671ae99847c3ae2e6384-Paper.pdf",
    "abstract": "We study a multi-armed bandit problem where the rewards exhibit regime switching. Specifically, the distributions of the random rewards generated from all arms are modulated by a common underlying state modeled as a finite-state Markov chain. The agent does not observe the underlying state and has to learn the transition matrix and the reward distributions. We propose a learning algorithm for this problem, building on spectral method-of-moments estimations for hidden Markov models, belief error control in partially observable Markov decision processes and upper-confidence-bound methods for online learning. We also establish an upper bound $O(T^{2/3}\\sqrt{\\log T})$ for the proposed learning algorithm where $T$ is the learning horizon. Finally, we conduct proof-of-concept experiments to illustrate the performance of the learning algorithm.",
    "authors": [
      "Zhou, Xiang",
      "Xiong, Yi",
      "Chen, Ningyuan",
      "GAO, Xuefeng"
    ]
  },
  {
    "id": "240c945bb72980130446fc2b40fbb8e0",
    "title": "MixACM: Mixup-Based Robustness Transfer via Distillation of Activated Channel Maps",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/240c945bb72980130446fc2b40fbb8e0-Paper.pdf",
    "abstract": "Deep neural networks are susceptible to adversarially crafted, small, and imperceptible changes in the natural inputs. The most effective defense mechanism against these examples is adversarial training which constructs adversarial examples during training by iterative maximization of loss. The model is then trained to minimize the loss on these constructed examples. This min-max optimization requires more data, larger capacity models, and additional computing resources. It also degrades the standard generalization performance of a model. Can we achieve robustness more efficiently? In this work, we explore this question from the perspective of knowledge transfer. First, we theoretically show the transferability of robustness from an adversarially trained teacher model to a student model with the help of mixup augmentation. Second, we propose a novel robustness transfer method called Mixup-Based Activated Channel Maps (MixACM) Transfer. MixACM transfers robustness from a robust teacher to a student by matching activated channel maps generated without expensive adversarial perturbations. Finally, extensive experiments on multiple datasets and different learning scenarios show our method can transfer robustness while also improving generalization on natural images. ",
    "authors": [
      "Muhammad, Awais",
      "Zhou, Fengwei",
      "Xie, Chuanlong",
      "Li, Jiawei",
      "Bae, Sung-Ho",
      "Li, Zhenguo"
    ]
  },
  {
    "id": "2417dc8af8570f274e6775d4d60496da",
    "title": "Localization, Convexity, and Star Aggregation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2417dc8af8570f274e6775d4d60496da-Paper.pdf",
    "abstract": "Offset Rademacher complexities have been shown to provide tight upper bounds for the square loss in a broad class of problems including improper statistical learning and online learning. We show that the offset complexity can be generalized to any loss that satisfies a certain general convexity condition. Further, we show that this condition is closely related to both exponential concavity and self-concordance, unifying apparently disparate results. By a novel geometric argument, many of our bounds translate to improper learning in a non-convex class with Audibert's star algorithm. Thus, the offset complexity provides a versatile analytic tool that covers both convex empirical risk minimization and improper learning under entropy conditions. Applying the method, we recover the optimal rates for proper and improper learning with the $p$-loss for $1 < p < \\infty$, and show that improper variants of empirical risk minimization can attain fast rates for logistic regression and other generalized linear models.",
    "authors": [
      "Vijaykumar, Suhas"
    ]
  },
  {
    "id": "242c100dc94f871b6d7215b868a875f8",
    "title": "Aligning Silhouette Topology for Self-Adaptive 3D Human Pose Recovery",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/242c100dc94f871b6d7215b868a875f8-Paper.pdf",
    "abstract": "Articulation-centric 2D/3D pose supervision forms the core training objective in most existing 3D human pose estimation techniques. Except for synthetic source environments, acquiring such rich supervision for each real target domain at deployment is highly inconvenient. However, we realize that standard foreground silhouette estimation techniques (on static camera feeds) remain unaffected by domain-shifts. Motivated by this, we propose a novel target adaptation framework that relies only on silhouette supervision to adapt a source-trained model-based regressor. However, in the absence of any auxiliary cue (multi-view, depth, or 2D pose), an isolated silhouette loss fails to provide a reliable pose-specific gradient and requires to be employed in tandem with a topology-centric loss. To this end, we develop a series of convolution-friendly spatial transformations in order to disentangle a topological-skeleton representation from the raw silhouette. Such a design paves the way to devise a Chamfer-inspired spatial topological-alignment loss via distance field computation, while effectively avoiding any gradient hindering spatial-to-pointset mapping. Experimental results demonstrate our superiority against prior-arts in self-adapting a source trained model to diverse unlabeled target domains, such as a) in-the-wild datasets, b) low-resolution image domains, and c) adversarially perturbed image domains (via UAP). ",
    "authors": [
      "Mugaludi, Ramesha Rakesh",
      "Kundu, Jogendra Nath",
      "Jampani, Varun",
      "R, Venkatesh Babu"
    ]
  },
  {
    "id": "243facb29564e7b448834a7c9d901201",
    "title": "Self-Adaptable Point Processes with Nonparametric Time Decays",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/243facb29564e7b448834a7c9d901201-Paper.pdf",
    "abstract": "Many applications involve multi-type event data. Understanding the complex influences of the events on each other is critical to discover useful knowledge and to predict future events and their types. Existing methods either ignore or partially account for these influences. Recent works use recurrent neural networks to model the event rate. While being highly expressive, they couple all the temporal dependencies in a black-box and can hardly extract meaningful knowledge. More important, most methods assume an exponential time decay of the influence strength, which is over-simplified and can miss many important strength varying patterns.  To overcome these limitations, we propose SPRITE, a $\\underline{S}$elf-adaptable  $\\underline{P}$oint p$\\underline{R}$ocess w$\\underline{I}$th nonparametric $\\underline{T}$ime d$\\underline{E}$cays, which can decouple the influences between every pair of the events and capture various time decays of the influence strengths. Specifically, we use an embedding to represent each event type and model the event influence as an unknown function of the embeddings and time span. We derive a general construction that can cover all possible time decaying functions. By placing Gaussian process (GP) priors over the latent functions and using Gauss-Legendre quadrature to obtain the integral in the construction, we can flexibly estimate all kinds of time-decaying influences, without restricting to any specific form or imposing derivative constraints that bring learning difficulties.  We then use weight space augmentation of GPs to develop an efficient stochastic variational learning algorithm. We show the advantages of our approach in both the ablation study and real-world applications. ",
    "authors": [
      "Pan, Zhimeng",
      "Wang, Zheng",
      "Phillips, Jeff M",
      "Zhe, Shandian"
    ]
  },
  {
    "id": "248024541dbda1d3fd75fe49d1a4df4d",
    "title": "Offline Meta Reinforcement Learning -- Identifiability Challenges and Effective Data Collection Strategies",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/248024541dbda1d3fd75fe49d1a4df4d-Paper.pdf",
    "abstract": "Consider the following instance of the Offline Meta Reinforcement Learning (OMRL) problem: given the complete training logs of $N$ conventional RL agents, trained on $N$ different tasks, design a meta-agent that can quickly maximize reward in a new, unseen task from the same task distribution. In particular, while each conventional RL agent explored and exploited its own different task, the meta-agent must identify regularities in the data that lead to effective exploration/exploitation in the unseen task. Here, we take a Bayesian RL (BRL) view, and seek to learn a Bayes-optimal policy from the offline data. Building on the recent VariBAD BRL approach, we develop an off-policy BRL method that learns to plan an  exploration strategy based on an adaptive neural belief estimate. However, learning to infer such a belief from offline data brings a new identifiability issue we term MDP ambiguity. We characterize the problem, and suggest resolutions via data collection and modification procedures.Finally, we evaluate our framework on a diverse set of domains, including difficult sparse reward tasks, and demonstrate learning of effective exploration behavior that is qualitatively different from the exploration used by any RL agent in the data. Our code is available online at \\url{https://github.com/Rondorf/BOReL}.",
    "authors": [
      "Dorfman, Ron",
      "Shenfeld, Idan",
      "Tamar, Aviv"
    ]
  },
  {
    "id": "24b43fb034a10d78bec71274033b4096",
    "title": "RoMA: Robust Model Adaptation for Offline Model-based Optimization",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/24b43fb034a10d78bec71274033b4096-Paper.pdf",
    "abstract": "We consider the problem of searching an input maximizing a black-box objective function given a static dataset of input-output queries. A popular approach to solving this problem is maintaining a proxy model, e.g., a deep neural network (DNN), that approximates the true objective function. Here, the main challenge is how to avoid adversarially optimized inputs during the search, i.e., the inputs where the DNN highly overestimates the true objective function. To handle the issue, we propose a new framework, coined robust model adaptation (RoMA), based on gradient-based optimization of inputs over the DNN. Specifically, it consists of two steps: (a) a pre-training strategy to robustly train the proxy model and (b) a novel adaptation procedure of the proxy model to have robust estimates for a specific set of candidate solutions. At a high level, our scheme utilizes the local smoothness prior to overcome the brittleness of the DNN. Experiments under various tasks show the effectiveness of RoMA compared with previous methods, obtaining state-of-the-art results, e.g., RoMA outperforms all at 4 out of 6 tasks and achieves runner-up results at the remaining tasks.",
    "authors": [
      "Yu, Sihyun",
      "Ahn, Sungsoo",
      "Song, Le",
      "Shin, Jinwoo"
    ]
  },
  {
    "id": "24cceab7ffc1118f5daaace13c670885",
    "title": "Flexible Option Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/24cceab7ffc1118f5daaace13c670885-Paper.pdf",
    "abstract": "Temporal abstraction in reinforcement learning (RL), offers the promise of improving generalization and knowledge transfer in complex environments, by propagating information more efficiently over time. Although option learning was initially formulated in a way that allows updating many options simultaneously, using off-policy, intra-option learning (Sutton, Precup & Singh, 1999) , many of the recent hierarchical reinforcement learning approaches only update a single option at a time: the option currently executing. We revisit and extend intra-option learning in the context of deep reinforcement learning, in order to enable updating all options consistent with current primitive action choices, without introducing any additional estimates. Our method can therefore be naturally adopted in most hierarchical RL frameworks. When we combine our approach with the option-critic algorithm for option discovery, we obtain significant improvements in performance and data-efficiency across a wide variety of domains. ",
    "authors": [
      "Klissarov, Martin",
      "Precup, Doina"
    ]
  },
  {
    "id": "24ec8468b67314c2013d215b77034476",
    "title": "Faster Directional Convergence of Linear Neural Networks under Spherically Symmetric Data",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/24ec8468b67314c2013d215b77034476-Paper.pdf",
    "abstract": "In this paper, we study gradient methods for training deep linear neural networks with binary cross-entropy loss. In particular, we show global directional convergence guarantees from a polynomial rate to a linear rate for (deep) linear networks with spherically symmetric data distribution, which can be viewed as a specific zero-margin dataset. Our results do not require the assumptions in other works such as small initial loss, presumed convergence of weight direction, or overparameterization. We also characterize our findings in experiments.",
    "authors": [
      "Lin, Dachao",
      "Sun, Ruoyu",
      "Zhang, Zhihua"
    ]
  },
  {
    "id": "250473494b245120a7eaf8b2e6b1f17c",
    "title": "Online Facility Location with Multiple Advice",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/250473494b245120a7eaf8b2e6b1f17c-Paper.pdf",
    "abstract": "Clustering is a central topic in unsupervised learning and its online formulation has received a lot of attention in recent years. In this paper, we study the classic facility location problem in the presence of multiple machine-learned advice. We design an algorithm with provable performance guarantees such that, if the advice is good, it outperforms the best-known online algorithms for the problem, and if it is bad it still matches their performance.We complement our theoretical analysis with an in-depth study of the performance of our algorithm, showing its effectiveness on synthetic and real-world data sets.",
    "authors": [
      "Almanza, Matteo",
      "Chierichetti, Flavio",
      "Lattanzi, Silvio",
      "Panconesi, Alessandro",
      "Re, Giuseppe"
    ]
  },
  {
    "id": "25048eb6a33209cb5a815bff0cf6887c",
    "title": "Credit Assignment in Neural Networks through Deep Feedback Control",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/25048eb6a33209cb5a815bff0cf6887c-Paper.pdf",
    "abstract": "The success of deep learning sparked interest in whether the brain learns by using similar techniques for assigning credit to each synaptic weight for its contribution to the network output. However, the majority of current attempts at biologically-plausible learning methods are either non-local in time, require highly specific connectivity motifs, or have no clear link to any known mathematical optimization method. Here, we introduce Deep Feedback Control (DFC), a new learning method that uses a feedback controller to drive a deep neural network to match a desired output target and whose control signal can be used for credit assignment. The resulting learning rule is fully local in space and time and approximates Gauss-Newton optimization for a wide range of feedback connectivity patterns. To further underline its biological plausibility, we relate DFC to a multi-compartment model of cortical pyramidal neurons with a local voltage-dependent synaptic plasticity rule, consistent with recent theories of dendritic processing. By combining dynamical system theory with mathematical optimization theory, we provide a strong theoretical foundation for DFC that we corroborate with detailed results on toy experiments and standard computer-vision benchmarks.",
    "authors": [
      "Meulemans, Alexander",
      "Tristany Farinha, Matilde",
      "Garcia Ordonez, Javier",
      "Vilimelis Aceituno, Pau",
      "Sacramento, Jo\u00e3o",
      "Grewe, Benjamin F."
    ]
  },
  {
    "id": "250dd56814ad7c50971ee4020519c6f5",
    "title": "Robust Online Correlation Clustering",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/250dd56814ad7c50971ee4020519c6f5-Paper.pdf",
    "abstract": "In correlation clustering we are given a set of points along with recommendations whether each pair of points should be placed in the same cluster or into separate clusters. The goal cluster the points to minimize disagreements from the recommendations. We study the correlation clustering problem in the online setting., where points arrive one at a time, and upon arrival the algorithm must make an irrevocable cluster assignment decision. While the online version is natural, there is a simple lower bound that rules out any algorithm with a non-trivial competitive ratio. In this work we go beyond worst case analysis, and show that the celebrated Pivot algorithm performs well when given access to a small number of random samples from the input. Moreover, we prove that Pivot is robust to additional adversarial perturbations of the sample set in this setting. We conclude with an empirical analysis validating our theoretical findings. ",
    "authors": [
      "Lattanzi, Silvio",
      "Moseley, Benjamin",
      "Vassilvitskii, Sergei",
      "Wang, Yuyan",
      "Zhou, Rudy"
    ]
  },
  {
    "id": "251bd0442dfcc53b5a761e050f8022b8",
    "title": "Neural Additive Models: Interpretable Machine Learning with Neural Nets",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/251bd0442dfcc53b5a761e050f8022b8-Paper.pdf",
    "abstract": "Deep neural networks (DNNs) are powerful black-box predictors that have achieved impressive performance on a wide variety of tasks. However, their accuracy comes at the cost of intelligibility: it is usually unclear how they make their decisions. This hinders their applicability to high stakes decision-making domains such as healthcare. We propose Neural Additive Models (NAMs) which combine some of the expressivity of DNNs with the inherent intelligibility of generalized additive models. NAMs learn a linear combination of neural networks that each attend to a single input feature. These networks are trained jointly and can learn arbitrarily complex relationships between their input feature and the output. Our experiments on regression and classification datasets show that NAMs are more accurate than widely used intelligible models such as logistic regression and shallow decision trees. They perform similarly to existing state-of-the-art generalized additive models in accuracy, but are more flexible because they are based on neural nets instead of boosted trees. To demonstrate this, we show how NAMs can be used for multitask learning on synthetic data and on the COMPAS recidivism data due to their composability, and demonstrate that the differentiability of NAMs allows them to train more complex interpretable models for COVID-19. ",
    "authors": [
      "Agarwal, Rishabh",
      "Melnick, Levi",
      "Frosst, Nicholas",
      "Zhang, Xuezhou",
      "Lengerich, Ben",
      "Caruana, Rich",
      "Hinton, Geoffrey E."
    ]
  },
  {
    "id": "251c5ffd6b62cc21c446c963c76cf214",
    "title": "Representation Learning for Event-based Visuomotor Policies",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/251c5ffd6b62cc21c446c963c76cf214-Paper.pdf",
    "abstract": "Event-based cameras are dynamic vision sensors that provide asynchronous measurements of changes in per-pixel brightness at a microsecond level. This makes them significantly faster than conventional frame-based cameras, and an appealing choice for high-speed robot navigation. While an interesting sensor modality, this asynchronously streamed event data poses a challenge for machine learning based computer vision techniques that are more suited for synchronous, frame-based data. In this paper, we present an event variational autoencoder through which compact representations can be learnt directly from asynchronous spatiotemporal event data. Furthermore, we show that such pretrained representations can be used for event-based reinforcement learning instead of end-to-end reward driven perception. We validate this framework of learning event-based visuomotor policies by applying it to an obstacle avoidance scenario in simulation. Compared to techniques that treat event data as images, we show that representations learnt from event streams result in faster policy training, adapt to different control capacities, and demonstrate a higher degree of robustness to environmental changes and sensor noise.",
    "authors": [
      "Vemprala, Sai",
      "Mian, Sami",
      "Kapoor, Ashish"
    ]
  },
  {
    "id": "251e16a2aac0ca4847adf561483381bf",
    "title": "Kernel Functional Optimisation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/251e16a2aac0ca4847adf561483381bf-Paper.pdf",
    "abstract": "Traditional methods for kernel selection rely on parametric kernel functions or a combination thereof and although the kernel hyperparameters are tuned, these methods often provide sub-optimal results due to the limitations induced by the parametric forms. In this paper, we propose a novel formulation for kernel selection using efficient Bayesian optimisation to find the best fitting non-parametric kernel. The kernel is expressed using a linear combination of functions sampled from a prior Gaussian Process (GP) defined by a hyperkernel. We also provide a mechanism to ensure the positive definiteness of the Gram matrix constructed using the resultant kernels. Our experimental results on GP regression and Support Vector Machine (SVM) classification tasks involving both synthetic functions and several real-world datasets show the superiority of our approach over the state-of-the-art.",
    "authors": [
      "Anjanapura Venkatesh, Arun Kumar",
      "Shilton, Alistair",
      "Rana, Santu",
      "Gupta, Sunil",
      "Venkatesh, Svetha"
    ]
  },
  {
    "id": "252a3dbaeb32e7690242ad3b556e626b",
    "title": "Generalized Shape Metrics on Neural Representations",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/252a3dbaeb32e7690242ad3b556e626b-Paper.pdf",
    "abstract": "Understanding the operation of biological and artificial networks remains a difficult and important challenge. To identify general principles, researchers are increasingly interested in surveying large collections of networks that are trained on, or biologically adapted to, similar tasks. A standardized set of analysis tools is now needed to identify how network-level covariates---such as architecture, anatomical brain region, and model organism---impact neural representations (hidden layer activations). Here, we provide a rigorous foundation for these analyses by defining a broad family of metric spaces that quantify representational dissimilarity. Using this framework, we modify existing representational similarity measures based on canonical correlation analysis and centered kernel alignment to satisfy the triangle inequality, formulate a novel metric that respects the inductive biases in convolutional layers, and identify approximate Euclidean embeddings that enable network representations to be incorporated into essentially any off-the-shelf machine learning method. We demonstrate these methods on large-scale datasets from biology (Allen Institute Brain Observatory) and deep learning (NAS-Bench-101). In doing so, we identify relationships between neural representations that are interpretable in terms of anatomical features and model performance.",
    "authors": [
      "Williams, Alex H",
      "Kunz, Erin",
      "Kornblith, Simon",
      "Linderman, Scott"
    ]
  },
  {
    "id": "253614bbac999b38b5b60cae531c4969",
    "title": "Diverse Message Passing for Attribute with Heterophily",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/253614bbac999b38b5b60cae531c4969-Paper.pdf",
    "abstract": "Most of the existing GNNs can be modeled via the Uniform Message Passing framework. This framework considers all the attributes of each node in its entirety, shares the uniform propagation weights along each edge,  and focuses on the uniform weight learning. The design of this framework possesses two prerequisites, the simplification of homophily and heterophily to the node-level property and the ignorance of attribute differences. Unfortunately, different attributes possess diverse characteristics. In this paper, the network homophily rate defined with  respect to the node labels is extended to attribute homophily rate by taking the attributes as weak labels. Based on this attribute homophily rate, we propose a Diverse Message Passing (DMP) framework, which specifies every attribute propagation weight on each edge. Besides, we propose two specific strategies to significantly reduce the computational complexity of DMP to prevent the overfitting issue.  By investigating the spectral characteristics, existing spectral GNNs are actually equivalent to a degenerated version of DMP.  From the perspective of numerical optimization, we provide a theoretical analysis to demonstrate DMP's powerful representation ability and the ability of alleviating the over-smoothing issue.  Evaluations on various  real networks demonstrate the superiority of our DMP on  handling the networks with heterophily  and alleviating the over-smoothing issue, compared to the existing state-of-the-arts.",
    "authors": [
      "Yang, Liang",
      "Li, Mengzhe",
      "Liu, Liyang",
      "niu, bingxin",
      "Wang, Chuan",
      "Cao, Xiaochun",
      "Guo, Yuanfang"
    ]
  },
  {
    "id": "256bf8e6923a52fda8ddf7dc050a1148",
    "title": "Towards Robust Bisimulation Metric Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/256bf8e6923a52fda8ddf7dc050a1148-Paper.pdf",
    "abstract": "Learned representations in deep reinforcement learning (DRL) have to extract task-relevant information from complex observations, balancing between robustness to distraction and informativeness to the policy. Such stable and rich representations, often learned via modern function approximation techniques, can enable practical application of the policy improvement theorem, even in high-dimensional continuous state-action spaces. Bisimulation metrics offer one solution to this representation learning problem, by collapsing functionally similar states together in representation space, which promotes invariance to noise and distractors. In this work, we generalize value function approximation bounds for on-policy bisimulation metrics to non-optimal policies and approximate environment dynamics. Our theoretical results help us identify embedding pathologies that may occur in practical use. In particular, we find that these issues stem from an underconstrained dynamics model and an unstable dependence of the embedding norm on the reward signal in environments with sparse rewards. Further, we propose a set of practical remedies: (i) a norm constraint on the representation space, and (ii) an extension of prior approaches with intrinsic rewards and latent space regularization. Finally, we provide evidence that the resulting method is not only more robust to sparse reward functions, but also able to solve challenging continuous control tasks with observational distractions, where prior methods fail.",
    "authors": [
      "Kemertas, Mete",
      "Aumentado-Armstrong, Tristan"
    ]
  },
  {
    "id": "2578eb9cdf020730f77793e8b58e165a",
    "title": "Beyond BatchNorm: Towards a Unified Understanding of Normalization in Deep Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2578eb9cdf020730f77793e8b58e165a-Paper.pdf",
    "abstract": "Inspired by BatchNorm, there has been an explosion of normalization layers in deep learning. Recent works have identified a multitude of beneficial properties in BatchNorm to explain its success. However, given the pursuit of alternative normalization layers, these properties need to be generalized so that any given layer's success/failure can be accurately predicted. In this work, we take a first step towards this goal by extending known properties of BatchNorm in randomly initialized deep neural networks (DNNs) to several recently proposed normalization layers. Our primary findings follow: (i) similar to BatchNorm, activations-based normalization layers can prevent exponential growth of activations in ResNets, but parametric techniques require explicit remedies; (ii) use of GroupNorm can ensure an informative forward propagation, with different samples being assigned dissimilar activations, but increasing group size results in increasingly indistinguishable activations for different samples, explaining slow convergence speed in models with LayerNorm; and (iii) small group sizes result in large gradient norm in earlier layers, hence explaining training instability issues in Instance Normalization and illustrating a speed-stability tradeoff in GroupNorm. Overall, our analysis reveals a unified set of mechanisms that underpin the success of normalization methods in deep learning, providing us with a compass to systematically explore the vast design space of DNN normalization layers.",
    "authors": [
      "Lubana, Ekdeep S",
      "Dick, Robert",
      "Tanaka, Hidenori"
    ]
  },
  {
    "id": "258be18e31c8188555c2ff05b4d542c3",
    "title": "Representation Learning Beyond Linear Prediction Functions",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/258be18e31c8188555c2ff05b4d542c3-Paper.pdf",
    "abstract": "Recent papers on the theory of representation learning has shown the importance of a quantity called diversity when generalizing from a set of source tasks to a target task. Most of these papers assume that the function mapping shared representations to predictions is linear, for both source and target tasks. In practice, researchers in deep learning use different numbers of extra layers following the pretrained model based on the difficulty of the new task. This motivates us to ask whether diversity can be achieved when source tasks and the target task use different prediction function spaces beyond linear functions. We show that diversity holds even if the target task uses a neural network with multiple layers, as long as source tasks use linear functions. If source tasks use nonlinear prediction functions, we provide a negative result by showing that depth-1 neural networks with ReLu activation function need exponentially many source tasks to achieve diversity. For a general function class, we find that eluder dimension gives a lower bound on the number of tasks required for diversity. Our theoretical results imply that simpler tasks generalize better. Though our theoretical results are shown for the global minimizer of empirical risks, their qualitative predictions still hold true for gradient-based optimization algorithms as verified by our simulations on deep neural networks.",
    "authors": [
      "Xu, Ziping",
      "Tewari, Ambuj"
    ]
  },
  {
    "id": "25e2a30f44898b9f3e978b1786dcd85c",
    "title": "Volume Rendering of Neural Implicit Surfaces",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/25e2a30f44898b9f3e978b1786dcd85c-Paper.pdf",
    "abstract": "Neural volume rendering became increasingly popular recently due to its success in synthesizing novel views of a scene from a sparse set of input images. So far, the geometry learned by neural volume rendering techniques was modeled using a generic density function. Furthermore, the geometry itself was extracted using an arbitrary level set of the density function leading to a noisy, often low fidelity reconstruction.The goal of this paper is to improve geometry representation and reconstruction in neural volume rendering. We achieve that by modeling the volume density as a function of the geometry. This is in contrast to previous work modeling the geometry as a function of the volume density. In more detail, we define the volume density function as Laplace's cumulative distribution function (CDF) applied to a signed distance function (SDF) representation. This simple density representation has three benefits: (i) it provides a useful inductive bias to the geometry learned in the neural volume rendering process; (ii) it facilitates a bound on the opacity approximation error, leading to an accurate sampling of the viewing ray. Accurate sampling is important to provide a precise coupling of geometry and radiance; and (iii) it allows efficient unsupervised disentanglement of shape and appearance in volume rendering.Applying this new density representation to challenging scene multiview datasets produced high quality geometry reconstructions, outperforming relevant baselines. Furthermore, switching shape and appearance between scenes is possible due to the disentanglement of the two. ",
    "authors": [
      "Yariv, Lior",
      "Gu, Jiatao",
      "Kasten, Yoni",
      "Lipman, Yaron"
    ]
  },
  {
    "id": "260c2432a0eecc28ce03c10dadc078a4",
    "title": "MAUVE: Measuring the Gap Between Neural Text and Human Text using Divergence Frontiers",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/260c2432a0eecc28ce03c10dadc078a4-Paper.pdf",
    "abstract": "As major progress is made in open-ended text generation, measuring how close machine-generated text is to human language remains a critical open problem. We introduce Mauve, a comparison measure for open-ended text generation, which directly compares the learnt distribution from a text generation model to the distribution of human-written text using divergence frontiers. Mauve scales up to modern text generation models by computing information divergences in a quantized embedding space. Through an extensive empirical study on three open-ended generation tasks, we find that Mauve identifies known properties of generated text, scales naturally with model size, and correlates with human judgments, with fewer restrictions than existing distributional evaluation metrics.",
    "authors": [
      "Pillutla, Krishna",
      "Swayamdipta, Swabha",
      "Zellers, Rowan",
      "Thickstun, John",
      "Welleck, Sean",
      "Choi, Yejin",
      "Harchaoui, Zaid"
    ]
  },
  {
    "id": "26337353b7962f533d78c762373b3318",
    "title": "Accurately Solving Rod Dynamics with Graph Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/26337353b7962f533d78c762373b3318-Paper.pdf",
    "abstract": "Iterative solvers are widely used to accurately simulate physical systems. These solvers require initial guesses to generate a sequence of improving approximate solutions. In this contribution, we introduce a novel method to accelerate iterative solvers for rod dynamics with graph networks (GNs) by predicting the initial guesses to reduce the number of iterations. Unlike existing methods that aim to learn physical systems in an end-to-end manner, our approach guarantees long-term stability and therefore leads to more accurate solutions. Furthermore, our method improves the run time performance of traditional iterative solvers for rod dynamics. To explore our method we make use of position-based dynamics (PBD) as a common solver for physical systems and evaluate it by simulating the dynamics of elastic rods. Our approach is able to generalize across different initial conditions, discretizations, and realistic material properties. We demonstrate that it also performs well when taking discontinuous effects into account such as collisions between individual rods. Finally, to illustrate the scalability of our approach, we simulate complex 3D tree models composed of over a thousand individual branch segments swaying in wind fields.",
    "authors": [
      "Shao, Han",
      "Kugelstadt, Tassilo",
      "H\u00e4drich, Torsten",
      "Palubicki, Wojtek",
      "Bender, Jan",
      "Pirk, Soeren",
      "Michels, Dominik L"
    ]
  },
  {
    "id": "2639ba2137371773aa1e64e7735cdb30",
    "title": "Limiting fluctuation and trajectorial stability of multilayer neural networks with mean field training",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2639ba2137371773aa1e64e7735cdb30-Paper.pdf",
    "abstract": "The mean field theory of multilayer neural networks centers around a particular infinite-width scaling, in which the learning dynamics is shown to be closely tracked by the mean field limit. A random fluctuation around this infinite-width limit is expected from a large-width expansion to the next order. This fluctuation has been studied only in the case of shallow networks, where previous works employ heavily technical notions or additional formulation ideas amenable only to that case. Treatment of the multilayer case has been missing, with the chief difficulty in finding a formulation that must capture the stochastic dependency across not only time but also depth.In this work, we initiate the study of the fluctuation in the case of multilayer networks, at any network depth. Leveraging on the neuronal embedding framework recently introduced by Nguyen and Pham, we systematically derive a system of dynamical equations, called the second-order mean field limit, that captures the limiting fluctuation distribution. We demonstrate through the framework the complex interaction among neurons in this second-order mean field limit, the stochasticity with cross-layer dependency and the nonlinear time evolution inherent in the limiting fluctuation. A limit theorem is proven to relate quantitatively this limit to the fluctuation realized by large-width networks.We apply the result to show a stability property of gradient descent mean field training: in the large-width regime, along the training trajectory, it progressively biases towards a solution with \"minimal fluctuation\" (in fact, vanishing fluctuation) in the learned output function, even after the network has been initialized at or has converged (sufficiently fast) to a global optimum. This extends a similar phenomenon previously shown only for shallow networks with a squared loss in the empirical risk minimization setting, to multilayer networks with a loss function that is not necessarily convex in a more general setting.",
    "authors": [
      "Pham, Huy Tuan",
      "Nguyen, Phan-Minh"
    ]
  },
  {
    "id": "26405399c51ad7b13b504e74eb7c696c",
    "title": "Medical Dead-ends and Learning to Identify High-Risk States and Treatments",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/26405399c51ad7b13b504e74eb7c696c-Paper.pdf",
    "abstract": "Machine learning has successfully framed many sequential decision making problems as either supervised prediction, or optimal decision-making policy identification via reinforcement learning. In data-constrained offline settings, both approaches may fail as they assume fully optimal behavior or rely on exploring alternatives that may not exist. We introduce an inherently different approach that identifies \"dead-ends\" of a state space. We focus on patient condition in the intensive care unit, where a \"medical dead-end\" indicates that a patient will expire, regardless of all potential future treatment sequences. We postulate \"treatment security\" as avoiding treatments with probability proportional to their chance of leading to dead-ends, present a formal proof, and frame discovery as an RL problem. We then train three independent deep neural models for automated state construction, dead-end discovery and confirmation. Our empirical results discover that dead-ends exist in real clinical data among septic patients, and further reveal gaps between secure treatments and those administered. ",
    "authors": [
      "Fatemi, Mehdi",
      "Killian, Taylor W.",
      "Subramanian, Jayakumar",
      "Ghassemi, Marzyeh"
    ]
  },
  {
    "id": "26657d5ff9020d2abefe558796b99584",
    "title": "Overcoming the Convex Barrier for Simplex Inputs",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/26657d5ff9020d2abefe558796b99584-Paper.pdf",
    "abstract": "Recent progress in neural network verification has challenged the notion of a convex barrier, that is, an inherent weakness in the convex relaxation of the output of a neural network. Specifically, there now exists a tight relaxation for verifying the robustness of a neural network to $\\ell_\\infty$ input perturbations, as well as efficient primal and dual solvers for the relaxation. Buoyed by this success, we consider the problem of developing similar techniques for verifying robustness to input perturbations within the probability simplex. We prove a somewhat surprising result that, in this case, not only can one design a tight relaxation that overcomes the convex barrier, but the size of the relaxation remains linear in the number of neurons, thereby leading to simpler and more efficient algorithms. We establish the scalability of our overall approach via the specification of $\\ell_1$ robustness for CIFAR-10 and MNIST classification, where our approach improves the state of the art verified accuracy by up to $14.4\\%$. Furthermore, we establish its accuracy on a novel and highly challenging task of verifying the robustness of a multi-modal (text and image) classifier to arbitrary changes in its textual input. ",
    "authors": [
      "Behl, Harkirat Singh",
      "Kumar, M. Pawan",
      "Torr, Philip",
      "Dvijotham, Krishnamurthy"
    ]
  },
  {
    "id": "26901debb30ea03f0aa833c9de6b81e9",
    "title": "High-probability Bounds for Non-Convex Stochastic Optimization with Heavy Tails",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/26901debb30ea03f0aa833c9de6b81e9-Paper.pdf",
    "abstract": "We consider non-convex stochastic optimization using first-order algorithms for which the gradient estimates may have heavy tails. We show that a combination of gradient clipping, momentum, and normalized gradient descent yields convergence to critical points in high-probability with best-known rates for smooth losses when the gradients only have bounded $\\mathfrak{p}$th moments for some $\\mathfrak{p}\\in(1,2]$. We then consider the case of second-order smooth losses, which to our knowledge have not been studied in this setting, and again obtain high-probability bounds for any $\\mathfrak{p}$. Moreover, our results hold for arbitrary smooth norms, in contrast to the typical SGD analysis which requires a Hilbert space norm. Further, we show that after a suitable \"burn-in\" period, the objective value will monotonically decrease for every iteration until a critical point is identified, which provides intuition behind the popular practice of learning rate \"warm-up'' and also yields a last-iterate guarantee.",
    "authors": [
      "Cutkosky, Ashok",
      "Mehta, Harsh"
    ]
  },
  {
    "id": "26cd8ecadce0d4efd6cc8a8725cbd1f8",
    "title": "Batch Normalization Orthogonalizes Representations in Deep Random Networks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/26cd8ecadce0d4efd6cc8a8725cbd1f8-Paper.pdf",
    "abstract": "This paper underlines an elegant property of batch-normalization (BN): Successive batch normalizations with random linear updates make samples increasingly orthogonal. We establish a non-asymptotic characterization of the interplay between depth, width, and the orthogonality of deep representations. More precisely, we prove, under a mild assumption, the deviation of the representations from orthogonality rapidly decays with depth up to a term inversely proportional to the network width. This result has two main theoretical and practical implications: 1) Theoretically, as the depth grows, the distribution of the outputs contracts to a Wasserstein-2 ball around an isotropic normal distribution. Furthermore, the radius of this Wasserstein ball shrinks with the width of the network. 2) Practically, the orthogonality of the representations directly influences the performance of stochastic gradient descent (SGD). When representations are initially aligned, we observe SGD wastes many iterations to disentangle representations before the classification. Nevertheless, we experimentally show that starting optimization from orthogonal representations is sufficient to accelerate SGD, with no need for BN.",
    "authors": [
      "Daneshmand, Hadi",
      "Joudaki, Amir",
      "Bach, Francis"
    ]
  },
  {
    "id": "26d4b4313a7e5828856bc0791fca39a2",
    "title": "Support vector machines and linear regression coincide with very high-dimensional features",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/26d4b4313a7e5828856bc0791fca39a2-Paper.pdf",
    "abstract": "The support vector machine (SVM) and minimum Euclidean norm least squares regression are two fundamentally different approaches to fitting linear models, but they have recently been connected in models for very high-dimensional data through a phenomenon of support vector proliferation, where every training example used to fit an SVM becomes a support vector. In this paper, we explore the generality of this phenomenon and make the following contributions. First, we prove a super-linear lower bound on the dimension (in terms of sample size) required for support vector proliferation in independent feature models, matching the upper bounds from previous works. We further identify a sharp phase transition in Gaussian feature models, bound the width of this transition, and give experimental support for its universality. Finally, we hypothesize that this phase transition occurs only in much higher-dimensional settings in the $\\ell_1$ variant of the SVM, and we present a new geometric characterization of the problem that may elucidate this phenomenon for the general $\\ell_p$ case. ",
    "authors": [
      "Ardeshir, Navid",
      "Sanford, Clayton",
      "Hsu, Daniel J."
    ]
  },
  {
    "id": "26ddd45b02859e836d13d4b9fde34281",
    "title": "Coupled Segmentation and Edge Learning via Dynamic Graph Propagation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/26ddd45b02859e836d13d4b9fde34281-Paper.pdf",
    "abstract": "Image segmentation and edge detection are both central problems in perceptual grouping. It is therefore interesting to study how these two tasks can be coupled to benefit each other. Indeed, segmentation can be easily transformed into contour edges to guide edge learning. However, the converse is nontrivial since general edges may not always form closed contours. In this paper, we propose a principled end-to-end framework for coupled edge and segmentation learning, where edges are leveraged as pairwise similarity cues to guide segmentation. At the core of our framework is a recurrent module termed as dynamic graph propagation (DGP) layer that performs message passing on dynamically constructed graphs. The layer uses learned gating to dynamically select neighbors for message passing using max-pooling. The output from message passing is further gated with an edge signal to refine segmentation. Experiments demonstrate that the proposed framework is able to let both tasks mutually improve each other. On Cityscapes validation, our best model achieves 83.7% mIoU in semantic segmentation and 78.7% maximum F-score in semantic edge detection. Our method also leads to improved zero-shot robustness on Cityscapes with natural corruptions (Cityscapes-C).",
    "authors": [
      "Yu, Zhiding",
      "Huang, Rui",
      "Byeon, Wonmin",
      "Liu, Sifei",
      "Liu, Guilin",
      "Breuel, Thomas",
      "Anandkumar, Anima",
      "Kautz, Jan"
    ]
  },
  {
    "id": "274a10ffa06e434f2a94df765cac6bf4",
    "title": "Offline RL Without Off-Policy Evaluation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/274a10ffa06e434f2a94df765cac6bf4-Paper.pdf",
    "abstract": "Most prior approaches to offline reinforcement learning (RL) have taken an iterative actor-critic approach involving off-policy evaluation. In this paper we show that simply doing one step of constrained/regularized policy improvement using an on-policy Q estimate of the behavior policy performs surprisingly well. This one-step algorithm beats the previously reported results of iterative algorithms on a large portion of the D4RL benchmark. The one-step baseline achieves this strong performance while being notably simpler and more robust to hyperparameters than previously proposed iterative algorithms. We argue that the relatively poor performance of iterative approaches is a result of the high variance inherent in doing off-policy evaluation and magnified by the repeated optimization of policies against those estimates. In addition, we hypothesize that the strong performance of the one-step algorithm is due to a combination of favorable structure in the environment and behavior policy.",
    "authors": [
      "Brandfonbrener, David",
      "Whitney, Will",
      "Ranganath, Rajesh",
      "Bruna, Joan"
    ]
  },
  {
    "id": "274ad4786c3abca69fa097b85867d9a4",
    "title": "Continuous vs. Discrete Optimization of Deep Neural Networks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/274ad4786c3abca69fa097b85867d9a4-Paper.pdf",
    "abstract": "Existing analyses of optimization in deep learning are either continuous, focusing on (variants of) gradient flow, or discrete, directly treating (variants of) gradient descent.  Gradient flow is amenable to theoretical analysis, but is stylized and disregards computational efficiency.  The extent to which it represents gradient descent is an open question in the theory of deep learning.  The current paper studies this question.  Viewing gradient descent as an approximate numerical solution to the initial value problem of gradient flow, we find that the degree of approximation depends on the curvature around the gradient flow trajectory.  We then show that over deep neural networks with homogeneous activations, gradient flow trajectories enjoy favorable curvature, suggesting they are well approximated by gradient descent.  This finding allows us to translate an analysis of gradient flow over deep linear neural networks into a guarantee that gradient descent efficiently converges to global minimum almost surely under random initialization.  Experiments suggest that over simple deep neural networks, gradient descent with conventional step size is indeed close to gradient flow.  We hypothesize that the theory of gradient flows will unravel mysteries behind deep learning.",
    "authors": [
      "Elkabetz, Omer",
      "Cohen, Nadav"
    ]
  },
  {
    "id": "2754518221cfbc8d25c13a06a4cb8421",
    "title": "CrypTen: Secure Multi-Party Computation Meets Machine Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2754518221cfbc8d25c13a06a4cb8421-Paper.pdf",
    "abstract": "Secure multi-party computation (MPC) allows parties to perform computations on data while keeping that data private. This capability has great potential for machine-learning applications: it facilitates training of machine-learning models on private data sets owned by different parties, evaluation of one party's private model using another party's private data, etc. Although a range of studies implement machine-learning models via secure MPC, such implementations are not yet mainstream. Adoption of secure MPC is hampered by the absence of flexible software frameworks that `\"speak the language\" of machine-learning researchers and engineers. To foster adoption of secure MPC in machine learning, we present CrypTen: a software framework that exposes popular secure MPC primitives via abstractions that are common in modern machine-learning frameworks, such as tensor computations, automatic differentiation, and modular neural networks. This paper describes the design of CrypTen and measure its performance on state-of-the-art models for text classification, speech recognition, and image classification. Our benchmarks show that CrypTen's GPU support and high-performance communication between (an arbitrary number of) parties allows it to perform efficient private evaluation of modern machine-learning models under a semi-honest threat model. For example, two parties using CrypTen can securely predict phonemes in speech recordings using Wav2Letter faster than real-time. We hope that CrypTen will spur adoption of secure MPC in the machine-learning community.",
    "authors": [
      "Knott, Brian",
      "Venkataraman, Shobha",
      "Hannun, Awni",
      "Sengupta, Shubho",
      "Ibrahim, Mark",
      "van der Maaten, Laurens"
    ]
  },
  {
    "id": "27934a1f19d678a1377c257b9a780e80",
    "title": "Can contrastive learning avoid shortcut solutions?",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/27934a1f19d678a1377c257b9a780e80-Paper.pdf",
    "abstract": "The generalization of representations learned via contrastive learning depends crucially on what features of the data are extracted. However, we observe that the contrastive loss does not always sufficiently guide which features are extracted, a behavior that can negatively impact the performance on downstream tasks via \u201cshortcuts\", i.e., by inadvertently suppressing important predictive features.  We find that feature extraction is influenced by  the difficulty of the so-called instance discrimination task (i.e., the task of discriminating pairs of similar points from pairs of dissimilar ones). Although harder pairs improve the representation of some features, the improvement comes at the cost of suppressing previously well represented features. In response, we propose implicit feature modification (IFM), a method for altering positive and negative samples in order to guide contrastive models towards capturing a wider variety of predictive features. Empirically, we observe that IFM reduces feature suppression, and as a result improves performance on vision and medical imaging tasks. ",
    "authors": [
      "Robinson, Joshua",
      "Sun, Li",
      "Yu, Ke",
      "Batmanghelich, Kayhan",
      "Jegelka, Stefanie",
      "Sra, Suvrit"
    ]
  },
  {
    "id": "27d52bcb3580724eb4cbe9f2718a9365",
    "title": "See More for Scene: Pairwise Consistency Learning for Scene Classification",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/27d52bcb3580724eb4cbe9f2718a9365-Paper.pdf",
    "abstract": "Scene classification is a valuable classification subtask and has its own characteristics which still needs more in-depth studies. Basically, scene characteristics are distributed over the whole image, which cause the need of \u201cseeing\u201d comprehensive and informative regions. Previous works mainly focus on region discovery and aggregation, while rarely involves the inherent properties of CNN along with its potential ability to satisfy the requirements of scene classification. In this paper, we propose to understand scene images and the scene classification CNN models in terms of the focus area. From this new perspective, we find that large focus area is preferred in scene classification CNN models as a consequence of learning scene characteristics. Meanwhile, the analysis about existing training schemes helps us to understand the effects of focus area, and also raises the question about optimal training method for scene classification. Pursuing the better usage of scene characteristics, we propose a new learning scheme with a tailored loss in the goal of activating larger focus area on scene images. Since the supervision of the target regions to be enlarged is usually lacked, our alternative learning scheme is to erase already activated area, and allow the CNN models to activate more area during training. The proposed scheme is implemented by keeping the pairwise consistency between the output of  the erased image and its original one. In particular, a tailored loss is proposed to keep such pairwise consistency by leveraging category-relevance information. Experiments on Places365 show the significant improvements of our method with various CNNs. Our method shows an inferior result on the object-centric dataset, ImageNet, which experimentally indicates that it captures the unique characteristics of scenes.",
    "authors": [
      "Chen, Gongwei",
      "Song, Xinhang",
      "Wang, Bohan",
      "Jiang, Shuqiang"
    ]
  },
  {
    "id": "27debb435021eb68b3965290b5e24c49",
    "title": "Provable Guarantees for Self-Supervised Deep Learning with Spectral Contrastive Loss",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/27debb435021eb68b3965290b5e24c49-Paper.pdf",
    "abstract": "Recent works in self-supervised learning have advanced the state-of-the-art by relying on the contrastive learning paradigm, which learns representations by pushing positive pairs, or similar examples from the same class, closer together while keeping negative pairs far apart. Despite the empirical successes, theoretical foundations are limited -- prior analyses assume conditional independence of the positive pairs given the same class label, but recent empirical applications use heavily correlated positive pairs (i.e., data augmentations of the same image). Our work analyzes contrastive learning without assuming conditional independence of positive pairs using a novel concept of the augmentation graph on data.  Edges in this graph connect augmentations of the same data, and ground-truth classes naturally form connected sub-graphs. We propose a loss that performs spectral decomposition on the population augmentation graph and can be succinctly written as a contrastive learning objective on neural net representations. Minimizing this objective leads to features with provable accuracy guarantees under linear probe evaluation. By standard generalization bounds, these accuracy guarantees also hold when minimizing the training contrastive loss. In all, this work provides the first provable analysis for contrastive learning where the guarantees can apply to realistic empirical settings.",
    "authors": [
      "HaoChen, Jeff Z.",
      "Wei, Colin",
      "Gaidon, Adrien",
      "Ma, Tengyu"
    ]
  },
  {
    "id": "27e9661e033a73a6ad8cefcde965c54d",
    "title": "Greedy Approximation Algorithms for Active Sequential Hypothesis Testing",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/27e9661e033a73a6ad8cefcde965c54d-Paper.pdf",
    "abstract": "In the problem of \\emph{active sequential hypothesis testing} (ASHT), a learner seeks to identify the \\emph{true} hypothesis from among a known set of hypotheses. The learner is given a set of actions and knows the random distribution of the outcome of any action under any true hypothesis. Given a target error $\\delta>0$, the goal is to sequentially select the fewest number of actions so as to identify the true hypothesis with probability at least $1 - \\delta$. Motivated by applications in which the number of hypotheses or actions is massive (e.g., genomics-based cancer detection), we propose efficient (greedy, in fact) algorithms and provide the first approximation guarantees for ASHT, under two types of adaptivity. Both of our guarantees are independent of the number of actions and logarithmic in the number of hypotheses. We numerically evaluate the performance of our algorithms using both synthetic and real-world DNA mutation data, demonstrating that our algorithms outperform previously proposed heuristic policies by large margins.",
    "authors": [
      "Gan, Kyra",
      "Jia, Su",
      "Li, Andrew"
    ]
  },
  {
    "id": "28267ab848bcf807b2ed53c3a8f8fc8a",
    "title": "When False Positive is Intolerant: End-to-End  Optimization with Low FPR for Multipartite Ranking",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/28267ab848bcf807b2ed53c3a8f8fc8a-Paper.pdf",
    "abstract": "Multipartite ranking is a basic task in machine learning, where the Area Under the receiver operating characteristics Curve (AUC) is generally applied as the evaluation metric. Despite that AUC reflects the overall performance of the model, it is inconsistent with the expected performance in some application scenarios, where only a low False Positive Rate (FPR) is meaningful. To leverage high performance under low FPRs, we consider an alternative metric for multipartite ranking evaluating the True Positive Rate (TPR) at a given FPR, denoted as TPR@FPR. Unfortunately, the key challenge of direct  TPR@FPR optimization is two-fold: \\textbf{a)} the original objective function is not differentiable, making gradient backpropagation impossible; \\textbf{b)} the loss function could not be written as a sum of independent instance-wise terms, making mini-batch based optimization infeasible.      To address these issues, we propose a novel framework on top of the deep learning framework named \\textit{Cross-Batch Approximation for Multipartite Ranking (CBA-MR)}. In face of \\textbf{a)},  we propose a differentiable surrogate optimization problem where the instances having a short-time effect on FPR are rendered with different weights based on the random walk hypothesis. To tackle \\textbf{b)}, we propose a fast ranking estimation method, where the full-batch loss evaluation is replaced by a delayed update scheme with the help of an embedding cache. Finally, experimental results on four real-world benchmarks are provided to demonstrate the effectiveness of the proposed method.",
    "authors": [
      "Wen, Peisong",
      "Xu, Qianqian",
      "Yang, Zhiyong",
      "He, Yuan",
      "Huang, Qingming"
    ]
  },
  {
    "id": "285a25c17f351708754cdb6d56f3962e",
    "title": "Convex Polytope Trees",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/285a25c17f351708754cdb6d56f3962e-Paper.pdf",
    "abstract": "A decision tree is commonly restricted to use a single hyperplane to split the covariate space at each of its internal nodes. It often requires a large number of nodes to achieve high accuracy. In this paper, we propose convex polytope trees (CPT) to expand the family of decision trees by an interpretable generalization of their decision boundary. The splitting function at each node of CPT is based on the logical disjunction of a community of differently weighted probabilistic linear decision-makers, which also geometrically corresponds to a convex polytope in the covariate space. We use a nonparametric Bayesian prior at each node to infer the community's size, encouraging simpler decision boundaries by shrinking the number of polytope facets. We develop a greedy method to efficiently construct CPT and scalable end-to-end training algorithms for the tree parameters when the tree structure is given. We empirically demonstrate the efficiency of CPT over existing state-of-the-art decision trees in several real-world classification and regression tasks from diverse domains.",
    "authors": [
      "Armandpour, Mohammadreza",
      "Sadeghian, Ali",
      "Zhou, Mingyuan"
    ]
  },
  {
    "id": "285baacbdf8fda1de94b19282acd23e2",
    "title": "The Skellam Mechanism for Differentially Private Federated Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/285baacbdf8fda1de94b19282acd23e2-Paper.pdf",
    "abstract": "We introduce the multi-dimensional Skellam mechanism, a discrete differential privacy mechanism based on the difference of two independent Poisson random variables. To quantify its privacy guarantees, we analyze the privacy loss distribution via a numerical evaluation and provide a sharp bound on the R\u00e9nyi divergence between two shifted Skellam distributions. While useful in both centralized and distributed privacy applications, we investigate how it can be applied in the context of federated learning with secure aggregation under communication constraints. Our theoretical findings and extensive experimental evaluations demonstrate that the Skellam mechanism provides the same privacy-accuracy trade-offs as the continuous Gaussian mechanism, even when the precision is low. More importantly, Skellam is closed under summation and sampling from it only requires sampling from a Poisson distribution -- an efficient routine that ships with all machine learning and data analysis software packages. These features, along with its discrete nature and competitive privacy-accuracy trade-offs, make it an attractive practical alternative to the newly introduced discrete Gaussian mechanism. ",
    "authors": [
      "Agarwal, Naman",
      "Kairouz, Peter",
      "Liu, Ziyu"
    ]
  },
  {
    "id": "286674e3082feb7e5afb92777e48821f",
    "title": "Stability and Deviation Optimal Risk Bounds with Convergence Rate $O(1/n)$",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/286674e3082feb7e5afb92777e48821f-Paper.pdf",
    "abstract": "The sharpest known high probability generalization bounds for uniformly stable algorithms (Feldman, Vondrak, NeurIPS 2018, COLT, 2019), (Bousquet, Klochkov, Zhivotovskiy, COLT, 2020) contain a generally inevitable sampling error term of order $\\Theta(1/\\sqrt{n})$. When applied to excess risk bounds, this leads to suboptimal results in several standard stochastic convex optimization problems. We show that if the so-called Bernstein condition is satisfied, the term $\\Theta(1/\\sqrt{n})$ can be avoided, and high probability excess risk bounds of order up to $O(1/n)$ are possible via uniform stability. Using this result, we show a high probability excess risk bound with the rate $O(\\log n/n)$ for strongly convex and Lipschitz losses valid for \\emph{any} empirical risk minimization method. This resolves a question of Shalev-Shwartz, Shamir, Srebro, and Sridharan (COLT, 2009). We discuss how $O(\\log n/n)$ high probability excess risk bounds are possible for projected gradient descent in the case of strongly convex and Lipschitz losses without the usual smoothness assumption.",
    "authors": [
      "Klochkov, Yegor",
      "Zhivotovskiy, Nikita"
    ]
  },
  {
    "id": "28891cb4ab421830acc36b1f5fd6c91e",
    "title": "SketchGen: Generating Constrained CAD Sketches",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/28891cb4ab421830acc36b1f5fd6c91e-Paper.pdf",
    "abstract": "Computer-aided design (CAD) is the most widely used modeling approach for technical design. The typical starting point in these designs is 2D sketches which can later be extruded and combined to obtain complex three-dimensional assemblies. Such sketches are typically composed of parametric primitives, such as points, lines, and circular arcs, augmented with geometric constraints linking the primitives, such as coincidence, parallelism, or orthogonality. Sketches can be represented as graphs, with the primitives as nodes and the constraints as edges. Training a model to automatically generate CAD sketches can enable several novel workflows, but is challenging due to the complexity of the graphs and the heterogeneity of the primitives and constraints. In particular, each type of primitive and constraint may require a record of different size and parameter types.We propose SketchGen as a generative model based on a transformer architecture to address the heterogeneity problem by carefully designing a sequential language for the primitives and constraints that allows distinguishing between different primitive or constraint types and their parameters, while encouraging our model to re-use information across related parameters, encoding shared structure. A particular highlight of our work is the ability to produce primitives linked via constraints that enables the final output to be further regularized via a constraint solver. We evaluate our model by demonstrating constraint prediction for given sets of primitives and full sketch generation from scratch, showing that our approach significantly out performs the state-of-the-art in CAD sketch generation.",
    "authors": [
      "Para, Wamiq",
      "Bhat, Shariq",
      "Guerrero, Paul",
      "Kelly, Tom",
      "Mitra, Niloy",
      "Guibas, Leonidas J.",
      "Wonka, Peter"
    ]
  },
  {
    "id": "288cd2567953f06e460a33951f55daaf",
    "title": "CLDA: Contrastive Learning for Semi-Supervised Domain Adaptation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/288cd2567953f06e460a33951f55daaf-Paper.pdf",
    "abstract": "Unsupervised Domain Adaptation (UDA) aims to align the labeled source distribution with the unlabeled target distribution to obtain domain invariant predictive models. However, the application of well-known UDA approaches does not generalize well in Semi-Supervised Domain Adaptation (SSDA) scenarios where few labeled samples from the target domain are available.This paper proposes a simple Contrastive Learning framework for semi-supervised Domain Adaptation (CLDA) that attempts to bridge the intra-domain gap between the labeled and unlabeled target distributions and the inter-domain gap between source and unlabeled target distribution in SSDA. We suggest employing class-wise contrastive learning to reduce the inter-domain gap and instance-level contrastive alignment between the original(input image) and strongly augmented unlabeled target images to minimize the intra-domain discrepancy. We have empirically shown that both of these modules complement each other to achieve superior performance. Experiments on three well-known domain adaptation benchmark datasets, namely DomainNet, Office-Home, and Office31, demonstrate the effectiveness of our approach. CLDA achieves state-of-the-art results on all the above datasets.",
    "authors": [
      "Singh, Ankit"
    ]
  },
  {
    "id": "28ce9bc954876829eeb56ff46da8e1ab",
    "title": "Differentially Private n-gram Extraction",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/28ce9bc954876829eeb56ff46da8e1ab-Paper.pdf",
    "abstract": "We revisit the problem of $n$-gram extraction in the differential privacy setting. In this problem, given a corpus of private text data, the goal is to release as many $n$-grams as possible while preserving user level privacy. Extracting $n$-grams is a fundamental subroutine in many NLP applications such as sentence completion, auto response generation for emails, etc. The problem also arises in other applications such as sequence mining, trajectory analysis, etc., and is a generalization of recently studied differentially private set union (DPSU) by Gopi et al. (2020). In this paper, we develop a new differentially private algorithm for this problem which, in our experiments, significantly outperforms the state-of-the-art. Our improvements stem from combining recent advances in DPSU, privacy accounting, and new heuristics for pruning in the tree-based approach initiated by Chen et al. (2012).",
    "authors": [
      "Kim, Kunho",
      "Gopi, Sivakanth",
      "Kulkarni, Janardhan",
      "Yekhanin, Sergey"
    ]
  },
  {
    "id": "291d43c696d8c3704cdbe0a72ade5f6c",
    "title": "Capturing implicit hierarchical structure in 3D biomedical images with self-supervised hyperbolic representations",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/291d43c696d8c3704cdbe0a72ade5f6c-Paper.pdf",
    "abstract": "We consider the task of representation learning for unsupervised segmentation of 3D voxel-grid biomedical images. We show that models that capture implicit hierarchical relationships between subvolumes are better suited for this task. To that end, we consider encoder-decoder architectures with a hyperbolic latent space, to explicitly capture hierarchical relationships present in subvolumes of the data. We propose utilizing a 3D hyperbolic variational autoencoder with a novel gyroplane convolutional layer to map from the embedding space back to 3D images. To capture these relationships, we introduce an essential self-supervised loss---in addition to the standard VAE loss---which infers approximate hierarchies and encourages implicitly related subvolumes to be mapped closer in the embedding space. We present experiments on synthetic datasets along with a dataset from the medical domain to validate our hypothesis.",
    "authors": [
      "Hsu, Joy",
      "Gu, Jeffrey",
      "Wu, Gong",
      "Chiu, Wah",
      "Yeung, Serena"
    ]
  },
  {
    "id": "29301521774ff3cbd26652b2d5c95996",
    "title": "Noisy Recurrent Neural Networks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/29301521774ff3cbd26652b2d5c95996-Paper.pdf",
    "abstract": "We provide a general framework for studying recurrent neural networks (RNNs) trained by injecting noise into hidden states. Specifically, we consider RNNs that can be viewed as discretizations of stochastic differential equations driven by input data. This framework allows us to study the implicit regularization effect of general noise injection schemes by deriving an approximate explicit regularizer in the small noise regime. We find that, under reasonable assumptions, this implicit regularization promotes flatter minima; it biases towards models with more stable dynamics; and, in classification tasks, it favors models with larger classification margin. Sufficient conditions for global stability are obtained, highlighting the phenomenon of stochastic stabilization, where noise injection can improve stability during training. Our theory is supported by empirical results which demonstrate that the RNNs have improved robustness with respect to various input perturbations.",
    "authors": [
      "Lim, Soon Hoe",
      "Erichson, N. Benjamin",
      "Hodgkinson, Liam",
      "Mahoney, Michael W."
    ]
  },
  {
    "id": "29539ed932d32f1c56324cded92c07c2",
    "title": "Matrix encoding networks for neural combinatorial optimization",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/29539ed932d32f1c56324cded92c07c2-Paper.pdf",
    "abstract": "Machine Learning (ML) can help solve combinatorial optimization (CO) problems better. A popular approach is to use a neural net to compute on the parameters of a given CO problem and extract useful information that guides the search for good solutions. Many CO problems of practical importance can be specified in a matrix form of parameters quantifying the relationship between two groups of items. There is currently no neural net model, however, that takes in such matrix-style relationship data as an input. Consequently, these types of CO problems have been out of reach for ML engineers. In this paper, we introduce Matrix Encoding Network (MatNet) and show how conveniently it takes in and processes parameters of such complex CO problems. Using an end-to-end model based on MatNet, we solve asymmetric traveling salesman (ATSP) and flexible flow shop (FFSP) problems as the earliest neural approach. In particular, for a class of FFSP we have tested MatNet on, we demonstrate a far superior empirical performance to any methods (neural or not) known to date.",
    "authors": [
      "Kwon, Yeong-Dae",
      "Choo, Jinho",
      "Yoon, Iljoo",
      "Park, Minah",
      "Park, Duwon",
      "Gwon, Youngjune"
    ]
  },
  {
    "id": "29586cb449c90e249f1f09a0a4ee245a",
    "title": "When Is Unsupervised Disentanglement Possible?",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/29586cb449c90e249f1f09a0a4ee245a-Paper.pdf",
    "abstract": "A common assumption in many domains is that high dimensional data are a smooth nonlinear function of a small number of independent factors. When is it possible to recover the factors from unlabeled data? In the context of deep models this problem is called \u201cdisentanglement\u201d and was recently shown to be impossible without additional strong assumptions [17, 19]. In this paper, we show that the assumption of local isometry together with non-Gaussianity of the factors, is sufficient to provably recover disentangled representations from data. We leverage recent advances in deep generative models to construct manifolds of highly realistic images for which the ground truth latent representation is known, and test whether modern and classical methods succeed in recovering the latent factors. For many different manifolds, we find that a spectral method that explicitly optimizes local isometry and non-Gaussianity consistently finds the correct latent factors, while baseline deep autoencoders do not. We propose how to encourage deep autoencoders to find encodings that satisfy local isometry and show that this helps them discover disentangled representations. Overall, our results suggest that in some realistic settings, unsupervised disentanglement is provably possible, without any domain-specific assumptions.",
    "authors": [
      "Horan, Daniella",
      "Richardson, Eitan",
      "Weiss, Yair"
    ]
  },
  {
    "id": "2983e3047c0c730d3b7c022584717f3f",
    "title": "Continuous Latent Process Flows",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2983e3047c0c730d3b7c022584717f3f-Paper.pdf",
    "abstract": "Partial observations of continuous time-series dynamics at arbitrary time stamps exist in many disciplines. Fitting this type of data using statistical models with continuous dynamics is not only promising at an intuitive level but also has practical benefits, including the ability to generate continuous trajectories and to perform inference on previously unseen time stamps. Despite exciting progress in this area, the existing models still face challenges in terms of their representational power and the quality of their variational approximations. We tackle these challenges with continuous latent process flows (CLPF), a principled architecture decoding continuous latent processes into continuous observable processes using a time-dependent normalizing flow driven by a stochastic differential equation. To optimize our model using maximum likelihood, we propose a novel piecewise construction of a variational posterior process and derive the corresponding variational lower bound using trajectory re-weighting. Our ablation studies demonstrate the effectiveness of our contributions in various inference tasks on irregular time grids. Comparisons to state-of-the-art baselines show our model's favourable performance on both synthetic and real-world time-series data.",
    "authors": [
      "Deng, Ruizhi",
      "Brubaker, Marcus A.",
      "Mori, Greg",
      "Lehrmann, Andreas"
    ]
  },
  {
    "id": "298f587406c914fad5373bb689300433",
    "title": "Perturbation-based Regret Analysis of Predictive Control in Linear Time Varying Systems",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/298f587406c914fad5373bb689300433-Paper.pdf",
    "abstract": "We study predictive control in a setting where the dynamics are time-varying and linear, and the costs are time-varying and well-conditioned. At each time step, the controller receives the exact predictions of costs, dynamics, and disturbances for the future $k$ time steps. We show that when the prediction window $k$ is sufficiently large, predictive control is input-to-state stable and achieves a dynamic regret of $O(\\lambda^k T)$, where $\\lambda < 1$ is a positive constant. This is the first dynamic regret bound on the predictive control of linear time-varying systems. We also show a variation of predictive control obtains the first competitive bound for the control of linear time-varying systems:  $1 + O(\\lambda^k)$. Our results are derived using a novel proof framework based on a perturbation bound that characterizes how a small change to the system parameters impacts the optimal trajectory.",
    "authors": [
      "Lin, Yiheng",
      "Hu, Yang",
      "Shi, Guanya",
      "Sun, Haoyuan",
      "Qu, Guannan",
      "Wierman, Adam"
    ]
  },
  {
    "id": "299a23a2291e2126b91d54f3601ec162",
    "title": "Dataset Distillation with Infinitely Wide Convolutional Networks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/299a23a2291e2126b91d54f3601ec162-Paper.pdf",
    "abstract": "The effectiveness of machine learning algorithms arises from being able to extract useful features from large amounts of data. As model and dataset sizes increase, dataset distillation methods that compress large datasets into significantly smaller yet highly performant ones will become valuable in terms of training efficiency and useful feature extraction. To that end, we apply a novel distributed kernel-based meta-learning framework to achieve state-of-the-art results for dataset distillation using infinitely wide convolutional neural networks. For instance, using only 10  datapoints (0.02% of original dataset), we obtain over 65% test accuracy on CIFAR-10 image classification task, a dramatic improvement over the previous best test accuracy of 40%. Our state-of-the-art results extend across many other settings for MNIST, Fashion-MNIST, CIFAR-10, CIFAR-100, and SVHN. Furthermore, we perform some preliminary analyses of our distilled datasets to shed light on how they differ from naturally occurring data.",
    "authors": [
      "Nguyen, Timothy",
      "Novak, Roman",
      "Xiao, Lechao",
      "Lee, Jaehoon"
    ]
  },
  {
    "id": "299dc35e747eb77177d9cea10a802da2",
    "title": "SPANN: Highly-efficient Billion-scale Approximate Nearest Neighborhood Search",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/299dc35e747eb77177d9cea10a802da2-Paper.pdf",
    "abstract": "The in-memory algorithms for approximate nearest neighbor search (ANNS) have achieved great success for fast high-recall search, but are extremely expensive when handling very large scale database. Thus, there is an increasing request for the hybrid ANNS solutions with small memory and inexpensive solid-state drive (SSD). In this paper, we present a simple but efficient memory-disk hybrid indexing and search system, named SPANN, that follows the inverted index methodology. It stores the centroid points of the posting lists in the memory and the large posting lists in the disk. We guarantee both disk-access efficiency (low  latency) and high recall by effectively reducing the disk-access number and retrieving high-quality posting lists. In the index-building stage, we adopt a hierarchical balanced clustering algorithm to balance the length of posting lists and augment the posting list by adding the points in the closure of the corresponding clusters. In the search stage, we use a query-aware scheme to dynamically prune the access of unnecessary posting lists.  Experiment results demonstrate that SPANN is 2X faster than the state-of-the-art ANNS solution DiskANN to reach the same recall quality 90% with same memory cost in three billion-scale datasets. It can reach 90% recall@1 and recall@10 in just around one millisecond with only about 10% of original memory cost.  Code is available at: https://github.com/microsoft/SPTAG.",
    "authors": [
      "Chen, Qi",
      "Zhao, Bing",
      "Wang, Haidong",
      "Li, Mingqin",
      "Liu, Chuanjie",
      "Li, Zengzhong",
      "Yang, Mao",
      "Wang, Jingdong"
    ]
  },
  {
    "id": "29c0c0ee223856f336d7ea8052057753",
    "title": "Distilling Object Detectors with Feature Richness",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/29c0c0ee223856f336d7ea8052057753-Paper.pdf",
    "abstract": "In recent years, large-scale deep models have achieved great success, but the huge computational complexity and massive storage requirements make it a great challenge to deploy them in resource-limited devices. As a model compression and acceleration method, knowledge distillation effectively improves the performance of small models by transferring the dark knowledge from the teacher detector. However, most of the existing distillation-based detection methods mainly imitating features near bounding boxes, which suffer from two limitations. First, they ignore the beneficial features outside the bounding boxes. Second, these methods imitate some features which are mistakenly regarded as the background by the teacher detector. To address the above issues, we propose a novel Feature-Richness Score (FRS) method to choose important features that improve generalized detectability during distilling. The proposed method effectively retrieves the important features outside the bounding boxes and removes the detrimental features within the bounding boxes. Extensive experiments show that our methods achieve excellent performance on both anchor-based and anchor-free detectors. For example, RetinaNet with ResNet-50 achieves 39.7% in mAP on the COCO2017 dataset, which even surpasses the ResNet-101 based teacher detector 38.9% by 0.8%. Our implementation is available at https://github.com/duzhixing/FRS.",
    "authors": [
      "Zhixing, Du",
      "Zhang, Rui",
      "Chang, Ming",
      "zhang, xishan",
      "Liu, Shaoli",
      "Chen, Tianshi",
      "Chen, Yunji"
    ]
  },
  {
    "id": "29d74915e1b323676bfc28f91b3c4802",
    "title": "Analysis of one-hidden-layer neural networks via the resolvent method",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/29d74915e1b323676bfc28f91b3c4802-Paper.pdf",
    "abstract": "In this work, we investigate the asymptotic spectral density of the random feature matrix $M = Y Y^*$ with $Y = f(WX)$ generated by a single-hidden-layer neural network, where $W$ and $X$ are random rectangular matrices with i.i.d. centred entries and $f$ is a non-linear smooth function which is applied entry-wise. We prove that the Stieltjes transform of the limiting spectral distribution approximately satisfies a quartic self-consistent equation, which is exactly the equation obtained by [Pennington, Worah 2017] and [Benigni, P\u00e9ch\u00e9 2019] with the moment method. We extend the previous results to the case of additive bias $Y=f(WX+B)$ with $B$ being an independent rank-one Gaussian random matrix, closer modelling the neural network infrastructures encountered in practice. Our key finding is that in the case of additive bias it is impossible to choose an activation function preserving the layer-to-layer singular value distribution, in sharp contrast to the bias-free case where a simple integral constraint is sufficient to achieve isospectrality. To obtain the asymptotics for the empirical spectral density we follow the resolvent method from random matrix theory via the cumulant expansion. We find that this approach is more robust and less combinatorial than the moment method and expect that it will apply also for models where the combinatorics of the former become intractable. The resolvent method has been widely employed, but compared to previous works, it is applied here to non-linear random matrices.",
    "authors": [
      "Piccolo, Vanessa",
      "Schr\u00f6der, Dominik"
    ]
  },
  {
    "id": "29daf9442f3c0b60642b14c081b4a556",
    "title": "Grounding Spatio-Temporal Language with Transformers",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/29daf9442f3c0b60642b14c081b4a556-Paper.pdf",
    "abstract": "Language is an interface to the outside world. In order for embodied agents to use it, language must be grounded in other, sensorimotor modalities. While there is an extended literature studying how machines can learn grounded language, the topic of how to learn spatio-temporal linguistic concepts is still largely uncharted. To make progress in this direction, we here introduce a novel spatio-temporal language grounding task where the goal is to learn the meaning of spatio-temporal descriptions of behavioral traces of an embodied agent. This is achieved by training a truth function that predicts if a description matches a given history of observations. The descriptions involve time-extended predicates in past and present tense as well as spatio-temporal references to objects in the scene. To study the role of architectural biases in this task, we train several models including multimodal Transformer architectures; the latter implement different attention computations between words and objects across space and time. We test models on two classes of generalization: 1) generalization to new sentences, 2) generalization to grammar primitives. We observe that maintaining object identity in the attention computation of our Transformers is instrumental to achieving good performance on generalization overall, and that summarizing object traces in a single token has little influence on performance. We then discuss how this opens new perspectives for language-guided autonomous embodied agents.",
    "authors": [
      "Karch, Tristan",
      "Teodorescu, Laetitia",
      "Hofmann, Katja",
      "Moulin-Frier, Cl\u00e9ment",
      "Oudeyer, Pierre-Yves"
    ]
  },
  {
    "id": "2a10665525774fa2501c2c8c4985ce61",
    "title": "Learning where to learn: Gradient sparsity in meta and continual learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2a10665525774fa2501c2c8c4985ce61-Paper.pdf",
    "abstract": "Finding neural network weights that generalize well from small datasets is difficult. A promising approach is to learn a weight initialization such that a small number of weight changes results in low generalization error. We show that this form of meta-learning can be improved by letting the learning algorithm decide which weights to change, i.e., by learning where to learn. We find that patterned sparsity emerges from this process, with the pattern of sparsity varying on a problem-by-problem basis. This selective sparsity results in better generalization and less interference in a range of few-shot and continual learning problems. Moreover, we find that sparse learning also emerges in a more expressive model where learning rates are meta-learned. Our results shed light on an ongoing debate on whether meta-learning can discover adaptable features and suggest that learning by sparse gradient descent is a powerful inductive bias for meta-learning systems.",
    "authors": [
      "von Oswald, Johannes",
      "Zhao, Dominic",
      "Kobayashi, Seijin",
      "Schug, Simon",
      "Caccia, Massimo",
      "Zucchet, Nicolas",
      "Sacramento, Jo\u00e3o"
    ]
  },
  {
    "id": "2a2717956118b4d223ceca17ce3865e2",
    "title": "Domain Invariant Representation Learning with Domain Density Transformations",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2a2717956118b4d223ceca17ce3865e2-Paper.pdf",
    "abstract": "Domain generalization refers to the problem where we aim to train a model on data from a set of source domains so that the model can generalize to unseen target domains. Naively training a model on the aggregate set of data (pooled from all source domains) has been shown to perform suboptimally, since the information learned by that model might be domain-specific and generalize imperfectly to target domains. To tackle this problem, a predominant domain generalization approach is to  learn some domain-invariant information for the prediction task, aiming at a good generalization across domains. In this paper, we propose a theoretically grounded method to learn a domain-invariant representation by enforcing the representation network to be invariant under all transformation functions among domains. We next introduce the use of generative adversarial networks to learn such domain transformations in a possible implementation of our method in practice. We demonstrate the effectiveness of our method on several widely used datasets for the domain generalization problem, on all of which we achieve competitive results with state-of-the-art models.",
    "authors": [
      "Nguyen, A. Tuan",
      "Tran, Toan",
      "Gal, Yarin",
      "Baydin, Atilim Gunes"
    ]
  },
  {
    "id": "2a38a4a9316c49e5a833517c45d31070",
    "title": "PlayVirtual: Augmenting Cycle-Consistent Virtual Trajectories for Reinforcement Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2a38a4a9316c49e5a833517c45d31070-Paper.pdf",
    "abstract": "Learning good feature representations is important for deep reinforcement learning (RL). However, with limited experience, RL often suffers from data inefficiency for training. For un-experienced or less-experienced trajectories (i.e., state-action sequences), the lack of data limits the use of them for better feature learning. In this work, we propose a novel method, dubbed PlayVirtual, which augments cycle-consistent virtual trajectories to enhance the data efficiency for RL feature representation learning. Specifically, PlayVirtual predicts future states in a latent space based on the current state and action by a dynamics model and then predicts the previous states by a backward dynamics model, which forms a trajectory cycle. Based on this, we augment the actions to generate a large amount of virtual state-action trajectories. Being free of groudtruth state supervision, we enforce a trajectory to meet the cycle consistency constraint, which can significantly enhance the data efficiency. We validate the effectiveness of our designs on the Atari and DeepMind Control Suite benchmarks. Our method achieves the state-of-the-art performance on both benchmarks. Our code is available at https://github.com/microsoft/Playvirtual.",
    "authors": [
      "Yu, Tao",
      "Lan, Cuiling",
      "Zeng, Wenjun",
      "Feng, Mingxiao",
      "Zhang, Zhizheng",
      "Chen, Zhibo"
    ]
  },
  {
    "id": "2a79ea27c279e471f4d180b08d62b00a",
    "title": "Efficient Equivariant Network",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2a79ea27c279e471f4d180b08d62b00a-Paper.pdf",
    "abstract": "Convolutional neural networks (CNNs) have dominated the field of Computer Vision and achieved great success due to their built-in translation equivariance. Group equivariant CNNs (G-CNNs) that incorporate more equivariance can significantly improve the performance of conventional CNNs. However, G-CNNs are faced with two major challenges: \\emph{spatial-agnostic problem} and \\emph{expensive computational cost}. In this work, we propose a general framework of previous equivariant models, which includes G-CNNs and equivariant self-attention layers as special cases. Under this framework, we explicitly decompose the feature aggregation operation into a kernel generator and an encoder, and decouple the spatial and extra geometric dimensions in the computation. Therefore, our filters are essentially dynamic rather than being spatial-agnostic. We further show that our \\emph{E}quivariant model is parameter \\emph{E}fficient and computation \\emph{E}fficient by complexity analysis, and also data \\emph{E}fficient by experiments, so we call our model $E^4$-Net. Extensive experiments verify that our model can significantly improve previous works with smaller model size.Especially, under the setting of training on $1/5$ data of CIFAR10, our model improves G-CNNs by $5\\%+$ accuracy,while using only $56\\%$ parameters and $68\\%$ FLOPs.",
    "authors": [
      "He, Lingshen",
      "Chen, Yuxuan",
      "shen, zhengyang",
      "Dong, Yiming",
      "Wang, Yisen",
      "Lin, Zhouchen"
    ]
  },
  {
    "id": "2a8009525763356ad5e3bb48b7475b4d",
    "title": "Unifying Gradient Estimators for Meta-Reinforcement Learning  via Off-Policy Evaluation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2a8009525763356ad5e3bb48b7475b4d-Paper.pdf",
    "abstract": "Model-agnostic meta-reinforcement learning requires estimating the Hessian matrix of value functions. This is challenging from an implementation perspective, as repeatedly differentiating policy gradient estimates may lead to biased Hessian estimates. In this work, we provide a unifying framework for estimating higher-order derivatives of value functions, based on off-policy evaluation. Our framework interprets a number of prior approaches as special cases and elucidates the bias and variance trade-off of Hessian estimates. This framework also opens the door to a new family of estimates, which can be easily implemented with auto-differentiation libraries, and lead to performance gains in practice.",
    "authors": [
      "Tang, Yunhao",
      "Kozuno, Tadashi",
      "Rowland, Mark",
      "Munos, Remi",
      "Valko, Michal"
    ]
  },
  {
    "id": "2adcefe38fbcd3dcd45908fbab1bf628",
    "title": "Even your Teacher Needs Guidance: Ground-Truth Targets Dampen Regularization Imposed by Self-Distillation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2adcefe38fbcd3dcd45908fbab1bf628-Paper.pdf",
    "abstract": "Knowledge distillation is classically a procedure where a neural network is trained on the output of another network along with the original targets in order to transfer knowledge between the architectures. The special case of self-distillation, where the network architectures are identical, has been observed to improve generalization accuracy. In this paper, we consider an iterative variant of self-distillation in a kernel regression setting, in which successive steps incorporate both model outputs and the ground-truth targets. This allows us to provide the first theoretical results on the importance of using the weighted ground-truth targets in self-distillation. Our focus is on fitting nonlinear functions to training data with a weighted mean square error objective function suitable for distillation, subject to $\\ell_2$ regularization of the model parameters. We show that any such function obtained with self-distillation can be calculated directly as a function of the initial fit, and that infinite distillation steps yields the same optimization problem as the original with amplified regularization. Furthermore, we provide a closed form solution for the optimal choice of weighting parameter at each step, and show how to efficiently estimate this weighting parameter for deep learning and significantly reduce the computational requirements compared to a grid search.",
    "authors": [
      "Borup, Kenneth",
      "Andersen, Lars N"
    ]
  },
  {
    "id": "2adcfc3929e7c03fac3100d3ad51da26",
    "title": "Compressing Neural Networks: Towards Determining the Optimal Layer-wise Decomposition",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2adcfc3929e7c03fac3100d3ad51da26-Paper.pdf",
    "abstract": "We present a novel global compression framework for deep neural networks that automatically analyzes each layer to identify the optimal per-layer compression ratio, while simultaneously achieving the desired overall compression. Our algorithm hinges on the idea of compressing each convolutional (or fully-connected) layer by slicing its channels into multiple groups and decomposing each group via low-rank decomposition. At the core of our algorithm is the derivation of layer-wise error bounds from the Eckart\u2013Young\u2013Mirsky theorem. We then leverage these bounds to frame the compression problem as an optimization problem where we wish to minimize the maximum compression error across layers and propose an efficient algorithm towards a solution. Our experiments indicate that our method outperforms existing low-rank compression approaches across a wide range of networks and data sets. We believe that our results open up new avenues for future research into the global performance-size trade-offs of modern neural networks.",
    "authors": [
      "Liebenwein, Lucas",
      "Maalouf, Alaa",
      "Feldman, Dan",
      "Rus, Daniela"
    ]
  },
  {
    "id": "2aedcba61ca55ceb62d785c6b7f10a83",
    "title": "Equilibrium and non-Equilibrium regimes in the learning of Restricted Boltzmann Machines",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2aedcba61ca55ceb62d785c6b7f10a83-Paper.pdf",
    "abstract": "Training Restricted Boltzmann Machines (RBMs) has been challenging for a long time due to the difficulty of computing precisely the log-likelihood gradient. Over the past decades, many works have proposed more or less successful recipes but without studying systematically the crucial quantity of the problem: the mixing time i.e. the number of MCMC iterations needed to sample completely new configurations from a model. In this work, we show that this mixing time plays a crucial role in the behavior and stability of the trained model, and that RBMs operate in two well-defined distinct regimes, namely equilibrium and out-of-equilibrium, depending on the interplay between this mixing time of the model and the number of MCMC steps, $k$, used to approximate the gradient.  We further show empirically that this mixing time increases along the learning, which often implies a transition from one regime to another as soon as $k$ becomes smaller than this time.In particular, we show that using the popular $k$ (persistent) contrastive divergence approaches, with $k$ small, the dynamics of the fitted model are extremely slow and often dominated by strong out-of-equilibrium effects. On the contrary, RBMs trained in equilibrium display much faster dynamics, and a smooth convergence to dataset-like configurations during the sampling.Finally, we discuss how to exploit in practice both regimes depending on the task one aims to fulfill: (i) short $k$s can be used to generate convincing samples in short learning times, (ii) large $k$ (or increasingly large) must be used to learn the correct equilibrium distribution of the RBM. Finally, the existence of these two operational regimes seems to be a general property of energy based models trained via likelihood maximization.",
    "authors": [
      "Decelle, Aur\u00e9lien",
      "Furtlehner, Cyril",
      "Seoane, Beatriz"
    ]
  },
  {
    "id": "2b0aa0d9e30ea3a55fc271ced8364536",
    "title": "Imitation with Neural Density Models",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2b0aa0d9e30ea3a55fc271ced8364536-Paper.pdf",
    "abstract": "We propose a new framework for Imitation Learning (IL) via density estimation of the expert's occupancy measure followed by Maximum Occupancy Entropy Reinforcement Learning (RL) using the density as a reward. Our approach maximizes a non-adversarial model-free RL objective that provably lower bounds reverse Kullback\u2013Leibler divergence between occupancy measures of the expert and imitator. We present a practical IL algorithm, Neural Density Imitation (NDI), which obtains state-of-the-art demonstration efficiency on benchmark control tasks. ",
    "authors": [
      "Kim, Kuno",
      "Jindal, Akshat",
      "Song, Yang",
      "Song, Jiaming",
      "Sui, Yanan",
      "Ermon, Stefano"
    ]
  },
  {
    "id": "2b0f658cbffd284984fb11d90254081f",
    "title": "Accurate Point Cloud Registration with Robust Optimal Transport",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2b0f658cbffd284984fb11d90254081f-Paper.pdf",
    "abstract": "This work investigates the use of robust optimal transport (OT) for shape matching. Specifically, we show that recent OT solvers improve both optimization-based and deep learning methods for point cloud registration, boosting accuracy at an affordable computational cost. This manuscript starts with a practical overview of modern OT theory. We then provide solutions to the main difficulties in using this framework for shape matching. Finally, we showcase the performance of transport-enhanced registration models on a wide range of challenging tasks: rigid registration for partial shapes; scene flow estimation on the Kitti dataset; and nonparametric registration of lung vascular trees between inspiration and expiration. Our OT-based methods achieve state-of-the-art results on Kitti and for the challenging lung registration task, both in terms of accuracy and scalability. We also release PVT1010, a new public dataset of 1,010 pairs of lung vascular trees with densely sampled points. This dataset provides a challenging use case for point cloud registration algorithms with highly complex shapes and deformations. Our work demonstrates that robust OT enables fast pre-alignment and fine-tuning for a wide range of registration models, thereby providing a new key method for the computer vision toolbox. Our code and dataset are available online at: https://github.com/uncbiag/robot.",
    "authors": [
      "Shen, Zhengyang",
      "Feydy, Jean",
      "Liu, Peirong",
      "Curiale, Ariel H",
      "San Jose Estepar, Ruben",
      "San Jose Estepar, Raul",
      "Niethammer, Marc"
    ]
  },
  {
    "id": "2b323d6eb28422cef49b266557dd31ad",
    "title": "Simple steps are all you need: Frank-Wolfe and generalized self-concordant functions",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2b323d6eb28422cef49b266557dd31ad-Paper.pdf",
    "abstract": "Generalized self-concordance is a key property present in the objective function of many important learning problems.  We establish the convergence rate of a simple Frank-Wolfe variant that uses the open-loop step size strategy $\\gamma_t = 2/(t+2)$, obtaining a $\\mathcal{O}(1/t)$ convergence rate for this class of functions in terms of primal gap and Frank-Wolfe gap, where $t$ is the iteration count. This avoids the use of second-order information or the need to estimate local smoothness parameters of previous work. We also show improved convergence rates for various common cases, e.g., when the feasible region under consideration is uniformly convex or polyhedral.",
    "authors": [
      "Carderera, Alejandro",
      "Besan\u00e7on, Mathieu",
      "Pokutta, Sebastian"
    ]
  },
  {
    "id": "2b38c2df6a49b97f706ec9148ce48d86",
    "title": "Automatic Data Augmentation for Generalization in Reinforcement Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2b38c2df6a49b97f706ec9148ce48d86-Paper.pdf",
    "abstract": "Deep reinforcement learning (RL) agents often fail to generalize beyond their training environments. To alleviate this problem, recent work has proposed the use of data augmentation. However, different tasks tend to benefit from different types of augmentations and selecting the right one typically requires expert knowledge. In this paper, we introduce three approaches for automatically finding an effective augmentation for any RL task. These are combined with two novel regularization terms for the policy and value function, required to make the use of data augmentation theoretically sound for actor-critic algorithms. Our method achieves a new state-of-the-art on the Procgen benchmark and outperforms popular RL algorithms on DeepMind Control tasks with distractors. In addition, our agent learns policies and representations which are more robust to changes in the environment that are irrelevant for solving the task, such as the background. ",
    "authors": [
      "Raileanu, Roberta",
      "Goldstein, Maxwell",
      "Yarats, Denis",
      "Kostrikov, Ilya",
      "Fergus, Rob"
    ]
  },
  {
    "id": "2b3bf3eee2475e03885a110e9acaab61",
    "title": "Blending Anti-Aliasing into Vision Transformer",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2b3bf3eee2475e03885a110e9acaab61-Paper.pdf",
    "abstract": "The transformer architectures, based on self-attention mechanism and convolution-free design, recently found superior performance and booming applications in computer vision. However, the discontinuous patch-wise tokenization process implicitly introduces jagged artifacts into attention maps, arising the traditional problem of aliasing for vision transformers. Aliasing effect occurs when discrete patterns are used to produce high frequency or continuous information, resulting in the indistinguishable distortions. Recent researches have found that modern convolution networks still suffer from this phenomenon. In this work, we analyze the uncharted problem of aliasing in vision transformer and explore to incorporate anti-aliasing properties. Specifically, we propose a plug-and-play Aliasing-Reduction Module (ARM) to alleviate the aforementioned issue. We investigate the effectiveness and generalization of the proposed method across multiple tasks and various vision transformer families. This lightweight design consistently attains a clear boost over several famous structures. Furthermore, our module also improves data efficiency and robustness of vision transformers.",
    "authors": [
      "Qian, Shengju",
      "Shao, Hao",
      "Zhu, Yi",
      "Li, Mu",
      "Jia, Jiaya"
    ]
  },
  {
    "id": "2b515e2bdd63b7f034269ad747c93a42",
    "title": "A Trainable Spectral-Spatial Sparse Coding Model for Hyperspectral Image Restoration",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2b515e2bdd63b7f034269ad747c93a42-Paper.pdf",
    "abstract": "Hyperspectral imaging offers new perspectives for diverse applications, ranging from the monitoring of the environment using airborne or satellite remote sensing, precision farming, food safety, planetary exploration, or astrophysics. Unfortunately, the spectral diversity of information comes at the expense of various sources of degradation,  and the lack of accurate ground-truth \"clean\" hyperspectral signals acquired on the spot makes restoration tasks challenging.  In particular, training deep neural networks for restoration is difficult, in contrast to traditional RGB imaging problems where deep models tend to shine. In this paper, we advocate instead for a hybrid approach based on sparse coding principles that retain the interpretability of classical techniques encoding domain knowledge with handcrafted image priors, while allowing to train model parameters end-to-end without massive amounts of data. We show on various denoising benchmarks that our method is computationally efficient and  significantly outperforms the state of the art.",
    "authors": [
      "Bodrito, Theo",
      "Zouaoui, Alexandre",
      "Chanussot, Jocelyn",
      "Mairal, Julien"
    ]
  },
  {
    "id": "2b6921f2c64dee16ba21ebf17f3c2c92",
    "title": "Posterior Collapse and Latent Variable Non-identifiability",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2b6921f2c64dee16ba21ebf17f3c2c92-Paper.pdf",
    "abstract": "Variational autoencoders model high-dimensional data by positinglow-dimensional latent variables that are mapped through a flexibledistribution parametrized by a neural network. Unfortunately,variational autoencoders often suffer from posterior collapse: theposterior of the latent variables is equal to its prior, rendering thevariational autoencoder useless as a means to produce meaningfulrepresentations. Existing approaches to posterior collapse oftenattribute it to the use of neural networks or optimization issues dueto variational approximation. In this paper, we consider posteriorcollapse as a problem of latent variable non-identifiability. We provethat the posterior collapses if and only if the latent variables arenon-identifiable in the generative model. This fact implies thatposterior collapse is not a phenomenon specific to the use of flexibledistributions or approximate inference. Rather, it can occur inclassical probabilistic models even with exact inference, which wealso demonstrate. Based on these results, we propose a class oflatent-identifiable variational autoencoders, deep generative modelswhich enforce identifiability without sacrificing flexibility. Thismodel class resolves the problem of latent variablenon-identifiability by leveraging bijective Brenier maps andparameterizing them with input convex neural networks, without specialvariational inference objectives or optimization tricks. Acrosssynthetic and real datasets, latent-identifiable variationalautoencoders outperform existing methods in mitigating posteriorcollapse and providing meaningful representations of the data.",
    "authors": [
      "Wang, Yixin",
      "Blei, David",
      "Cunningham, John P."
    ]
  },
  {
    "id": "2b6bb5354a56ce256116b6b307a1ea10",
    "title": "The Benefits of Implicit Regularization from SGD in Least Squares Problems",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2b6bb5354a56ce256116b6b307a1ea10-Paper.pdf",
    "abstract": "Stochastic gradient descent (SGD) exhibits strong algorithmic regularization effects in practice, which has been hypothesized to play an important role in the generalization of modern machine learning approaches. In this work, we seek to understand these issues in the simpler setting of linear regression (including both underparameterized and overparameterized regimes), where our goal is to make sharp instance-based comparisons of the implicit regularization afforded by (unregularized) average SGD with the explicit regularization of ridge regression. For a broad class of least squares problem instances (that are natural in high-dimensional settings), we show: (1) for every problem instance and for every ridge parameter, (unregularized) SGD, when provided with \\emph{logarithmically} more samples than that provided to the ridge algorithm, generalizes no worse than the ridge solution (provided SGD uses a tuned constant stepsize); (2) conversely, there exist instances (in this wide problem class) where optimally-tuned ridge regression requires \\emph{quadratically} more samples than SGD in order to have the same generalization performance. Taken together, our results show that, up to the logarithmic factors, the generalization performance of SGD is always no worse than that of ridge regression in a wide range of overparameterized problems, and, in fact, could be much better for some problem instances. More generally, our results show how algorithmic regularization has important consequences even in simpler (overparameterized) convex settings.",
    "authors": [
      "Zou, Difan",
      "Wu, Jingfeng",
      "Braverman, Vladimir",
      "Gu, Quanquan",
      "Foster, Dean P.",
      "Kakade, Sham"
    ]
  },
  {
    "id": "2b763288faedb7707c0748abe015ab6c",
    "title": "Generalization of Model-Agnostic Meta-Learning Algorithms: Recurring and Unseen Tasks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2b763288faedb7707c0748abe015ab6c-Paper.pdf",
    "abstract": "In this paper, we study the generalization properties of Model-Agnostic Meta-Learning (MAML) algorithms for supervised learning problems. We focus on the setting in which we train the MAML model over $m$ tasks, each with $n$ data points, and characterize its generalization error from two points of view: First, we assume the new task at test time is one of the training tasks, and we show that, for strongly convex objective functions, the expected excess population loss is bounded by $\\mathcal{O}(1/mn)$. Second, we consider the MAML algorithm's generalization to an unseen task and show that the resulting generalization error depends on the total variation distance between the underlying distributions of the new task and the tasks observed during the training process. Our proof techniques rely on the connections between algorithmic stability and generalization bounds of algorithms. In particular, we propose a new definition of stability for meta-learning algorithms, which allows us to capture the role of both the number of tasks $m$ and number of samples per task $n$ on the generalization error of MAML. ",
    "authors": [
      "Fallah, Alireza",
      "Mokhtari, Aryan",
      "Ozdaglar, Asuman"
    ]
  },
  {
    "id": "2ba8698b79439589fdd2b0f7218d8b07",
    "title": "Factored Policy Gradients: Leveraging Structure for Efficient Learning in MOMDPs",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2ba8698b79439589fdd2b0f7218d8b07-Paper.pdf",
    "abstract": "Policy gradient methods can solve complex tasks but often fail when the dimensionality of the action-space or objective multiplicity grow very large. This occurs, in part, because the variance on score-based gradient estimators scales quadratically. In this paper, we address this problem through a factor baseline which exploits independence structure encoded in a novel action-target influence network. Factored policy gradients (FPGs), which follow, provide a common framework for analysing key state-of-the-art algorithms, are shown to generalise traditional policy gradients, and yield a principled way of incorporating prior knowledge of a problem domain's generative processes. We provide an analysis of the proposed estimator and identify the conditions under which variance is reduced. The algorithmic aspects of FPGs are discussed, including optimal policy factorisation, as characterised by minimum biclique coverings, and the implications for the bias variance trade-off of incorrectly specifying the network. Finally, we demonstrate the performance advantages of our algorithm on large-scale bandit and traffic intersection problems,  providing a novel contribution to the latter in the form of a spatial approximation.",
    "authors": [
      "Spooner, Thomas",
      "Vadori, Nelson",
      "Ganesh, Sumitra"
    ]
  },
  {
    "id": "2bcab9d935d219641434683dd9d18a03",
    "title": "MarioNette: Self-Supervised Sprite Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2bcab9d935d219641434683dd9d18a03-Paper.pdf",
    "abstract": "Artists and video game designers often construct 2D animations using libraries of sprites---textured patches of objects and characters. We propose a deep learning approach that decomposes sprite-based video animations into a disentangled representation of recurring graphic elements in a self-supervised manner. By jointly learning a dictionary of possibly transparent patches and training a network that places them onto a canvas, we deconstruct sprite-based content into a sparse, consistent, and explicit representation that can be easily used in downstream tasks, like editing or analysis. Our framework offers a promising approach for discovering recurring visual patterns in image collections without supervision. ",
    "authors": [
      "Smirnov, Dmitriy",
      "GHARBI, MICHAEL",
      "Fisher, Matthew",
      "Guizilini, Vitor",
      "Efros, Alexei",
      "Solomon, Justin M."
    ]
  },
  {
    "id": "2bce32ed409f5ebcee2a7b417ad9beed",
    "title": "RLlib Flow: Distributed Reinforcement Learning is a Dataflow Problem",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2bce32ed409f5ebcee2a7b417ad9beed-Paper.pdf",
    "abstract": "Researchers and practitioners in the field of reinforcement learning (RL) frequently leverage parallel computation, which has led to a plethora of new algorithms and systems in the last few years. In this paper, we re-examine the challenges posed by distributed RL and try to view it through the lens of an old idea: distributed dataflow. We show that viewing RL as a dataflow problem leads to highly composable and performant implementations. We propose RLlib Flow, a hybrid actor-dataflow programming model for distributed RL, and validate its practicality by porting the full suite of algorithms in RLlib, a widely adopted distributed RL library. Concretely, RLlib Flow provides 2-9$\\times$ code savings in real production code and enables the composition of multi-agent algorithms not possible by end users before. The open-source code is available as part of RLlib at https://github.com/ray-project/ray/tree/master/rllib.",
    "authors": [
      "Liang, Eric",
      "Wu, Zhanghao",
      "Luo, Michael",
      "Mika, Sven",
      "Gonzalez, Joseph E.",
      "Stoica, Ion"
    ]
  },
  {
    "id": "2bd235c31c97855b7ef2dc8b414779af",
    "title": "Improve Agents without Retraining: Parallel Tree Search with Off-Policy Correction",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2bd235c31c97855b7ef2dc8b414779af-Paper.pdf",
    "abstract": "Tree Search (TS) is crucial to some of the most influential successes in reinforcement learning. Here, we tackle two major challenges with TS that limit its usability: \\textit{distribution shift} and \\textit{scalability}. We first discover and analyze a counter-intuitive phenomenon: action selection through TS and a pre-trained value function often leads to lower performance compared to the original pre-trained agent, even when having access to the exact state and reward in future steps. We show this is due to a distribution shift to areas where value estimates are highly inaccurate and analyze this effect using Extreme Value theory. To overcome this problem, we introduce a novel off-policy correction term that accounts for the mismatch between the pre-trained value and its corresponding TS policy by penalizing under-sampled trajectories. We prove that our correction eliminates the above mismatch and bound the probability of sub-optimal action selection. Our correction significantly improves pre-trained Rainbow agents without any further training, often more than doubling their scores on Atari games. Next, we address the scalability issue given by the computational complexity of exhaustive TS that scales exponentially with the tree depth. We introduce Batch-BFS: a GPU breadth-first search that advances all nodes in each depth of the tree simultaneously. Batch-BFS reduces runtime by two orders of magnitude and, beyond inference, enables also training with TS of depths that were not feasible before. We train DQN agents from scratch using TS and show improvement in several Atari games compared to both the original DQN and the more advanced Rainbow. We will share the code upon publication.",
    "authors": [
      "Dalal, Gal",
      "Hallak, Assaf",
      "Dalton, Steven",
      "frosio, iuri",
      "Mannor, Shie",
      "Chechik, Gal"
    ]
  },
  {
    "id": "2bd388f731f26312bfc0fe30da009595",
    "title": "Redesigning the Transformer Architecture with Insights from Multi-particle Dynamical Systems",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2bd388f731f26312bfc0fe30da009595-Paper.pdf",
    "abstract": "The Transformer and its variants have been proven to be efficient sequence learners in many different domains. Despite their staggering success, a critical issue has been the enormous number of parameters that must be trained (ranging from $10^7$ to $10^{11}$) along with the quadratic complexity of dot-product attention. In this work, we investigate the problem of approximating the two central components of the Transformer --- multi-head self-attention and point-wise feed-forward transformation, with reduced parameter space and computational complexity. We build upon recent developments in analyzing deep neural networks as numerical solvers of ordinary differential equations. Taking advantage of an analogy between Transformer stages and the evolution of a dynamical system of multiple interacting particles, we formulate a temporal evolution scheme, \\name, to bypass costly dot-product attention over multiple stacked layers.  We perform exhaustive experiments with \\name\\ on well-known encoder-decoder as well as encoder-only tasks. We observe that the degree of approximation (or inversely, the degree of parameter reduction) has different effects on the performance, depending on the task. While in the encoder-decoder regime, \\name\\ delivers performances comparable to the original Transformer, in encoder-only tasks it consistently outperforms Transformer along with several subsequent variants.",
    "authors": [
      "Dutta, Subhabrata",
      "Gautam, Tanya",
      "Chakrabarti, Soumen",
      "Chakraborty, Tanmoy"
    ]
  },
  {
    "id": "2bd7f907b7f5b6bbd91822c0c7b835f6",
    "title": "Exploring Architectural Ingredients of Adversarially Robust Deep Neural Networks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2bd7f907b7f5b6bbd91822c0c7b835f6-Paper.pdf",
    "abstract": "Deep neural networks (DNNs) are known to be vulnerable to adversarial attacks. A range of defense methods have been proposed to train adversarially robust DNNs, among which adversarial training has demonstrated promising results. However, despite preliminary understandings developed for adversarial training, it is still not clear, from the architectural perspective, what configurations can lead to more robust DNNs. In this paper, we address this gap via a comprehensive investigation on the impact of network width and depth on the robustness of adversarially trained DNNs. Specifically, we make the following key observations: 1) more parameters (higher model capacity) does not necessarily help adversarial robustness; 2) reducing capacity at the last stage (the last group of blocks) of the network can actually improve adversarial robustness; and 3) under the same parameter budget, there exists an optimal architectural configuration for adversarial robustness. We also provide a theoretical analysis explaning why such network configuration can help robustness. These architectural insights can help design adversarially robust DNNs. ",
    "authors": [
      "Huang, Hanxun",
      "Wang, Yisen",
      "Erfani, Sarah",
      "Gu, Quanquan",
      "Bailey, James",
      "Ma, Xingjun"
    ]
  },
  {
    "id": "2be8328f41144106f7144802f2367487",
    "title": "Center Smoothing: Certified Robustness for Networks with Structured Outputs",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2be8328f41144106f7144802f2367487-Paper.pdf",
    "abstract": "The study of provable adversarial robustness has mostly been limited to classification tasks and models with one-dimensional real-valued outputs. We extend the scope of certifiable robustness to problems with more general and structured outputs like sets, images, language, etc. We model the output space as a metric space under a distance/similarity function, such as intersection-over-union, perceptual similarity, total variation distance, etc. Such models are used in many machine learning problems like image segmentation, object detection, generative models, image/audio-to-text systems, etc. Based on a robustness technique called randomized smoothing, our center smoothing procedure can produce models with the guarantee that the change in the output, as measured by the distance metric, remains small for any norm-bounded adversarial perturbation of the input. We apply our method to create certifiably robust models with disparate output spaces -- from sets to images -- and show that it yields meaningful certificates without significantly degrading the performance of the base model.",
    "authors": [
      "Kumar, Aounon",
      "Goldstein, Tom"
    ]
  },
  {
    "id": "2c27a260f16ad3098393cc529f391f4a",
    "title": "Breaking the Linear Iteration Cost Barrier for Some Well-known Conditional Gradient Methods Using MaxIP Data-structures",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2c27a260f16ad3098393cc529f391f4a-Paper.pdf",
    "abstract": "Conditional gradient methods (CGM) are widely used in modern machine learning. CGM's overall running time usually consists of two parts: the number of iterations and the cost of each iteration. Most efforts focus on reducing the number of iterations as a means to reduce the overall running time. In this work, we focus on improving the per iteration cost of CGM. The bottleneck step in most CGM is maximum inner product search (MaxIP), which requires a linear scan over the parameters.  In practice, approximate MaxIP data-structures are found to be helpful heuristics. However, theoretically, nothing is known about the combination of approximate MaxIP data-structures and CGM. In this work, we answer this question positively by providing a formal framework to combine the locality sensitive hashing type approximate MaxIP data-structures with CGM algorithms.  As a result, we show the first algorithm, where the cost per iteration is sublinear in the number of parameters, for many fundamental optimization algorithms, e.g., Frank-Wolfe, Herding algorithm, and policy gradient.",
    "authors": [
      "Xu, Zhaozhuo",
      "Song, Zhao",
      "Shrivastava, Anshumali"
    ]
  },
  {
    "id": "2c29d89cc56cdb191c60db2f0bae796b",
    "title": "Neural Regression, Representational Similarity, Model Zoology & Neural Taskonomy at Scale in Rodent Visual Cortex",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2c29d89cc56cdb191c60db2f0bae796b-Paper.pdf",
    "abstract": "How well do deep neural networks fare as models of mouse visual cortex? A majority of research to date suggests results far more mixed than those produced in the modeling of primate visual cortex. Here, we perform a large-scale benchmarking of dozens of deep neural network models in mouse visual cortex with both representational similarity analysis and neural regression. Using the Allen Brain Observatory's 2-photon calcium-imaging dataset of activity in over 6,000 reliable rodent visual cortical neurons recorded in response to natural scenes, we replicate previous findings and resolve previous discrepancies, ultimately demonstrating that modern neural networks can in fact be used to explain activity in the mouse visual cortex to a more reasonable degree than previously suggested. Using our benchmark as an atlas, we offer preliminary answers to overarching questions about levels of analysis (e.g. do models that better predict the representations of individual neurons also predict representational similarity across neural populations?); questions about the properties of models that best predict the visual system overall (e.g. is convolution or category-supervision necessary to better predict neural activity?); and questions about the mapping between biological and artificial representations (e.g. does the information processing hierarchy in deep nets match the anatomical hierarchy of mouse visual cortex?). Along the way, we catalogue a number of models (including vision transformers, MLP-Mixers, normalization free networks, Taskonomy encoders and self-supervised models) outside the traditional circuit of convolutional object recognition. Taken together, our results provide a reference point for future ventures in the deep neural network modeling of mouse visual cortex, hinting at novel combinations of mapping method, architecture, and task to more fully characterize the computational motifs of visual representation in a species so central to neuroscience, but with a perceptual physiology and ecology markedly different from the ones we study in primates.",
    "authors": [
      "Conwell, Colin",
      "Mayo, David",
      "Barbu, Andrei",
      "Buice, Michael",
      "Alvarez, George",
      "Katz, Boris"
    ]
  },
  {
    "id": "2c463dfdde588f3bfc60d53118c10d6b",
    "title": "A Topological Perspective on Causal Inference",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2c463dfdde588f3bfc60d53118c10d6b-Paper.pdf",
    "abstract": "This paper presents a topological learning-theoretic perspective on causal inference by introducing a series of topologies defined on general spaces of structural causal models (SCMs). As an illustration of the framework we prove a topological causal hierarchy theorem, showing that substantive assumption-free causal inference is possible only in a meager set of SCMs. Thanks to a known correspondence between open sets in the weak topology and statistically verifiable hypotheses, our results show that inductive assumptions sufficient to license valid causal inferences are statistically unverifiable in principle. Similar to no-free-lunch theorems for statistical inference, the present results clarify the inevitability of substantial assumptions for causal inference. An additional benefit of our topological approach is that it easily accommodates SCMs with infinitely many variables. We finally suggest that our framework may be helpful for the positive project of exploring and assessing alternative causal-inductive assumptions.",
    "authors": [
      "Ibeling, Duligur",
      "Icard, Thomas"
    ]
  },
  {
    "id": "2c6ae45a3e88aee548c0714fad7f8269",
    "title": "Parameter Inference with Bifurcation Diagrams",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2c6ae45a3e88aee548c0714fad7f8269-Paper.pdf",
    "abstract": "Estimation of parameters in differential equation models can be achieved by applying learning algorithms to quantitative time-series data. However, sometimes it is only possible to measure qualitative changes of a system in response to a controlled condition. In dynamical systems theory, such change points are known as bifurcations and lie on a function of the controlled condition called the bifurcation diagram. In this work, we propose a gradient-based approach for inferring the parameters of differential equations that produce a user-specified bifurcation diagram. The cost function contains an error term that is minimal when the model bifurcations match the specified targets and a bifurcation measure which has gradients that push optimisers towards bifurcating parameter regimes. The gradients can be computed without the need to differentiate through the operations of the solver that was used to compute the diagram. We demonstrate parameter inference with minimal models which explore the space of saddle-node and pitchfork diagrams and the genetic toggle switch from synthetic biology. Furthermore, the cost landscape allows us to organise models in terms of topological and geometric equivalence.",
    "authors": [
      "Szep, Gregory",
      "Dalchau, Neil",
      "Csik\u00e1sz-Nagy, Attila"
    ]
  },
  {
    "id": "2c7f9ccb5a39073e24babc3a4cb45e60",
    "title": "Scalable Thompson Sampling using Sparse Gaussian Process Models",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2c7f9ccb5a39073e24babc3a4cb45e60-Paper.pdf",
    "abstract": "Thompson Sampling (TS) from Gaussian Process (GP) models is a powerful tool for the optimization of black-box functions. Although TS enjoys strong theoretical guarantees and convincing empirical performance, it incurs a large computational overhead that scales polynomially with the optimization budget. Recently, scalable TS methods based on sparse GP models have been proposed to increase the scope of TS, enabling its application to problems that are sufficiently multi-modal, noisy or combinatorial to require more than a few hundred evaluations to be solved. However, the approximation error introduced by sparse GPs invalidates all existing regret bounds. In this work, we perform a theoretical and empirical analysis of scalable TS. We provide theoretical guarantees and show that the drastic reduction in computational complexity of scalable TS can be enjoyed without loss in the regret performance over the standard TS. These conceptual claims are validated for practical implementations of scalable TS on synthetic benchmarks and as part of a real-world high-throughput molecular design task.",
    "authors": [
      "Vakili, Sattar",
      "Moss, Henry",
      "Artemev, Artem",
      "Dutordoir, Vincent",
      "Picheny, Victor"
    ]
  },
  {
    "id": "2c8c3a57383c63caef6724343eb62257",
    "title": "Robust Counterfactual Explanations on Graph Neural Networks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2c8c3a57383c63caef6724343eb62257-Paper.pdf",
    "abstract": "Massive deployment of Graph Neural Networks (GNNs) in high-stake applications generates a strong demand for explanations that are robust to noise and align well with human intuition. Most existing methods generate explanations by identifying a subgraph of an input graph that has a strong correlation with the prediction. These explanations are not robust to noise because independently optimizing the correlation for a single input can easily overfit noise. Moreover, they are not counterfactual because removing an identified subgraph from an input graph does not necessarily change the prediction result. In this paper, we propose a novel method to generate robust counterfactual explanations on GNNs by explicitly modelling the common decision logic of GNNs on similar input graphs. Our explanations are naturally robust to noise because they are produced from the common decision boundaries of a GNN that govern the predictions of many similar input graphs. The explanations are also counterfactual because removing the set of edges identified by an explanation from the input graph changes the prediction significantly. Exhaustive experiments on many public datasets demonstrate the superior performance of our method.",
    "authors": [
      "Bajaj, Mohit",
      "Chu, Lingyang",
      "Xue, Zi Yu",
      "Pei, Jian",
      "Wang, Lanjun",
      "Lam, Peter Cho-Ho",
      "Zhang, Yong"
    ]
  },
  {
    "id": "2cb274e6ce940f47beb8011d8ecb1462",
    "title": "Similarity and Matching of Neural Network Representations",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2cb274e6ce940f47beb8011d8ecb1462-Paper.pdf",
    "abstract": "We employ a toolset --- dubbed Dr. Frankenstein --- to analyse the similarity of representations in deep neural networks. With this toolset we aim to match the activations on given layers of two trained neural networks by joining them with a stitching layer. We demonstrate that the inner representations emerging in deep convolutional neural networks with the same architecture but different initialisations can be matched with a surprisingly high degree of accuracy even with a single, affine stitching layer. We choose the stitching layer from several possible classes of linear transformations and investigate their performance and properties. The task of matching representations is closely related to notions of similarity. Using this toolset we also provide a novel viewpoint on the current line of research regarding similarity indices of neural network representations: the perspective of the performance on a task.",
    "authors": [
      "Csisz\u00e1rik, Adri\u00e1n",
      "K\u0151r\u00f6si-Szab\u00f3, P\u00e9ter",
      "Matszangosz, \u00c1kos",
      "Papp, Gergely",
      "Varga, D\u00e1niel"
    ]
  },
  {
    "id": "2cb6b10338a7fc4117a80da24b582060",
    "title": "DOCTOR: A Simple Method for Detecting Misclassification Errors",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2cb6b10338a7fc4117a80da24b582060-Paper.pdf",
    "abstract": "Deep neural networks (DNNs) have shown to perform very well on large scale object recognition problems and lead to widespread use for real-world applications, including situations where DNN are implemented as \u201cblack boxes\u201d.  A promising approach to secure their use is to accept decisions that are likely to be correct while discarding the others.  In this work, we propose DOCTOR, a simple method that aims to identify whether the prediction of a DNN classifier should (or should not) be trusted so that, consequently, it would be possible to accept it or to reject it. Two scenarios are investigated: Totally Black Box (TBB) where only the soft-predictions are available and Partially Black Box (PBB) where gradient-propagation to perform input pre-processing is allowed. Empirically, we show that DOCTOR outperforms all state-of-the-art methods on various well-known images and sentiment analysis datasets. In particular, we observe a reduction of up to 4% of the false rejection rate (FRR) in the PBB scenario. DOCTOR can be applied to any pre-trained model, it does not require prior information about the underlying dataset and is as simple as the simplest available methods in the literature.",
    "authors": [
      "Granese, Federica",
      "Romanelli, Marco",
      "Gorla, Daniele",
      "Palamidessi, Catuscia",
      "Piantanida, Pablo"
    ]
  },
  {
    "id": "2d1b2a5ff364606ff041650887723470",
    "title": "Contrastive Laplacian Eigenmaps",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2d1b2a5ff364606ff041650887723470-Paper.pdf",
    "abstract": "Graph contrastive learning attracts/disperses node representations for similar/dissimilar node pairs under some notion of similarity. It may be combined with a low-dimensional embedding of nodes to preserve intrinsic and structural properties of a graph. In this paper, we extend the celebrated Laplacian Eigenmaps with contrastive learning, and call them COntrastive Laplacian EigenmapS (COLES). Starting from a GAN-inspired contrastive formulation, we show that the Jensen-Shannon divergence underlying many contrastive graph embedding models fails under disjoint positive and negative distributions, which may naturally emerge during sampling in the contrastive setting. In contrast, we demonstrate analytically that COLES essentially minimizes a surrogate of Wasserstein distance, which is known to cope well under disjoint distributions. Moreover, we show that the loss of COLES belongs to the family of so-called block-contrastive losses, previously shown to be superior compared to pair-wise losses typically used by contrastive methods. We show on popular benchmarks/backbones that COLES offers favourable accuracy/scalability compared to DeepWalk, GCN, Graph2Gauss, DGI and GRACE baselines.",
    "authors": [
      "Zhu, Hao",
      "Sun, Ke",
      "Koniusz, Peter"
    ]
  },
  {
    "id": "2d1bcedd27b586d2a9562a0f8e076b41",
    "title": "Machine learning structure preserving brackets for forecasting irreversible processes",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2d1bcedd27b586d2a9562a0f8e076b41-Paper.pdf",
    "abstract": "Forecasting of time-series data requires imposition of inductive biases to obtain predictive extrapolation, and recent works have imposed Hamiltonian/Lagrangian form to preserve structure for systems with \\emph{reversible} dynamics. In this work we present a novel parameterization of dissipative brackets from metriplectic dynamical systems appropriate for learning \\emph{irreversible} dynamics with unknown a priori model form. The process learns generalized Casimirs for energy and entropy guaranteed to be conserved and nondecreasing, respectively. Furthermore, for the case of added thermal noise, we guarantee exact preservation of a fluctuation-dissipation theorem, ensuring thermodynamic consistency. We provide benchmarks for dissipative systems demonstrating learned dynamics are more robust and generalize better than either \"black-box\" or penalty-based approaches.",
    "authors": [
      "Lee, Kookjin",
      "Trask, Nathaniel",
      "Stinis, Panos"
    ]
  },
  {
    "id": "2d290e496d16c9dcaa9b4ded5cac10cc",
    "title": "On the Variance of the Fisher Information for Deep Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2d290e496d16c9dcaa9b4ded5cac10cc-Paper.pdf",
    "abstract": "In the realm of deep learning, the Fisher information matrix (FIM) gives novel insights and useful tools to characterize the loss landscape, perform second-order optimization, and build geometric learning theories. The exact FIM is either unavailable in closed form or too expensive to compute. In practice, it is almost always estimated based on empirical samples.  We investigate two such estimators based on two equivalent representations of the FIM --- both unbiased and consistent. Their estimation quality is naturally gauged by their variance given in closed form. We analyze how the parametric structure of a deep neural network can affect the variance. The meaning of this variance measure and its upper bounds are then discussed in the context of deep learning.",
    "authors": [
      "Soen, Alexander",
      "Sun, Ke"
    ]
  },
  {
    "id": "2d3acd3e240c61820625fff66a19938f",
    "title": "A$^2$-Net: Learning Attribute-Aware Hash Codes for Large-Scale Fine-Grained Image Retrieval",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2d3acd3e240c61820625fff66a19938f-Paper.pdf",
    "abstract": "Our work focuses on tackling large-scale fine-grained image retrieval as ranking the images depicting the concept of interests (i.e., the same sub-category labels) highest based on the fine-grained details in the query. It is desirable to alleviate the challenges of both fine-grained nature of small inter-class variations with large intra-class variations and explosive growth of fine-grained data for such a practical task. In this paper, we propose an Attribute-Aware hashing Network (A$^2$-Net) for generating attribute-aware hash codes to not only make the retrieval process efficient, but also establish explicit correspondences between hash codes and visual attributes. Specifically, based on the captured visual representations by attention, we develop an encoder-decoder structure network of a reconstruction task to unsupervisedly distill high-level attribute-specific vectors from the appearance-specific visual representations without attribute annotations. A$^2$-Net is also equipped with a feature decorrelation constraint upon these attribute vectors to enhance their representation abilities. Finally, the required hash codes are generated by the attribute vectors driven by preserving original similarities. Qualitative experiments on five benchmark fine-grained datasets show our superiority over competing methods. More importantly, quantitative results demonstrate the obtained hash codes can strongly correspond to certain kinds of crucial properties of fine-grained objects.",
    "authors": [
      "Wei, Xiu-Shen",
      "Shen, Yang",
      "Sun, Xuhao",
      "Ye, Han-Jia",
      "Yang, Jian"
    ]
  },
  {
    "id": "2d3d9d5373f378108cdbd30a3c52bd3e",
    "title": "Shape Registration in the Time of Transformers",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2d3d9d5373f378108cdbd30a3c52bd3e-Paper.pdf",
    "abstract": "In this paper, we propose a transformer-based procedure for the efficient registration of non-rigid 3D point clouds. The proposed approach is data-driven and adopts for the first time the transformers architecture in the registration task. Our method is general and applies to different settings. Given a fixed template with some desired properties (e.g. skinning weights or other animation cues), we can register raw acquired data to it, thereby transferring all the template properties to the input geometry. Alternatively, given a pair of shapes, our method can register the first onto the second (or vice-versa), obtaining a high-quality dense correspondence between the two.In both contexts, the quality of our results enables us to target real applications such as texture transfer and shape interpolation.Furthermore, we also show that including an estimation of the underlying density of the surface eases the learning process. By exploiting the potential of this architecture, we can train our model requiring only a sparse set of ground truth correspondences ($10\\sim20\\%$ of the total points). The proposed model and the analysis that we perform pave the way for future exploration of transformer-based architectures for registration and matching applications. Qualitative and quantitative evaluations demonstrate that our pipeline outperforms state-of-the-art methods for deformable and unordered 3D data registration on different datasets and scenarios.",
    "authors": [
      "Trappolini, Giovanni",
      "Cosmo, Luca",
      "Moschella, Luca",
      "Marin, Riccardo",
      "Melzi, Simone",
      "Rodol\u00e0, Emanuele"
    ]
  },
  {
    "id": "2d4027d6df9c0256b8d4474ce88f8c88",
    "title": "Brick-by-Brick: Combinatorial Construction with Deep Reinforcement Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2d4027d6df9c0256b8d4474ce88f8c88-Paper.pdf",
    "abstract": "Discovering a solution in a combinatorial space is prevalent in many real-world problems but it is also challenging due to diverse complex constraints and the vast number of possible combinations. To address such a problem, we introduce a novel formulation, combinatorial construction, which requires a building agent to assemble unit primitives (i.e., LEGO bricks) sequentially -- every connection between two bricks must follow a fixed rule, while no bricks mutually overlap. To construct a target object, we provide incomplete knowledge about the desired target (i.e., 2D images) instead of exact and explicit volumetric information to the agent. This problem requires a comprehensive understanding of partial information and long-term planning to append a brick sequentially, which leads us to employ reinforcement learning. The approach has to consider a variable-sized action space where a large number of invalid actions, which would cause overlap between bricks, exist. To resolve these issues, our model, dubbed Brick-by-Brick, adopts an action validity prediction network that efficiently filters invalid actions for an actor-critic network. We demonstrate that the proposed method successfully learns to construct an unseen object conditioned on a single image or multiple views of a target object.",
    "authors": [
      "Chung, Hyunsoo",
      "Kim, Jungtaek",
      "Knyazev, Boris",
      "Lee, Jinhwi",
      "Taylor, Graham W.",
      "Park, Jaesik",
      "Cho, Minsu"
    ]
  },
  {
    "id": "2d95666e2649fcfc6e3af75e09f5adb9",
    "title": "Dissecting the Diffusion Process in Linear Graph Convolutional Networks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2d95666e2649fcfc6e3af75e09f5adb9-Paper.pdf",
    "abstract": "Graph Convolutional Networks (GCNs) have attracted more and more attentions in recent years. A typical GCN layer consists of a linear feature propagation step and a nonlinear transformation step. Recent works show that a linear GCN can achieve comparable performance to the original non-linear GCN while being much more computationally efficient. In this paper, we dissect the feature propagation steps of linear GCNs from a perspective of continuous graph diffusion, and analyze why linear GCNs fail to benefit from more propagation steps. Following that, we propose Decoupled Graph Convolution (DGC) that decouples the terminal time and the feature propagation steps, making it more flexible and capable of exploiting a very large number of feature propagation steps. Experiments demonstrate that our proposed DGC improves linear GCNs by a large margin and makes them competitive with many modern variants of non-linear GCNs.",
    "authors": [
      "Wang, Yifei",
      "Wang, Yisen",
      "Yang, Jiansheng",
      "Lin, Zhouchen"
    ]
  },
  {
    "id": "2d969e2cee8cfa07ce7ca0bb13c7a36d",
    "title": "Dynamic Grained Encoder for Vision Transformers",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2d969e2cee8cfa07ce7ca0bb13c7a36d-Paper.pdf",
    "abstract": "Transformers, the de-facto standard for language modeling, have been recently applied for vision tasks. This paper introduces sparse queries for vision transformers to exploit the intrinsic spatial redundancy of natural images and save computational costs. Specifically, we propose a Dynamic Grained Encoder for vision transformers, which can adaptively assign a suitable number of queries to each spatial region. Thus it achieves a fine-grained representation in discriminative regions while keeping high efficiency. Besides, the dynamic grained encoder is compatible with most vision transformer frameworks. Without bells and whistles, our encoder allows the state-of-the-art vision transformers to reduce computational complexity by 40%-60% while maintaining comparable performance on image classification. Extensive experiments on object detection and segmentation further demonstrate the generalizability of our approach. Code is available at https://github.com/StevenGrove/vtpack.",
    "authors": [
      "Song, Lin",
      "Zhang, Songyang",
      "Liu, Songtao",
      "Li, Zeming",
      "He, Xuming",
      "Sun, Hongbin",
      "Sun, Jian",
      "Zheng, Nanning"
    ]
  },
  {
    "id": "2dace78f80bc92e6d7493423d729448e",
    "title": "Understanding Negative Samples in Instance Discriminative Self-supervised Representation Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2dace78f80bc92e6d7493423d729448e-Paper.pdf",
    "abstract": "Instance discriminative self-supervised representation learning has been attracted attention thanks to its unsupervised nature and informative feature representation for downstream tasks. In practice, it commonly uses a larger number of negative samples than the number of supervised classes. However, there is an inconsistency in the existing analysis; theoretically, a large number of negative samples degrade classification performance on a downstream supervised task, while empirically, they improve the performance. We provide a novel framework to analyze this empirical result regarding negative samples using the coupon collector's problem. Our bound can implicitly incorporate the supervised loss of the downstream task in the self-supervised loss by increasing the number of negative samples. We confirm that our proposed analysis holds on real-world benchmark datasets.",
    "authors": [
      "Nozawa, Kento",
      "Sato, Issei"
    ]
  },
  {
    "id": "2de5d16682c3c35007e4e92982f1a2ba",
    "title": "On UMAP's True Loss Function",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2de5d16682c3c35007e4e92982f1a2ba-Paper.pdf",
    "abstract": "UMAP has supplanted $t$-SNE as state-of-the-art for visualizing high-dimensional datasets in many disciplines, but the reason for its success is not well understood. In this work, we investigate UMAP's sampling based optimization scheme in detail. We derive UMAP's true loss function in closed form and find that it differs from the published one in a dataset size dependent way. As a consequence, we show that UMAP does not aim to reproduce its theoretically motivated high-dimensional UMAP similarities. Instead, it tries to reproduce  similarities that only encode the $k$ nearest neighbor graph, thereby challenging the previous understanding of UMAP's effectiveness. Alternatively, we consider the implicit balancing of attraction and repulsion due to the negative sampling to be key to UMAP's success. We corroborate our theoretical findings on toy and single cell RNA sequencing data.",
    "authors": [
      "Damrich, Sebastian",
      "Hamprecht, Fred A."
    ]
  },
  {
    "id": "2dffbc474aa176b6dc957938c15d0c8b",
    "title": "Fast Pure Exploration via Frank-Wolfe",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2dffbc474aa176b6dc957938c15d0c8b-Paper.pdf",
    "abstract": "We study the problem of active pure exploration with fixed confidence in generic stochastic bandit environments. The goal of the learner is to answer a query about the environment with a given level of certainty while minimizing her sampling budget. For this problem, instance-specific lower bounds on the expected sample complexity reveal the optimal proportions of arm draws an Oracle algorithm would apply. These proportions solve an optimization problem whose tractability strongly depends on the structural properties of the environment, but may be instrumental in the design of efficient learning algorithms. We devise Frank-Wolfe-based Sampling (FWS), a simple algorithm whose sample complexity matches the lower bounds for a wide class of pure exploration problems. The algorithm is computationally efficient as, to learn and track the optimal proportion of arm draws, it relies on a single iteration of Frank-Wolfe algorithm applied to the lower-bound optimization problem. We apply FWS to various pure exploration tasks, including best arm identification in unstructured, thresholded, linear, and Lipschitz bandits. Despite its simplicity, FWS is competitive compared to state-of-art algorithms.",
    "authors": [
      "Wang, Po-An",
      "Tzeng, Ruo-Chun",
      "Proutiere, Alexandre"
    ]
  },
  {
    "id": "2e3d2c4f33a7a1f58bc6c81cacd21e9c",
    "title": "iFlow: Numerically Invertible Flows for Efficient Lossless Compression via a Uniform Coder",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2e3d2c4f33a7a1f58bc6c81cacd21e9c-Paper.pdf",
    "abstract": "It was estimated that the world produced $59 ZB$ ($5.9 \\times 10^{13} GB$) of data in 2020, resulting in the enormous costs of both data storage and transmission. Fortunately, recent advances in deep generative models have spearheaded a new class of so-called \"neural compression\" algorithms, which significantly outperform traditional codecs in terms of compression ratio. Unfortunately, the application of neural compression garners little commercial interest due to its limited bandwidth; therefore, developing highly efficient frameworks is of critical practical importance. In this paper, we discuss lossless compression using normalizing flows which have demonstrated a great capacity for achieving high compression ratios. As such, we introduce iFlow, a new method for achieving efficient lossless compression. We first propose Modular Scale Transform (MST) and a novel family of numerically invertible flow transformations based on MST. Then we introduce the Uniform Base Conversion System (UBCS), a fast uniform-distribution codec incorporated into iFlow, enabling efficient compression. iFlow achieves state-of-the-art compression ratios and is $5 \\times$ quicker than other high-performance schemes. Furthermore, the techniques presented in this paper can be used to accelerate coding time for a broad class of flow-based algorithms. ",
    "authors": [
      "Zhang, Shifeng",
      "Kang, Ning",
      "Ryder, Tom",
      "Li, Zhenguo"
    ]
  },
  {
    "id": "2e5c2cb8d13e8fba78d95211440ba326",
    "title": "History Aware Multimodal Transformer for Vision-and-Language Navigation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2e5c2cb8d13e8fba78d95211440ba326-Paper.pdf",
    "abstract": "Vision-and-language navigation (VLN) aims to build autonomous visual agents that follow instructions and navigate in real scenes. To remember previously visited locations and actions taken, most approaches to VLN implement memory using recurrent states. Instead, we introduce a History Aware Multimodal Transformer (HAMT) to incorporate a long-horizon history into multimodal decision making. HAMT efficiently encodes all the past panoramic observations via a hierarchical vision transformer (ViT), which first encodes individual images with ViT, then models spatial relation between images in a panoramic observation and finally takes into account temporal relation between panoramas in the history. It, then, jointly combines text, history and current observation to predict the next action. We first train HAMT end-to-end using several proxy tasks including single step action prediction and spatial relation prediction, and then use reinforcement learning to further improve the navigation policy. HAMT achieves new state of the art on a broad range of VLN tasks, including VLN with fine-grained instructions (R2R, RxR), high-level instructions (R2R-Last, REVERIE), dialogs (CVDN) as well as long-horizon VLN (R4R, R2R-Back). We demonstrate HAMT to be particularly effective for navigation tasks with longer trajectories. ",
    "authors": [
      "Chen, Shizhe",
      "Guhur, Pierre-Louis",
      "Schmid, Cordelia",
      "Laptev, Ivan"
    ]
  },
  {
    "id": "2e6d9c6052e99fcdfa61d9b9da273ca2",
    "title": "Meta Two-Sample Testing: Learning Kernels for Testing with Limited Data ",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2e6d9c6052e99fcdfa61d9b9da273ca2-Paper.pdf",
    "abstract": "Modern kernel-based two-sample tests have shown great success in distinguishing complex, high-dimensional distributions by learning appropriate kernels (or, as a special case, classifiers). Previous work, however, has assumed that many samples are observed from both of the distributions being distinguished. In realistic scenarios with very limited numbers of data samples, it can be challenging to identify a kernel powerful enough to distinguish complex distributions. We address this issue by introducing the problem of meta two-sample testing (M2ST), which aims to exploit (abundant) auxiliary data on related tasks to find an algorithm that can quickly identify a powerful test on new target tasks. We propose two specific algorithms for this task: a generic scheme which improves over baselines, and a more tailored approach which performs even better. We provide both theoretical justification and empirical evidence that our proposed meta-testing schemes outperform learning kernel-based tests directly from scarce observations, and identify when such schemes will be successful.",
    "authors": [
      "Liu, Feng",
      "Xu, Wenkai",
      "Lu, Jie",
      "Sutherland, Danica J."
    ]
  },
  {
    "id": "2e855f9489df0712b4bd8ea9e2848c5a",
    "title": "Process for Adapting Language Models to Society (PALMS) with Values-Targeted Datasets",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2e855f9489df0712b4bd8ea9e2848c5a-Paper.pdf",
    "abstract": "Language models can generate harmful and biased outputs and exhibit undesirable behavior according to a given cultural context. We propose a Process for Adapting Language Models to Society (PALMS) with Values-Targeted Datasets, an iterative process to significantly change model behavior by crafting and fine-tuning on a dataset that reflects a predetermined set of target values. We evaluate our process using three metrics: quantitative metrics with human evaluations that score output adherence to a target value, toxicity scoring on outputs; and qualitative metrics analyzing the most common word associated with a given social category. Through each iteration, we add additional training dataset examples based on observed shortcomings from evaluations. PALMS performs significantly better on all metrics compared to baseline and control models for a broad range of GPT-3 language model sizes without compromising capability integrity. We find that the effectiveness of PALMS increases with model size. We show that significantly adjusting language model behavior is feasible with a small, hand-curated dataset.",
    "authors": [
      "Solaiman, Irene",
      "Dennison, Christy"
    ]
  },
  {
    "id": "2e907f44e0a9616314cf3d964d4e3c93",
    "title": "The Lazy Online Subgradient Algorithm is Universal on Strongly Convex Domains",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2e907f44e0a9616314cf3d964d4e3c93-Paper.pdf",
    "abstract": "We study Online Lazy Gradient Descent for optimisation on a strongly convex domain. The algorithm is known to achieve  $O(\\sqrt N)$ regret against adversarial opponents; here we show it is universal in the sense that it also achieves $O(\\log N)$ expected regret against i.i.d opponents. This improves upon the more complex meta-algorithm of Huang et al \\cite{FTLBall} that only gets $O(\\sqrt {N \\log N})$ and $ O(\\log N)$ bounds. In addition  we show that, unlike  for the simplex, order bounds for pseudo-regret and expected regret are equivalent for strongly convex domains.",
    "authors": [
      "Anderson, Daron",
      "Leith, Douglas"
    ]
  },
  {
    "id": "2e92962c0b6996add9517e4242ea9bdc",
    "title": "Computer-Aided Design as Language",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2e92962c0b6996add9517e4242ea9bdc-Paper.pdf",
    "abstract": "Computer-Aided Design (CAD) applications are used in manufacturing to model everything from coffee mugs to sports cars. These programs are complex and require years of training and experience to master. A component of all CAD models particularly difficult to make are the highly structured 2D sketches that lie at the heart of every 3D construction. In this work, we propose a machine learning model capable of automatically generating such sketches. Through this, we pave the way for developing intelligent tools that would help engineers create better designs with less effort. The core of our method is a combination of a general-purpose language modeling technique alongside an off-the-shelf data serialization protocol. Additionally, we explore several extensions allowing us to gain finer control over the generation process. We show that our approach has enough flexibility to accommodate the complexity of the domain and performs well for both unconditional synthesis and image-to-sketch translation.",
    "authors": [
      "Ganin, Yaroslav",
      "Bartunov, Sergey",
      "Li, Yujia",
      "Keller, Ethan",
      "Saliceti, Stefano"
    ]
  },
  {
    "id": "2e976ab88a42d723d9f2ee6027b707f5",
    "title": "COHESIV: Contrastive Object and Hand Embedding Segmentation In Video",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2e976ab88a42d723d9f2ee6027b707f5-Paper.pdf",
    "abstract": "In this paper we learn to segment hands and hand-held objects from motion. Our system takes a single RGB image and hand location as input to segment the hand and hand-held object. For learning, we generate responsibility maps that show how well a hand's motion explains other pixels' motion in video. We use these responsibility maps as pseudo-labels to train a weakly-supervised neural network using an attention-based similarity loss and contrastive loss. Our system outperforms alternate methods, achieving good performance on the 100DOH, EPIC-KITCHENS, and HO3D datasets.",
    "authors": [
      "Shan, Dandan",
      "Higgins, Richard",
      "Fouhey, David"
    ]
  },
  {
    "id": "2e9f978b222a956ba6bdf427efbd9ab3",
    "title": "ByPE-VAE: Bayesian Pseudocoresets Exemplar VAE",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2e9f978b222a956ba6bdf427efbd9ab3-Paper.pdf",
    "abstract": "Recent studies show that advanced priors play a major role in deep generative models. Exemplar VAE, as a variant of VAE with an exemplar-based prior, has achieved impressive results. However, due to the nature of model design, an exemplar-based model usually requires vast amounts of data to participate in training, which leads to huge computational complexity. To address this issue, we propose Bayesian Pseudocoresets Exemplar VAE (ByPE-VAE), a new variant of VAE with a prior based on Bayesian pseudocoreset. The proposed prior is conditioned on a small-scale pseudocoreset rather than the whole dataset for reducing the computational cost and avoiding overfitting. Simultaneously, we obtain the optimal pseudocoreset via a stochastic optimization algorithm during VAE training aiming to minimize the Kullback-Leibler divergence between the prior based on the pseudocoreset and that based on the whole dataset. Experimental results show that ByPE-VAE can achieve competitive improvements over the state-of-the-art VAEs in the tasks of density estimation, representation learning, and generative data augmentation. Particularly, on a basic VAE architecture, ByPE-VAE is up to 3 times faster than Exemplar VAE while almost holding the performance. Code is available at \\url{https://github.com/Aiqz/ByPE-VAE}.",
    "authors": [
      "Ai, Qingzhong",
      "HE, LIRONG",
      "LIU, SHIYU",
      "Xu, Zenglin"
    ]
  },
  {
    "id": "2ea1202aed1e0ce30d41be4919b0cc99",
    "title": "Recovery Analysis for Plug-and-Play Priors using the Restricted Eigenvalue Condition",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2ea1202aed1e0ce30d41be4919b0cc99-Paper.pdf",
    "abstract": "The plug-and-play priors (PnP) and regularization by denoising (RED) methods have become widely used for solving inverse problems by leveraging pre-trained deep denoisers as image priors.  While the empirical imaging performance and the theoretical convergence properties of these algorithms have been widely investigated, their recovery properties have not previously been theoretically analyzed.  We address this gap by showing how to establish theoretical recovery guarantees for PnP/RED by assuming that the solution of these methods lies near the fixed-points of a deep neural network. We also present numerical results comparing the recovery performance of PnP/RED in compressive sensing against that of recent compressive sensing algorithms based on generative models. Our numerical results suggest that PnP with a pre-trained artifact removal network provides significantly better results compared to the existing state-of-the-art methods.",
    "authors": [
      "Liu, Jiaming",
      "Asif, Salman",
      "Wohlberg, Brendt",
      "Kamilov, Ulugbek"
    ]
  },
  {
    "id": "2ea6241cf767c279cf1e80a790df1885",
    "title": "Group Equivariant Subsampling",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2ea6241cf767c279cf1e80a790df1885-Paper.pdf",
    "abstract": "Subsampling is used in convolutional neural networks (CNNs) in the form of pooling or strided convolutions, to reduce the spatial dimensions of feature maps and to allow the receptive fields to grow exponentially with depth. However, it is known that such subsampling operations are not translation equivariant, unlike convolutions that are translation equivariant. Here, we first introduce translation equivariant subsampling/upsampling layers that can be used to construct exact translation equivariant CNNs. We then generalise these layers beyond translations to general groups, thus proposing group equivariant subsampling/upsampling. We use these layers to construct group equivariant autoencoders (GAEs) that allow us to learn low-dimensional equivariant representations. We empirically verify on images that the representations are indeed equivariant to input translations and rotations, and thus generalise well to unseen positions and orientations. We further use GAEs in models that learn object-centric representations on multi-object datasets, and show improved data efficiency and decomposition compared to non-equivariant baselines.",
    "authors": [
      "Xu, Jin",
      "Kim, Hyunjik",
      "Rainforth, Thomas",
      "Teh, Yee"
    ]
  },
  {
    "id": "2eb5657d37f474e4c4cf01e4882b8962",
    "title": "Data Sharing and Compression for Cooperative Networked Control",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2eb5657d37f474e4c4cf01e4882b8962-Paper.pdf",
    "abstract": "Sharing forecasts of network timeseries data, such as cellular or electricity load patterns, can improve independent control applications ranging from traffic scheduling to power generation. Typically, forecasts are designed without knowledge of a downstream controller's task objective, and thus simply optimize for mean prediction error. However, such task-agnostic representations are often too large to stream over a communication network and do not emphasize salient temporal features for cooperative control. This paper presents a solution to learn succinct, highly-compressed forecasts that are co-designed with a modular controller's task objective. Our simulations with real cellular, Internet-of-Things (IoT), and electricity load data show we can improve a model predictive controller's performance by at least 25% while transmitting 80% less data than the competing method. Further, we present theoretical compression results for a networked variant of the classical linear quadratic regulator (LQR) control problem.",
    "authors": [
      "Cheng, Jiangnan",
      "Pavone, Marco",
      "Katti, Sachin",
      "Chinchali, Sandeep",
      "Tang, Ao"
    ]
  },
  {
    "id": "2ed80f6311c1825feb854d78fa969d34",
    "title": "Hyperbolic Procrustes Analysis Using Riemannian Geometry",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2ed80f6311c1825feb854d78fa969d34-Paper.pdf",
    "abstract": "Label-free alignment between datasets collected at different times, locations, or by different instruments is a fundamental scientific task. Hyperbolic spaces have recently provided a fruitful foundation for the development of informative representations of hierarchical data. Here, we take a purely geometric approach for label-free alignment of hierarchical datasets and introduce hyperbolic Procrustes analysis (HPA). HPA consists of new implementations of the three prototypical Procrustes analysis components: translation, scaling, and rotation, based on the Riemannian geometry of the Lorentz model of hyperbolic space. We analyze the proposed components, highlighting their useful properties for alignment. The efficacy of HPA, its theoretical properties, stability and computational efficiency are demonstrated in simulations. In addition, we showcase its performance on three batch correction tasks involving gene expression and mass cytometry data. Specifically, we demonstrate high-quality unsupervised batch effect removal from data acquired at different sites and with different technologies that outperforms recent methods for label-free alignment in hyperbolic spaces.",
    "authors": [
      "Lin, Ya-Wei Eileen",
      "Kluger, Yuval",
      "Talmon, Ronen"
    ]
  },
  {
    "id": "2f2b265625d76a6704b08093c652fd79",
    "title": "No Fear of Heterogeneity: Classifier Calibration for Federated Learning with Non-IID Data",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2f2b265625d76a6704b08093c652fd79-Paper.pdf",
    "abstract": "A central challenge in training classification models in the real-world federated system is learning with non-IID data. To cope with this, most of the existing works involve enforcing regularization in local optimization or improving the model aggregation scheme at the server. Other works also share public datasets or synthesized samples to supplement the training of under-represented classes or introduce a certain level of personalization. Though effective, they lack a deep understanding of how the data heterogeneity affects each layer of a deep classification model. In this paper, we bridge this gap by performing an experimental analysis of the representations learned by different layers. Our observations are surprising: (1) there exists a greater bias in the classifier than other layers, and (2) the classification performance can be significantly improved by post-calibrating the classifier after federated training. Motivated by the above findings, we propose a novel and simple algorithm called Classifier Calibration with Virtual Representations (CCVR), which adjusts the classifier using virtual representations sampled from an approximated gaussian mixture model. Experimental results demonstrate that CCVR achieves state-of-the-art performance on popular federated learning benchmarks including CIFAR-10, CIFAR-100, and CINIC-10. We hope that our simple yet effective method can shed some light on the future research of federated learning with non-IID data. ",
    "authors": [
      "Luo, Mi",
      "Chen, Fei",
      "Hu, Dapeng",
      "Zhang, Yifan",
      "Liang, Jian",
      "Feng, Jiashi"
    ]
  },
  {
    "id": "2f2cd5c753d3cee48e47dbb5bbaed331",
    "title": "Preconditioned Gradient Descent for Over-Parameterized Nonconvex Matrix Factorization",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2f2cd5c753d3cee48e47dbb5bbaed331-Paper.pdf",
    "abstract": "In practical instances of nonconvex matrix factorization, the rank of the true solution $r^{\\star}$ is often unknown, so the rank $r$of the model can be over-specified as $r>r^{\\star}$. This over-parameterized regime of matrix factorization significantly slows down the convergence of local search algorithms, from a linear rate with $r=r^{\\star}$ to a sublinear rate when $r>r^{\\star}$. We propose an inexpensive preconditioner for the matrix sensing variant of nonconvex matrix factorization that restores the convergence rate of gradient descent back to linear, even in the over-parameterized case, while also making it agnostic to possible ill-conditioning in the ground truth. Classical gradient descent in a neighborhood of the solution slows down due to the need for the model matrix factor to become singular. Our key result is that this singularity can be corrected by $\\ell_{2}$ regularization with a specific range of values for the damping parameter. In fact, a good damping parameter can be inexpensively estimated from the current iterate. The resulting algorithm, which we call preconditioned gradient descent or PrecGD, is stable under noise, and converges linearly to an information theoretically optimal error bound. Our numerical experiments find that PrecGD works equally well in restoring the linear convergence of other variants of nonconvex matrix factorization in the over-parameterized regime.",
    "authors": [
      "Zhang, Jialun",
      "Fattahi, Salar",
      "Zhang, Richard Y"
    ]
  },
  {
    "id": "2f37d10131f2a483a8dd005b3d14b0d9",
    "title": "Improving Contrastive Learning on Imbalanced Data via Open-World Sampling",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2f37d10131f2a483a8dd005b3d14b0d9-Paper.pdf",
    "abstract": "Contrastive learning approaches have achieved great success in learning visual representations with few labels of the target classes. That implies a tantalizing possibility of scaling them up beyond a curated \u201cseed\" benchmark, to incorporating more unlabeled images from the internet-scale external sources to enhance its performance. However, in practice, larger amount of unlabeled data will require more computing resources due to the bigger model size and longer training needed. Moreover, open-world unlabeled data usually follows an implicit long-tail class or attribute distribution, many of which also do not belong to the target classes. Blindly leveraging all unlabeled data hence can lead to the data imbalance as well as distraction issues. This motivates us to seek a principled approach to strategically select unlabeled data from an external source, in order to learn generalizable, balanced and diverse representations for relevant classes. In this work, we present an open-world unlabeled data sampling framework called Model-Aware K-center (MAK), which follows three simple principles: (1) tailness, which encourages sampling of examples from tail classes, by sorting the empirical contrastive loss expectation (ECLE) of samples over random data augmentations; (2) proximity, which rejects the out-of-distribution outliers that may distract training; and (3) diversity, which ensures diversity in the set of sampled examples. Empirically, using ImageNet-100-LT (without labels) as the seed dataset and two \u201cnoisy\u201d external data sources, we demonstrate that MAK can consistently improve both the overall representation quality and the class balancedness of the learned features, as evaluated via linear classifier evaluation on full-shot and few-shot settings. Thecode is available at: https://github.com/VITA-Group/MAK.",
    "authors": [
      "Jiang, Ziyu",
      "Chen, Tianlong",
      "Chen, Ting",
      "Wang, Zhangyang"
    ]
  },
  {
    "id": "2f3c6a4cd8af177f6456e7e51a916ff3",
    "title": "Searching for Efficient Transformers for Language Modeling",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2f3c6a4cd8af177f6456e7e51a916ff3-Paper.pdf",
    "abstract": "Large Transformer models have been central to recent advances in natural language processing. The training and inference costs of these models, however, have grown rapidly and become prohibitively expensive. Here we aim to reduce the costs of Transformers by searching for a more efficient variant. Compared to previous approaches, our search is performed at a lower level, over the primitives that define a Transformer TensorFlow program. We identify an architecture, named Primer, that has a smaller training cost than the original Transformer and other variants for auto-regressive language modeling. Primer\u2019s improvements can be mostly attributed to two simple modifications: squaring ReLU activations and adding a depthwise convolution layer after each Q, K, and V projection in self-attention.Experiments show Primer\u2019s gains over Transformer increase as compute scale grows and follow a power law with respect to quality at optimal model sizes. We also verify empirically that Primer can be dropped into different codebases to significantly speed up training without additional tuning. For example, at a 500M parameter size, Primer improves the original T5 architecture on C4 auto-regressive language modeling, reducing the training cost by 4X. Furthermore, the reduced training cost means Primer needs much less compute to reach a target one-shot performance. For instance, in a 1.9B parameter configuration similar to GPT-3 XL, Primer uses 1/3 of the training compute to achieve the same one-shot performance as Transformer. We open source our models and several comparisons in T5 to help with reproducibility.",
    "authors": [
      "So, David",
      "Ma\u0144ke, Wojciech",
      "Liu, Hanxiao",
      "Dai, Zihang",
      "Shazeer, Noam",
      "Le, Quoc V"
    ]
  },
  {
    "id": "2f4ccb0f7a84f335affb418aee08a6df",
    "title": "Scaling Ensemble Distribution Distillation to Many Classes with Proxy Targets",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2f4ccb0f7a84f335affb418aee08a6df-Paper.pdf",
    "abstract": "Ensembles of machine learning models yield improved system performance as well as robust and interpretable uncertainty estimates; however, their inference costs can be prohibitively high. Ensemble Distribution Distillation (EnD$^2$) is an approach that allows a single model to efficiently capture both the predictive performance and uncertainty estimates of an ensemble. For classification, this is achieved by training a Dirichlet distribution over the ensemble members' output distributions via the maximum likelihood criterion. Although theoretically principled, this work shows that the criterion exhibits poor convergence when applied to large-scale tasks where the number of classes is very high. Specifically, we show that for the Dirichlet log-likelihood criterion classes with low probability induce larger gradients than high-probability classes. Hence during training the model focuses on the distribution of the ensemble tail-class probabilities rather than the probability of the correct and closely related classes. We propose a new training objective which minimizes the reverse KL-divergence to a \\emph{Proxy-Dirichlet} target derived from the ensemble. This loss resolves the gradient issues of EnD$^2$, as we demonstrate both theoretically and empirically on the ImageNet, LibriSpeech, and WMT17 En-De datasets containing 1000, 5000, and 40,000 classes, respectively.",
    "authors": [
      "Ryabinin, Max",
      "Malinin, Andrey",
      "Gales, Mark"
    ]
  },
  {
    "id": "2fd5d41ec6cfab47e32164d5624269b1",
    "title": "Multi-Person 3D Motion Prediction with Multi-Range Transformers",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/2fd5d41ec6cfab47e32164d5624269b1-Paper.pdf",
    "abstract": "We propose a novel framework for multi-person 3D motion trajectory prediction. Our key observation is that a human's action and behaviors may highly depend on the other persons around. Thus, instead of predicting each human pose trajectory in isolation, we introduce a Multi-Range Transformers model which contains of a local-range encoder for individual motion and a global-range encoder for social interactions. The Transformer decoder then performs prediction for each person by taking a corresponding pose as a query which attends to both local and global-range encoder features. Our model not only outperforms state-of-the-art methods on long-term 3D motion prediction, but also generates diverse social interactions. More interestingly, our model can even predict 15-person motion simultaneously by automatically dividing the persons into different interaction groups.  Project page with code is available at https://jiashunwang.github.io/MRT/.",
    "authors": [
      "Wang, Jiashun",
      "Xu, Huazhe",
      "Narasimhan, Medhini",
      "Wang, Xiaolong"
    ]
  },
  {
    "id": "3016a447172f3045b65f5fc83e04b554",
    "title": "STEM: A Stochastic Two-Sided Momentum Algorithm Achieving Near-Optimal Sample and Communication Complexities for Federated Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3016a447172f3045b65f5fc83e04b554-Paper.pdf",
    "abstract": "Federated Learning (FL) refers to the paradigm where multiple worker nodes (WNs) build a joint model by using local data. Despite extensive research, for a generic non-convex FL problem, it is not clear, how to choose the WNs' and the server's update directions, the minibatch sizes, and the local update frequency, so that the WNs use the minimum number of samples and communication rounds to achieve the desired solution. This work addresses the above question and considers a class of stochastic algorithms where the WNs perform a few local updates before communication. We show that when both the WN's and the server's directions are chosen based on certain stochastic momentum estimator, the algorithm requires $\\tilde{\\mathcal{O}}(\\epsilon^{-3/2})$ samples and $\\tilde{\\mathcal{O}}(\\epsilon^{-1})$ communication rounds to compute an $\\epsilon$-stationary solution. To the best of our knowledge, this is the first FL algorithm that achieves such {\\it near-optimal} sample and communication complexities simultaneously.  Further, we show that there is a trade-off curve between local update frequencies and local minibatch sizes, on which the above sample and communication complexities can be maintained. {Finally,   we show that for the classical FedAvg (a.k.a. Local SGD, which is a momentum-less special case of the STEM), a similar trade-off curve exists, albeit with worse sample and communication complexities. Our insights on this trade-off provides guidelines for choosing the four important design elements for FL algorithms, the update frequency, directions, and minibatch sizes to achieve the best performance.} ",
    "authors": [
      "Khanduri, Prashant",
      "SHARMA, PRANAY",
      "Yang, Haibo",
      "Hong, Mingyi",
      "Liu, Jia",
      "Rajawat, Ketan",
      "Varshney, Pramod"
    ]
  },
  {
    "id": "307eb8ee16198da891c521eca21464c1",
    "title": "Bubblewrap: Online tiling and real-time flow prediction on neural manifolds",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/307eb8ee16198da891c521eca21464c1-Paper.pdf",
    "abstract": "While most classic studies of function in experimental neuroscience have focused on the coding properties of individual neurons, recent developments in recording technologies have resulted in an increasing emphasis on the dynamics of neural populations. This has given rise to a wide variety of models for analyzing population activity in relation to experimental variables, but direct testing of many neural population hypotheses requires intervening in the system based on current neural state, necessitating models capable of inferring neural state online. Existing approaches, primarily based on dynamical systems, require strong parametric assumptions that are easily violated in the noise-dominated regime and do not scale well to the thousands of data channels in modern experiments. To address this problem, we propose a method that combines fast, stable dimensionality reduction with a soft tiling of the resulting neural manifold, allowing dynamics to be approximated as a probability flow between tiles. This method can be fit efficiently using online expectation maximization, scales to tens of thousands of tiles, and outperforms existing methods when dynamics are noise-dominated or feature multi-modal transition probabilities. The resulting model can be trained at kiloHertz data rates, produces accurate approximations of neural dynamics within minutes, and generates predictions on submillisecond time scales. It retains predictive performance throughout many time steps into the future and is fast enough to serve as a component of closed-loop causal experiments.",
    "authors": [
      "Draelos, Anne",
      "Gupta, Pranjal",
      "Jun, Na Young",
      "Sriworarat, Chaichontat",
      "Pearson, John"
    ]
  },
  {
    "id": "3083202a936b7d0ef8b680d7ae73fa1a",
    "title": "The Semi-Random Satisfaction of Voting Axioms",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3083202a936b7d0ef8b680d7ae73fa1a-Paper.pdf",
    "abstract": "We initiate the work towards a comprehensive picture of the worst average-case satisfaction of  voting axioms in semi-random models, to provide a finer and more realistic foundation for comparing voting rules. We adopt the semi-random model and formulation in [Xia 2020],  where an adversary chooses arbitrarily correlated ``ground truth'' preferences for the agents, on top of which random noises are added. We  focus on characterizing the semi-random satisfaction of two well-studied voting axioms:  Condorcet criterion and participation. We prove that  for any fixed number of alternatives, when the number of voters $n$ is sufficiently large, the semi-random satisfaction of the Condorcet criterion under a wide range of voting rules is $1$, $1-\\exp(-\\Theta(n))$, $\\Theta(n^{-0.5})$, $ \\exp(-\\Theta(n))$, or being $\\Theta(1)$ and $1-\\Theta(1)$ at the same time; and the semi-random satisfaction of participation is  $1-\\Theta(n^{-0.5})$.   Our results  address  open questions by Berg and Lepelley in 1994, and also  confirm the following high-level message: the Condorcet criterion is a bigger concern than participation under realistic models. ",
    "authors": [
      "Xia, Lirong"
    ]
  },
  {
    "id": "30a237d18c50f563cba4531f1db44acf",
    "title": "Deep Marching Tetrahedra: a Hybrid Representation for High-Resolution 3D Shape Synthesis",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/30a237d18c50f563cba4531f1db44acf-Paper.pdf",
    "abstract": "We introduce DMTet, a deep 3D conditional generative model that can synthesize high-resolution 3D shapes using simple user guides such as coarse voxels. It marries the merits of implicit and explicit 3D representations by leveraging a novel hybrid 3D representation. Compared to the current implicit approaches, which are trained to regress the signed distance values, DMTet directly optimizes for the reconstructed surface, which enables us to synthesize finer geometric details with fewer artifacts. Unlike deep 3D generative models that directly generate explicit representations such as meshes, our model can synthesize shapes with arbitrary topology. The core of DMTet includes a deformable tetrahedral grid that encodes a discretized signed distance function and a differentiable marching tetrahedra layer that converts the implicit signed distance representation to the explicit surface mesh representation. This combination allows joint optimization of the surface geometry and topology as well as generation of the hierarchy of subdivisions using reconstruction and adversarial losses defined explicitly on the surface mesh. Our approach significantly outperforms existing work on conditional shape synthesis from coarse voxel inputs, trained on a dataset of complex 3D animal shapes. Project page: https://nv-tlabs.github.io/DMTet/.",
    "authors": [
      "Shen, Tianchang",
      "Gao, Jun",
      "Yin, Kangxue",
      "Liu, Ming-Yu",
      "Fidler, Sanja"
    ]
  },
  {
    "id": "30d411fdc0e6daf092a74354094359bb",
    "title": "Learning to Combine Per-Example Solutions for Neural Program Synthesis",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/30d411fdc0e6daf092a74354094359bb-Paper.pdf",
    "abstract": "The goal of program synthesis from examples is to find a computer program that is consistent with a given set of input-output examples. Most learning-based approaches try to find a program that satisfies all examples at once. Our work, by contrast, considers an approach that breaks the problem into two stages: (a) find programs that satisfy only one example, and (b) leverage these per-example solutions to yield a program that satisfies all examples. We introduce the Cross Aggregator neural network module based on a multi-head attention mechanism that learns to combine the cues present in these per-example solutions to synthesize a global solution. Evaluation across programs of different lengths and under two different experimental settings reveal that when given the same time budget, our technique significantly improves the success rate over PCCoder [Zohar et. al 2018] and other ablation baselines.",
    "authors": [
      "Shrivastava, Disha",
      "Larochelle, Hugo",
      "Tarlow, Daniel"
    ]
  },
  {
    "id": "30d454f09b771b9f65e3eaf6e00fa7bd",
    "title": "On Success and Simplicity: A Second Look at Transferable Targeted Attacks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/30d454f09b771b9f65e3eaf6e00fa7bd-Paper.pdf",
    "abstract": "Achieving transferability of targeted attacks is reputed to be remarkably difficult. The current state of the art has resorted to resource-intensive solutions that necessitate training model(s) for each target class with additional data. In our investigation, we find, however, that simple transferable attacks which require neither model training nor additional data can achieve surprisingly strong targeted transferability. This insight has been overlooked until now, mainly because the widespread practice of attacking with only few iterations has largely limited the attack convergence to optimal targeted transferability. In particular, we, for the first time, identify that a very simple logit loss can largely surpass the commonly adopted cross-entropy loss, and yield even better results than the resource-intensive state of the art. Our analysis spans a variety of transfer scenarios, especially including three new, realistic scenarios: an ensemble transfer scenario with little model similarity, a worse-case scenario with low-ranked target classes, and also a real-world attack on the Google Cloud Vision API. Results in these new transfer scenarios demonstrate that the commonly adopted, easy scenarios cannot fully reveal the actual strength of different attacks and may cause misleading comparative results. We also show the usefulness of the simple logit loss for generating targeted universal adversarial perturbations in a data-free manner. Overall, the aim of our analysis is to inspire a more meaningful evaluation on targeted transferability. Code is available at https://github.com/ZhengyuZhao/Targeted-Tansfer.",
    "authors": [
      "Zhao, Zhengyu",
      "Liu, Zhuoran",
      "Larson, Martha"
    ]
  },
  {
    "id": "30d4e6422cd65c7913bc9ce62e078b79",
    "title": "Provably efficient, succinct, and precise explanations",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/30d4e6422cd65c7913bc9ce62e078b79-Paper.pdf",
    "abstract": "We consider the problem of explaining the predictions of an arbitrary blackbox model $f$: given query access to $f$ and an instance $x$, output a small set of $x$'s features that in conjunction essentially determines $f(x)$. We design an efficient algorithm with provable guarantees on the succinctness and precision of the explanations that it returns. Prior algorithms were either efficient but lacked such guarantees, or achieved such guarantees but were inefficient.   We obtain our algorithm via a connection to the problem of {\\sl implicitly} learning decision trees.  The implicit nature of this learning task allows for efficient algorithms even when the complexity of~$f$ necessitates an intractably large surrogate decision tree.  We solve the implicit learning problem by bringing together techniques from learning theory, local computation algorithms, and complexity theory.   Our approach of \u201cexplaining by implicit learning\u201d shares elements of two previously disparate methods for post-hoc explanations, global and local explanations, and we make the case that it enjoys advantages of both.",
    "authors": [
      "Blanc, Guy",
      "Lange, Jane",
      "Tan, Li-Yang"
    ]
  },
  {
    "id": "30f8f6b940d1073d8b6a5eebc46dd6e5",
    "title": "Refined Learning Bounds for Kernel and Approximate $k$-Means",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/30f8f6b940d1073d8b6a5eebc46dd6e5-Paper.pdf",
    "abstract": "Kernel $k$-means is one of the most popular approaches to clustering and its theoretical properties have been investigated for decades. However, the existing state-of-the-art risk bounds are of order $\\mathcal{O}(k/\\sqrt{n})$, which do not match with the stated lower bound $\\Omega(\\sqrt{k/n})$ in terms of $k$, where $k$ is the number of clusters and $n$ is the size of the training set. In this paper, we study the statistical properties of kernel $k$-means and Nystr\\\"{o}m-based kernel $k$-means, and obtain optimal clustering risk bounds, which improve the existing risk bounds. Particularly, based on a refined upper bound of Rademacher complexity [21], we first derive an optimal risk bound of rate $\\mathcal{O}(\\sqrt{k/n})$ for empirical risk minimizer (ERM), and further extend it to general cases beyond ERM. Then, we analyze the statistical effect of computational approximations of Nystr\\\"{o}m kernel $k$-means, and prove that it achieves the same statistical accuracy as the original kernel $k$-means considering only $\\Omega(\\sqrt{nk})$ Nystr\\\"{o}m landmark points. We further relax the restriction of landmark points from $\\Omega(\\sqrt{nk})$ to $\\Omega(\\sqrt{n})$ under a mild condition. Finally, we validate the theoretical findings via numerical experiments.",
    "authors": [
      "Liu, Yong"
    ]
  },
  {
    "id": "310614fca8fb8e5491295336298c340f",
    "title": "Learning Causal Semantic Representation for Out-of-Distribution Prediction",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/310614fca8fb8e5491295336298c340f-Paper.pdf",
    "abstract": "Conventional supervised learning methods, especially deep ones, are found to be sensitive to out-of-distribution (OOD) examples, largely because the learned representation mixes the semantic factor with the variation factor due to their domain-specific correlation, while only the semantic factor causes the output. To address the problem, we propose a Causal Semantic Generative model (CSG) based on a causal reasoning so that the two factors are modeled separately, and develop methods for OOD prediction from a single training domain, which is common and challenging. The methods are based on the causal invariance principle, with a novel design in variational Bayes for both efficient learning and easy prediction. Theoretically, we prove that under certain conditions, CSG can identify the semantic factor by fitting training data, and this semantic-identification guarantees the boundedness of OOD generalization error and the success of adaptation. Empirical study shows improved OOD performance over prevailing baselines.",
    "authors": [
      "Liu, Chang",
      "Sun, Xinwei",
      "Wang, Jindong",
      "Tang, Haoyue",
      "Li, Tao",
      "Qin, Tao",
      "Chen, Wei",
      "Liu, Tie-Yan"
    ]
  },
  {
    "id": "310b60949d2b6096903d7e8a539b20f5",
    "title": "A first-order primal-dual method with adaptivity to local smoothness",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/310b60949d2b6096903d7e8a539b20f5-Paper.pdf",
    "abstract": "We consider the problem of finding a saddle point for the convex-concave objective $\\min_x \\max_y f(x) + \\langle Ax, y\\rangle - g^*(y)$, where $f$ is a convex function with locally Lipschitz gradient and $g$ is convex and possibly non-smooth. We propose an adaptive version of the Condat-V\u0169 algorithm, which alternates between primal gradient steps and dual proximal steps. The method achieves stepsize adaptivity through a simple rule involving $\\|A\\|$ and the norm of recently computed gradients of $f$. Under standard assumptions, we prove an $\\mathcal{O}(k^{-1})$ ergodic convergence rate. Furthermore, when $f$ is also locally strongly convex and $A$ has full row rank we show that our method converges with a linear rate. Numerical experiments are provided for illustrating the practical performance of the algorithm.",
    "authors": [
      "Vladarean, Maria-Luiza",
      "Malitsky, Yura",
      "Cevher, Volkan"
    ]
  },
  {
    "id": "310ce61c90f3a46e340ee8257bc70e93",
    "title": "A Theory-Driven Self-Labeling Refinement Method for Contrastive Representation Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/310ce61c90f3a46e340ee8257bc70e93-Paper.pdf",
    "abstract": "For an image  query, unsupervised contrastive learning  labels crops of  the same image as positives,  and other image crops as  negatives. Although intuitive, such a native label assignment strategy cannot reveal the underlying semantic similarity between a  query and  its positives and negatives, and impairs performance,  since some negatives are  semantically similar to  the query or even share the same semantic class as the query.  In this work, we first  prove that for  contrastive learning,  inaccurate label assignment heavily  impairs its generalization for semantic instance discrimination, while accurate labels  benefit its generalization.  Inspired by this theory, we  propose   a novel self-labeling refinement approach for contrastive learning. It improves the label quality via two complementary  modules:  (i)  self-labeling refinery (SLR) to  generate accurate labels and (ii)  momentum mixup (MM)  to enhance similarity between query and its positive. SLR uses a positive of a query to estimate  semantic similarity between  a query and its positive and negatives, and  combines estimated similarity with  vanilla label assignment in contrastive learning to  iteratively generate  more accurate and informative soft labels. We theoretically show that our SLR can exactly recover the true semantic  labels of  label-corrupted  data, and  supervises   networks to achieve zero prediction  error on classification tasks.  MM randomly  combines   queries and  positives to increase  semantic similarity between the generated virtual queries and their positives so as to improves label accuracy.  Experimental results on CIFAR10,  ImageNet, VOC and COCO show the effectiveness of our method.  ",
    "authors": [
      "Zhou, Pan",
      "Xiong, Caiming",
      "Yuan, Xiaotong",
      "Hoi, Steven Chu Hong"
    ]
  },
  {
    "id": "312ecfdfa8b239e076b114498ce21905",
    "title": "Adversarial Robustness with Semi-Infinite Constrained Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/312ecfdfa8b239e076b114498ce21905-Paper.pdf",
    "abstract": "Despite strong performance in numerous applications, the fragility of deep learning to input perturbations has raised serious questions about its use in safety-critical domains.  While adversarial training can mitigate this issue in practice, state-of-the-art methods are increasingly application-dependent, heuristic in nature, and suffer from fundamental trade-offs between nominal performance and robustness. Moreover, the problem of finding worst-case perturbations is non-convex and underparameterized, both of which engender a non-favorable optimization landscape. Thus, there is a gap between the theory and practice of robust learning, particularly with respect to when and why adversarial training works.  In this paper, we take a constrained learning approach to address these questions and to provide a theoretical foundation for robust learning. In particular, we leverage semi-infinite optimization and non-convex duality theory to show that adversarial training is equivalent to a statistical problem over perturbation distributions. Notably, we show that a myriad of previous robust training techniques can be recovered for particular, sub-optimal choices of these distributions. Using these insights, we then propose a hybrid Langevin Markov Chain Monte Carlo approach for which several common algorithms (e.g., PGD) are special cases. Finally, we show that our approach can mitigate the trade-off between nominal and robust performance, yielding state-of-the-art results on MNIST and CIFAR-10.  Our code is available at: https://github.com/arobey1/advbench.",
    "authors": [
      "Robey, Alexander",
      "Chamon, Luiz",
      "Pappas, George J.",
      "Hassani, Hamed",
      "Ribeiro, Alejandro"
    ]
  },
  {
    "id": "312f1ba2a72318edaaa995a67835fad5",
    "title": "Conformal Time-series Forecasting",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/312f1ba2a72318edaaa995a67835fad5-Paper.pdf",
    "abstract": "Current approaches for multi-horizon time series forecasting using recurrent neural networks (RNNs) focus on issuing point estimates, which is insufficient for decision-making in critical application domains where an uncertainty estimate is also required. Existing approaches for uncertainty quantification in RNN-based time-series forecasts are limited as they may require significant alterations to the underlying model architecture, may be computationally complex, may be difficult to calibrate, may incur high sample complexity, and may not provide theoretical guarantees on frequentist coverage. In this paper, we extend the inductive conformal prediction framework to the time-series forecasting setup, and propose a lightweight algorithm to address all of the above limitations, providing uncertainty estimates with theoretical guarantees for any multi-horizon forecast predictor and any dataset with minimal exchangeability assumptions. We demonstrate the effectiveness of our approach by comparing it with existing benchmarks on a variety of synthetic and real-world datasets.",
    "authors": [
      "Stankeviciute, Kamile",
      "M. Alaa, Ahmed",
      "van der Schaar, Mihaela"
    ]
  },
  {
    "id": "314450613369e0ee72d0da7f6fee773c",
    "title": "A 3D Generative Model for Structure-Based Drug Design",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/314450613369e0ee72d0da7f6fee773c-Paper.pdf",
    "abstract": "We study a fundamental problem in structure-based drug design --- generating molecules that bind to specific protein binding sites. While we have witnessed the great success of deep generative models in drug design, the existing methods are mostly string-based or graph-based. They are limited by the lack of spatial information and thus unable to be applied to structure-based design tasks. Particularly, such models have no or little knowledge of how molecules interact with their target proteins exactly in 3D space. In this paper, we propose a 3D generative model that generates molecules given a designated 3D protein binding site. Specifically, given a binding site as the 3D context, our model estimates the probability density of atom's occurrences in 3D space --- positions that are more likely to have atoms will be assigned higher probability. To generate 3D molecules, we propose an auto-regressive sampling scheme --- atoms are sampled sequentially from the learned distribution until there is no room for new atoms. Combined with this sampling scheme, our model can generate valid and diverse molecules, which could be applicable to various structure-based molecular design tasks such as molecule sampling and linker design. Experimental results demonstrate that molecules sampled from our model exhibit high binding affinity to specific targets and good drug properties such as drug-likeness even if the model is not explicitly optimized for them.",
    "authors": [
      "Luo, Shitong",
      "Guan, Jiaqi",
      "Ma, Jianzhu",
      "Peng, Jian"
    ]
  },
  {
    "id": "3152e3b1e52e2cb123363787d5f76c95",
    "title": "Bootstrapping the Error of Oja's Algorithm ",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3152e3b1e52e2cb123363787d5f76c95-Paper.pdf",
    "abstract": "We consider the problem of quantifying uncertainty for the estimation error of the leading eigenvector from Oja's algorithm for streaming principal component analysis, where the data are generated IID from some unknown distribution.  By combining classical tools from the U-statistics literature with recent results on high-dimensional central limit theorems for quadratic forms of random vectors and concentration of matrix products, we establish a weighted $\\chi^2$ approximation result for the $\\sin^2$ error between the population eigenvector and the output of Oja\u2019s algorithm. Since estimating the covariance matrix associated with the approximating distribution requires knowledge of unknown model parameters, we propose a multiplier bootstrap algorithm that may be updated in an online manner.  We establish conditions under which the bootstrap distribution is close to the corresponding sampling distribution with high probability, thereby establishing the bootstrap as a consistent inferential method in an appropriate asymptotic regime. ",
    "authors": [
      "Lunde, Robert",
      "Sarkar, Purnamrita",
      "Ward, Rachel"
    ]
  },
  {
    "id": "31784d9fc1fa0d25d04eae50ac9bf787",
    "title": "Landscape analysis of an improved power method for tensor decomposition",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/31784d9fc1fa0d25d04eae50ac9bf787-Paper.pdf",
    "abstract": "In this work, we consider the optimization formulation for symmetric tensor decomposition recently introduced in the Subspace Power Method (SPM) of Kileel and Pereira.  Unlike popular alternative functionals for tensor decomposition, the SPM objective function has the desirable properties that its maximal value is known in advance, and its global optima are exactly the rank-1 components of the tensor when the input is sufficiently low-rank.  We analyze the non-convex optimization landscape associated with the SPM objective.  Our analysis accounts for working with noisy tensors.  We derive quantitative bounds such that any second-order critical point with SPM objective value exceeding the bound must equal a tensor component in the noiseless case, and must approximate a tensor component in the noisy case. For decomposing tensors of size $D^{\\times m}$, we obtain a near-global guarantee up to rank $\\widetilde{o}(D^{\\lfloor m/2 \\rfloor})$ under a random tensor model, and a global guarantee up to rank $\\mathcal{O}(D)$ assuming deterministic frame conditions.  This implies that SPM with suitable initialization is a provable, efficient, robust algorithm for low-rank symmetric tensor decomposition.  We conclude with numerics that show a practical preferability for using the SPM functional over a more established counterpart.",
    "authors": [
      "Kileel, Joe",
      "Klock, Timo",
      "M Pereira, Jo\u00e3o"
    ]
  },
  {
    "id": "31839b036f63806cba3f47b93af8ccb5",
    "title": "Curriculum Offline Imitating Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/31839b036f63806cba3f47b93af8ccb5-Paper.pdf",
    "abstract": "Offline reinforcement learning (RL) tasks require the agent to learn from a pre-collected dataset with no further interactions with the environment. Despite the potential to surpass the behavioral policies, RL-based methods are generally impractical due to the training instability and bootstrapping the extrapolation errors, which always require careful hyperparameter tuning via online evaluation. In contrast, offline imitation learning (IL) has no such issues since it learns the policy directly without estimating the value function by bootstrapping. However, IL is usually limited in the capability of the behavioral policy and tends to learn a mediocre behavior from the dataset collected by the mixture of policies. In this paper, we aim to take advantage of IL but mitigate such a drawback. Observing that behavior cloning is able to imitate neighboring policies with less data, we propose \\textit{Curriculum Offline Imitation Learning (COIL)}, which utilizes an experience picking strategy to make the agent imitate from adaptive neighboring policies with a higher return, and improves the current policy along curriculum stages. On continuous control benchmarks, we compare COIL against both imitation-based methods and RL-based methods, showing that COIL not only avoids just learning a mediocre behavior on mixed datasets but is also even competitive with state-of-the-art offline RL methods.",
    "authors": [
      "Liu, Minghuan",
      "Zhao, Hanye",
      "Yang, Zhengyu",
      "Shen, Jian",
      "Zhang, Weinan",
      "Zhao, Li",
      "Liu, Tie-Yan"
    ]
  },
  {
    "id": "31857b449c407203749ae32dd0e7d64a",
    "title": "Robust Pose Estimation in Crowded Scenes with Direct Pose-Level Inference",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/31857b449c407203749ae32dd0e7d64a-Paper.pdf",
    "abstract": "Multi-person pose estimation in crowded scenes is challenging because overlapping and occlusions make it difficult to detect person bounding boxes and infer pose cues from individual keypoints. To address those issues, this paper proposes a direct pose-level inference strategy that is free of bounding box detection and keypoint grouping. Instead of inferring individual keypoints, the Pose-level Inference Network (PINet) directly infers the complete pose cues for a person from his/her visible body parts. PINet first applies the Part-based Pose Generation (PPG) to infer multiple coarse poses for each person from his/her body parts. Those coarse poses are refined by the Pose Refinement module through incorporating pose priors, and finally are fused in the Pose Fusion module. PINet relies on discriminative body parts to differentiate overlapped persons, and applies visual body cues to infer the global pose cues.  Experiments on several crowded scenes pose estimation benchmarks demonstrate the superiority of PINet. For instance, it achieves 59.8% AP on the OCHuman dataset, outperforming the recent works by a large margin.",
    "authors": [
      "Wang, Dongkai",
      "Zhang, Shiliang",
      "Hua, Gang"
    ]
  },
  {
    "id": "31917677a66c6eddd3ab1f68b0679e2f",
    "title": "Ising Model Selection Using $\\ell_{1}$-Regularized Linear Regression: A Statistical Mechanics Analysis",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/31917677a66c6eddd3ab1f68b0679e2f-Paper.pdf",
    "abstract": "We theoretically analyze the typical learning performance of $\\ell_{1}$-regularized linear regression ($\\ell_1$-LinR) for Ising model selection using the replica method from statistical mechanics. For typical random regular graphs in the paramagnetic phase, an accurate estimate of the typical sample complexity of $\\ell_1$-LinR is obtained.   Remarkably, despite the model misspecification, $\\ell_1$-LinR is model selection consistent with the same order of sample complexity as $\\ell_{1}$-regularized logistic regression ($\\ell_1$-LogR), i.e., $M=\\mathcal{O}\\left(\\log N\\right)$,  where $N$ is the number of variables of the Ising model. Moreover, we provide an efficient method to accurately predict the non-asymptotic behavior of $\\ell_1$-LinR for moderate $M, N$, such as precision and recall. Simulations show a fairly good agreement between theoretical predictions and experimental results, even for graphs with many loops, which supports our findings. Although this paper mainly focuses on $\\ell_1$-LinR, our method is readily applicable for precisely characterizing the typical learning performances of a wide class of  $\\ell_{1}$-regularized $M$-estimators including $\\ell_1$-LogR and interaction screening. ",
    "authors": [
      "Meng, Xiangming",
      "Obuchi, Tomoyuki",
      "Kabashima, Yoshiyuki"
    ]
  },
  {
    "id": "31b3b31a1c2f8a370206f111127c0dbd",
    "title": "Conformal Prediction using Conditional Histograms",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/31b3b31a1c2f8a370206f111127c0dbd-Paper.pdf",
    "abstract": "This paper develops a conformal method to compute prediction intervals for non-parametric regression that can automatically adapt to skewed data. Leveraging black-box machine learning algorithms to estimate the conditional distribution of the outcome using histograms, it translates their output into the shortest prediction intervals with approximate conditional coverage. The resulting prediction intervals provably have marginal coverage in finite samples, while asymptotically achieving conditional coverage and optimal length if the black-box model is consistent. Numerical experiments with simulated and real data demonstrate improved performance compared to state-of-the-art alternatives, including conformalized quantile regression and other distributional conformal prediction approaches.",
    "authors": [
      "Sesia, Matteo",
      "Romano, Yaniv"
    ]
  },
  {
    "id": "31c0b36aef265d9221af80872ceb62f9",
    "title": "Contrastive Graph Poisson Networks: Semi-Supervised Learning with Extremely Limited Labels",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/31c0b36aef265d9221af80872ceb62f9-Paper.pdf",
    "abstract": "Graph Neural Networks (GNNs) have achieved remarkable performance in the task of semi-supervised node classification. However, most existing GNN models require sufficient labeled data for effective network training. Their performance can be seriously degraded when labels are extremely limited. To address this issue, we propose a new framework termed Contrastive Graph Poisson Networks (CGPN) for node classification under extremely limited labeled data. Specifically, our CGPN derives from variational inference; integrates a newly designed Graph Poisson Network (GPN) to effectively propagate the limited labels to the entire graph and a normal GNN, such as Graph Attention Network, that flexibly guides the propagation of GPN; applies a contrastive objective to further exploit the supervision information from the learning process of GPN and GNN models. Essentially, our CGPN can enhance the learning performance of GNNs under extremely limited labels by contrastively propagating the limited labels to the entire graph. We conducted extensive experiments on different types of datasets to demonstrate the superiority of CGPN. ",
    "authors": [
      "Wan, Sheng",
      "Zhan, Yibing",
      "Liu, Liu",
      "Yu, Baosheng",
      "Pan, Shirui",
      "Gong, Chen"
    ]
  },
  {
    "id": "31ca0ca71184bbdb3de7b20a51e88e90",
    "title": "Collaborative Uncertainty in Multi-Agent Trajectory Forecasting",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/31ca0ca71184bbdb3de7b20a51e88e90-Paper.pdf",
    "abstract": "Uncertainty modeling is critical in trajectory-forecasting systems for both interpretation and safety reasons. To better predict the future trajectories of multiple agents, recent works have introduced interaction modules to capture interactions among agents. This approach leads to correlations among the predicted trajectories. However, the uncertainty brought by such correlations is neglected. To fill this gap, we propose a novel concept, collaborative uncertainty (CU), which models the uncertainty resulting from the interaction module. We build a general CU-based framework to make a prediction model learn the future trajectory and the corresponding uncertainty. The CU-based framework is integrated as a plugin module to current state-of-the-art (SOTA) systems and deployed in two special cases based on multivariate Gaussian and Laplace distributions. In each case, we conduct extensive experiments on two synthetic datasets and two public, large-scale benchmarks of trajectory forecasting. The results are promising: 1) The results of synthetic datasets show that CU-based framework allows the model to nicely rebuild the ground-truth distribution. 2) The results of trajectory forecasting benchmarks demonstrate that the CU-based framework steadily helps SOTA systems improve their performances. Specially, the proposed CU-based framework helps VectorNet improve by 57 cm regarding Final Displacement Error on nuScenes dataset. 3) The visualization results of CU illustrate that the value of CU is highly related to the amount of the interactive information among agents.",
    "authors": [
      "Tang, Bohan",
      "Zhong, Yiqi",
      "Neumann, Ulrich",
      "Wang, Gang",
      "Chen, Siheng",
      "Zhang, Ya"
    ]
  },
  {
    "id": "321cf86b4c9f5ddd04881a44067c2a5a",
    "title": "Network-to-Network Regularization: Enforcing Occam's Razor to Improve Generalization",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/321cf86b4c9f5ddd04881a44067c2a5a-Paper.pdf",
    "abstract": "What makes a classifier have the ability to generalize? There have been a lot of important attempts to address this question, but a clear answer is still elusive. Proponents of complexity theory find that the complexity of the classifier's function space is key to deciding generalization, whereas other recent work reveals that classifiers which extract invariant feature representations are likely to generalize better. Recent theoretical and empirical studies, however, have shown that even within a classifier's function space, there can be significant differences in the ability to generalize. Specifically, empirical studies have shown that among functions which have a good training data fit, functions with lower Kolmogorov complexity (KC) are likely to generalize better, while the opposite is true for functions of higher KC. Motivated by these findings, we propose, in this work, a novel measure of complexity called Kolmogorov Growth (KG), which we use to derive new generalization error bounds that only depend on the final choice of the classification function. Guided by the bounds, we propose a novel way of regularizing neural networks by constraining the network trajectory to remain in the low KG zone during training. Minimizing KG while learning is akin to applying the Occam's razor to neural networks. The proposed approach, called network-to-network regularization, leads to clear improvements in the generalization ability of classifiers. We verify this for three popular image datasets (MNIST, CIFAR-10, CIFAR-100) across varying training data sizes. Empirical studies find that conventional training of neural networks, unlike network-to-network regularization, leads to networks of high KG and lower test accuracies. Furthermore, we present the benefits of N2N regularization in the scenario where the training data labels are noisy. Using N2N regularization, we achieve competitive performance on MNIST, CIFAR-10 and CIFAR-100 datasets with corrupted training labels, significantly improving network performance compared to standard cross-entropy baselines in most cases. These findings illustrate the many benefits obtained from imposing a function complexity prior like Kolmogorov Growth during the training process.  ",
    "authors": [
      "Ghosh, Rohan",
      "Motani, Mehul"
    ]
  },
  {
    "id": "325995af77a0e8b06d1204a171010b3a",
    "title": "Generalized and Discriminative Few-Shot Object Detection via SVD-Dictionary Enhancement",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/325995af77a0e8b06d1204a171010b3a-Paper.pdf",
    "abstract": "Few-shot object detection (FSOD) aims to detect new objects based on few annotated samples. To alleviate the impact of few samples, enhancing the generalization and discrimination abilities of detectors on new objects plays an important role. In this paper, we explore employing Singular Value Decomposition (SVD) to boost both the generalization and discrimination abilities. In specific, we propose a novel method, namely, SVD-Dictionary enhancement, to build two separated spaces based on the sorted singular values. Concretely, the eigenvectors corresponding to larger singular values are used to build the generalization space in which localization is performed, as these eigenvectors generally suppress certain variations (e.g., the variation of styles) and contain intrinsical characteristics of objects. Meanwhile, since the eigenvectors corresponding to relatively smaller singular values may contain richer category-related information, we can utilize them to build the discrimination space in which classification is performed. Dictionary learning is further leveraged to capture high-level discriminative information from the discrimination space, which is beneficial for improving detection accuracy. In the experiments, we separately verify the effectiveness of our method on PASCAL VOC and COCO benchmarks. Particularly, for the 2-shot case in VOC split1, our method significantly outperforms the baseline by 6.2\\%. Moreover, visualization analysis shows that our method is instrumental in doing FSOD.",
    "authors": [
      "WU, Aming",
      "Zhao, Suqi",
      "Deng, Cheng",
      "Liu, Wei"
    ]
  },
  {
    "id": "325eaeac5bef34937cfdc1bd73034d17",
    "title": "Conditioning Sparse Variational Gaussian Processes for Online Decision-making",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/325eaeac5bef34937cfdc1bd73034d17-Paper.pdf",
    "abstract": "With a principled representation of uncertainty and closed form posterior updates, Gaussian processes (GPs) are a natural choice for online decision making. However, Gaussian processes typically require at least $\\mathcal{O}(n^2)$ computations for $n$ training points, limiting their general applicability. Stochastic variational Gaussian processes (SVGPs) can provide scalable inference for a dataset of fixed size, but are difficult to efficiently condition on new data. We propose online variational conditioning (OVC), a procedure for efficiently conditioning SVGPs in an online setting that does not require re-training through the evidence lower bound with the addition of new data. OVC enables the pairing of SVGPs with advanced look-ahead acquisition functions for black-box optimization, even with non-Gaussian likelihoods. We show OVC provides compelling performance in a range of applications including active learning of malaria incidence, and reinforcement learning on MuJoCo simulated robotic control tasks.",
    "authors": [
      "Maddox, Wesley J.",
      "Stanton, Samuel",
      "Wilson, Andrew G."
    ]
  },
  {
    "id": "326a8c055c0d04f5b06544665d8bb3ea",
    "title": "Spherical Motion Dynamics: Learning Dynamics of Normalized Neural Network using SGD and Weight Decay",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/326a8c055c0d04f5b06544665d8bb3ea-Paper.pdf",
    "abstract": "In this paper, we comprehensively reveal the learning dynamics of normalized neural network using Stochastic Gradient Descent (with momentum) and Weight Decay (WD), named as Spherical Motion Dynamics (SMD). Most related works focus on studying behavior of effective learning rate\" inequilibrium\" state, i.e. assuming weight norm remains unchanged. However, their discussion on why this equilibrium can be reached is either absent or less convincing. Our work directly explores the cause of equilibrium, as a special state of SMD. Specifically, 1) we introduce the assumptions that can lead to equilibrium state in SMD, and prove equilibrium can be reached in a linear rate regime under given assumptions; 2) we propose ``angular update\" as a substitute for effective learning rate to depict the state of SMD, and derive the theoretical value of angular update in equilibrium state; 3) we verify our assumptions and theoretical results on various large-scale computer vision tasks including ImageNet and MSCOCO with standard settings. Experiment results show our theoretical findings agree well with empirical observations. We also show that the behavior of angular update in SMD can produce interesting effect to the optimization of neural network in practice.",
    "authors": [
      "Wan, Ruosi",
      "Zhu, Zhanxing",
      "Zhang, Xiangyu",
      "Sun, Jian"
    ]
  },
  {
    "id": "327af0f71f7acdfd882774225f04775f",
    "title": "Imitating Deep Learning Dynamics via  Locally Elastic Stochastic Differential Equations",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/327af0f71f7acdfd882774225f04775f-Paper.pdf",
    "abstract": "Understanding the training dynamics of deep learning models is perhaps a necessary step toward demystifying the effectiveness of these models. In particular, how do training data from different classes gradually become separable in their feature spaces when training neural networks using stochastic gradient descent? In this paper, we model the evolution of features during deep learning training using a set of stochastic differential equations (SDEs) that each corresponding to a training sample. As a crucial ingredient in our modeling strategy, each SDE contains a drift term that reflects the impact of backpropagation at an input on the features of all samples. Our main finding uncovers a sharp phase transition phenomenon regarding the intra-class impact: if the SDEs are locally elastic in the sense that the impact is more significant on samples from the same class as the input, the features of training data become linearly separable---meaning vanishing training loss; otherwise, the features are not separable, no matter how long the training time is. In the presence of local elasticity, moreover, an analysis of our SDEs shows the emergence of a simple geometric structure called neural collapse of the features. Taken together, our results shed light on the decisive role of local elasticity underlying the training dynamics of neural networks. We corroborate our theoretical analysis with experiments on a synthesized dataset of geometric shapes as well as on CIFAR-10.",
    "authors": [
      "Zhang, Jiayao",
      "Wang, Hua",
      "Su, Weijie"
    ]
  },
  {
    "id": "32b127307a606effdcc8e51f60a45922",
    "title": "Probabilistic Forecasting: A Level-Set Approach",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/32b127307a606effdcc8e51f60a45922-Paper.pdf",
    "abstract": "Large-scale time series panels have become ubiquitous over the last years in areas such as retail, operational metrics, IoT, and medical domain (to name only a few). This has resulted in a need for forecasting techniques that effectively leverage all available data by learning across all time series in each panel. Among the desirable properties of forecasting techniques, being able to generate probabilistic predictions ranks among the top. In this paper, we therefore present Level Set Forecaster (LSF), a simple yet effective general approach to transform a point estimator into a probabilistic one. By recognizing the connection of our algorithm to random forests (RFs) and quantile regression forests (QRFs), we are able to prove consistency guarantees of our approach under mild assumptions on the underlying point estimator. As a byproduct, we prove the first consistency results for QRFs under the CART-splitting criterion. Empirical experiments show that our approach, equipped with tree-based models as the point estimator, rivals state-of-the-art deep learning models in terms of forecasting accuracy.",
    "authors": [
      "Hasson, Hilaf",
      "Wang, Bernie",
      "Januschowski, Tim",
      "Gasthaus, Jan"
    ]
  },
  {
    "id": "32b991e5d77ad140559ffb95522992d0",
    "title": "Roto-translated Local Coordinate Frames For Interacting Dynamical Systems",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/32b991e5d77ad140559ffb95522992d0-Paper.pdf",
    "abstract": "Modelling interactions is critical in learning complex dynamical systems, namely systems of interacting objects with highly non-linear and time-dependent behaviour. A large class of such systems can be formalized as $\\textit{geometric graphs}$, $\\textit{i.e.}$ graphs with nodes positioned in the Euclidean space given an $\\textit{arbitrarily}$ chosen global coordinate system, for instance vehicles in a traffic scene. Notwithstanding the arbitrary global coordinate system, the governing dynamics of the respective dynamical systems are invariant to rotations and translations, also known as $\\textit{Galilean invariance}$. As ignoring these invariances leads to worse generalization, in this work we propose local coordinate systems per node-object to induce roto-translation invariance to the geometric graph of the interacting dynamical system. Further, the local coordinate systems allow for a natural definition of anisotropic filtering in graph neural networks. Experiments in traffic scenes, 3D motion capture, and colliding particles demonstrate the proposed approach comfortably outperforms the recent state-of-the-art.",
    "authors": [
      "Kofinas, Miltiadis",
      "Nagaraja, Naveen",
      "Gavves, Efstratios"
    ]
  },
  {
    "id": "32b9e74c8f60958158eba8d1fa372971",
    "title": "ParK: Sound and Efficient Kernel Ridge Regression by Feature Space Partitions",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/32b9e74c8f60958158eba8d1fa372971-Paper.pdf",
    "abstract": "We introduce ParK, a new large-scale solver for kernel ridge regression. Our approach combines partitioning with random projections and iterative optimization to reduce space and time complexity while provably maintaining the same statistical accuracy. In particular, constructing suitable partitions directly in the feature space rather than in the input space, we promote orthogonality between the local estimators, thus ensuring that key quantities such as local effective dimension and bias remain under control. We characterize the statistical-computational tradeoff of our model, and demonstrate the effectiveness of our method by numerical experiments on large-scale datasets.",
    "authors": [
      "Carratino, Luigi",
      "Vigogna, Stefano",
      "Calandriello, Daniele",
      "Rosasco, Lorenzo"
    ]
  },
  {
    "id": "32bbf7b2bc4ed14eb1e9c2580056a989",
    "title": "Scaling Gaussian Processes with Derivative Information Using Variational Inference",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/32bbf7b2bc4ed14eb1e9c2580056a989-Paper.pdf",
    "abstract": "Gaussian processes with derivative information are useful in many settings where derivative information is available, including numerous Bayesian optimization and regression tasks that arise in the natural sciences. Incorporating derivative observations, however, comes with a dominating $O(N^3D^3)$ computational cost when training on $N$ points in $D$ input dimensions. This is intractable for even moderately sized problems. While recent work has addressed this intractability in the low-$D$ setting, the high-$N$, high-$D$ setting is still unexplored and of great value, particularly as machine learning problems increasingly become high dimensional. In this paper, we introduce methods to achieve fully scalable Gaussian process regression with derivatives using variational inference. Analogous to the use of inducing values to sparsify the labels of a training set, we introduce the concept of inducing directional derivatives to sparsify the partial derivative information of the training set. This enables us to construct a variational posterior that incorporates derivative information but whose size depends neither on the full dataset size $N$ nor the full dimensionality $D$. We demonstrate the full scalability of our approach on a variety of tasks, ranging from a high dimensional Stellarator fusion regression task to training graph convolutional neural networks on PubMed using Bayesian optimization. Surprisingly, we additionally find that our approach can improve regression performance even in settings where only label data is available.",
    "authors": [
      "Padidar, Misha",
      "Zhu, Xinran",
      "Huang, Leo",
      "Gardner, Jacob",
      "Bindel, David"
    ]
  },
  {
    "id": "32cfdce9631d8c7906e8e9d6e68b514b",
    "title": "On the Representation of Solutions to Elliptic PDEs in Barron Spaces",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/32cfdce9631d8c7906e8e9d6e68b514b-Paper.pdf",
    "abstract": "Numerical solutions to high-dimensional partial differential equations (PDEs) based on neural networks have seen exciting developments. This paper derives complexity estimates of the solutions of $d$-dimensional second-order elliptic PDEs in the Barron space, that is a set of functions admitting the integral of certain parametric ridge function against a probability measure on the parameters. We prove under some appropriate assumptions that if the coefficients and the source term of the elliptic PDE lie in Barron spaces, then the solution of the PDE is $\\epsilon$-close with respect to the $H^1$ norm to a Barron function. Moreover, we prove dimension-explicit bounds for the Barron norm of this approximate solution, depending at most polynomially on the dimension $d$ of the PDE. As a direct consequence of the complexity estimates, the solution of the PDE can be approximated on any bounded domain by a two-layer neural network with respect to the $H^1$ norm with a dimension-explicit convergence rate.",
    "authors": [
      "Chen, Ziang",
      "Lu, Jianfeng",
      "Lu, Yulong"
    ]
  },
  {
    "id": "32e19424b63cc63077a4031b87fb1010",
    "title": "A/B Testing for Recommender Systems in a Two-sided Marketplace",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/32e19424b63cc63077a4031b87fb1010-Paper.pdf",
    "abstract": "Two-sided marketplaces are standard business models of many online platforms (e.g., Amazon, Facebook, LinkedIn), wherein the platforms have consumers, buyers or content viewers on one side and producers, sellers or content-creators on the other. Consumer side measurement of the impact of a treatment variant can be done via simple online A/B testing. Producer side measurement is more challenging because the producer experience depends on the treatment assignment of the consumers. Existing approaches for producer side measurement are either based on graph cluster-based randomization or on certain treatment propagation assumptions. The former approach results in low-powered experiments as the producer-consumer network density increases and the latter approach lacks a strict notion of error control. In this paper, we propose (i) a quantification of the quality of a producer side experiment design, and (ii) a new experiment design mechanism that generates high-quality experiments based on this quantification. Our approach, called UniCoRn (Unifying Counterfactual Rankings), provides explicit control over the quality of the experiment and its computation cost. Further, we prove that our experiment design is optimal to the proposed design quality measure. Our approach is agnostic to the density of the producer-consumer network and does not rely on any treatment propagation assumption. Moreover, unlike the existing approaches, we do not need to know the underlying network in advance, making this widely applicable to the industrial setting where the underlying network is unknown and challenging to predict a priori due to its dynamic nature. We use simulations to validate our approach and compare it against existing methods. We also deployed UniCoRn in an edge recommendation application that serves tens of millions of members and billions of edge recommendations daily.",
    "authors": [
      "Nandy, Preetam",
      "Venugopalan, Divya",
      "Lo, Chun",
      "Chatterjee, Shaunak"
    ]
  },
  {
    "id": "32e54441e6382a7fbacbbbaf3c450059",
    "title": "Retiring Adult: New Datasets for Fair Machine Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/32e54441e6382a7fbacbbbaf3c450059-Paper.pdf",
    "abstract": "Although the fairness community has recognized the importance of data, researchers in the area primarily rely on UCI Adult when it comes to tabular data. Derived from a 1994 US Census survey, this dataset has appeared in hundreds of research papers where it served as the basis for the development and comparison of many algorithmic fairness interventions. We reconstruct a superset of the UCI Adult data from available US Census sources and reveal idiosyncrasies of the UCI Adult dataset that limit its external validity. Our primary contribution is a suite of new datasets derived from US Census surveys that extend the existing data ecosystem for research on fair machine learning. We create prediction tasks relating to income, employment, health, transportation, and housing. The data span multiple years and all states of the United States, allowing researchers to study temporal shift and geographic variation. We highlight a broad initial sweep of new empirical insights relating to trade-offs between fairness criteria, performance of algorithmic interventions, and the role of distribution shift based on our new datasets. Our findings inform ongoing debates, challenge some existing narratives, and point to future research directions.",
    "authors": [
      "Ding, Frances",
      "Hardt, Moritz",
      "Miller, John",
      "Schmidt, Ludwig"
    ]
  },
  {
    "id": "333222170ab9edca4785c39f55221fe7",
    "title": "Cardinality constrained submodular maximization for random streams",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/333222170ab9edca4785c39f55221fe7-Paper.pdf",
    "abstract": "We consider the problem of maximizing submodular functions in single-pass streaming and secretaries-with-shortlists models, both with random arrival order.For cardinality constrained monotone functions, Agrawal, Shadravan, and Stein~\\cite{SMC19} gave a single-pass $(1-1/e-\\varepsilon)$-approximation algorithm using only linear memory, but their exponential dependence on $\\varepsilon$ makes it impractical even for $\\varepsilon=0.1$.We simplify both the algorithm and the analysis, obtaining an exponential improvement in the $\\varepsilon$-dependence (in particular, $O(k/\\varepsilon)$ memory).Extending these techniques, we also give a simple $(1/e-\\varepsilon)$-approximation for non-monotone functions in $O(k/\\varepsilon)$ memory. For the monotone case, we also give a corresponding unconditional hardness barrier of $1-1/e+\\varepsilon$ for single-pass algorithms in randomly ordered streams, even assuming unlimited computation. Finally, we show that the algorithms are simple to implement and work well on real world datasets.",
    "authors": [
      "Liu, Paul",
      "Rubinstein, Aviad",
      "Vondrak, Jan",
      "Zhao, Junyao"
    ]
  },
  {
    "id": "3341f6f048384ec73a7ba2e77d2db48b",
    "title": "Self-Instantiated Recurrent Units with Dynamic Soft Recursion",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3341f6f048384ec73a7ba2e77d2db48b-Paper.pdf",
    "abstract": "While standard recurrent neural networks explicitly impose a chain structure on different forms of data, they do not have an explicit bias towards recursive self-instantiation where the extent of recursion is dynamic.  Given diverse and even growing data modalities (e.g., logic, algorithmic input and output, music, code, images, and language) that can be expressed in sequences and may benefit from more architectural flexibility, we propose the self-instantiated recurrent unit (Self-IRU) with a novel inductive bias towards dynamic soft recursion. On one hand, theSelf-IRU is characterized by recursive self-instantiation via its gating functions, i.e., gating mechanisms of the Self-IRU are controlled by instances of the Self-IRU itself, which are repeatedly invoked in a recursive fashion. On the other hand, the extent of the Self-IRU recursion is controlled by gates whose values are between 0 and 1 and may vary across the temporal dimension of sequences,  enabling dynamic soft recursion depth at each time step. The architectural flexibility and effectiveness of our proposed approach are demonstrated across multiple data modalities. For example, the Self-IRU achieves state-of-the-art performance on the logical inference dataset [Bowman et al., 2014] even when comparing with competitive models that have access to ground-truth syntactic information.",
    "authors": [
      "Zhang, Aston",
      "Tay, Yi",
      "Shen, Yikang",
      "Chan, Alvin",
      "ZHANG, SHUAI"
    ]
  },
  {
    "id": "334467d41d5cf21e234465a1530ba647",
    "title": "Sparse Uncertainty Representation in Deep Learning with Inducing Weights",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/334467d41d5cf21e234465a1530ba647-Paper.pdf",
    "abstract": "Bayesian Neural Networks and deep ensembles represent two modern paradigms of uncertainty quantification in deep learning. Yet these approaches struggle to scale mainly due to memory inefficiency, requiring parameter storage several times that of their deterministic counterparts. To address this, we augment each weight matrix with a small inducing weight matrix, projecting the uncertainty quantification into a lower dimensional space. We further extend Matheron\u2019s conditional Gaussian sampling rule to enable fast weight sampling, which enables our inference method to maintain reasonable run-time as compared with ensembles. Importantly, our approach achieves competitive performance to the state-of-the-art in prediction and uncertainty estimation tasks with fully connected neural networks and ResNets, while reducing the parameter size to $\\leq 24.3\\%$ of that of a single neural network.",
    "authors": [
      "Ritter, Hippolyt",
      "Kukla, Martin",
      "Zhang, Cheng",
      "Li, Yingzhen"
    ]
  },
  {
    "id": "33853141e0873909be88f5c3e6144cc6",
    "title": "Scalable Inference of Sparsely-changing Gaussian Markov Random Fields ",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/33853141e0873909be88f5c3e6144cc6-Paper.pdf",
    "abstract": "We study the problem of inferring time-varying Gaussian Markov random fields, where the underlying graphical model is both sparse and changes {sparsely} over time. Most of the existing methods for the inference of time-varying Markov random fields (MRFs) rely on the \\textit{regularized maximum likelihood estimation} (MLE), that typically suffer from weak statistical guarantees and high computational time. Instead, we introduce a new class of constrained optimization problems for the inference of sparsely-changing Gaussian MRFs (GMRFs). The proposed optimization problem is formulated based on the exact $\\ell_0$ regularization, and can be solved in near-linear time and memory. Moreover, we show that the proposed estimator enjoys a provably small estimation error. We derive sharp statistical guarantees in the high-dimensional regime, showing that such problems can be learned with as few as one sample per time period. Our proposed method is extremely efficient in practice: it can accurately estimate sparsely-changing GMRFs with more than 500 million variables in less than one hour.",
    "authors": [
      "Fattahi, Salar",
      "Gomez, Andres"
    ]
  },
  {
    "id": "33a854e247155d590883b93bca53848a",
    "title": "Grad2Task: Improved Few-shot Text Classification Using Gradients for Task Representation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/33a854e247155d590883b93bca53848a-Paper.pdf",
    "abstract": "Large pretrained language models (LMs) like BERT have improved performance in many disparate natural language processing (NLP) tasks. However, fine tuning such models requires a large number of training examples for each target task. Simultaneously, many realistic NLP problems are \"few shot\", without a sufficiently large training set. In this work, we propose a novel conditional neural process-based approach for few-shot text classification that learns to transfer from other diverse tasks with rich annotation. Our key idea is to represent each task using gradient information from a base model and to train an adaptation network that modulates a text classifier conditioned on the task representation. While previous task-aware few-shot learners represent tasks by input encoding, our novel task representation is more powerful, as the gradient captures input-output relationships of a task. Experimental results show that our approach outperforms traditional fine-tuning, sequential transfer learning, and state-of-the-art meta learning approaches on a collection of diverse few-shot tasks. We further conducted analysis and ablations to justify our design choices.",
    "authors": [
      "Wang, Jixuan",
      "Wang, Kuan-Chieh",
      "Rudzicz, Frank",
      "Brudno, Michael"
    ]
  },
  {
    "id": "33b3214d792caf311e1f00fd22b392c5",
    "title": "Learnability of Linear Thresholds from Label Proportions",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/33b3214d792caf311e1f00fd22b392c5-Paper.pdf",
    "abstract": "We study the problem of properly learning linear threshold functions (LTFs) in the learning from label proportions (LLP) framework. In this, the learning is on a collection of bags of feature-vectors with only the proportion of labels available for each bag. First, we provide an algorithm that, given a collection of such bags each of size at most two whose label proportions are consistent with (i.e., the bags are satisfied by) an unknown LTF, efficiently produces an LTF that satisfies at least $(2/5)$-fraction of the bags. If all the bags are non-monochromatic (i.e., bags of size two with differently labeled feature-vectors) the algorithm satisfies at least $(1/2)$-fraction of them. For the special case of OR over the $d$-dimensional boolean vectors, we give an algorithm which computes an LTF achieving an additional $\\Omega(1/d)$ in accuracy for the two cases.Our main result provides evidence that these algorithmic bounds cannot be significantly improved, even for learning monotone ORs using LTFs. We prove that it is NP-hard, given a collection of non-monochromatic bags which are all satisfied by some monotone OR, to compute any function of constantly many LTFs that satisfies  $(1/2 + \\varepsilon)$-fraction of the bags for any constant $\\varepsilon > 0$. This bound is tight for the non-monochromatic bags case.The above is in contrast to the usual supervised learning setup (i.e., unit-sized bags) in which LTFs are efficiently learnable to arbitrary accuracy using linear programming, and even a trivial algorithm (any LTF or its complement) achieves an accuracy of $1/2$. These techniques however, fail in the LLP setting. Indeed, we show that the LLP learning of LTFs (even for the special case of monotone ORs) using LTFs dramatically increases in complexity as soon as bags of size two are allowed.Our work gives the first inapproximability for LLP learning LTFs, and a strong complexity separation between LLP  and traditional supervised learning.",
    "authors": [
      "Saket, Rishi"
    ]
  },
  {
    "id": "33ceb07bf4eeb3da587e268d663aba1a",
    "title": "A variational approximate posterior for the deep Wishart process",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/33ceb07bf4eeb3da587e268d663aba1a-Paper.pdf",
    "abstract": "Recent work introduced deep kernel processes as an entirely kernel-based alternative to NNs (Aitchison et al. 2020). Deep kernel processes flexibly learn good top-layer representations by alternately sampling the kernel from a distribution over positive semi-definite matrices and performing nonlinear transformations. A particular deep kernel process, the deep Wishart process (DWP), is of particular interest because its prior can be made equivalent to deep Gaussian process (DGP) priors for kernels that can be expressed entirely in terms of Gram matrices. However, inference in DWPs has not yet been possible due to the lack of sufficiently flexible distributions over positive semi-definite matrices. Here, we give a novel approach to obtaining flexible distributions over positive semi-definite matrices by generalising the Bartlett decomposition of the Wishart probability density. We use this new distribution to develop an approximate posterior for the DWP that includes dependency across layers. We develop a doubly-stochastic inducing-point inference scheme for the DWP and show experimentally that inference in the DWP can improve performance over doing inference in a DGP with the equivalent prior.",
    "authors": [
      "Ober, Sebastian",
      "Aitchison, Laurence"
    ]
  },
  {
    "id": "33d6548e48d4318ceb0e3916a79afc84",
    "title": "Neural Pseudo-Label Optimism for the Bank Loan Problem",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/33d6548e48d4318ceb0e3916a79afc84-Paper.pdf",
    "abstract": "We study a class of classification problems best exemplified by the \\emph{bank loan} problem, where a lender decides whether or not to issue a loan. The lender only observes whether a customer will repay a loan if the loan is issued to begin with, and thus modeled decisions affect what data is available to the lender for future decisions. As a result, it is possible for the lender's algorithm to ``get stuck'' with a self-fulfilling model. This model never corrects its false negatives, since it never sees the true label for rejected data, thus accumulating infinite regret. In the case of linear models, this issue can be addressed by adding optimism directly into the model predictions. However, there are few methods that extend to the function approximation case using Deep Neural Networks. We present Pseudo-Label Optimism (PLOT), a conceptually and computationally simple method for this setting applicable to DNNs. \\PLOT{} adds an optimistic label to the subset of decision points the current model is deciding on, trains the model on all data so far (including these points along with their optimistic labels), and finally uses the resulting \\emph{optimistic} model for decision making. \\PLOT{} achieves competitive performance on a set of three challenging benchmark problems, requiring minimal hyperparameter tuning. We also show that \\PLOT{} satisfies a logarithmic regret guarantee, under a Lipschitz and logistic mean label model, and under a separability condition on the data. ",
    "authors": [
      "Pacchiano, Aldo",
      "Singh, Shaun",
      "Chou, Edward",
      "Berg, Alex",
      "Foerster, Jakob"
    ]
  },
  {
    "id": "33ebd5b07dc7e407752fe773eed20635",
    "title": "Visualizing the Emergence of Intermediate Visual Patterns in DNNs",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/33ebd5b07dc7e407752fe773eed20635-Paper.pdf",
    "abstract": "This paper proposes a method to visualize the discrimination power of intermediate-layer visual patterns encoded by a DNN. Specifically, we visualize (1) how the DNN gradually learns regional visual patterns in each intermediate layer during the training process, and (2) the effects of the DNN using non-discriminative patterns in low layers to construct disciminative patterns in middle/high layers through the forward propagation. Based on our visualization method, we can quantify knowledge points (i.e. the number of discriminative visual patterns) learned by the DNN to evaluate the representation capacity of the DNN. Furthermore, this method also provides new insights into signal-processing behaviors of existing deep-learning techniques, such as adversarial attacks and knowledge distillation.",
    "authors": [
      "Li, Mingjie",
      "Wang, Shaobo",
      "Zhang, Quanshi"
    ]
  },
  {
    "id": "3413ce14d52b87557e87e2c1518c2cbe",
    "title": "Learning 3D Dense Correspondence via Canonical Point Autoencoder",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3413ce14d52b87557e87e2c1518c2cbe-Paper.pdf",
    "abstract": "We propose a canonical point autoencoder (CPAE) that predicts dense correspondences between 3D shapes of the same category. The autoencoder performs two key functions: (a) encoding an arbitrarily ordered point cloud to a canonical primitive, e.g., a sphere, and (b) decoding the primitive back to the original input instance shape. As being placed in the bottleneck, this primitive plays a key role to map all the unordered point clouds on the canonical surface, and to be reconstructed in an ordered fashion. Once trained, points from different shape instances that are mapped to the same locations on the primitive surface are determined to be a pair of correspondence. Our method does not require any form of annotation or self-supervised part segmentation network and can handle unaligned input point clouds within a certain rotation range. Experimental results on 3D semantic keypoint transfer and part segmentation transfer show that our model performs favorably against state-of-the-art correspondence learning methods.",
    "authors": [
      "Cheng, An-Chieh",
      "Li, Xueting",
      "Sun, Min",
      "Yang, Ming-Hsuan",
      "Liu, Sifei"
    ]
  },
  {
    "id": "344ef5151be171062f42f03e69663ecf",
    "title": "Speech-T: Transducer for Text to Speech and Beyond",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/344ef5151be171062f42f03e69663ecf-Paper.pdf",
    "abstract": "Neural Transducer (e.g., RNN-T) has been widely used in automatic speech recognition (ASR) due to its capabilities of efficiently modeling monotonic alignments between input and output sequences and naturally supporting streaming inputs. Considering that monotonic alignments are also critical to text to speech (TTS) synthesis and streaming TTS is also an important application scenario, in this work, we explore the possibility of applying Transducer to TTS and more. However, it is challenging because it is difficult to trade off the emission (continuous mel-spectrogram prediction) probability and transition (ASR Transducer predicts blank token to indicate transition to next input) probability when calculating the output probability lattice in Transducer, and it is not easy to learn the alignments between text and speech through the output probability lattice. We propose SpeechTransducer (Speech-T for short), a Transformer based Transducer model that 1) uses a new forward algorithm to separate the transition prediction from the continuous mel-spectrogram prediction when calculating the output probability lattice, and uses a diagonal constraint in the probability lattice to help the alignment learning; 2) supports both full-sentence or streaming TTS by adjusting the look-ahead context; and 3) further supports both TTS and ASR together for the first time, which enjoys several advantages including fewer parameters as well as streaming synthesis and recognition in a single model. Experiments on LJSpeech datasets demonstrate that Speech-T 1) is more robust than the attention based autoregressive TTS model due to its inherent monotonic alignments between text and speech; 2) naturally supports streaming TTS with good voice quality; and 3) enjoys the benefit of joint modeling TTS and ASR in a single network.",
    "authors": [
      "Chen, Jiawei",
      "Tan, Xu",
      "Leng, Yichong",
      "Xu, Jin",
      "Wen, Guihua",
      "Qin, Tao",
      "Liu, Tie-Yan"
    ]
  },
  {
    "id": "3473decccb0509fb264818a7512a8b9b",
    "title": "Multi-modal Dependency Tree for Video Captioning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3473decccb0509fb264818a7512a8b9b-Paper.pdf",
    "abstract": "Generating fluent and relevant language to describe visual content is critical for the video captioning task. Many existing methods generate captions using sequence models that predict words in a left-to-right order. In this paper, we investigate a graph-structured model for caption generation by explicitly modeling the hierarchical structure in the sentences to further improve the fluency and relevance of sentences. To this end, we propose a novel video captioning method that generates a sentence by first constructing a multi-modal dependency tree and then traversing the constructed tree, where the syntactic structure and semantic relationship in the sentence are represented by the tree topology. To take full advantage of the information from both vision and language, both the visual and textual representation features are encoded into each tree node. Different from existing dependency parsing methods that generate uni-modal dependency trees for language understanding, our method construct s multi-modal dependency trees for language generation of images and videos. We also propose a tree-structured reinforcement learning algorithm to effectively optimize the captioning model where a novel reward is designed by evaluating the semantic consistency between the generated sub-tree and the ground-truth tree. Extensive experiments on several video captioning datasets demonstrate the effectiveness of the proposed method. ",
    "authors": [
      "Zhao, Wentian",
      "Wu, Xinxiao",
      "Luo, Jiebo"
    ]
  },
  {
    "id": "347665597cbfaef834886adbb848011f",
    "title": "Greedy and Random Quasi-Newton Methods with Faster Explicit Superlinear Convergence",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/347665597cbfaef834886adbb848011f-Paper.pdf",
    "abstract": "In this paper, we follow Rodomanov and Nesterov\u2019s work to study quasi-Newton methods. We focus on the common SR1 and BFGS quasi-Newton methods to establish better explicit (local) superlinear convergence rates. First, based on the greedy quasi-Newton update which greedily selects the direction to maximize a certain measure of progress, we improve the convergence rate to a condition-number-free superlinear convergence rate. Second, based on the random quasi-Newton update that selects the direction randomly from a spherically symmetric distribution, we show the same superlinear convergence rate established as above. Our analysis is closely related to the approximation of a given Hessian matrix, unconstrained quadratic objective, as well as the general strongly convex, smooth, and strongly self-concordant functions.",
    "authors": [
      "Lin, Dachao",
      "Ye, Haishan",
      "Zhang, Zhihua"
    ]
  },
  {
    "id": "348a38cd25abeab0e440f37510e9b1fa",
    "title": "Neural Tangent Kernel Maximum Mean Discrepancy",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/348a38cd25abeab0e440f37510e9b1fa-Paper.pdf",
    "abstract": "We present a novel neural network Maximum Mean Discrepancy (MMD) statistic by identifying a new connection between neural tangent kernel (NTK) and MMD. This connection enables us to develop a computationally efficient and memory-efficient approach to compute the MMD statistic and perform NTK based two-sample tests towards addressing the long-standing challenge of memory and computational complexity of the MMD statistic, which is essential for online implementation to assimilating new samples. Theoretically, such a connection allows us to understand the NTK test statistic properties, such as the Type-I error and testing power for performing the two-sample test, by adapting existing theories for kernel MMD. Numerical experiments on synthetic and real-world datasets validate the theory and demonstrate the effectiveness of the proposed NTK-MMD statistic.",
    "authors": [
      "Cheng, Xiuyuan",
      "Xie, Yao"
    ]
  },
  {
    "id": "34adeb8e3242824038aa65460a47c29e",
    "title": "Subgraph Federated Learning with Missing Neighbor Generation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/34adeb8e3242824038aa65460a47c29e-Paper.pdf",
    "abstract": "Graphs have been widely used in data mining and machine learning due to their unique representation of real-world objects and their interactions. As graphs are getting bigger and bigger nowadays, it is common to see their subgraphs separately collected and stored in multiple local systems. Therefore, it is natural to consider the subgraph federated learning setting, where each local system holds a small subgraph that may be biased from the distribution of the whole graph. Hence, the subgraph federated learning aims to collaboratively train a powerful and generalizable graph mining model without directly sharing their graph data. In this work, towards the novel yet realistic setting of subgraph federated learning, we propose two major techniques: (1) FedSage, which trains a GraphSage model based on FedAvg to integrate node features, link structures, and task labels on multiple local subgraphs; (2) FedSage+, which trains a missing neighbor generator along FedSage to deal with missing links across local subgraphs. Empirical results on four real-world graph datasets with synthesized subgraph federated learning settings demonstrate the effectiveness and efficiency of our proposed techniques. At the same time, consistent theoretical implications are made towards their generalization ability on the global graphs.",
    "authors": [
      "ZHANG, Ke",
      "Yang, Carl",
      "Li, Xiaoxiao",
      "Sun, Lichao",
      "Yiu, Siu Ming"
    ]
  },
  {
    "id": "34f98c7c5d7063181da890ea8d25265a",
    "title": "Bellman-consistent Pessimism for Offline Reinforcement Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/34f98c7c5d7063181da890ea8d25265a-Paper.pdf",
    "abstract": "The use of pessimism, when reasoning about datasets lacking exhaustive exploration has recently gained prominence in offline reinforcement learning. Despite the robustness it adds to the algorithm, overly pessimistic reasoning can be equally damaging in precluding the discovery of good policies, which is an issue for the popular bonus-based pessimism. In this paper, we introduce the notion of Bellman-consistent pessimism for general function approximation: instead of calculating a point-wise lower bound for the value function, we implement pessimism at the initial state over the set of functions consistent with the Bellman equations. Our theoretical guarantees only require Bellman closedness as standard in the exploratory setting, in which case bonus-based pessimism fails to provide guarantees.  Even in the special case of linear function approximation where stronger expressivity assumptions hold, our result improves upon a recent bonus-based approach by $\\mathcal O(d)$ in its sample complexity (when the action space is finite). Remarkably, our algorithms automatically adapt to the best bias-variance tradeoff in the hindsight, whereas most prior approaches require tuning extra hyperparameters a priori.",
    "authors": [
      "Xie, Tengyang",
      "Cheng, Ching-An",
      "Jiang, Nan",
      "Mineiro, Paul",
      "Agarwal, Alekh"
    ]
  },
  {
    "id": "3501672ebc68a5524629080e3ef60aef",
    "title": "Can You Learn an Algorithm?  Generalizing from Easy to Hard Problems with Recurrent Networks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3501672ebc68a5524629080e3ef60aef-Paper.pdf",
    "abstract": "Deep neural networks are powerful machines for visual pattern recognition, but reasoning tasks that are easy for humans may still be difficult for neural models. Humans possess the ability to extrapolate reasoning strategies learned on simple problems to solve harder examples, often by thinking for longer. For example, a person who has learned to solve small mazes can easily extend the very same search techniques to solve much larger mazes by spending more time.  In computers, this behavior is often achieved through the use of algorithms, which scale to arbitrarily hard problem instances at the cost of more computation. In contrast, the sequential computing budget of feed-forward neural networks is limited by their depth, and networks trained on simple problems have no way of extending their reasoning to accommodate harder problems. In this work, we show that recurrent networks trained to solve simple problems with few recurrent steps can indeed solve much more complex problems simply by performing additional recurrences during inference. We demonstrate this algorithmic behavior of recurrent networks on prefix sum computation, mazes, and chess.  In all three domains, networks trained on simple problem instances are able to extend their reasoning abilities at test time simply by \"thinking for longer.\"",
    "authors": [
      "Schwarzschild, Avi",
      "Borgnia, Eitan",
      "Gupta, Arjun",
      "Huang, Furong",
      "Vishkin, Uzi",
      "Goldblum, Micah",
      "Goldstein, Tom"
    ]
  },
  {
    "id": "35309226eb45ec366ca86a4329a2b7c3",
    "title": "Sub-Linear Memory: How to Make Performers SLiM",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/35309226eb45ec366ca86a4329a2b7c3-Paper.pdf",
    "abstract": "Transformer architectures have become very popular yet the original implementation requires  $O(L^2)$ in serial time and memory as functions of input length $L$. Recent works proposed various linear self-attention mechanisms, scaling only as $O(L)$ for serial computation. We conduct a thorough complexity analysis of Performers, a class which includes most recent linear Transformer mechanisms. We note a remarkable computational flexibility: the gradient computation can be performed with no approximations using sublinear memory as a function of $L$ (in addition to negligible storage for the input sequence), at a cost of greater time complexity in the parallel setting. In the extreme case, a Performer consumes only $O(1)$ memory, and still requires $O(L)$ time. Due to complete backward-compatibility, this discovered time-memory tradeoff can be used for fine-tuning on low-memory devices in a decentralized fashion without any server computations.",
    "authors": [
      "Likhosherstov, Valerii",
      "Choromanski, Krzysztof M.",
      "Davis, Jared Quincy",
      "Song, Xingyou",
      "Weller, Adrian"
    ]
  },
  {
    "id": "3556a3018cce3076e27dbbf9645b44d5",
    "title": "Efficient Learning of Discrete-Continuous Computation Graphs",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3556a3018cce3076e27dbbf9645b44d5-Paper.pdf",
    "abstract": "Numerous models for supervised and reinforcement learning benefit from combinations of discrete and continuous model components. End-to-end learnable discrete-continuous models are compositional, tend to generalize better, and are more interpretable. A popular approach to building discrete-continuous computation graphs is that of integrating discrete probability distributions into neural networks using stochastic softmax tricks. Prior work has mainly focused on computation graphs with a single discrete component on each of the graph's execution paths. We analyze the behavior of more complex stochastic computations graphs with multiple sequential discrete components. We show that it is challenging to optimize the parameters of these models, mainly due to small  gradients and local minima. We then propose two new strategies to overcome these challenges. First, we show that increasing the scale parameter of the Gumbel noise perturbations during training improves the learning behavior. Second, we propose dropout residual connections specifically tailored to stochastic, discrete-continuous computation graphs. With an extensive set of experiments, we show that we can train complex discrete-continuous models which one cannot train with standard stochastic softmax tricks. We also show that complex discrete-stochastic models generalize better than their continuous counterparts on several benchmark datasets.",
    "authors": [
      "Friede, David",
      "Niepert, Mathias"
    ]
  },
  {
    "id": "3569df159ec477451530c4455b2a9e86",
    "title": "VQ-GNN: A Universal Framework to Scale up Graph Neural Networks using Vector Quantization",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3569df159ec477451530c4455b2a9e86-Paper.pdf",
    "abstract": "Most state-of-the-art Graph Neural Networks (GNNs) can be defined as a form of graph convolution which can be realized by message passing between direct neighbors or beyond. To scale such GNNs to large graphs, various neighbor-, layer-, or subgraph-sampling techniques are proposed to alleviate the \"neighbor explosion\" problem by considering only a small subset of messages passed to the nodes in a mini-batch. However, sampling-based methods are difficult to apply to GNNs that utilize many-hops-away or global context each layer, show unstable performance for different tasks and datasets, and do not speed up model inference. We propose a principled and fundamentally different approach, VQ-GNN, a universal framework to scale up any convolution-based GNNs using Vector Quantization (VQ) without compromising the performance. In contrast to sampling-based techniques, our approach can effectively preserve all the messages passed to a mini-batch of nodes by learning and updating a small number of quantized reference vectors of global node representations, using VQ within each GNN layer. Our framework avoids the \"neighbor explosion\" problem of GNNs using quantized representations combined with a low-rank version of the graph convolution matrix. We show that such a compact low-rank version of the gigantic convolution matrix is sufficient both theoretically and experimentally. In company with VQ, we design a novel approximated message passing algorithm and a nontrivial back-propagation rule for our framework. Experiments on various types of GNN backbones demonstrate the scalability and competitive performance of our framework on large-graph node classification and link prediction benchmarks.",
    "authors": [
      "Ding, Mucong",
      "Kong, Kezhi",
      "Li, Jingling",
      "Zhu, Chen",
      "Dickerson, John",
      "Huang, Furong",
      "Goldstein, Tom"
    ]
  },
  {
    "id": "357cfba15668cc2e1e73111e09d54383",
    "title": "Overcoming Catastrophic Forgetting in Incremental Few-Shot Learning by Finding Flat Minima",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/357cfba15668cc2e1e73111e09d54383-Paper.pdf",
    "abstract": "This paper considers incremental few-shot learning, which requires a model to continually recognize new categories with only a few examples provided. Our study shows that existing methods severely suffer from catastrophic forgetting, a well-known problem in incremental learning, which is aggravated due to data scarcity and imbalance in the few-shot setting. Our analysis further suggests that to prevent catastrophic forgetting, actions need to be taken in the primitive stage -- the training of base classes instead of later few-shot learning sessions. Therefore, we propose to search for flat local minima of the base training objective function and then fine-tune the model parameters within the flat region on new tasks. In this way, the model can efficiently learn new classes while preserving the old ones. Comprehensive experimental results demonstrate that our approach outperforms all prior state-of-the-art methods and is very close to the approximate upper bound. The source code is available at https://github.com/moukamisama/F2M.",
    "authors": [
      "SHI, Guangyuan",
      "CHEN, JIAXIN",
      "Zhang, Wenlong",
      "Zhan, Li-Ming",
      "Wu, Xiao-Ming"
    ]
  },
  {
    "id": "35936504a37d53e03abdfbc7318d9ec7",
    "title": "Functional Neural Networks for Parametric Image Restoration Problems",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/35936504a37d53e03abdfbc7318d9ec7-Paper.pdf",
    "abstract": "Almost every single image restoration problem has a closely related parameter, such as the scale factor in super-resolution, the noise level in image denoising, and the quality factor in JPEG deblocking. Although recent studies on image restoration problems have achieved great success due to the development of deep neural networks, they handle the parameter involved in an unsophisticated way. Most previous researchers either treat problems with different parameter levels as independent tasks, and train a specific model for each parameter level; or simply ignore the parameter, and train a single model for all parameter levels. The two popular approaches have their own shortcomings. The former is inefficient in computing and the latter is ineffective in performance. In this work, we propose a novel system called functional neural network (FuncNet) to solve a parametric image restoration problem with a single model. Unlike a plain neural network, the smallest conceptual element of our FuncNet is no longer a floating-point variable, but a function of the parameter of the problem. This feature makes it both efficient and effective for a parametric problem. We apply FuncNet to super-resolution, image denoising, and JPEG deblocking. The experimental results show the superiority of our FuncNet on all three parametric image restoration tasks over the state of the arts.",
    "authors": [
      "Luo, Fangzhou",
      "Wu, Xiaolin",
      "Guo, Yanhui"
    ]
  },
  {
    "id": "35a12c43227f217207d4e06ffefe39d3",
    "title": "Intrinsic Dimension, Persistent Homology and Generalization in Neural Networks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/35a12c43227f217207d4e06ffefe39d3-Paper.pdf",
    "abstract": "Disobeying the classical wisdom of statistical learning theory, modern deep neural networks generalize well even though they typically contain millions of parameters. Recently, it has been shown that the trajectories of iterative optimization algorithms can possess \\emph{fractal structures}, and their generalization error can be formally linked to the complexity of such fractals. This complexity is measured by the fractal's \\emph{intrinsic dimension}, a quantity usually much smaller than the number of parameters in the network. Even though this perspective provides an explanation for why overparametrized networks would not overfit, computing the intrinsic dimension (\\eg, for monitoring generalization during training) is a notoriously difficult task,  where existing methods typically fail even in moderate ambient dimensions. In this study, we consider this problem from the lens of topological data analysis (TDA) and develop a generic computational tool that is built on rigorous mathematical foundations. By making a novel connection between learning theory and TDA, we first illustrate that the generalization error can be equivalently bounded in terms of a notion called the 'persistent homology dimension' (PHD), where, compared with prior work, our approach does not require any additional geometrical or statistical assumptions on the training dynamics. Then, by utilizing recently established theoretical results and TDA tools, we develop an efficient algorithm to estimate PHD in the scale of modern deep neural networks and further provide visualization tools to help understand generalization in deep learning. Our experiments show that the proposed approach can efficiently compute a network's intrinsic dimension in a variety of settings, which is predictive of the generalization error. ",
    "authors": [
      "Birdal, Tolga",
      "Lou, Aaron",
      "Guibas, Leonidas J.",
      "Simsekli, Umut"
    ]
  },
  {
    "id": "35cf8659cfcb13224cbd47863a34fc58",
    "title": "GemNet: Universal Directional Graph Neural Networks for Molecules",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/35cf8659cfcb13224cbd47863a34fc58-Paper.pdf",
    "abstract": "Effectively predicting molecular interactions has the potential to accelerate molecular dynamics by multiple orders of magnitude and thus revolutionize chemical simulations. Graph neural networks (GNNs) have recently shown great successes for this task, overtaking classical methods based on fixed molecular kernels. However, they still appear very limited from a theoretical perspective, since regular GNNs cannot distinguish certain types of graphs. In this work we close this gap between theory and practice. We show that GNNs with directed edge embeddings and two-hop message passing are indeed universal approximators for predictions that are invariant to translation, and equivariant to permutation and rotation. We then leverage these insights and multiple structural improvements to propose the geometric message passing neural network (GemNet). We demonstrate the benefits of the proposed changes in multiple ablation studies. GemNet outperforms previous models on the COLL, MD17, and OC20 datasets by 34%, 41%, and 20%, respectively, and performs especially well on the most challenging molecules. Our implementation is available online.",
    "authors": [
      "Gasteiger, Johannes",
      "Becker, Florian",
      "G\u00fcnnemann, Stephan"
    ]
  },
  {
    "id": "36165c62f7b7df72863d470d73302627",
    "title": "Loss function based second-order Jensen inequality and its application to particle variational inference",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/36165c62f7b7df72863d470d73302627-Paper.pdf",
    "abstract": "Bayesian model averaging, obtained as the expectation of a likelihood function by a posterior distribution, has been widely used for prediction, evaluation of uncertainty, and model selection. Various approaches have been developed to efficiently capture the information in the posterior distribution; one such approach is the optimization of a set of models simultaneously with interaction to ensure the diversity of the individual models in the same way as ensemble learning. A representative approach is particle variational inference (PVI), which uses an ensemble of models as an empirical approximation for the posterior distribution. PVI iteratively updates each model with a repulsion force to ensure the diversity of the optimized models. However, despite its promising performance, a theoretical understanding of this repulsion and its association with the generalization ability remains unclear. In this paper, we tackle this problem in light of PAC-Bayesian analysis. First, we provide a new second-order Jensen inequality, which has the repulsion term based on the loss function. Thanks to the repulsion term, it is tighter than the standard Jensen inequality. Then, we derive a novel generalization error bound and show that it can be reduced by enhancing the diversity of models. Finally, we derive a new PVI that optimizes the generalization error bound directly. Numerical experiments demonstrate that the performance of the proposed PVI compares favorably with existing methods in the experiment.",
    "authors": [
      "Futami, Futoshi",
      "Iwata, Tomoharu",
      "ueda, naonori",
      "Sato, Issei",
      "Sugiyama, Masashi"
    ]
  },
  {
    "id": "362387494f6be6613daea643a7706a42",
    "title": "Detecting and Adapting to Irregular Distribution Shifts in Bayesian Online Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/362387494f6be6613daea643a7706a42-Paper.pdf",
    "abstract": "We consider the problem of online learning in the presence of distribution shifts that occur at an unknown rate and of unknown intensity. We derive a new Bayesian online inference approach to simultaneously infer these distribution shifts and adapt the model to the detected changes by integrating ideas from change point detection, switching dynamical systems, and Bayesian online learning. Using a binary \u2018change variable,\u2019 we construct an informative prior such that--if a change is detected--the model partially erases the information of past model updates by tempering to facilitate adaptation to the new data distribution. Furthermore, the approach uses beam search to track multiple change-point hypotheses and selects the most probable one in hindsight. Our proposed method is model-agnostic, applicable in both supervised and unsupervised learning settings, suitable for an environment of concept drifts or covariate drifts, and yields improvements over state-of-the-art Bayesian online learning approaches.",
    "authors": [
      "Li, Aodong",
      "Boyd, Alex",
      "Smyth, Padhraic",
      "Mandt, Stephan"
    ]
  },
  {
    "id": "362c99307cdc3f2d8b410652386a9dd1",
    "title": "Asynchronous Decentralized SGD with Quantized and Local Updates",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/362c99307cdc3f2d8b410652386a9dd1-Paper.pdf",
    "abstract": "Decentralized optimization is emerging as a viable alternative for scalable distributed machine learning, but also introduces new challenges in terms of synchronization costs.  To this end, several communication-reduction techniques, such as non-blocking communication, quantization, and local steps, have been explored in the decentralized setting. Due to the complexity of analyzing optimization in such a relaxed setting, this line of work often assumes \\emph{global} communication rounds, which require additional synchronization. In this paper, we consider decentralized optimization in the simpler, but harder to analyze, \\emph{asynchronous gossip} model, in which communication occurs in discrete, randomly chosen pairings among nodes. Perhaps surprisingly, we show that a variant of SGD called \\emph{SwarmSGD} still converges in this setting, even if \\emph{non-blocking communication}, \\emph{quantization}, and \\emph{local steps} are all applied \\emph{in conjunction}, and even if the node data distributions and underlying graph topology are both \\emph{heterogenous}. Our analysis is based on a new connection with multi-dimensional load-balancing processes. We implement this algorithm and deploy it in a super-computing environment, showing that it can outperform previous decentralized methods in terms of end-to-end training time, and that it can even rival carefully-tuned large-batch SGD for certain tasks. ",
    "authors": [
      "Nadiradze, Giorgi",
      "Sabour, Amirmojtaba",
      "Davies, Peter",
      "Li, Shigang",
      "Alistarh, Dan"
    ]
  },
  {
    "id": "367147f1755502d9bc6189f8e2c3005d",
    "title": "Stochastic Shortest Path: Minimax, Parameter-Free and Towards Horizon-Free Regret",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/367147f1755502d9bc6189f8e2c3005d-Paper.pdf",
    "abstract": "We study the problem of learning in the stochastic shortest path (SSP) setting, where an agent seeks to minimize the expected cost accumulated before reaching a goal state. We design a novel model-based algorithm EB-SSP that carefully skews the empirical transitions and perturbs the empirical costs with an exploration bonus to induce an optimistic SSP problem whose associated value iteration scheme is guaranteed to converge. We prove that EB-SSP achieves the minimax regret rate $\\widetilde{O}(B_{\\star} \\sqrt{S A K})$, where $K$ is the number of episodes, $S$ is the number of states, $A$ is the number of actions and $B_{\\star}$ bounds the expected cumulative cost of the optimal policy from any state, thus closing the gap with the lower bound. Interestingly, EB-SSP obtains this result while being parameter-free, i.e., it does not require any prior knowledge of $B_{\\star}$, nor of $T_{\\star}$, which bounds the expected time-to-goal of the optimal policy from any state. Furthermore, we illustrate various cases (e.g., positive costs, or general costs when an order-accurate estimate of $T_{\\star}$ is available) where the regret only contains a logarithmic dependence on $T_{\\star}$, thus yielding the first (nearly) horizon-free regret bound beyond the finite-horizon MDP setting.",
    "authors": [
      "Tarbouriech, Jean",
      "Zhou, Runlong",
      "Du, Simon S.",
      "Pirotta, Matteo",
      "Valko, Michal",
      "Lazaric, Alessandro"
    ]
  },
  {
    "id": "36bedb6eb7152f39b16328448942822b",
    "title": "Nested Counterfactual Identification from Arbitrary Surrogate Experiments",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/36bedb6eb7152f39b16328448942822b-Paper.pdf",
    "abstract": "The Ladder of Causation describes three qualitatively different types of activities an agent may be interested in engaging in, namely, seeing (observational), doing (interventional), and imagining (counterfactual) (Pearl and Mackenzie, 2018). The inferential challenge imposed by the causal hierarchy is that data is collected by an agent observing or intervening in a system (layers 1 and 2), while its goal may be to understand what would have happened had it taken a different course of action, contrary to what factually ended up happening (layer 3). While there exists a solid understanding of the conditions under which cross-layer inferences are allowed from observations to interventions, the results are somewhat scarcer when targeting counterfactual quantities. In this paper, we study the identification of nested counterfactuals from an arbitrary combination of observations and experiments. Specifically, building on a more explicit definition of nested counterfactuals, we prove the counterfactual unnesting theorem (CUT), which allows one to map arbitrary nested counterfactuals to unnested ones. For instance, applications in mediation and fairness analysis usually evoke notions of direct, indirect, and spurious effects, which naturally require nesting. Second, we introduce a sufficient and necessary graphical condition for counterfactual identification from an arbitrary combination of observational and experimental distributions. Lastly, we develop an efficient and complete algorithm for identifying nested counterfactuals; failure of the algorithm returning an expression for a query implies it is not identifiable.",
    "authors": [
      "Correa, Juan",
      "Lee, Sanghack",
      "Bareinboim, Elias"
    ]
  },
  {
    "id": "36f4d832825380f102846560a5104c90",
    "title": "Sim and Real: Better Together",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/36f4d832825380f102846560a5104c90-Paper.pdf",
    "abstract": "Simulation is used extensively in autonomous systems, particularly in robotic manipulation. By far, the most common approach is to train a controller in simulation, and then use it as an initial starting point for the real system. We demonstrate how to learn simultaneously from both simulation and interaction with the real environment. We propose an algorithm for balancing the large number of samples from the high throughput but less accurate simulation and the low-throughput, high-fidelity and costly samples from the real environment. We achieve that by maintaining a replay buffer for each environment the agent interacts with. We analyze such multi-environment interaction theoretically, and provide convergence properties, through a novel theoretical replay buffer analysis.  We demonstrate the efficacy of our method on a sim-to-real environment.",
    "authors": [
      "Di-Castro, Shirli",
      "Di Castro, Dotan",
      "Mannor, Shie"
    ]
  },
  {
    "id": "371bce7dc83817b7893bcdeed13799b5",
    "title": "Trustworthy Multimodal Regression with Mixture of Normal-inverse Gamma Distributions",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/371bce7dc83817b7893bcdeed13799b5-Paper.pdf",
    "abstract": "Multimodal regression is a fundamental task, which integrates the information from different sources to improve the performance of follow-up applications. However, existing methods mainly focus on improving the performance and often ignore the confidence of prediction for diverse situations. In this study, we are devoted to trustworthy multimodal regression which is critical in cost-sensitive domains. To this end, we introduce a novel Mixture of Normal-Inverse Gamma distributions (MoNIG) algorithm, which efficiently estimates uncertainty in principle for adaptive integration of different modalities and produces a trustworthy regression result. Our model can be dynamically aware of uncertainty for each modality, and also robust for corrupted modalities. Furthermore, the proposed MoNIG ensures explicitly representation of (modality-specific/global) epistemic and aleatoric uncertainties, respectively. Experimental results on both synthetic and different real-world data demonstrate the effectiveness and trustworthiness of our method on various multimodal regression tasks (e.g., temperature prediction for superconductivity, relative location prediction for CT slices, and multimodal sentiment analysis).",
    "authors": [
      "Ma, Huan",
      "Han, Zongbo",
      "Zhang, Changqing",
      "Fu, Huazhu",
      "Zhou, Joey Tianyi",
      "Hu, Qinghua"
    ]
  },
  {
    "id": "37693cfc748049e45d87b8c7d8b9aacd",
    "title": "An Empirical Study of Adder Neural Networks for Object Detection",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/37693cfc748049e45d87b8c7d8b9aacd-Paper.pdf",
    "abstract": "Adder neural networks (AdderNets) have shown impressive performance on image classification with only addition operations, which are more energy efficient than traditional convolutional neural networks built with multiplications. Compared with classification, there is a strong demand on reducing the energy consumption of modern object detectors via AdderNets for real-world applications such as autonomous driving and face detection. In this paper, we present an empirical study of AdderNets for object detection. We first reveal that the batch normalization statistics in the pre-trained adder backbone should not be frozen, since the relatively large feature variance of AdderNets. Moreover, we insert more shortcut connections in the neck part and design a new feature fusion architecture for avoiding the sparse features of adder layers. We present extensive ablation studies to explore several design choices of adder detectors. Comparisons with state-of-the-arts are conducted on COCO and PASCAL VOC benchmarks. Specifically, the proposed Adder FCOS achieves a 37.8% AP on the COCO val set, demonstrating comparable performance to that of the convolutional counterpart with an about $1.4\\times$ energy reduction.",
    "authors": [
      "Chen, Xinghao",
      "Xu, Chang",
      "Dong, Minjing",
      "XU, Chunjing",
      "Wang, Yunhe"
    ]
  },
  {
    "id": "376c6b9ff3bedbbea56751a84fffc10c",
    "title": "Does Knowledge Distillation Really Work?",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/376c6b9ff3bedbbea56751a84fffc10c-Paper.pdf",
    "abstract": "Knowledge distillation is a popular technique for training a small student network to emulate a larger teacher model, such as an ensemble of networks. We show that while knowledge distillation can improve student generalization, it does not typically work as it is commonly understood: there often remains a surprisingly large discrepancy between the predictive distributions of the teacher and the student, even in cases when the student has the capacity to perfectly match the teacher. We identify difficulties in optimization as a key reason for why the student is unable to match the teacher. We also show how the details of the dataset used for distillation play a role in how closely the student matches the teacher --- and that more closely matching the teacher paradoxically does not always lead to better student generalization.",
    "authors": [
      "Stanton, Samuel",
      "Izmailov, Pavel",
      "Kirichenko, Polina",
      "Alemi, Alexander A.",
      "Wilson, Andrew G."
    ]
  },
  {
    "id": "37cfff3c04f95b22bcf166df586cd7a9",
    "title": "Teachable Reinforcement Learning via Advice Distillation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/37cfff3c04f95b22bcf166df586cd7a9-Paper.pdf",
    "abstract": "Training automated agents to complete complex tasks in interactive environments is challenging: reinforcement learning requires careful hand-engineering of reward functions, imitation learning requires specialized infrastructure and access to a human expert, and learning from intermediate forms of supervision (like binary preferences) is time-consuming and extracts little information from each human intervention. Can we overcome these challenges by building agents that learn from rich, interactive feedback instead? We propose a new supervision paradigm for interactive learning based on \"teachable\" decision-making systems that learn from structured advice provided by an external teacher. We begin by formalizing a class of human-in-the-loop decision making problems in which multiple forms of teacher-provided advice are available to a learner. We then describe a simple learning algorithm for these problems that first learns to interpret advice, then learns from advice to complete tasks even in the absence of human supervision. In puzzle-solving, navigation, and locomotion domains, we show that agents that learn from advice can acquire new skills with significantly less human supervision than standard reinforcement learning algorithms and often less than imitation learning.",
    "authors": [
      "Watkins, Olivia",
      "Gupta, Abhishek",
      "Darrell, Trevor",
      "Abbeel, Pieter",
      "Andreas, Jacob"
    ]
  },
  {
    "id": "37ecd27608480aa3569a511a638ca74f",
    "title": "Antipodes of Label Differential Privacy: PATE and ALIBI",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/37ecd27608480aa3569a511a638ca74f-Paper.pdf",
    "abstract": "We consider the privacy-preserving machine learning (ML) setting where the trained model must satisfy differential privacy (DP) with respect to the labels of the training examples. We propose two novel approaches based on, respectively, the Laplace mechanism and the PATE framework, and demonstrate their effectiveness on standard benchmarks.While recent work by Ghazi et al. proposed Label DP schemes based on a randomized response mechanism, we argue that additive Laplace noise coupled with Bayesian inference (ALIBI) is a better fit for typical ML tasks. Moreover, we show how to achieve very strong privacy levels in some regimes, with our adaptation of the PATE framework that builds on recent advances in semi-supervised learning.We complement theoretical analysis of our algorithms' privacy guarantees with empirical evaluation of their memorization properties. Our evaluation suggests that comparing different algorithms according to their provable DP guarantees can be misleading and favor a less private algorithm with a tighter analysis.Code for implementation of algorithms and memorization attacks is available from https://github.com/facebookresearch/labeldpantipodes.",
    "authors": [
      "Malek Esmaeili, Mani",
      "Mironov, Ilya",
      "Prasad, Karthik",
      "Shilov, Igor",
      "Tramer, Florian"
    ]
  },
  {
    "id": "37f0e884fbad9667e38940169d0a3c95",
    "title": "Visual Search Asymmetry: Deep Nets and Humans Share Similar Inherent Biases",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/37f0e884fbad9667e38940169d0a3c95-Paper.pdf",
    "abstract": "Visual search is a ubiquitous and often challenging daily task, exemplified by looking for the car keys at home or a friend in a crowd. An intriguing property of some classical search tasks is an asymmetry such that finding a target A among distractors B can be easier than finding B among A. To elucidate the mechanisms responsible for asymmetry in visual search, we propose a computational model that takes a target and a search image as inputs and produces a sequence of eye movements until the target is found. The model integrates eccentricity-dependent visual recognition with target-dependent top-down cues. We compared the model against human behavior in six paradigmatic search tasks that show asymmetry in humans. Without prior exposure to the stimuli or task-specific training, the model provides a plausible mechanism for search asymmetry. We hypothesized that the polarity of search asymmetry arises from experience with the natural environment. We tested this hypothesis by training the model on augmented versions of ImageNet where the biases of natural images were either removed or reversed. The polarity of search asymmetry disappeared or was altered depending on the training protocol. This study highlights how classical perceptual properties can emerge in neural network models, without the need for task-specific training, but rather as a consequence of the statistical properties of the developmental diet fed to the model. All source code and data are publicly available at https://github.com/kreimanlab/VisualSearchAsymmetry.",
    "authors": [
      "Gupta, Shashi Kant",
      "Zhang, Mengmi",
      "WU, CHIA-CHIEN",
      "Wolfe, Jeremy",
      "Kreiman, Gabriel"
    ]
  },
  {
    "id": "38181d991caac98be8fb2ecb8bd0f166",
    "title": "On the Universality of Graph Neural Networks on Large Random Graphs",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/38181d991caac98be8fb2ecb8bd0f166-Paper.pdf",
    "abstract": "We study the approximation power of Graph Neural Networks (GNNs) on latent position random graphs. In the large graph limit, GNNs are known to converge to certain ``continuous'' models known as c-GNNs, which directly enables a study of their approximation power on random graph models. In the absence of input node features however, just as GNNs are limited by the Weisfeiler-Lehman isomorphism test, c-GNNs will be severely limited on simple random graph models. For instance, they will fail to distinguish the communities of a well-separated Stochastic Block Model (SBM) with constant degree function. Thus, we consider recently proposed architectures that augment GNNs with unique node identifiers, referred to as Structural GNNs here (SGNNs). We study the convergence of SGNNs to their continuous counterpart (c-SGNNs) in the large random graph limit, under new conditions on the node identifiers. We then show that c-SGNNs are strictly more powerful than c-GNNs in the continuous limit, and prove their universality on several random graph models of interest, including most SBMs and a large class of random geometric graphs. Our results cover both permutation-invariant and permutation-equivariant architectures.",
    "authors": [
      "Keriven, Nicolas",
      "Bietti, Alberto",
      "Vaiter, Samuel"
    ]
  },
  {
    "id": "384babc3e7faa44cf1ca671b74499c3b",
    "title": "Inverse Reinforcement Learning in a Continuous State Space with Formal Guarantees",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/384babc3e7faa44cf1ca671b74499c3b-Paper.pdf",
    "abstract": "Inverse Reinforcement Learning (IRL) is the problem of finding a reward function which describes observed/known expert behavior.  The IRL setting is remarkably useful for automated control, in situations where the reward function is difficult to specify manually or as a means to extract agent preference. In this work, we provide a new IRL algorithm for the continuous state space setting with unknown transition dynamics by modeling the system using a basis of orthonormal functions. Moreover, we provide a proof of correctness and formal guarantees on the sample and time complexity of our algorithm.  Finally, we present synthetic experiments to corroborate our theoretical guarantees.",
    "authors": [
      "Dexter, Gregory",
      "Bello, Kevin",
      "Honorio, Jean"
    ]
  },
  {
    "id": "38811c5285e34e2e3319ab7d9f2cfa5b",
    "title": "Adversarial Attacks on Graph Classifiers via Bayesian Optimisation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/38811c5285e34e2e3319ab7d9f2cfa5b-Paper.pdf",
    "abstract": "Graph neural networks, a popular class of models effective in a wide range of graph-based learning tasks, have been shown to be vulnerable to adversarial attacks. While the majority of the literature focuses on such vulnerability in node-level classification tasks, little effort has been dedicated to analysing adversarial attacks on graph-level classification, an important problem with numerous real-life applications such as biochemistry and social network analysis. The few existing methods often require unrealistic setups, such as access to internal information of the victim models, or an impractically-large number of queries. We present a novel Bayesian optimisation-based attack method for graph classification models. Our method is black-box, query-efficient and parsimonious with respect to the perturbation applied. We empirically validate the effectiveness and flexibility of the proposed method on a wide range of graph classification tasks involving varying graph properties, constraints and modes of attack. Finally, we analyse common interpretable patterns behind the adversarial samples produced, which may shed further light on the adversarial robustness of graph classification models. ",
    "authors": [
      "Wan, Xingchen",
      "Kenlay, Henry",
      "Ru, Robin",
      "Blaas, Arno",
      "Osborne, Michael A",
      "Dong, Xiaowen"
    ]
  },
  {
    "id": "38b4f06e27fd4f6fdcceabc6f5c068ea",
    "title": "Regulating algorithmic filtering on social media",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/38b4f06e27fd4f6fdcceabc6f5c068ea-Paper.pdf",
    "abstract": "By filtering the content that users see, social media platforms have the ability to influence users' perceptions and decisions, from their dining choices to their voting preferences. This influence has drawn scrutiny, with many calling for regulations on filtering algorithms, but designing and enforcing regulations remains challenging. In this work, we examine three questions. First, given a regulation, how would one design an audit to enforce it? Second, does the audit impose a performance cost on the platform? Third, how does the audit affect the content that the platform is incentivized to filter? In response to these questions, we propose a method such that, given a regulation, an auditor can test whether that regulation is met with only black-box access to the filtering algorithm. We then turn to the platform's perspective. The platform's goal is to maximize an objective function while meeting regulation. We find that there are conditions under which the regulation does not place a high performance cost on the platform and, notably, that content diversity can play a key role in aligning the interests of the platform and regulators.",
    "authors": [
      "Cen, Sarah",
      "Shah, Devavrat"
    ]
  },
  {
    "id": "38eb982ee635354d3febf457beeee736",
    "title": "argmax centroid",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/38eb982ee635354d3febf457beeee736-Paper.pdf",
    "abstract": "We propose a general method to construct centroid approximation for the distribution of maximum points of a random function (a.k.a. argmax distribution), which finds broad applications in machine learning. Our method optimizes a set of centroid points to compactly approximate the argmax distribution with a simple objective function, without explicitly drawing exact samples from the argmax distribution. Theoretically, the argmax centroid method can be shown to minimize a surrogate of Wasserstein distance between the ground-truth argmax distribution and the centroid approximation under proper conditions. We demonstrate the applicability and effectiveness of our method on a variety of real-world multi-task learning applications, including few-shot image classification, personalized dialogue systems and multi-target domain adaptation.",
    "authors": [
      "Gong, Chengyue",
      "Ye, Mao",
      "Liu, Qiang"
    ]
  },
  {
    "id": "38ef4b66cb25e92abe4d594acb841471",
    "title": "Contrastive Learning of Global and Local Video Representations",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/38ef4b66cb25e92abe4d594acb841471-Paper.pdf",
    "abstract": "Contrastive learning has delivered impressive results for various tasks in the self-supervised regime. However, existing approaches optimize for learning representations specific to downstream scenarios, i.e., global representations suitable for tasks such as classification or local representations for tasks such as detection and localization. While they produce satisfactory results in the intended downstream scenarios, they often fail to generalize to tasks that they were not originally designed for. In this work, we propose to learn video representations that generalize to both the tasks which require global semantic information (e.g., classification) and the tasks that require local fine-grained spatio-temporal information (e.g., localization). We achieve this by optimizing two contrastive objectives that together encourage our model to learn global-local visual information given audio signals. We show that the two objectives mutually improve the generalizability of the learned global-local representations, significantly outperforming their disjointly learned counterparts. We demonstrate our approach on various tasks including action/sound classification, lipreading, deepfake detection, event and sound localization.",
    "authors": [
      "ma, shuang",
      "Zeng, Zhaoyang",
      "McDuff, Daniel",
      "Song, Yale"
    ]
  },
  {
    "id": "39144da5a6180c47885443c83547ec14",
    "title": "BooVI: Provably Efficient Bootstrapped Value Iteration",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/39144da5a6180c47885443c83547ec14-Paper.pdf",
    "abstract": "Despite the tremendous success of reinforcement learning (RL) with function approximation, efficient exploration remains a significant challenge, both practically and theoretically. In particular, existing theoretically grounded RL algorithms based on upper confidence bounds (UCBs), such as optimistic least-squares value iteration (LSVI), are often incompatible with practically powerful function approximators, such as neural networks. In this paper, we develop a variant of \\underline{boo}tstrapped LS\\underline{VI}, namely BooVI, which bridges such a gap between practice and theory. Practically, BooVI drives exploration through (re)sampling, making it compatible with general function approximators. Theoretically, BooVI inherits the worst-case $\\tilde{O}(\\sqrt{d^3 H^3 T})$-regret of optimistic LSVI in the episodic linear setting. Here $d$ is the feature dimension, $H$ is the episode horizon, and $T$ is the total number of steps.",
    "authors": [
      "Liu, Boyi",
      "Cai, Qi",
      "Yang, Zhuoran",
      "Wang, Zhaoran"
    ]
  },
  {
    "id": "3937230de3c8041e4da6ac3246a888e8",
    "title": "Do Wider Neural Networks Really Help Adversarial Robustness?",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3937230de3c8041e4da6ac3246a888e8-Paper.pdf",
    "abstract": "Adversarial training is a powerful type of defense against adversarial examples. Previous empirical results suggest that adversarial training requires wider networks for better performances. However, it remains elusive how does neural network width affect model robustness. In this paper, we carefully examine the relationship between network width and model robustness. Specifically, we show that the model robustness is closely related to the tradeoff between natural accuracy and perturbation stability, which is controlled by the robust regularization parameter \u03bb. With the same \u03bb, wider networks can achieve better natural accuracy but worse perturbation stability, leading to a potentially worse overall model robustness. To understand the origin of this phenomenon, we further relate the perturbation stability with the network's local Lipschitzness. By leveraging recent results on neural tangent kernels, we theoretically show that wider networks tend to have worse perturbation stability. Our analyses suggest that: 1) the common strategy of first fine-tuning \u03bb on small networks and then directly use it for wide model training could lead to deteriorated model robustness; 2) one needs to properly enlarge \u03bb to unleash the robustness potential of wider models fully. Finally, we propose a new Width Adjusted Regularization (WAR) method that adaptively enlarges \u03bb on wide models and significantly saves the tuning time.",
    "authors": [
      "Wu, Boxi",
      "Chen, Jinghui",
      "Cai, Deng",
      "He, Xiaofei",
      "Gu, Quanquan"
    ]
  },
  {
    "id": "3941c4358616274ac2436eacf67fae05",
    "title": "Exploring the Limits of Out-of-Distribution Detection",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3941c4358616274ac2436eacf67fae05-Paper.pdf",
    "abstract": "Near out-of-distribution detection (OOD) is a major challenge for deep neural networks. We demonstrate that large-scale pre-trained transformers can significantly improve the state-of-the-art (SOTA) on a range of near OOD tasks across different data modalities. For instance, on CIFAR-100 vs CIFAR-10 OOD detection, we improve the AUROC from 85% (current SOTA) to more than 96% using Vision Transformers pre-trained on ImageNet21k. On a challenging genomics OOD detection benchmark, we improve the AUROC from 66% to 77% using transformer and unsupervised pre-training.  To further improve performance, we explore the few-shot outlier exposure setting where a few examples from outlier classes may be available; we show that  pre-trained transformers are particularly well-suited for outlier exposure, and that the AUROC of OOD detection on CIFAR-100 vs CIFAR-10  can be improved to 98.7% with just 1 image per OOD class, and 99.46% with 10 images per OOD class. For multi-modal image-text pre-trained transformers such as CLIP, we explore a new way of using just the names of outlier classes as a sole source of information without any accompanying images, and show that this outperforms previous SOTA on standard OOD benchmark tasks. ",
    "authors": [
      "Fort, Stanislav",
      "Ren, Jie",
      "Lakshminarayanan, Balaji"
    ]
  },
  {
    "id": "3953630da28e5181cffca1278517e3cf",
    "title": "ABC: Auxiliary Balanced Classifier for Class-imbalanced Semi-supervised Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3953630da28e5181cffca1278517e3cf-Paper.pdf",
    "abstract": "Existing semi-supervised learning (SSL) algorithms typically assume class-balanced datasets, although the class distributions of many real world datasets are imbalanced. In general, classifiers trained on a class-imbalanced dataset are biased toward the majority classes. This issue becomes more problematic for SSL algorithms because they utilize the biased prediction of unlabeled data for training. However, traditional class-imbalanced learning techniques, which are designed for labeled data, cannot be readily combined with SSL algorithms. We propose a scalable class-imbalanced SSL algorithm that can effectively use unlabeled data, while mitigating class imbalance by introducing an auxiliary balanced classifier (ABC) of a single layer, which is attached to a representation layer of an existing SSL algorithm. The ABC is trained with a class-balanced loss of a minibatch, while using high-quality representations learned from all data points in the minibatch using the backbone SSL algorithm to avoid overfitting and information loss. Moreover, we use consistency regularization, a recent SSL technique for utilizing unlabeled data in a modified way, to train the ABC to be balanced among the classes by selecting unlabeled data with the same probability for each class. The proposed algorithm achieves state-of-the-art performance in various class-imbalanced SSL experiments using four benchmark datasets.",
    "authors": [
      "Lee, Hyuck",
      "Shin, Seungjae",
      "Kim, Heeyoung"
    ]
  },
  {
    "id": "39799c18791e8d7eb29704fc5bc04ac8",
    "title": "BCD Nets: Scalable Variational Approaches for Bayesian Causal Discovery",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/39799c18791e8d7eb29704fc5bc04ac8-Paper.pdf",
    "abstract": "A structural equation model (SEM) is an effective framework to reason over causal relationships represented via a directed acyclic graph (DAG).Recent advances have enabled effective maximum-likelihood point estimation of DAGs from observational data. However, a point estimate may not accurately capture the uncertainty in inferring the underlying graph in practical scenarios, wherein the true DAG is non-identifiable and/or the observed dataset is limited.We propose Bayesian Causal Discovery Nets (BCD Nets), a variational inference framework for estimating a distribution over DAGs characterizing a linear-Gaussian SEM.Developing a full Bayesian posterior over DAGs is challenging due to the the discrete and combinatorial nature of graphs.We analyse key design choices for scalable VI over DAGs, such as 1) the parametrization of DAGs via an expressive variational family, 2) a continuous relaxation that enables low-variance stochastic optimization, and 3) suitable priors over the latent variables.We provide a series of experiments on real and synthetic data showing that BCD Nets outperform maximum-likelihood methods on standard causal discovery metrics such as structural Hamming distance in low data regimes. ",
    "authors": [
      "Cundy, Chris",
      "Grover, Aditya",
      "Ermon, Stefano"
    ]
  },
  {
    "id": "398410ece9d7343091093a2a7f8ee381",
    "title": "Discovering Dynamic Salient Regions for Spatio-Temporal Graph Neural Networks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/398410ece9d7343091093a2a7f8ee381-Paper.pdf",
    "abstract": "Graph Neural Networks are perfectly suited to capture latent interactions between various entities in the spatio-temporal domain (e.g. videos). However, when an explicit structure is not available, it is not obvious what atomic elements should be represented as nodes. Current works generally use pre-trained object detectors or fixed, predefined regions to extract graph nodes. Improving upon this, our proposed model learns nodes that dynamically attach to well-delimited salient regions, which are relevant for a higher-level task, without using any object-level supervision. Constructing these localized, adaptive nodes gives our model inductive bias towards object-centric representations and we show that it discovers regions that are well correlated with objects in the video. In extensive ablation studies and experiments on two challenging datasets, we show superior performance to previous graph neural networks models for video classification.",
    "authors": [
      "Duta, Iulia",
      "Nicolicioiu, Andrei",
      "Leordeanu, Marius"
    ]
  },
  {
    "id": "398475c83b47075e8897a083e97eb9f0",
    "title": "Information-constrained optimization: can adaptive processing of gradients help?",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/398475c83b47075e8897a083e97eb9f0-Paper.pdf",
    "abstract": "We revisit first-order optimization under local information constraints such as local privacy, gradient quantization, and computational constraints limiting access to a few coordinates of the gradient. In this setting, the optimization algorithm is not allowed to directly access the complete output of the gradient oracle, but only gets limited information about it subject to the local information constraints.   We study the role of adaptivity in processing the gradient output to obtain this limited information from it, and obtain tight or nearly tight bounds for both convex and strongly convex optimization when adaptive gradient processing is allowed.",
    "authors": [
      "Acharya, Jayadev",
      "Canonne, Clement",
      "Mayekar, Prathamesh",
      "Tyagi, Himanshu"
    ]
  },
  {
    "id": "39ae2ed11b14a4ccb41d35e9d1ba5d11",
    "title": "Towards Calibrated Model for Long-Tailed Visual Recognition from Prior Perspective",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/39ae2ed11b14a4ccb41d35e9d1ba5d11-Paper.pdf",
    "abstract": "Real-world data universally confronts a severe class-imbalance problem and exhibits a long-tailed distribution, i.e., most labels are associated with limited instances. The na\u00efve models supervised by such datasets would prefer dominant labels, encounter a serious generalization challenge and become poorly calibrated. We propose two novel methods from the prior perspective to alleviate this dilemma. First, we deduce a balance-oriented data augmentation named Uniform Mixup (UniMix) to promote mixup in long-tailed scenarios, which adopts advanced mixing factor and sampler in favor of the minority. Second, motivated by the Bayesian theory, we figure out the Bayes Bias (Bayias), an inherent bias caused by the inconsistency of prior, and compensate it as a modification on standard cross-entropy loss. We further prove that both the proposed methods ensure the classification calibration theoretically and empirically. Extensive experiments verify that our strategies contribute to a better-calibrated model, and their combination achieves state-of-the-art performance on CIFAR-LT, ImageNet-LT, and iNaturalist 2018.",
    "authors": [
      "Xu, Zhengzhuo",
      "Chai, Zenghao",
      "Yuan, Chun"
    ]
  },
  {
    "id": "39d0a8908fbe6c18039ea8227f827023",
    "title": "Learning to Draw: Emergent Communication through Sketching",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/39d0a8908fbe6c18039ea8227f827023-Paper.pdf",
    "abstract": "Evidence that visual communication preceded written language and provided a basis for it goes back to prehistory, in forms such as cave and rock paintings depicting traces of our distant ancestors. Emergent communication research has sought to explore how agents can learn to communicate in order to collaboratively solve tasks. Existing research has focused on language, with a learned communication channel transmitting sequences of discrete tokens between the agents. In this work, we explore a visual communication channel between agents that are allowed to draw with simple strokes. Our agents are parameterised by deep neural networks, and the drawing procedure is differentiable, allowing for end-to-end training. In the framework of a referential communication game, we demonstrate that agents can not only successfully learn to communicate by drawing, but with appropriate inductive biases, can do so in a fashion that humans can interpret. We hope to encourage future research to consider visual communication as a more flexible and directly interpretable alternative of training collaborative agents.",
    "authors": [
      "Mihai, Daniela",
      "Hare, Jonathon"
    ]
  },
  {
    "id": "39d4b545fb02556829aab1db805021c3",
    "title": "Self-Supervised Learning of Event-Based Optical Flow with Spiking Neural Networks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/39d4b545fb02556829aab1db805021c3-Paper.pdf",
    "abstract": "The field of neuromorphic computing promises extremely low-power and low-latency sensing and processing. Challenges in transferring learning algorithms from traditional artificial neural networks (ANNs) to spiking neural networks (SNNs) have so far prevented their application to large-scale, complex regression tasks. Furthermore, realizing a truly asynchronous and fully neuromorphic pipeline that maximally attains the abovementioned benefits involves rethinking the way in which this pipeline takes in and accumulates information. In the case of perception, spikes would be passed as-is and one-by-one between an event camera and an SNN, meaning all temporal integration of information must happen inside the network. In this article, we tackle these two problems. We focus on the complex task of learning to estimate optical flow from event-based camera inputs in a self-supervised manner, and modify the state-of-the-art ANN training pipeline to encode minimal temporal information in its inputs. Moreover, we reformulate the self-supervised loss function for event-based optical flow to improve its convexity. We perform experiments with various types of recurrent ANNs and SNNs using the proposed pipeline. Concerning SNNs, we investigate the effects of elements such as parameter initialization and optimization, surrogate gradient shape, and adaptive neuronal mechanisms. We find that initialization and surrogate gradient width play a crucial part in enabling learning with sparse inputs, while the inclusion of adaptivity and learnable neuronal parameters can improve performance. We show that the performance of the proposed ANNs and SNNs are on par with that of the current state-of-the-art ANNs trained in a self-supervised manner.",
    "authors": [
      "Hagenaars, Jesse",
      "Paredes-Valles, Federico",
      "de Croon, Guido"
    ]
  },
  {
    "id": "3a15c7d0bbe60300a39f76f8a5ba6896",
    "title": "On the Value of Infinite Gradients in Variational Autoencoder Models",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3a15c7d0bbe60300a39f76f8a5ba6896-Paper.pdf",
    "abstract": "A number of recent studies of continuous variational autoencoder (VAE) models have noted, either directly or indirectly, the tendency of various parameter gradients to drift towards infinity during training.  Because such gradients could potentially contribute to numerical instabilities, and are often framed as a problematic phenomena to be avoided, it may be tempting to shift to alternative energy functions that guarantee bounded gradients.  But it remains an open question: What might the unintended consequences of such a restriction be?  To address this issue, we examine how unbounded gradients relate to the regularization of a broad class of autoencoder-based architectures, including VAE models, as applied to data lying on or near a low-dimensional manifold (e.g., natural images).  Our main finding is that, if the ultimate goal is to simultaneously avoid over-regularization (high reconstruction errors, sometimes referred to as posterior collapse) and under-regularization (excessive latent dimensions are not pruned from the model), then an autoencoder-based energy function with infinite gradients around optimal representations is provably required per a certain technical sense which we carefully detail.  Given that both over- and under-regularization can directly lead to poor generated sample quality or suboptimal feature selection, this result suggests that heuristic modifications to or constraints on the VAE energy function may at times be ill-advised, and large gradients should be accommodated to the extent possible.",
    "authors": [
      "Dai, Bin",
      "Wenliang, Li",
      "Wipf, David"
    ]
  },
  {
    "id": "3a4496776767aaa99f9804d0905fe584",
    "title": "Online Robust Reinforcement Learning with Model Uncertainty",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3a4496776767aaa99f9804d0905fe584-Paper.pdf",
    "abstract": "Robust reinforcement learning (RL) is to find a policy that optimizes the worst-case performance over an uncertainty set of MDPs. In this paper, we focus on model-free robust RL, where the uncertainty set is defined to be centering at a misspecified MDP that generates samples, and is assumed to be unknown. We develop a sample-based approach to estimate the unknown uncertainty set, and design robust Q-learning algorithm (tabular case) and robust TDC algorithm (function approximation setting), which can be implemented in an online and incremental fashion.  For the robust Q-learning algorithm, we prove that it converges to the optimal robust Q function, and for the robust TDC algorithm, we prove that it converges asymptotically to some stationary points. Unlike the results in [Roy et al., 2017], our algorithms do not need any additional conditions on the discount factor to guarantee the convergence. We further characterize the finite-time error bounds of the two algorithms, and show that both the robust Q-learning and robust TDC algorithms converge as fast as their vanilla counterparts (within a constant factor). Our numerical experiments further demonstrate the robustness of our algorithms. Our approach can be readily extended to robustify many other algorithms, e.g., TD, SARSA, and other GTD algorithms. ",
    "authors": [
      "Wang, Yue",
      "Zou, Shaofeng"
    ]
  },
  {
    "id": "3a61ed715ee66c48bacf237fa7bb5289",
    "title": "Neural View Synthesis and Matching for Semi-Supervised Few-Shot Learning of 3D Pose",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3a61ed715ee66c48bacf237fa7bb5289-Paper.pdf",
    "abstract": "We study the problem of learning to estimate the 3D object pose from a few labelled examples and a collection of unlabelled data. Our main contribution is a learning framework, neural view synthesis and matching, that can transfer the 3D pose annotation from the labelled to unlabelled images reliably, despite unseen 3D views and nuisance variations such as the object shape, texture, illumination or scene context. In our approach, objects are represented as 3D cuboid meshes composed of feature vectors at each mesh vertex. The model is initialized from a few labelled images and is subsequently used to synthesize feature representations of unseen 3D views. The synthesized views are matched with the feature representations of unlabelled images to generate pseudo-labels of the 3D pose. The pseudo-labelled data is, in turn, used to train the feature extractor such that the features at each mesh vertex are more invariant across varying 3D views of the object. Our model is trained in an EM-type manner alternating between increasing the 3D pose invariance of the feature extractor and annotating unlabelled data through neural view synthesis and matching. We demonstrate the effectiveness of the proposed semi-supervised learning framework for 3D pose estimation on the PASCAL3D+ and KITTI datasets. We find that our approach outperforms all baselines by a wide margin, particularly in an extreme few-shot setting where only 7 annotated images are given. Remarkably, we observe that our model also achieves an exceptional robustness in out-of-distribution scenarios that involve partial occlusion.",
    "authors": [
      "Wang, Angtian",
      "Mei, Shenxiao",
      "Yuille, Alan L.",
      "Kortylewski, Adam"
    ]
  },
  {
    "id": "3b24156ad560a696116454056bc88ab4",
    "title": "Sharp Impossibility Results for Hyper-graph Testing",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3b24156ad560a696116454056bc88ab4-Paper.pdf",
    "abstract": "In a broad Degree-Corrected Mixed-Membership (DCMM) setting, we test whether a non-uniform hypergraph has only one community or has multiple communities. Since both the null and alternative hypotheses have many unknown parameters, the challenge is, given an alternative, how to identify the null that is hardest to separate from the alternative. We approach this by proposing a degree matching strategy where the main idea is leveraging the theory for tensor scaling to create a least favorable pair of hypotheses. We present a  result on standard  minimax lower bound theory and a result on Region of Impossibility (which is more informative than the minimax lower bound). We show that our lower bounds are tight by introducing a new test that attains the lower bound up to a logarithmic factor. We also discuss the case where the hypergraphs may have mixed-memberships.",
    "authors": [
      "Jin, Jiashun",
      "Ke, Zheng Tracy",
      "Liang, Jiajun"
    ]
  },
  {
    "id": "3b3fff6463464959dcd1b68d0320f781",
    "title": "Evaluating Gradient Inversion Attacks and Defenses in Federated Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3b3fff6463464959dcd1b68d0320f781-Paper.pdf",
    "abstract": "Gradient inversion attack (or input recovery from gradient) is an emerging threat to the security and privacy preservation of Federated learning, whereby malicious eavesdroppers or participants in the protocol can recover (partially) the clients' private data. This paper evaluates existing attacks and defenses. We find that some attacks make strong assumptions about the setup. Relaxing such assumptions can substantially weaken these attacks. We then evaluate the benefits of three proposed defense mechanisms against gradient inversion attacks. We show the trade-offs of privacy leakage and data utility of these defense methods, and find that combining them in an appropriate manner makes the attack less effective, even under the original strong assumptions. We also estimate the computation cost of end-to-end recovery of a single image under each evaluated defense. Our findings suggest that the state-of-the-art attacks can currently be defended against with minor data utility loss, as summarized in a list of potential strategies.",
    "authors": [
      "Huang, Yangsibo",
      "Gupta, Samyak",
      "Song, Zhao",
      "Li, Kai",
      "Arora, Sanjeev"
    ]
  },
  {
    "id": "3b712de48137572f3849aabd5666a4e3",
    "title": "Faster Non-asymptotic Convergence for Double Q-learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3b712de48137572f3849aabd5666a4e3-Paper.pdf",
    "abstract": "Double Q-learning (Hasselt, 2010) has gained significant success in practice due to its effectiveness in overcoming the overestimation issue of Q-learning. However, the theoretical understanding of double Q-learning is rather limited. The only existing finite-time analysis was recently established in (Xiong et al. 2020), where the polynomial learning rate adopted in the analysis typically yields a slower convergence rate. This paper tackles the more challenging case of a constant learning rate, and develops new analytical tools that improve the existing convergence rate by orders of magnitude. Specifically, we show that synchronous double Q-learning attains an $\\epsilon$-accurate global optimum with a time complexity of $\\tilde{\\Omega}\\left(\\frac{\\ln D}{(1-\\gamma)^7\\epsilon^2} \\right)$, and the asynchronous algorithm achieves a time complexity of $\\tilde{\\Omega}\\left(\\frac{L}{(1-\\gamma)^7\\epsilon^2} \\right)$, where $D$ is the cardinality of the state-action space, $\\gamma$ is the discount factor, and $L$ is a parameter related to the sampling strategy for asynchronous double Q-learning. These results improve the existing convergence rate by the order of magnitude in terms of its dependence on all major parameters $(\\epsilon,1-\\gamma, D, L)$.  This paper presents a substantial step toward the full understanding of the fast convergence of double-Q learning.",
    "authors": [
      "Zhao, Lin",
      "Xiong, Huaqing",
      "Liang, Yingbin"
    ]
  },
  {
    "id": "3b92d18aa7a6176dd37d372bc2f1eb71",
    "title": "Towards Tight Communication Lower Bounds for Distributed Optimisation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3b92d18aa7a6176dd37d372bc2f1eb71-Paper.pdf",
    "abstract": "We consider a standard distributed optimisation setting where $N$ machines, each holding a $d$-dimensional function $f_i$, aim to jointly minimise the sum of the functions $\\sum_{i = 1}^N f_i (x)$.  This problem arises naturally in  large-scale distributed optimisation, where a standard solution is to apply variants of (stochastic) gradient descent. We focus on the communication complexity of this problem: our main result provides the first fully unconditional bounds on total number of bits which need to be sent and received by the $N$ machines to solve this problem under point-to-point communication, within a given error-tolerance. Specifically, we show that $\\Omega( Nd \\log d / N\\varepsilon)$ total bits need to be communicated between the machines to find an additive $\\epsilon$-approximation to the minimum of $\\sum_{i = 1}^N f_i (x)$. The result holds for both deterministic and randomised algorithms, and, importantly, requires no assumptions on the algorithm structure. The lower bound is tight under certain restrictions on parameter values, and is matched within constant factors for quadratic objectives by a new variant of quantised gradient descent, which we describe and analyse. Our results bring over tools from communication complexity to distributed optimisation, which has potential for further  applications.",
    "authors": [
      "Korhonen, Janne H.",
      "Alistarh, Dan"
    ]
  },
  {
    "id": "3bbca1d243b01b47c2bf42b29a8b265c",
    "title": "Fast Multi-Resolution Transformer Fine-tuning for Extreme Multi-label Text Classification",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3bbca1d243b01b47c2bf42b29a8b265c-Paper.pdf",
    "abstract": "Extreme multi-label text classification~(XMC) seeks to find relevant labels from an extreme large label collection for a given text input. Many real-world applications can be formulated as XMC problems, such as recommendation systems, document tagging and semantic search. Recently, transformer based XMC methods, such as X-Transformer and LightXML, have shown significant improvement over other XMC methods. Despite leveraging pre-trained transformer models for text representation, the fine-tuning procedure of transformer models on large label space still has lengthy computational time even with powerful GPUs. In this paper, we propose a novel recursive approach, XR-Transformer to accelerate the procedure through recursively fine-tuning transformer models on a series of multi-resolution objectives related to the original XMC objective function. Empirical results show that XR-Transformer takes significantly less training time compared to other transformer-based XMC models while yielding better state-of-the-art results. In particular, on the public Amazon-3M dataset with 3 million labels, XR-Transformer is not only 20x faster than X-Transformer but also improves the Precision@1 from 51% to 54%.",
    "authors": [
      "Zhang, Jiong",
      "Chang, Wei-Cheng",
      "Yu, Hsiang-Fu",
      "Dhillon, Inderjit"
    ]
  },
  {
    "id": "3bbfdde8842a5c44a0323518eec97cbe",
    "title": "HRFormer: High-Resolution Vision Transformer for Dense Predict",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3bbfdde8842a5c44a0323518eec97cbe-Paper.pdf",
    "abstract": "We present a High-Resolution Transformer (HRFormer) that learns high-resolution representations for dense prediction tasks, in contrast to the original Vision Transformer that produces low-resolution representations and has high memory and computational cost. We take advantage of the multi-resolution parallel design introduced in high-resolution convolutional networks (HRNet [45]), along with local-window self-attention that performs self-attention over small non-overlapping image windows [21], for improving the memory and computation efficiency. In addition, we introduce a convolution into the FFN to exchange information across the disconnected image windows. We demonstrate the effectiveness of the HighResolution Transformer on both human pose estimation and semantic segmentation tasks, e.g., HRFormer outperforms Swin transformer [27] by 1.3 AP on COCO pose estimation with 50% fewer parameters and 30% fewer FLOPs. Code is available at: https://github.com/HRNet/HRFormer",
    "authors": [
      "YUAN, YUHUI",
      "Fu, Rao",
      "Huang, Lang",
      "Lin, Weihong",
      "Zhang, Chao",
      "Chen, Xilin",
      "Wang, Jingdong"
    ]
  },
  {
    "id": "3bc31a430954d8326605fc690ed22f4d",
    "title": "Manifold Topology Divergence: a Framework for Comparing Data Manifolds. ",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3bc31a430954d8326605fc690ed22f4d-Paper.pdf",
    "abstract": "We propose a framework for comparing data manifolds, aimed, in particular, towards the evaluation of deep generative models. We describe a novel tool, Cross-Barcode(P,Q), that, given a pair of distributions in a high-dimensional space, tracks multiscale topology spacial discrepancies between manifolds on which the distributions are concentrated. Based on the Cross-Barcode, we introduce the Manifold Topology Divergence score (MTop-Divergence) and apply it to assess the performance of deep generative models in various domains: images, 3D-shapes, time-series, and on different datasets: MNIST, Fashion MNIST, SVHN, CIFAR10, FFHQ, market stock data, ShapeNet. We demonstrate that the MTop-Divergence accurately detects various degrees of mode-dropping, intra-mode collapse, mode invention, and image disturbance. Our algorithm scales well (essentially linearly) with the increase of the dimension of the ambient high-dimensional space. It is one of the first TDA-based methodologies that can be applied universally to datasets of different sizes and dimensions, including the ones on which the most recent GANs in the visual domain are trained. The proposed method is domain agnostic and does not rely on pre-trained networks.",
    "authors": [
      "Barannikov, Serguei",
      "Trofimov, Ilya",
      "Sotnikov, Grigorii",
      "Trimbach, Ekaterina",
      "Korotin, Alexander",
      "Filippov, Alexander",
      "Burnaev, Evgeny"
    ]
  },
  {
    "id": "3bd4017318837e92a66298c7855f4427",
    "title": "Weak-shot Fine-grained Classification via Similarity Transfer",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3bd4017318837e92a66298c7855f4427-Paper.pdf",
    "abstract": "Recognizing fine-grained categories remains a challenging task, due to the subtle distinctions among different subordinate categories, which results in the need of abundant annotated samples. To alleviate the data-hungry problem, we consider the problem of learning novel categories from web data with the support of a clean set of base categories, which is referred to as weak-shot learning. In this setting, we propose a method called SimTrans to transfer pairwise semantic similarity from base categories to novel categories. Specifically, we firstly train a similarity net on clean data, and then leverage the transferred similarity to denoise web training data using two simple yet effective strategies. In addition, we apply adversarial loss on similarity net to enhance the transferability of similarity. Comprehensive experiments demonstrate the effectiveness of our weak-shot setting and our SimTrans method. ",
    "authors": [
      "Chen, Junjie",
      "Niu, Li",
      "Liu, Liu",
      "Zhang, Liqing"
    ]
  },
  {
    "id": "3c057cb2b41f22c0e740974d7a428918",
    "title": "Shape your Space: A Gaussian Mixture Regularization Approach to Deterministic Autoencoders",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3c057cb2b41f22c0e740974d7a428918-Paper.pdf",
    "abstract": "Variational Autoencoders (VAEs) are powerful probabilistic models to learn representations of complex data distributions. One important limitation of VAEs is the strong prior assumption that latent representations learned by the model follow a simple uni-modal Gaussian distribution. Further, the variational training procedure poses considerable practical challenges. Recently proposed regularized autoencoders offer a deterministic autoencoding framework, that simplifies the original VAE objective and is significantly easier to train. Since these models only provide weak control over the learned latent distribution, they require an ex-post density estimation step to generate samples comparable to those of VAEs. In this paper, we propose a simple and end-to-end trainable deterministic autoencoding framework, that efficiently shapes the latent space of the model during training and utilizes the capacity of expressive multi-modal latent distributions. The proposed training procedure provides direct evidence if the latent distribution adequately captures complex aspects of the encoded data. We show in experiments the expressiveness and sample quality of our model in various challenging continuous and discrete domains. An implementation is available at https://github.com/boschresearch/GMM_DAE.",
    "authors": [
      "Saseendran, Amrutha",
      "Skubch, Kathrin",
      "Falkner, Stefan",
      "Keuper, Margret"
    ]
  },
  {
    "id": "3c63ec7be1b6c49e6c308397023fd8cd",
    "title": "An Even More Optimal Stochastic Optimization Algorithm: Minibatching and Interpolation Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3c63ec7be1b6c49e6c308397023fd8cd-Paper.pdf",
    "abstract": "We present and analyze an algorithm for optimizing smooth and convex or strongly convex objectives using minibatch stochastic gradient estimates. The algorithm is optimal with respect to its dependence on both the minibatch size and minimum expected loss simultaneously. This improves over the optimal method of Lan, which is insensitive to the minimum expected loss; over the optimistic acceleration of Cotter et al., which has suboptimal dependence on the minibatch size; and over the algorithm of Liu and Belkin, which is limited to least squares problems and is also similarly suboptimal.  Applied to interpolation learning, the improvement over Cotter et al.~and Liu and Belkin translates to a linear, rather than square-root, parallelization speedup.  ",
    "authors": [
      "Woodworth, Blake E.",
      "Srebro, Nathan"
    ]
  },
  {
    "id": "3c88c1db16b9523b4dcdcd572aa1e16a",
    "title": "Indexed Minimum Empirical Divergence for Unimodal Bandits",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3c88c1db16b9523b4dcdcd572aa1e16a-Paper.pdf",
    "abstract": "We consider a stochastic multi-armed bandit problem specified by a set of one-dimensional family exponential distributions endowed with a unimodal structure. The unimodal structure is of practical relevance for several applications. We introduce IMED-UB, an algorithm that exploits provably optimally the unimodal-structure, by adapting to this setting the Indexed Minimum Empirical Divergence (IMED) algorithm introduced by Honda and Takemura (2015).  Owing to our proof technique, we are able to provide a concise finite-time analysis of the IMED-UB algorithm, that is simple and yet yields asymptotic optimality. We finally provide numerical experiments showing that IMED-UB competes favorably with the recently introduced state-of-the-art algorithms.",
    "authors": [
      "SABER, Hassan",
      "M\u00e9nard, Pierre",
      "Maillard, Odalric-Ambrym"
    ]
  },
  {
    "id": "3c8a49145944fed2bbcaade178a426c4",
    "title": "SOAT: A Scene- and Object-Aware Transformer for Vision-and-Language Navigation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3c8a49145944fed2bbcaade178a426c4-Paper.pdf",
    "abstract": "Natural language instructions for visual navigation often use scene descriptions (e.g., bedroom) and object references (e.g., green chairs) to provide a breadcrumb trail to a goal location. This work presents a transformer-based vision-and-language navigation (VLN) agent that uses two different visual encoders -- a scene classification network and an object detector -- which produce features that match these two distinct types of visual cues. In our method, scene features contribute high-level contextual information that supports object-level processing. With this design, our model is able to use vision-and-language pretraining (i.e., learning the alignment between images and text from large-scale web data) to substantially improve performance on the Room-to-Room (R2R) and Room-Across-Room (RxR) benchmarks. Specifically, our approach leads to improvements of 1.8% absolute in SPL on R2R and 3.7% absolute in SR on RxR. Our analysis reveals even larger gains for navigation instructions that contain six or more object references, which further suggests that our approach is better able to use object features and align them to references in the instructions.",
    "authors": [
      "Moudgil, Abhinav",
      "Majumdar, Arjun",
      "Agrawal, Harsh",
      "Lee, Stefan",
      "Batra, Dhruv"
    ]
  },
  {
    "id": "3ce3bd7d63a2c9c81983cc8e9bd02ae5",
    "title": "A Normative and Biologically Plausible Algorithm for Independent Component Analysis",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3ce3bd7d63a2c9c81983cc8e9bd02ae5-Paper.pdf",
    "abstract": "The brain effortlessly solves blind source separation (BSS) problems, but the algorithm it uses remains elusive. In signal processing, linear BSS problems are often solved by Independent Component Analysis (ICA). To serve as a model of a biological circuit, the ICA neural network (NN) must satisfy at least the following requirements: 1. The algorithm must operate in the online setting where data samples are streamed one at a time, and the NN computes the sources on the fly without storing any significant fraction of the data in memory. 2. The synaptic weight update is local, i.e., it depends only on the biophysical variables present in the vicinity of a synapse. Here, we propose a novel objective function for ICA from which we derive a biologically plausible NN, including both the neural architecture and the synaptic learning rules. Interestingly, our algorithm relies on modulating synaptic plasticity by the total activity of the output neurons. In the brain, this could be accomplished by neuromodulators, extracellular calcium, local field potential, or nitric oxide. ",
    "authors": [
      "Bahroun, Yanis",
      "Chklovskii, Dmitri",
      "Sengupta, Anirvan"
    ]
  },
  {
    "id": "3cec07e9ba5f5bb252d13f5f431e4bbb",
    "title": "Regret Bounds for Gaussian-Process Optimization in Large Domains",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3cec07e9ba5f5bb252d13f5f431e4bbb-Paper.pdf",
    "abstract": "The goal of this paper is to characterize Gaussian-Process optimization in the setting where the function domain is large relative to the number of admissible function evaluations, i.e., where it is impossible to find the global optimum. We provide upper bounds on the suboptimality (Bayesian simple regret) of the solution found by optimization strategies that are closely related to the widely used expected improvement (EI) and upper confidence bound (UCB) algorithms. These regret bounds illuminate the relationship between the number of evaluations, the domain size (i.e. cardinality of finite domains / Lipschitz constant of the covariance function in continuous domains), and the optimality of the retrieved function value.In particular, we show that even when the number of evaluations is far too small to find the global optimum, we can find nontrivial function values (e.g. values that achieve a certain ratio with the optimal value).",
    "authors": [
      "Wuethrich, Manuel",
      "Sch\u00f6lkopf, Bernhard",
      "Krause, Andreas"
    ]
  },
  {
    "id": "3cf2559725a9fdfa602ec8c887440f32",
    "title": "Deeply Shared Filter Bases for  Parameter-Efficient  Convolutional Neural Networks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3cf2559725a9fdfa602ec8c887440f32-Paper.pdf",
    "abstract": "Modern convolutional neural networks (CNNs) have massive identical convolution blocks, and, hence, recursive sharing of parameters across these blocks has been proposed to reduce the amount of parameters.  However, naive sharing of parameters poses many challenges such as limited representational power and the vanishing/exploding gradients problem of recursively shared parameters. In this paper, we present a recursive convolution block design and training method, in which a recursively shareable part, or a filter basis, is separated and learned while effectively avoiding the vanishing/exploding gradients problem during training. We show that the unwieldy vanishing/exploding gradients problem can be controlled by enforcing the elements of the filter basis orthonormal, and empirically demonstrate that the proposed orthogonality regularization improves the flow of gradients during training. Experimental results on image classification and object detection show that our approach, unlike previous parameter-sharing approaches, does not trade performance to save parameters and consistently outperforms over parameterized counterpart networks. This superior performance demonstrates that the proposed recursive convolution block design and the orthogonality regularization not only prevent performance degradation, but also consistently improve the representation capability while a significant amount of parameters are recursively shared.",
    "authors": [
      "Kang, Woochul",
      "Kim, Daeyeon"
    ]
  },
  {
    "id": "3d191ef6e236bd1b9bdb9ff4743c47fe",
    "title": "On Optimal Robustness to Adversarial Corruption in Online Decision Problems",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3d191ef6e236bd1b9bdb9ff4743c47fe-Paper.pdf",
    "abstract": "This paper considers two fundamental sequential decision-making problems: the problem of prediction with expert advice and the multi-armed bandit problem.  We focus on stochastic regimes in which an adversary may corrupt losses, and we investigate what level of robustness can be achieved against adversarial corruption.  The main contribution of this paper is to show that optimal robustness can be expressed by a square-root dependency on the amount of corruption.  More precisely, we show that two classes of algorithms, anytime Hedge with decreasing learning rate and algorithms with second-order regret bounds, achieve $O( \\frac{\\log N}{\\Delta} + \\sqrt{ \\frac{C \\log N }{\\Delta} } )$-regret, where $N, \\Delta$, and $C$ represent the number of experts, the gap parameter, and the corruption level, respectively.  We further provide a matching lower bound, which means that this regret bound is tight up to a constant factor. For the multi-armed bandit problem, we also provide a nearly-tight lower bound up to a logarithmic factor.",
    "authors": [
      "Ito, Shinji"
    ]
  },
  {
    "id": "3d36c07721a0a5a96436d6c536a132ec",
    "title": "Directed Spectrum Measures Improve Latent Network Models Of Neural Populations",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3d36c07721a0a5a96436d6c536a132ec-Paper.pdf",
    "abstract": "Systems neuroscience aims to understand how networks of neurons distributed throughout the brain mediate computational tasks. One popular approach to identify those networks is to first calculate measures of neural activity (e.g. power spectra) from multiple brain regions, and then apply a linear factor model to those measures. Critically, despite the established role of directed communication between brain regions in neural computation, measures of directed communication have been rarely utilized in network estimation because they are incompatible with the implicit assumptions of the linear factor model approach. Here, we develop a novel spectral measure of directed communication called the Directed Spectrum (DS). We prove that it is compatible with the implicit assumptions of linear factor models, and we provide a method to estimate the DS. We demonstrate that latent linear factor models of DS measures better capture underlying brain networks in both simulated and real neural recording data compared to available alternatives. Thus, linear factor models of the Directed Spectrum offer neuroscientists a simple and effective way to explicitly model directed communication in networks of neural populations.",
    "authors": [
      "Gallagher, Neil",
      "Dzirasa, Kafui",
      "Carlson, David"
    ]
  },
  {
    "id": "3d3d286a8d153a4a58156d0e02d8570c",
    "title": "Uncertainty-Based Offline Reinforcement Learning with Diversified Q-Ensemble",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3d3d286a8d153a4a58156d0e02d8570c-Paper.pdf",
    "abstract": "Offline reinforcement learning (offline RL), which aims to find an optimal policy from a previously collected static dataset, bears algorithmic difficulties due to function approximation errors from out-of-distribution (OOD) data points. To this end, offline RL algorithms adopt either a constraint or a penalty term that explicitly guides the policy to stay close to the given dataset. However, prior methods typically require accurate estimation of the behavior policy or sampling from OOD data points, which themselves can be a non-trivial problem. Moreover, these methods under-utilize the generalization ability of deep neural networks and often fall into suboptimal solutions too close to the given dataset. In this work, we propose an uncertainty-based offline RL method that takes into account the confidence of the Q-value prediction and does not require any estimation or sampling of the data distribution. We show that the clipped Q-learning, a technique widely used in online RL, can be leveraged to successfully penalize OOD data points with high prediction uncertainties. Surprisingly, we find that it is possible to substantially outperform existing offline RL methods on various tasks by simply increasing the number of Q-networks along with the clipped Q-learning. Based on this observation, we propose an ensemble-diversified actor-critic algorithm that reduces the number of required ensemble networks down to a tenth compared to the naive ensemble while achieving state-of-the-art performance on most of the D4RL benchmarks considered.",
    "authors": [
      "An, Gaon",
      "Moon, Seungyong",
      "Kim, Jang-Hyun",
      "Song, Hyun Oh"
    ]
  },
  {
    "id": "3d4893419e57449fb290647149f738d4",
    "title": "Distribution-free inference for regression: discrete, continuous, and in between",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3d4893419e57449fb290647149f738d4-Paper.pdf",
    "abstract": "In data analysis problems where we are not able to rely on distributional assumptions, what types of inference guarantees can still be obtained? Many popular methods, such as holdout methods, cross-validation methods, and conformal prediction, are able to provide distribution-free guarantees for predictive inference, but the problem of providing inference for the underlying regression function (for example, inference on the conditional mean $\\mathbb{E}[Y|X]$) is more challenging. In the setting where the features $X$ are continuously distributed, recent work has established that any confidence interval for $\\mathbb{E}[Y|X]$ must have non-vanishing width, even as sample size tends to infinity. At the other extreme, if $X$ takes only a small number of possible values, then inference on $\\mathbb{E}[Y|X]$ is trivial to achieve. In this work, we study the problem in settings in between these two extremes. We find that there are several distinct regimes in between the finite setting and the continuous setting, where vanishing-width confidence intervals are achievable if and only if the effective support size of the distribution of $X$ is smaller than the square of the sample size.",
    "authors": [
      "Lee, Yonghoon",
      "Barber, Rina"
    ]
  },
  {
    "id": "3d7d9461075eb7c37fbbfcad1d7042c1",
    "title": "Statistical Inference with M-Estimators on Adaptively Collected Data",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3d7d9461075eb7c37fbbfcad1d7042c1-Paper.pdf",
    "abstract": "Bandit algorithms are increasingly used in real-world sequential decision-making problems. Associated with this is an increased desire to be able to use the resulting datasets to answer scientific questions like: Did one type of ad lead to more purchases? In which contexts is a mobile health intervention effective? However, classical statistical approaches fail to provide valid confidence intervals when used with data collected with bandit algorithms. Alternative methods have recently been developed for simple models (e.g., comparison of means). Yet there is a lack of general methods for  conducting statistical inference using more complex models on data collected with (contextual) bandit algorithms; for example, current methods cannot be used for valid inference on parameters in a logistic regression model for a binary reward. In this  work, we develop theory justifying the use of M-estimators---which  includes estimators based on empirical risk minimization as well as maximum likelihood---on data collected with adaptive algorithms, including (contextual) bandit algorithms. Specifically, we show that M-estimators, modified with particular adaptive weights, can be used  to construct asymptotically valid confidence regions for a variety of inferential targets.",
    "authors": [
      "Zhang, Kelly",
      "Janson, Lucas",
      "Murphy, Susan"
    ]
  },
  {
    "id": "3d863b367aa379f71c7afc0c9cdca41d",
    "title": "NeuroLKH: Combining Deep Learning Model with Lin-Kernighan-Helsgaun Heuristic for Solving the Traveling Salesman Problem",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3d863b367aa379f71c7afc0c9cdca41d-Paper.pdf",
    "abstract": "We present NeuroLKH, a novel algorithm that combines deep learning with the strong traditional heuristic Lin-Kernighan-Helsgaun (LKH) for solving Traveling Salesman Problem. Specifically, we train a Sparse Graph Network (SGN) with supervised learning for edge scores and unsupervised learning for node penalties, both of which are critical for improving the performance of LKH. Based on the output of SGN, NeuroLKH creates the edge candidate set and transforms edge distances to guide the searching process of LKH. Extensive experiments firmly demonstrate that, by training one model on a wide range of problem sizes, NeuroLKH significantly outperforms LKH and generalizes well to much larger sizes. Also, we show that NeuroLKH can be applied to other routing problems such as Capacitated Vehicle Routing Problem (CVRP), Pickup and Delivery Problem (PDP), and CVRP with Time Windows (CVRPTW).",
    "authors": [
      "Xin, Liang",
      "Song, Wen",
      "Cao, Zhiguang",
      "Zhang, Jie"
    ]
  },
  {
    "id": "3d98b79ac6c8d1cef43d7bf1dadf8647",
    "title": "LSH-SMILE: Locality Sensitive Hashing Accelerated Simulation and Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3d98b79ac6c8d1cef43d7bf1dadf8647-Paper.pdf",
    "abstract": "The advancement of deep neural networks over the last decade has enabled progress in scientific knowledge discovery in the form of learning Partial Differential Equations (PDEs) directly from experiment data. Nevertheless, forward simulation and backward learning of large-scale dynamic systems require handling billions of mutually interacting elements, the scale of which overwhelms current computing architectures. We propose Locality Sensitive Hashing Accelerated Simulation and Learning (LSH-SMILE), a unified framework to scale up both forward simulation and backward learning of physics systems. LSH-SMILE takes advantage of (i) the locality of PDE updates, (ii) similar temporal dynamics shared by multiple elements. LSH-SMILE hashes elements with similar dynamics into a single hash bucket and handles their updates at once. This allows LSH-SMILE to scale with respect to the number of non-empty hash buckets, a drastic improvement over conventional approaches. Theoretically, we prove a novel bound on the errors introduced by LSH-SMILE. Experimentally, we demonstrate that LSH-SMILE simulates physics systems at comparable quality with exact approaches, but with way less time and space complexity. Such savings also translate to better learning performance due to LSH-SMILE's ability to propagate gradients over a long duration.",
    "authors": [
      "Sima, Chonghao",
      "Xue, Yexiang"
    ]
  },
  {
    "id": "3dc4876f3f08201c7c76cb71fa1da439",
    "title": "Meta-learning with an Adaptive Task Scheduler",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3dc4876f3f08201c7c76cb71fa1da439-Paper.pdf",
    "abstract": "To benefit the learning of a new task, meta-learning has been proposed to transfer a well-generalized meta-model learned from various meta-training tasks. Existing meta-learning algorithms randomly sample meta-training tasks with a uniform probability, under the assumption that tasks are of equal importance. However, it is likely that tasks are detrimental with noise or imbalanced given a limited number of meta-training tasks. To prevent the meta-model from being corrupted by such detrimental tasks or dominated by tasks in the majority, in this paper, we propose an adaptive task scheduler (ATS) for the meta-training process. In ATS, for the first time, we design a neural scheduler to decide which meta-training tasks to use next by predicting the probability being sampled for each candidate task, and train the scheduler to optimize the generalization capacity of the meta-model to unseen tasks. We identify two meta-model-related factors as the input of the neural scheduler, which characterize the difficulty of a candidate task to the meta-model. Theoretically, we show that a scheduler taking the two factors into account improves the meta-training loss and also the optimization landscape. Under the setting of meta-learning with noise and limited budgets, ATS improves the performance on both miniImageNet and a real-world drug discovery benchmark by up to 13% and 18%, respectively, compared to state-of-the-art task schedulers.",
    "authors": [
      "Yao, Huaxiu",
      "Wang, Yu",
      "Wei, Ying",
      "Zhao, Peilin",
      "Mahdavi, Mehrdad",
      "Lian, Defu",
      "Finn, Chelsea"
    ]
  },
  {
    "id": "3dcaf04c357c577a857f3ffadc555f9b",
    "title": "Neural Active Learning with Performance Guarantees",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3dcaf04c357c577a857f3ffadc555f9b-Paper.pdf",
    "abstract": "We investigate the problem of active learning in the streaming setting in non-parametric regimes, where the labels are stochastically generated from a class of functions on which we make no assumptions whatsoever. We rely on recently proposed Neural Tangent Kernel (NTK) approximation tools to construct a suitable neural embedding that determines the feature space the algorithm operates on and the learned model computed atop. Since the shape of the label requesting threshold is tightly related to the complexity of the function to be learned, which is a-priori unknown, we also derive a version of the algorithm which is agnostic to any prior knowledge. This algorithm relies on a regret balancing scheme to solve the resulting online model selection problem, and is computationally efficient. We prove joint guarantees on the cumulative regret and number of requested labels which depend on the complexity of the labeling function at hand. In the linear case, these guarantees recover known minimax results of the generalization error as a function of the label complexity in a standard statistical learning setting.",
    "authors": [
      "Wang, Zhilei",
      "Awasthi, Pranjal",
      "Dann, Christoph",
      "Sekhari, Ayush",
      "Gentile, Claudio"
    ]
  },
  {
    "id": "3de568f8597b94bda53149c7d7f5958c",
    "title": "A Gradient Method for Multilevel Optimization",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3de568f8597b94bda53149c7d7f5958c-Paper.pdf",
    "abstract": "Although application examples of multilevel optimization have already been discussed since the 1990s, the development of solution methods was almost limited to bilevel cases due to the difficulty of the problem. In recent years, in machine learning, Franceschi et al. have proposed a method for solving bilevel optimization problems by replacing their lower-level problems with the $T$ steepest descent update equations with some prechosen iteration number $T$. In this paper, we have developed a gradient-based algorithm for multilevel optimization with $n$ levels based on their idea and proved that our reformulation asymptotically converges to the original multilevel problem. As far as we know, this is one of the first algorithms with some theoretical guarantee for multilevel optimization. Numerical experiments show that a trilevel hyperparameter learning model considering data poisoning produces more stable prediction results than an existing bilevel hyperparameter learning model in noisy data settings.",
    "authors": [
      "Sato, Ryo",
      "Tanaka, Mirai",
      "Takeda, Akiko"
    ]
  },
  {
    "id": "3def184ad8f4755ff269862ea77393dd",
    "title": "Edge Representation Learning with Hypergraphs",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3def184ad8f4755ff269862ea77393dd-Paper.pdf",
    "abstract": "Graph neural networks have recently achieved remarkable success in representing graph-structured data, with rapid progress in both the node embedding and graph pooling methods. Yet, they mostly focus on capturing information from the nodes considering their connectivity, and not much work has been done in representing the edges, which are essential components of a graph. However, for tasks such as graph reconstruction and generation, as well as graph classification tasks for which the edges are important for discrimination, accurately representing edges of a given graph is crucial to the success of the graph representation learning. To this end, we propose a novel edge representation learning framework based on Dual Hypergraph Transformation (DHT), which transforms the edges of a graph into the nodes of a hypergraph. This dual hypergraph construction allows us to apply message-passing techniques for node representations to edges. After obtaining edge representations from the hypergraphs, we then cluster or drop edges to obtain holistic graph-level edge representations. We validate our edge representation learning method with hypergraphs on diverse graph datasets for graph representation and generation performance, on which our method largely outperforms existing graph representation learning methods. Moreover, our edge representation learning and pooling method also largely outperforms state-of-the-art graph pooling methods on graph classification, not only because of its accurate edge representation learning, but also due to its lossless compression of the nodes and removal of irrelevant edges for effective message-passing.",
    "authors": [
      "Jo, Jaehyeong",
      "Baek, Jinheon",
      "Lee, Seul",
      "Kim, Dongki",
      "Kang, Minki",
      "Hwang, Sung Ju"
    ]
  },
  {
    "id": "3df07fdae1ab273a967aaa1d355b8bb6",
    "title": "One Question Answering Model for Many Languages with Cross-lingual Dense Passage Retrieval",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3df07fdae1ab273a967aaa1d355b8bb6-Paper.pdf",
    "abstract": "We present Cross-lingual Open-Retrieval Answer Generation (CORA), the first unified many-to-many question answering (QA) model that can answer questions across many languages, even for ones without language-specific annotated data or knowledge sources.We introduce a new dense passage retrieval algorithm that is trained to retrieve documents across languages for a question.Combined with a  multilingual autoregressive generation model, CORA answers directly in the target language without any translation or in-language retrieval modules as used in prior work. We propose an iterative training method that automatically extends annotated data available only in high-resource languages to low-resource ones. Our results show that CORA substantially outperforms the previous state of the art on multilingual open QA benchmarks across 26 languages, 9 of which are unseen during training. Our analyses show the significance of cross-lingual retrieval and generation in many languages, particularly under low-resource settings. ",
    "authors": [
      "Asai, Akari",
      "Yu, Xinyan",
      "Kasai, Jungo",
      "Hajishirzi, Hanna"
    ]
  },
  {
    "id": "3df1d4b96d8976ff5986393e8767f5b2",
    "title": "LEADS: Learning Dynamical Systems that Generalize Across Environments",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3df1d4b96d8976ff5986393e8767f5b2-Paper.pdf",
    "abstract": "When modeling dynamical systems from real-world data samples, the distribution of data often changes according to the environment in which they are captured, and the dynamics of the system itself vary from one environment to another. Generalizing across environments thus challenges the conventional frameworks. The classical settings suggest either considering data as i.i.d and learning a single model to cover all situations or learning environment-specific models. Both are sub-optimal: the former disregards the discrepancies between environments leading to biased solutions, while the latter does not exploit their potential commonalities and is prone to scarcity problems. We propose LEADS, a novel framework that leverages the commonalities and discrepancies among known environments to improve model generalization. This is achieved with a tailored training formulation aiming at capturing common dynamics within a shared model while additional terms capture environment-specific dynamics. We ground our approach in theory, exhibiting a decrease in sample complexity w.r.t classical alternatives.  We show how theory and practice coincides on the simplified case of linear dynamics. Moreover, we instantiate this framework for neural networks and evaluate it experimentally on representative families of nonlinear dynamics. We show that this new setting can exploit knowledge extracted from environment-dependent data and improves generalization for both known and novel environments.",
    "authors": [
      "Yin, Yuan",
      "Ayed, Ibrahim",
      "de B\u00e9zenac, Emmanuel",
      "Baskiotis, Nicolas",
      "Gallinari, Patrick"
    ]
  },
  {
    "id": "3dfe2f633108d604df160cd1b01710db",
    "title": "Storchastic: A Framework for General Stochastic Automatic Differentiation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3dfe2f633108d604df160cd1b01710db-Paper.pdf",
    "abstract": "Modelers use automatic differentiation (AD) of computation graphs to implement complex Deep Learning models without defining gradient computations. Stochastic AD extends AD to stochastic computation graphs with sampling steps, which arise when modelers handle the intractable expectations common in Reinforcement Learning and Variational Inference. However, current methods for stochastic AD are limited: They are either only applicable to continuous random variables and differentiable functions, or can only use simple but high variance score-function estimators. To overcome these limitations, we introduce Storchastic, a new framework for AD of stochastic computation graphs. Storchastic allows the modeler to choose from a wide variety of gradient estimation methods at each sampling step, to optimally reduce the variance of the gradient estimates. Furthermore, Storchastic is provably unbiased for estimation of any-order gradients, and generalizes variance reduction techniques to higher-order gradient estimates. Finally, we implement Storchastic as a PyTorch library at github.com/HEmile/storchastic.",
    "authors": [
      "Krieken, Emile",
      "Tomczak, Jakub",
      "Ten Teije, Annette"
    ]
  },
  {
    "id": "3e33b970f21d2fc65096871ea0d2c6e4",
    "title": "Concentration inequalities under sub-Gaussian and sub-exponential conditions",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3e33b970f21d2fc65096871ea0d2c6e4-Paper.pdf",
    "abstract": "We prove analogues of the popular bounded difference inequality (also called McDiarmid's inequality) for functions of independent random variables under sub-gaussian and sub-exponential conditions. Applied to vector-valued concentration and the method of Rademacher complexities these inequalities allow an easy extension of uniform convergence results for PCA and linear regression to the case potentially unbounded input- and output variables.",
    "authors": [
      "Maurer, Andreas",
      "Pontil, Massimiliano"
    ]
  },
  {
    "id": "3e6260b81898beacda3d16db379ed329",
    "title": "Variance-Aware Off-Policy Evaluation with Linear Function Approximation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3e6260b81898beacda3d16db379ed329-Paper.pdf",
    "abstract": "We study the off-policy evaluation (OPE) problem in reinforcement learning with linear function approximation, which aims to estimate the value function of a target policy based on the offline data collected by a behavior policy. We propose to incorporate the variance information of the value function to improve the sample efficiency of OPE. More specifically, for time-inhomogeneous episodic linear Markov decision processes (MDPs), we propose an algorithm, \\texttt{VA-OPE}, which uses the estimated variance of the value function to reweight the Bellman residual in Fitted Q-Iteration. We show that our algorithm achieves a tighter error bound than the best-known result. We also provide a fine-grained characterization of the distribution shift between the behavior policy and the target policy. Extensive numerical experiments corroborate our theory.",
    "authors": [
      "Min, Yifei",
      "Wang, Tianhao",
      "Zhou, Dongruo",
      "Gu, Quanquan"
    ]
  },
  {
    "id": "3e98410c45ea98addec555019bbae8eb",
    "title": "A Provably Efficient Sample Collection Strategy for Reinforcement Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3e98410c45ea98addec555019bbae8eb-Paper.pdf",
    "abstract": "One of the challenges in online reinforcement learning (RL) is that the agent needs to trade off the exploration of the environment and the exploitation of the samples to optimize its behavior. Whether we optimize for regret, sample complexity, state-space coverage or model estimation, we need to strike a different exploration-exploitation trade-off. In this paper, we propose to tackle the exploration-exploitation problem following a decoupled approach composed of: 1) An \"objective-specific\" algorithm that (adaptively) prescribes how many samples to collect at which states, as if it has access to a generative model (i.e., a simulator of the environment); 2) An \"objective-agnostic\" sample collection exploration strategy responsible for generating the prescribed samples as fast as possible. Building on recent methods for exploration in the stochastic shortest path problem, we first provide an algorithm that, given as input the number of samples $b(s,a)$ needed in each state-action pair, requires $\\widetilde{O}(B D + D^{3/2} S^2 A)$ time steps to collect the $B=\\sum_{s,a} b(s,a)$ desired samples, in any unknown communicating MDP with $S$ states, $A$ actions and diameter $D$. Then we show how this general-purpose exploration algorithm can be paired with \"objective-specific\" strategies that prescribe the sample requirements to tackle a variety of settings \u2014 e.g., model estimation, sparse reward discovery, goal-free cost-free exploration in communicating MDPs \u2014 for which we obtain improved or novel sample complexity guarantees.",
    "authors": [
      "Tarbouriech, Jean",
      "Pirotta, Matteo",
      "Valko, Michal",
      "Lazaric, Alessandro"
    ]
  },
  {
    "id": "3e9f7c16bd1cdea78f8e2eea72dfdfbe",
    "title": "Improved Regret Bounds for Tracking Experts with Memory",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3e9f7c16bd1cdea78f8e2eea72dfdfbe-Paper.pdf",
    "abstract": "We address the problem of sequential prediction with expert advice in a non-stationary environment with long-term memory guarantees in the sense of Bousquet and Warmuth [4]. We give a linear-time algorithm that improves on the best known regret bound [27]. This algorithm incorporates a relative entropy projection step. This projection is advantageous over previous weight-sharing approaches in that weight updates may come with implicit costs as in for example portfolio optimization. We give an algorithm to compute this projection step in linear time, which may be of independent interest.",
    "authors": [
      "Robinson, James",
      "Herbster, Mark"
    ]
  },
  {
    "id": "3ea2db50e62ceefceaf70a9d9a56a6f4",
    "title": "Robustness of Graph Neural Networks at Scale",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3ea2db50e62ceefceaf70a9d9a56a6f4-Paper.pdf",
    "abstract": "Graph Neural Networks (GNNs) are increasingly important given their popularity and the diversity of applications. Yet, existing studies of their vulnerability to adversarial attacks rely on relatively small graphs. We address this gap and study how to attack and defend GNNs at scale. We propose two sparsity-aware first-order optimization attacks that maintain an efficient representation despite optimizing over a number of parameters which is quadratic in the number of nodes. We show that common surrogate losses are not well-suited for global attacks on GNNs. Our alternatives can double the attack strength. Moreover, to improve GNNs' reliability we design a robust aggregation function, Soft Median, resulting in an effective defense at all scales. We evaluate our attacks and defense with standard GNNs on graphs more than 100 times larger compared to previous work. We even scale one order of magnitude further by extending our techniques to a scalable GNN.",
    "authors": [
      "Geisler, Simon",
      "Schmidt, Tobias",
      "\u015eirin, Hakan",
      "Z\u00fcgner, Daniel",
      "Bojchevski, Aleksandar",
      "G\u00fcnnemann, Stephan"
    ]
  },
  {
    "id": "3eb414bf1c2a66a09c185d60553417b8",
    "title": "Random Noise Defense Against Query-Based Black-Box Attacks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3eb414bf1c2a66a09c185d60553417b8-Paper.pdf",
    "abstract": "The query-based black-box attacks have raised serious threats to machine learning models in many real applications. In this work, we study a lightweight defense method, dubbed Random Noise Defense (RND), which adds proper Gaussian noise to each query. We conduct the theoretical analysis about the effectiveness of RND against query-based black-box attacks and the corresponding adaptive attacks. Our theoretical results reveal that the defense performance of RND is determined by the magnitude ratio between the noise induced by RND and the noise added by the attackers for gradient estimation or local search.  The large magnitude ratio leads to the stronger defense performance of RND, and it's also critical for mitigating adaptive attacks. Based on our analysis, we further propose to combine RND with a plausible Gaussian augmentation Fine-tuning (RND-GF). It enables RND to add larger noise to each query while maintaining the clean accuracy to obtain a better trade-off between clean accuracy and defense performance. Additionally, RND can be flexibly combined with the existing defense methods to further boost the adversarial robustness, such as adversarial training (AT). Extensive experiments on CIFAR-10 and ImageNet verify our theoretical findings and the effectiveness of RND and RND-GF. ",
    "authors": [
      "Qin, Zeyu",
      "Fan, Yanbo",
      "Zha, Hongyuan",
      "Wu, Baoyuan"
    ]
  },
  {
    "id": "3f1656d9668dffcf8119e3ecff873558",
    "title": "SADGA: Structure-Aware Dual Graph Aggregation Network for Text-to-SQL",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3f1656d9668dffcf8119e3ecff873558-Paper.pdf",
    "abstract": "The Text-to-SQL task, aiming to translate the natural language of the questions into SQL queries, has drawn much attention recently.  One of the most challenging problems of Text-to-SQL is how to generalize the trained model to the unseen database schemas, also known as the cross-domain Text-to-SQL task. The key lies in the generalizability of (i) the encoding method to model the question and the database schema and (ii) the question-schema linking method to learn the mapping between words in the question and tables/columns in the database schema. Focusing on the above two key issues, we propose a \\emph{Structure-Aware Dual Graph Aggregation Network} (SADGA) for cross-domain Text-to-SQL. In SADGA, we adopt the graph structure to provide a unified encoding model for both the natural language question and database schema. Based on the proposed unified modeling, we further devise a structure-aware aggregation method to learn the mapping between the question-graph and schema-graph. The structure-aware aggregation method is featured with \\emph{Global Graph Linking}, \\emph{Local Graph Linking} and \\emph{Dual-Graph Aggregation Mechanism}. We not only study the performance of our proposal empirically but also achieved 3rd place on the challenging Text-to-SQL benchmark Spider at the time of writing.",
    "authors": [
      "Cai, Ruichu",
      "Yuan, Jinjie",
      "Xu, Boyan",
      "Hao, Zhifeng"
    ]
  },
  {
    "id": "3f24bb08a5741e4197af64e1f93a5029",
    "title": "Near-Optimal Offline Reinforcement Learning via Double Variance Reduction",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3f24bb08a5741e4197af64e1f93a5029-Paper.pdf",
    "abstract": "We consider the problem of offline reinforcement learning (RL)  --- a well-motivated setting of RL that aims at policy optimization using only historical data. Despite its wide applicability, theoretical understandings of offline RL, such as its optimal sample complexity, remain largely open even in basic settings such as \\emph{tabular} Markov Decision Processes (MDPs).  In this paper, we propose \\emph{Off-Policy Double Variance Reduction} (OPDVR), a new variance reduction-based algorithm for offline RL. Our main result shows that OPDVR provably identifies an $\\epsilon$-optimal policy with $\\widetilde{O}(H^2/d_m\\epsilon^2)$ episodes of offline data in the finite-horizon \\emph{stationary transition} setting, where $H$ is the horizon length and $d_m$ is the minimal marginal state-action distribution induced by the behavior policy. This improves over the best-known upper bound by a factor of $H$. Moreover, we establish an information-theoretic lower bound of $\\Omega(H^2/d_m\\epsilon^2)$ which certifies that OPDVR is optimal up to logarithmic factors.  Lastly, we show that OPDVR also achieves rate-optimal sample complexity under alternative settings such as the finite-horizon MDPs with non-stationary transitions and the infinite horizon MDPs with discounted rewards.",
    "authors": [
      "Yin, Ming",
      "Bai, Yu",
      "Wang, Yu-Xiang"
    ]
  },
  {
    "id": "3f67fd97162d20e6fe27748b5b372509",
    "title": "Joint Modeling of Visual Objects and Relations for Scene Graph Generation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3f67fd97162d20e6fe27748b5b372509-Paper.pdf",
    "abstract": "An in-depth scene understanding usually requires recognizing all the objects and their relations in an image, encoded as a scene graph. Most existing approaches for scene graph generation first independently recognize each object and then predict their relations independently. Though these approaches are very efficient, they ignore the dependency between different objects as well as between their relations. In this paper, we propose a principled approach to jointly predict the entire scene graph by fully capturing the dependency between different objects and between their relations. Specifically, we establish a unified conditional random field (CRF) to model the joint distribution of all the objects and their relations in a scene graph. We carefully design the potential functions to enable relational reasoning among different objects according to knowledge graph embedding methods. We further propose an efficient and effective algorithm for inference based on mean-field variational inference, in which we first provide a warm initialization by independently predicting the objects and their relations according to the current model, followed by a few iterations of relational reasoning. Experimental results on both the relationship retrieval and zero-shot relationship retrieval tasks prove the efficiency and efficacy of our proposed approach.",
    "authors": [
      "Xu, Minghao",
      "Qu, Meng",
      "Ni, Bingbing",
      "Tang, Jian"
    ]
  },
  {
    "id": "3f9e3767ef3b10a0de4c256d7ef9805d",
    "title": "Going Beyond Linear Transformers with Recurrent Fast Weight Programmers",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3f9e3767ef3b10a0de4c256d7ef9805d-Paper.pdf",
    "abstract": "Transformers with linearised attention (''linear Transformers'') have demonstrated the practical scalability and effectiveness of outer product-based Fast Weight Programmers (FWPs) from the '90s. However, the original FWP formulation is more general than the one of linear Transformers: a slow neural network (NN) continually reprograms the weights of a fast NN with arbitrary architecture. In existing linear Transformers, both NNs are feedforward and consist of a single layer. Here we explore new variations by adding recurrence to the slow and fast nets. We evaluate our novel recurrent FWPs (RFWPs) on two synthetic algorithmic tasks (code execution and sequential ListOps), Wikitext-103 language models, and on the Atari 2600 2D game environment. Our models exhibit properties of Transformers and RNNs. In the reinforcement learning setting, we report large improvements over LSTM in several Atari games. Our code is public.",
    "authors": [
      "Irie, Kazuki",
      "Schlag, Imanol",
      "Csord\u00e1s, R\u00f3bert",
      "Schmidhuber, J\u00fcrgen"
    ]
  },
  {
    "id": "3fab5890d8113d0b5a4178201dc842ad",
    "title": "Reinforced Few-Shot Acquisition Function Learning for Bayesian Optimization",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3fab5890d8113d0b5a4178201dc842ad-Paper.pdf",
    "abstract": "Bayesian optimization (BO) conventionally relies on handcrafted acquisition functions (AFs) to sequentially determine the sample points. However, it has been widely observed in practice that the best-performing AF in terms of regret can vary significantly under different types of black-box functions. It has remained a challenge to design one AF that can attain the best performance over a wide variety of black-box functions. This paper aims to attack this challenge through the perspective of reinforced few-shot AF learning (FSAF). Specifically, we first connect the notion of AFs with Q-functions and view a deep Q-network (DQN) as a surrogate differentiable AF. While it serves as a natural idea to combine DQN and an existing few-shot learning method, we identify that such a direct combination does not perform well due to severe overfitting, which is particularly critical in BO due to the need of a versatile sampling policy. To address this, we present a Bayesian variant of DQN with the following three features: (i) It learns a distribution of Q-networks as AFs based on the Kullback-Leibler regularization framework. This inherently provides the uncertainty required in sampling for BO and mitigates overfitting. (ii) For the prior of the Bayesian DQN, we propose to use a demo policy induced by an off-the-shelf AF for better training stability. (iii) On the meta-level, we leverage the meta-loss of Bayesian model-agnostic meta-learning, which serves as a natural companion to the proposed FSAF. Moreover, with the proper design of the Q-networks, FSAF is general-purpose in that it is agnostic to the dimension and the cardinality of the input domain. Through extensive experiments, we demonstrate that the FSAF achieves comparable or better regrets than the state-of-the-art benchmarks on a wide variety of synthetic and real-world test functions.",
    "authors": [
      "Hsieh, Bing-Jing",
      "Hsieh, Ping-Chun",
      "Liu, Xi"
    ]
  },
  {
    "id": "3ff4cea152080fd7d692a8286a587a67",
    "title": "Forster Decomposition and Learning Halfspaces with Noise",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3ff4cea152080fd7d692a8286a587a67-Paper.pdf",
    "abstract": "A Forster transform is an operation that turns a multivariate distribution into one with good anti-concentration properties. While a Forster transform does not always exist, we show that any distribution can be efficiently decomposed as a disjoint mixture of few distributions for which a Forster transform exists and can be computed efficiently. As the main application of this result, we obtain the first polynomial-time algorithm for distribution-independent PAC learning of halfspaces in the Massart noise model with strongly polynomial sample complexity, i.e., independent of the bit complexity of the examples. Previous algorithms for this learning problem incurred sample complexity scaling polynomially with the bit complexity, even though such a dependence is not information-theoretically necessary.",
    "authors": [
      "Diakonikolas, Ilias",
      "Kane, Daniel",
      "Tzamos, Christos"
    ]
  },
  {
    "id": "3ffebb08d23c609875d7177ee769a3e9",
    "title": "Cortico-cerebellar networks as decoupling neural interfaces",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/3ffebb08d23c609875d7177ee769a3e9-Paper.pdf",
    "abstract": "The brain solves the credit assignment problem remarkably well. For credit to be assigned across neural networks they must, in principle, wait for specific neural computations to finish. How the brain deals with this inherent locking problem has remained unclear. Deep learning methods suffer from similar locking constraints both on the forward and feedback phase. Recently, decoupled neural interfaces (DNIs) were introduced as a solution to the forward and feedback locking problems in deep networks.Here we propose that a specialised brain region, the cerebellum, helps the cerebral cortex solve similar locking problems akin to DNIs. To demonstrate the potential of this framework we introduce a systems-level model in which a recurrent cortical network receives online temporal feedback predictions from a cerebellar module. We test this cortico-cerebellar recurrent neural network (ccRNN) model on a number of sensorimotor (line and digit drawing) and cognitive tasks (pattern recognition and caption generation) that have been shown to be cerebellar-dependent. In all tasks, we observe that ccRNNs facilitates learning while reducing ataxia-like behaviours, consistent with classical experimental observations. Moreover, our model also explains recent behavioural and neuronal observations while making several testable predictions across multiple levels.Overall, our work offers a novel perspective on the cerebellum as a brain-wide decoupling machine for efficient credit assignment and opens a new avenue between deep learning and neuroscience.",
    "authors": [
      "Pemberton, Joseph",
      "Boven, Ellen",
      "Apps, Richard",
      "Ponte Costa, Rui "
    ]
  },
  {
    "id": "40008b9a5380fcacce3976bf7c08af5b",
    "title": "To The Point: Correspondence-driven monocular 3D category reconstruction",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/40008b9a5380fcacce3976bf7c08af5b-Paper.pdf",
    "abstract": "We present To The Point (TTP), a method for reconstructing 3D objects from a single image using 2D to 3D correspondences given only foreground masks, a category specific template and optionally sparse keypoints for supervision.  We recover a 3D shape from a 2D image by first regressing the 2D positions corresponding to the 3D template vertices and then jointly estimating a rigid camera transform and non-rigid template deformation that optimally explain the 2D positions through the 3D shape projection. By relying on correspondences we use a simple per-sample optimization problem to replace CNN-based regression of camera pose and non-rigid deformation and thereby obtain substantially more accurate 3D reconstructions. We treat this optimization as a differentiable layer and train the whole system in an end-to-end manner using geometry-driven losses. We report systematic quantitative improvements on multiple categories and provide qualitative results comprising diverse shape, poses and texture prediction examples.",
    "authors": [
      "Kokkinos, Filippos",
      "Kokkinos, Iasonas"
    ]
  },
  {
    "id": "400e5e6a7ce0c754f281525fae75a873",
    "title": "Proper Value Equivalence",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/400e5e6a7ce0c754f281525fae75a873-Paper.pdf",
    "abstract": "One of the main challenges in model-based reinforcement learning (RL) is to decide which aspects of the environment should be modeled. The value-equivalence (VE) principle proposes a simple answer to this question: a model should capture the aspects of the environment that are relevant for value-based planning. Technically, VE distinguishes models based on a set of policies and a set of functions: a model is said to be VE to the environment if the Bellman operators it induces for the policies yield the correct result when applied to the functions. As the number of policies and functions increase, the set of VE models shrinks, eventually collapsing to a single point corresponding to a perfect model. A fundamental question underlying the VE principle is thus how to select the smallest sets of policies and functions that are sufficient for planning. In this paper we take an important step towards answering this question. We start by generalizing the concept of VE to order-$k$ counterparts defined with respect to $k$ applications of the Bellman operator. This leads to a family of VE classes that increase in size as $k \\rightarrow \\infty$. In the limit, all functions become value functions, and we have a special instantiation of VE which we call proper VE or simply PVE. Unlike VE, the PVE class may contain multiple models even in the limit when all value functions are used. Crucially, all these models are sufficient for planning, meaning that they will yield an optimal policy despite the fact that they may ignore many aspects of the environment. We construct a loss function for learning PVE models and argue that popular algorithms such as MuZero can be understood as minimizing an upper bound for this loss. We leverage this connection to propose a modification to MuZero and show that it can lead to improved performance in practice.",
    "authors": [
      "Grimm, Christopher",
      "Barreto, Andre",
      "Farquhar, Greg",
      "Silver, David",
      "Singh, Satinder"
    ]
  },
  {
    "id": "404dcc91b2aeaa7caa47487d1483e48a",
    "title": "Challenges and Opportunities in High Dimensional Variational Inference",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/404dcc91b2aeaa7caa47487d1483e48a-Paper.pdf",
    "abstract": "Current black-box variational inference (BBVI) methods require the user to make numerous design choices \u2013 such as the selection of variational objective and approximating family \u2013 yet there is little principled guidance on how to do so. We develop a conceptual framework and set of experimental tools to understand the effects of these choices, which we leverage to propose best practices for maximizing posterior approximation accuracy. Our approach is based on studying the pre-asymptotic tail behavior of the density ratios between the joint distribution and the variational approximation, then exploiting insights and tools from the importance sampling literature. Our framework and supporting experiments help to distinguish between the behavior of BBVI methods for approximating low-dimensional versus moderate-to-high-dimensional posteriors. In the latter case, we show that mass-covering variational objectives are difficult to optimize and do not improve accuracy, but flexible variational families can improve accuracy and the effectiveness of importance sampling \u2013 at the cost of additional optimization challenges. Therefore, for moderate-to-high-dimensional posteriors we recommend using the (mode-seeking) exclusive KL divergence since it is the easiest to optimize, and improving the variational family or using model parameter transformations to make the posterior and optimal variational approximation more similar. On the other hand, in low-dimensional settings, we show that heavy-tailed variational families and mass-covering divergences are effective and can increase the chances that the approximation can be improved by importance sampling. ",
    "authors": [
      "Dhaka, Akash Kumar",
      "Catalina, Alejandro",
      "Welandawe, Manushi",
      "Andersen, Michael R.",
      "Huggins, Jonathan",
      "Vehtari, Aki"
    ]
  },
  {
    "id": "4079016d940210b4ae9ae7d41c4a2065",
    "title": "On the Expressivity of Markov Reward",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4079016d940210b4ae9ae7d41c4a2065-Paper.pdf",
    "abstract": "Reward is the driving force for reinforcement-learning agents. This paper is dedicated to understanding the expressivity of reward as a way to capture tasks that we would want an agent to perform. We frame this study around three new abstract notions of \u201ctask\u201d that might be desirable: (1) a set of acceptable behaviors, (2) a partial ordering over behaviors, or (3) a partial ordering over trajectories. Our main results prove that while reward can express many of these tasks, there exist instances of each task type that no Markov reward function can capture. We then provide a set of polynomial-time algorithms that construct a Markov reward function that allows an agent to optimize tasks of each of these three types, and correctly determine when no such reward function exists. We conclude with an empirical study that corroborates and illustrates our theoretical findings.",
    "authors": [
      "Abel, David",
      "Dabney, Will",
      "Harutyunyan, Anna",
      "Ho, Mark K.",
      "Littman, Michael",
      "Precup, Doina",
      "Singh, Satinder"
    ]
  },
  {
    "id": "40cb228987243c91b2dd0b7c9c4a0856",
    "title": "One More Step Towards Reality: Cooperative Bandits with Imperfect Communication",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/40cb228987243c91b2dd0b7c9c4a0856-Paper.pdf",
    "abstract": "The cooperative bandit problem is increasingly becoming relevant due to its applications in large-scale decision-making. However, most research for this problem focuses exclusively on the setting with perfect communication, whereas in most real-world distributed settings, communication is often over stochastic networks, with arbitrary corruptions and delays. In this paper, we study cooperative bandit learning under three typical real-world communication scenarios, namely, (a) message-passing over stochastic time-varying networks, (b) instantaneous reward-sharing over a network with random delays, and (c) message-passing with adversarially corrupted rewards, including byzantine communication. For each of these environments, we propose decentralized algorithms that achieve competitive performance, along with near-optimal guarantees on the incurred group regret as well. Furthermore, in the setting with perfect  communication, we present an improved delayed-update algorithm that outperforms the existing state-of-the-art on various network topologies. Finally, we present tight network-dependent minimax lower bounds on the group regret. Our proposed algorithms are straightforward to implement and obtain competitive empirical performance.",
    "authors": [
      "Madhushani, Udari",
      "Dubey, Abhimanyu",
      "Leonard, Naomi",
      "Pentland, Alex"
    ]
  },
  {
    "id": "412604be30f701b1b1e3124c252065e6",
    "title": "Multi-Agent Reinforcement Learning in Stochastic Networked Systems",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/412604be30f701b1b1e3124c252065e6-Paper.pdf",
    "abstract": "We study multi-agent reinforcement learning (MARL) in a stochastic network of agents. The objective is to find localized policies that maximize the (discounted) global reward. In general, scalability is a challenge in this setting because the size of the global state/action space can be exponential in the number of agents. Scalable algorithms are only known in cases where dependencies are static, fixed and local, e.g., between neighbors in a fixed, time-invariant underlying graph. In this work, we propose a Scalable Actor Critic framework that applies in settings where the dependencies can be non-local and stochastic, and provide a finite-time error bound that shows how the convergence rate depends on the speed of information spread in the network.  Additionally, as a byproduct of our analysis, we obtain novel finite-time convergence results for a general stochastic approximation scheme and for temporal difference learning with state aggregation, which apply beyond the setting of MARL in networked systems.",
    "authors": [
      "Lin, Yiheng",
      "Qu, Guannan",
      "Huang, Longbo",
      "Wierman, Adam"
    ]
  },
  {
    "id": "41263b9a46f6f8f22668476661614478",
    "title": "Neural Scene Flow Prior",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/41263b9a46f6f8f22668476661614478-Paper.pdf",
    "abstract": "Before the deep learning revolution, many perception algorithms were based on runtime optimization in conjunction with a strong prior/regularization penalty. A prime example of this in computer vision is optical and scene flow. Supervised learning has largely displaced the need for explicit regularization. Instead, they rely on large amounts of labeled data to capture prior statistics, which are not always readily available for many problems. Although optimization is employed to learn the neural network, at runtime, the weights of this network are frozen. As a result, these learning solutions are domain-specific and do not generalize well to other statistically different scenarios. This paper revisits the scene flow problem that relies predominantly on runtime optimization and strong regularization. A central innovation here is the inclusion of a neural scene flow prior, which utilizes the architecture of neural networks as a new type of implicit regularizer. Unlike learning-based scene flow methods, optimization occurs at runtime, and our approach needs no offline datasets---making it ideal for deployment in new environments such as autonomous driving. We show that an architecture based exclusively on multilayer perceptrons (MLPs) can be used as a scene flow prior.  Our method attains competitive---if not better---results on scene flow benchmarks. Also, our neural prior's implicit and continuous scene flow representation allows us to estimate dense long-term correspondences across a sequence of point clouds. The dense motion information is represented by scene flow fields where points can be propagated through time by integrating motion vectors. We demonstrate such a capability by accumulating a sequence of lidar point clouds.",
    "authors": [
      "Li, Xueqian",
      "Kaesemodel Pontes, Jhony",
      "Lucey, Simon"
    ]
  },
  {
    "id": "412758d043dd247bddea07c7ec558c31",
    "title": "The future is log-Gaussian: ResNets and their infinite-depth-and-width limit at initialization",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/412758d043dd247bddea07c7ec558c31-Paper.pdf",
    "abstract": "Theoretical results show that neural networks can be approximated by Gaussian processes in the infinite-width limit. However, for fully connected networks, it has been previously shown that for any fixed network width, $n$, the Gaussian approximation gets worse as the network depth, $d$, increases. Given that modern networks are deep, this raises the question of how well modern architectures, like ResNets, are captured by the infinite-width limit. To provide a better approximation, we study ReLU ResNets in the infinite-depth-and-width limit, where \\emph{both} depth and width tend to infinity as their ratio, $d/n$, remains constant. In contrast to the Gaussian infinite-width limit, we show theoretically that the network exhibits log-Gaussian behaviour at initialization in the infinite-depth-and-width limit, with parameters depending on the ratio $d/n$. Using Monte Carlo simulations, we demonstrate that even basic properties of standard ResNet architectures are poorly captured by the Gaussian limit, but remarkably well captured by our log-Gaussian limit. Moreover, our analysis reveals that ReLU ResNets at initialization are hypoactivated: fewer than half of the ReLUs are activated. Additionally, we calculate the interlayer correlations, which have the effect of exponentially increasing the variance of the network output. Based on our analysis, we introduce \\emph{Balanced ResNets}, a simple architecture modification, which eliminates hypoactivation and interlayer correlations and is more amenable to theoretical analysis.",
    "authors": [
      "Li, Mufan",
      "Nica, Mihai",
      "Roy, Dan"
    ]
  },
  {
    "id": "4158f6d19559955bae372bb00f6204e4",
    "title": "Grammar-Based Grounded Lexicon Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4158f6d19559955bae372bb00f6204e4-Paper.pdf",
    "abstract": "We present Grammar-Based Grounded Language Learning (G2L2), a lexicalist approach toward learning a compositional and grounded meaning representation of language from grounded data, such as paired images and texts. At the core of G2L2 is a collection of lexicon entries, which map each word to a tuple of a syntactic type and a neuro-symbolic semantic program. For example, the word shiny has a syntactic type of adjective; its neuro-symbolic semantic program has the symbolic form $\\lambda x.\\textit{filter}(x, \\textbf{SHINY})$, where the concept SHINY is associated with a neural network embedding, which will be used to classify shiny objects. Given an input sentence, G2L2 first looks up the lexicon entries associated with each token. It then derives the meaning of the sentence as an executable neuro-symbolic program by composing lexical meanings based on syntax. The recovered meaning programs can be executed on grounded inputs. To facilitate learning in an exponentially-growing compositional space, we introduce a joint parsing and expected execution algorithm, which does local marginalization over derivations to reduce the training time. We evaluate G2L2 on two domains: visual reasoning and language-driven navigation. Results show that G2L2 can generalize from small amounts of data to novel compositions of words.",
    "authors": [
      "Mao, Jiayuan",
      "Shi, Freda",
      "Wu, Jiajun",
      "Levy, Roger",
      "Tenenbaum, Josh"
    ]
  },
  {
    "id": "41a60377ba920919939d83326ebee5a1",
    "title": "Distributed Deep Learning In Open Collaborations",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/41a60377ba920919939d83326ebee5a1-Paper.pdf",
    "abstract": "Modern deep learning applications require increasingly more compute to train state-of-the-art models. To address this demand, large corporations and institutions use dedicated High-Performance Computing clusters, whose construction and maintenance are both environmentally costly and well beyond the budget of most organizations. As a result, some research directions become the exclusive domain of a few large industrial and even fewer academic actors. To alleviate this disparity, smaller groups may pool their computational resources and run collaborative experiments that benefit all participants. This paradigm, known as grid- or volunteer computing, has seen successful applications in numerous scientific areas. However, using this approach for machine learning is difficult due to high latency, asymmetric bandwidth, and several challenges unique to volunteer computing. In this work, we carefully analyze these constraints and propose a novel algorithmic framework designed specifically for collaborative training. We demonstrate the effectiveness of our approach for SwAV and ALBERT pretraining in realistic conditions and achieve performance comparable to traditional setups at a fraction of the cost. Finally, we provide a detailed report of successful collaborative language model pretraining with nearly 50 participants.",
    "authors": [
      "Diskin, Michael",
      "Bukhtiyarov, Alexey",
      "Ryabinin, Max",
      "Saulnier, Lucile",
      "lhoest, quentin",
      "Sinitsin, Anton",
      "Popov, Dmitry",
      "Pyrkin, Dmitry V.",
      "Kashirin, Maxim",
      "Borzunov, Alexander",
      "Villanova del Moral, Albert",
      "Mazur, Denis",
      "Kobelev, Ilia",
      "Jernite, Yacine",
      "Wolf, Thomas",
      "Pekhimenko, Gennady"
    ]
  },
  {
    "id": "41a6fd31aa2e75c3c6d427db3d17ea80",
    "title": "Neural Ensemble Search for Uncertainty Estimation and Dataset Shift",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/41a6fd31aa2e75c3c6d427db3d17ea80-Paper.pdf",
    "abstract": "Ensembles of neural networks achieve superior performance compared to standalone networks in terms of accuracy, uncertainty calibration and robustness to dataset shift. Deep ensembles, a state-of-the-art method for uncertainty estimation, only ensemble random initializations of a fixed architecture. Instead, we propose two methods for automatically constructing ensembles with varying architectures, which implicitly trade-off individual architectures\u2019 strengths against the ensemble\u2019s diversity and exploit architectural variation as a source of diversity. On a variety of classification tasks and modern architecture search spaces, we show that the resulting ensembles outperform deep ensembles not only in terms of accuracy but also uncertainty calibration and robustness to dataset shift. Our further analysis and ablation studies provide evidence of higher ensemble diversity due to architectural variation, resulting in ensembles that can outperform deep ensembles, even when having weaker average base learners. To foster reproducibility, our code is available: https://github.com/automl/nes",
    "authors": [
      "Zaidi, Sheheryar",
      "Zela, Arber",
      "Elsken, Thomas",
      "Holmes, Chris C",
      "Hutter, Frank",
      "Teh, Yee"
    ]
  },
  {
    "id": "41bacf567aefc61b3076c74d8925128f",
    "title": "Finding Bipartite Components in Hypergraphs",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/41bacf567aefc61b3076c74d8925128f-Paper.pdf",
    "abstract": "Hypergraphs are important objects to model ternary or higher-order relations of objects, and have a number of applications in analysing many complex datasets occurring in practice.  In this work we study a new heat diffusion process in hypergraphs, and employ this process to design a polynomial-time algorithm that approximately finds bipartite components in a hypergraph.  We theoretically prove the performance of our proposed algorithm, and compare it against the previous state-of-the-art through extensive experimental analysis on both synthetic and real-world datasets. We find that our new algorithm consistently and significantly outperforms the previous state-of-the-art across a wide range of hypergraphs.",
    "authors": [
      "Macgregor, Peter",
      "Sun, He"
    ]
  },
  {
    "id": "41da609c519d77b29be442f8c1105647",
    "title": "Hit and Lead Discovery with Explorative RL and Fragment-based Molecule Generation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/41da609c519d77b29be442f8c1105647-Paper.pdf",
    "abstract": "Recently, utilizing reinforcement learning (RL) to generate molecules with desired properties has been highlighted as a promising strategy for drug design. Molecular docking program -- a physical simulation that estimates protein-small molecule binding affinity -- can be an ideal reward scoring function for RL, as it is a straightforward proxy of the therapeutic potential. Still, two imminent challenges exist for this task. First, the models often fail to generate chemically realistic and pharmacochemically acceptable molecules. Second, the docking score optimization is a difficult exploration problem that involves many local optima and less smooth surface with respect to molecular structure. To tackle these challenges, we propose a novel RL framework that generates pharmacochemically acceptable molecules with large docking scores. Our method -- Fragment-based generative RL with Explorative Experience replay for Drug design (FREED) -- constrains the generated molecules to a realistic and qualified chemical space and effectively explores the space to find drugs by coupling our fragment-based generation method and a novel error-prioritized experience replay (PER). We also show that our model performs well on both de novo and scaffold-based schemes. Our model produces molecules of higher quality compared to existing methods while achieving state-of-the-art performance on two of three targets in terms of the docking scores of the generated molecules. We further show with ablation studies that our method, predictive error-PER (FREED(PE)), significantly improves the model performance.",
    "authors": [
      "Yang, Soojung",
      "Hwang, Doyeong",
      "Lee, Seul",
      "Ryu, Seongok",
      "Hwang, Sung Ju"
    ]
  },
  {
    "id": "42299f06ee419aa5d9d07798b56779e2",
    "title": "Proxy Convexity: A Unified Framework for the Analysis of Neural Networks Trained by Gradient Descent",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/42299f06ee419aa5d9d07798b56779e2-Paper.pdf",
    "abstract": "Although the optimization objectives for learning neural networks are highly non-convex, gradient-based methods have been wildly successful at learning neural networks in practice. This juxtaposition has led to a number of recent studies on provable guarantees for neural networks trained by gradient descent. Unfortunately, the techniques in these works are often highly specific to the particular setup in each problem, making it difficult to generalize across different settings. To address this drawback in the literature, we propose a unified non-convex optimization framework for the analysis of neural network training. We introduce the notions of proxy convexity and proxy Polyak-Lojasiewicz (PL) inequalities, which are satisfied if the original objective function induces a proxy objective function that is implicitly minimized when using gradient methods. We show that stochastic gradient descent (SGD) on objectives satisfying proxy convexity or the proxy PL inequality leads to efficient guarantees for proxy objective functions. We further show that many existing guarantees for neural networks trained by gradient descent can be unified through proxy convexity and proxy PL inequalities. ",
    "authors": [
      "Frei, Spencer",
      "Gu, Quanquan"
    ]
  },
  {
    "id": "42778ef0b5805a96f9511e20b5611fce",
    "title": "Covariance-Aware Private Mean Estimation Without Private Covariance Estimation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/42778ef0b5805a96f9511e20b5611fce-Paper.pdf",
    "abstract": "We present two sample-efficient differentially private mean estimators for $d$-dimensional (sub)Gaussian distributions with unknown covariance. Informally, given $n \\gtrsim d/\\alpha^2$ samples from such a distribution with mean $\\mu$ and covariance $\\Sigma$, our estimators output $\\tilde\\mu$ such that $\\| \\tilde\\mu - \\mu \\|_{\\Sigma} \\leq \\alpha$, where $\\| \\cdot \\|_{\\Sigma}$ is the \\emph{Mahalanobis distance}. All previous estimators with the same guarantee either require strong a priori bounds on the covariance matrix or require $\\Omega(d^{3/2})$ samples.     Each of our estimators is based on a simple, general approach to designing differentially private mechanisms, but with novel technical steps to make the estimator private and sample-efficient. Our first estimator samples a point with approximately maximum Tukey depth using the exponential mechanism, but restricted to the set of points of large Tukey depth. Proving that this mechanism is private requires a novel analysis. Our second estimator perturbs the empirical mean of the data set with noise calibrated to the empirical covariance. Only the mean is released, however; the covariance is only used internally. Its sample complexity guarantees hold more generally for subgaussian distributions, albeit with a slightly worse dependence on the privacy parameter. For both estimators, careful preprocessing of the data is required to satisfy differential privacy.",
    "authors": [
      "Brown, Gavin",
      "Gaboardi, Marco",
      "Smith, Adam",
      "Ullman, Jonathan",
      "Zakynthinou, Lydia"
    ]
  },
  {
    "id": "427e3427c5f38a41bb9cb26525b22fba",
    "title": "Label consistency in overfitted generalized $k$-means",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/427e3427c5f38a41bb9cb26525b22fba-Paper.pdf",
    "abstract": "We provide theoretical guarantees for label consistency in generalized $k$-means problems, with an emphasis on the overfitted case where the number of clusters used by the algorithm is more than the ground truth. We provide conditions under which the estimated labels are close to a refinement of the true cluster labels. We consider both exact and approximate recovery of the labels. Our results hold for any constant-factor approximation to the $k$-means problem. The results are also model-free and only based on bounds on the maximum or average distance of the data points to the true cluster centers. These centers themselves are loosely defined and can be taken to be any set of points for which the aforementioned distances can be controlled. We show the usefulness of the results with applications to some manifold clustering problems.",
    "authors": [
      "Zhang, Linfan",
      "Amini, Arash"
    ]
  },
  {
    "id": "428fca9bc1921c25c5121f9da7815cde",
    "title": "Open-set Label Noise Can Improve Robustness Against Inherent Label Noise",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/428fca9bc1921c25c5121f9da7815cde-Paper.pdf",
    "abstract": "Learning with noisy labels is a practically challenging problem in weakly supervised learning. In the existing literature, open-set noises are always considered to be poisonous for generalization, similar to closed-set noises. In this paper, we empirically show that open-set noisy labels can be non-toxic and even benefit the robustness against inherent noisy labels. Inspired by the observations, we propose a simple yet effective regularization by introducing Open-set samples with Dynamic Noisy Labels (ODNL) into training. With ODNL, the extra capacity of the neural network can be largely consumed in a way that does not interfere with learning patterns from clean data. Through the lens of SGD noise, we show that the noises induced by our method are random-direction, conflict-free and biased, which may help the model converge to a flat minimum with superior stability and enforce the model to produce conservative predictions on Out-of-Distribution instances. Extensive experimental results on benchmark datasets with various types of noisy labels demonstrate that the proposed method not only enhances the performance of many existing robust algorithms but also achieves significant improvement on Out-of-Distribution detection tasks even in the label noise setting.",
    "authors": [
      "Wei, Hongxin",
      "Tao, Lue",
      "XIE, RENCHUNZI",
      "An, Bo"
    ]
  },
  {
    "id": "42a6845a557bef704ad8ac9cb4461d43",
    "title": "The Complexity of Sparse Tensor PCA",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/42a6845a557bef704ad8ac9cb4461d43-Paper.pdf",
    "abstract": "We study the problem of sparse tensor principal component analysis: given a tensor $\\pmb Y = \\pmb W + \\lambda x^{\\otimes p}$ with $\\pmb W \\in \\otimes^p \\mathbb{R}^n$ having i.i.d. Gaussian entries, the goal is to recover the $k$-sparse unit vector $x \\in \\mathbb{R}^n$. The model captures both sparse PCA (in its Wigner form)  and tensor PCA.For the highly sparse regime of $k \\leq \\sqrt{n}$, we present a family of algorithms that smoothly interpolates between a simple polynomial-time algorithm and the exponential-time exhaustive search algorithm. For any $1 \\leq t \\leq k$, our algorithms recovers the sparse vector for signal-to-noise ratio $\\lambda \\geq \\tilde{\\mathcal{O}} (\\sqrt{t} \\cdot (k/t)^{p/2})$ in time $\\tilde{\\mathcal{O}}(n^{p+t})$, capturing the state-of-the-art guarantees for the matrix settings (in both the polynomial-time and sub-exponential time regimes).Our results naturally extend to the case of $r$ distinct $k$-sparse signals with disjoint supports, with guarantees that are independent of the number of spikes. Even in the restricted case of sparse PCA, known algorithms only recover the sparse vectors for $\\lambda \\geq \\tilde{\\mathcal{O}}(k \\cdot r)$ while our algorithms require $\\lambda \\geq \\tilde{\\mathcal{O}}(k)$.Finally, by analyzing the low-degree likelihood ratio, we complement these algorithmic results with rigorous evidence illustrating the trade-offs between signal-to-noise ratio and running time. This lower bound captures the known lower bounds for both sparse  PCA and tensor PCA. In this general model, we observe a more intricate three-way trade-off between the number of samples $n$, the sparsity $k$, and the tensor power $p$.",
    "authors": [
      "Choo, Davin",
      "d'Orsi, Tommaso"
    ]
  },
  {
    "id": "42d6c7d61481d1c21bd1635f59edae05",
    "title": "Learning to Elect",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/42d6c7d61481d1c21bd1635f59edae05-Paper.pdf",
    "abstract": "Voting systems have a wide range of applications including recommender systems, web search, product design and elections. Limited by the lack of general-purpose analytical tools, it is difficult to hand-engineer desirable voting rules for each use case. For this reason, it is appealing to automatically discover voting rules geared towards each scenario. In this paper, we show that set-input neural network architectures such as Set Transformers, fully-connected graph networks and DeepSets are both theoretically and empirically well-suited for learning voting rules. In particular, we show that these network models can not only mimic a number of existing voting rules to compelling accuracy --- both position-based (such as Plurality and Borda) and comparison-based (such as Kemeny, Copeland and Maximin) --- but also discover near-optimal voting rules that maximize different social welfare functions. Furthermore, the learned voting rules generalize well to different voter utility distributions and election sizes unseen during training.",
    "authors": [
      "Anil, Cem",
      "Bao, Xuchan"
    ]
  },
  {
    "id": "433a6ea5429d6d75f0be9bf9da26e24c",
    "title": "KALE Flow: A Relaxed KL Gradient Flow for Probabilities with Disjoint Support",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/433a6ea5429d6d75f0be9bf9da26e24c-Paper.pdf",
    "abstract": "We study the gradient flow for a relaxed approximation to the Kullback-Leibler (KL) divergencebetween a moving source and a fixed target distribution.This approximation, termed theKALE (KL approximate lower-bound estimator), solves a regularized version ofthe Fenchel dual problem defining the KL over a restricted class of functions.When using a Reproducing Kernel Hilbert Space (RKHS) to define the functionclass, we show that the KALE continuously interpolates between the KL and theMaximum Mean Discrepancy (MMD). Like the MMD and other Integral ProbabilityMetrics, the KALE remains well defined for mutually singulardistributions. Nonetheless, the KALE inherits from the limiting KL a greater sensitivity to mismatch in the support of the distributions, compared with the MMD. These two properties make theKALE gradient flow particularly well suited when the target distribution is supported on a low-dimensional manifold. Under an assumption of sufficient smoothness of the trajectories, we show the global convergence of the KALE flow. We propose a particle implementation of the flow given initial samples from the source and the target distribution, which we use to empirically confirm the KALE's properties.",
    "authors": [
      "Glaser, Pierre",
      "Arbel, Michael",
      "Gretton, Arthur"
    ]
  },
  {
    "id": "437d46a857214c997956eaf0e3b21a55",
    "title": "When Is Generalizable Reinforcement Learning Tractable?",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/437d46a857214c997956eaf0e3b21a55-Paper.pdf",
    "abstract": "Agents trained by reinforcement learning (RL) often fail to generalize beyond the environment they were trained in, even when presented with new scenarios that seem similar to the training environment. We study the query complexity required to train RL agents that generalize to multiple environments. Intuitively, tractable generalization is only possible when the environments are similar or close in some sense. To capture this, we introduce Weak Proximity, a natural structural condition that requires the environments to have highly similar transition and reward functions and share a policy providing optimal value. Despite such shared structure, we prove that tractable generalization is impossible in the worst case. This holds even when each individual environment can be efficiently solved to obtain an optimal linear policy, and when the agent possesses a generative model. Our lower bound applies to the more complex task of representation learning for efficient generalization to multiple environments. On the positive side, we introduce Strong Proximity, a strengthened condition which we prove is sufficient for efficient generalization.",
    "authors": [
      "Malik, Dhruv",
      "Li, Yuanzhi",
      "Ravikumar, Pradeep"
    ]
  },
  {
    "id": "4392e631da381761421d5e1e0c3de25f",
    "title": "Relational Self-Attention: What's Missing in Attention for Video Understanding",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4392e631da381761421d5e1e0c3de25f-Paper.pdf",
    "abstract": "Convolution has been arguably the most important feature transform for modern neural networks, leading to the advance of deep learning.  Recent emergence of Transformer networks, which replace convolution layers with self-attention blocks,  has revealed the limitation of stationary convolution kernels and opened the door to the era of dynamic feature transforms. The existing dynamic transforms, including self-attention, however, are all limited for video understanding where correspondence relations in space and time, i.e., motion information, are crucial for effective representation. In this work, we introduce a relational feature transform, dubbed the relational self-attention (RSA), that leverages rich structures of spatio-temporal relations in videos by dynamically generating relational kernels and aggregating relational contexts. Our experiments and ablation studies show that the RSA network substantially outperforms convolution and self-attention counterparts, achieving the state of the art on the standard motion-centric benchmarks for video action recognition, such as Something-Something-V1&V2, Diving48, and FineGym. ",
    "authors": [
      "Kim, Manjin",
      "Kwon, Heeseung",
      "WANG, CHUNYU",
      "Kwak, Suha",
      "Cho, Minsu"
    ]
  },
  {
    "id": "43baa6762fa81bb43b39c62553b2970d",
    "title": "Towards Enabling Meta-Learning from Target Models",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/43baa6762fa81bb43b39c62553b2970d-Paper.pdf",
    "abstract": "Meta-learning can extract an inductive bias from previous learning experience and assist the training of new tasks. It is often realized through optimizing a meta-model with the evaluation loss of task-specific solvers. Most existing algorithms sample non-overlapping $\\mathit{support}$ sets and $\\mathit{query}$ sets to train and evaluate the solvers respectively due to simplicity ($\\mathcal{S}$/$\\mathcal{Q}$ protocol). Different from $\\mathcal{S}$/$\\mathcal{Q}$ protocol, we can also evaluate a task-specific solver by comparing it to a target model $\\mathcal{T}$, which is the optimal model for this task or a model that behaves well enough on this task ($\\mathcal{S}$/$\\mathcal{T}$ protocol). Although being short of research, $\\mathcal{S}$/$\\mathcal{T}$ protocol has unique advantages such as offering more informative supervision, but it is computationally expensive. This paper looks into this special evaluation method and takes a step towards putting it into practice. We find that with a small ratio of tasks armed with target models, classic meta-learning algorithms can be improved a lot without consuming many resources. We empirically verify the effectiveness of $\\mathcal{S}$/$\\mathcal{T}$ protocol in a typical application of meta-learning, $\\mathit{i.e.}$, few-shot learning. In detail, after constructing target models by fine-tuning the pre-trained network on those hard tasks, we match the task-specific solvers and target models via knowledge distillation.",
    "authors": [
      "Lu, Su",
      "Ye, Han-Jia",
      "Gan, Le",
      "Zhan, De-Chuan"
    ]
  },
  {
    "id": "43c656628a4a479e108ed86f7a28a010",
    "title": "A Near-Optimal Algorithm for Debiasing Trained Machine Learning Models",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/43c656628a4a479e108ed86f7a28a010-Paper.pdf",
    "abstract": "We present a scalable post-processing algorithm for debiasing trained models, including deep neural networks (DNNs), which we prove to be near-optimal by bounding its excess Bayes risk.  We empirically validate its advantages on standard benchmark datasets across both classical algorithms as well as modern DNN architectures and demonstrate that it outperforms previous post-processing methods while performing on par with in-processing. In addition, we show that the proposed algorithm is particularly effective for models trained at scale where post-processing is a natural and practical choice.",
    "authors": [
      "Alabdulmohsin, Ibrahim M.",
      "Lucic, Mario"
    ]
  },
  {
    "id": "43ec517d68b6edd3015b3edc9a11367b",
    "title": "GENESIS-V2: Inferring Unordered Object Representations without Iterative Refinement",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/43ec517d68b6edd3015b3edc9a11367b-Paper.pdf",
    "abstract": "Advances in unsupervised learning of object-representations have culminated in the development of a broad range of methods for unsupervised object segmentation and interpretable object-centric scene generation. These methods, however, are limited to simulated and real-world datasets with limited visual complexity. Moreover, object representations are often inferred using RNNs which do not scale well to large images or iterative refinement which avoids imposing an unnatural ordering on objects in an image but requires the a priori initialisation of a fixed number of object representations. In contrast to established paradigms, this work proposes an embedding-based approach in which embeddings of pixels are clustered in a differentiable fashion using a stochastic stick-breaking process. Similar to iterative refinement, this clustering procedure also leads to randomly ordered object representations, but without the need of initialising a fixed number of clusters a priori. This is used to develop a new model, GENESIS-v2, which can infer a variable number of object representations without using RNNs or iterative refinement. We show that GENESIS-v2 performs strongly in comparison to recent baselines in terms of unsupervised image segmentation and object-centric scene generation on established synthetic datasets as well as more complex real-world datasets.",
    "authors": [
      "Engelcke, Martin",
      "Parker Jones, Oiwi",
      "Posner, Ingmar"
    ]
  },
  {
    "id": "442b548e816f05640dec68f497ca38ac",
    "title": "How Data Augmentation affects Optimization for Linear Regression",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/442b548e816f05640dec68f497ca38ac-Paper.pdf",
    "abstract": "Though data augmentation has rapidly emerged as a key tool for optimization in modern machine learning, a clear picture of how augmentation schedules affect optimization and interact with optimization hyperparameters such as learning rate is nascent. In the spirit of classical convex optimization and recent work on implicit bias, the present work analyzes the effect of augmentation on optimization in the simple convex setting of linear regression with MSE loss.We find joint schedules for learning rate and data augmentation scheme under which augmented gradient descent provably converges and characterize the resulting minimum. Our results apply to arbitrary augmentation schemes, revealing complex interactions between learning rates and augmentations even in the convex setting. Our approach interprets augmented (S)GD as a stochastic optimization method for a time-varying sequence of proxy losses. This gives a unified way to analyze learning rate, batch size, and augmentations ranging from additive noise to random projections. From this perspective, our results, which also give rates of convergence, can be viewed as Monro-Robbins type conditions for augmented (S)GD. ",
    "authors": [
      "Hanin, Boris",
      "Sun, Yi"
    ]
  },
  {
    "id": "445e24b5f22cacb9d51a837c10e91a3f",
    "title": "An Exact Characterization of the Generalization Error for the Gibbs Algorithm",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/445e24b5f22cacb9d51a837c10e91a3f-Paper.pdf",
    "abstract": "Various approaches have been developed to upper bound the generalization error of a supervised learning algorithm. However, existing bounds are often loose and lack of guarantees. As a result, they may fail to characterize the exact generalization ability of a learning algorithm.Our main contribution is an exact characterization of the expected generalization error of the well-known Gibbs algorithm (a.k.a. Gibbs posterior) using symmetrized KL information between the input training samples and the output hypothesis. Our result can be applied to tighten existing expected generalization error and PAC-Bayesian bounds. Our approach is versatile, as it also characterizes the generalization error of the Gibbs algorithm with data-dependent regularizer and that of the Gibbs algorithm in the asymptotic regime, where it converges to the empirical risk minimization algorithm. Of particular relevance, our results highlight the role the symmetrized KL information plays in controlling the generalization error of the Gibbs algorithm.",
    "authors": [
      "Aminian, Gholamali",
      "Bu, Yuheng",
      "Toni, Laura",
      "Rodrigues, Miguel",
      "Wornell, Gregory"
    ]
  },
  {
    "id": "4476b929e30dd0c4e8bdbcc82c6ba23a",
    "title": "Subgaussian and Differentiable Importance Sampling for Off-Policy Evaluation and Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4476b929e30dd0c4e8bdbcc82c6ba23a-Paper.pdf",
    "abstract": "Importance Sampling (IS) is a widely used building block for a large variety of off-policy estimation and learning algorithms. However, empirical and theoretical studies have progressively shown that vanilla IS leads to poor estimations whenever the behavioral and target policies are too dissimilar. In this paper, we analyze the theoretical properties of the IS estimator by deriving a novel anticoncentration bound that formalizes the intuition behind its undesired behavior. Then, we propose a new class of IS transformations, based on the notion of power mean. To the best of our knowledge, the resulting estimator is the first to achieve, under certain conditions, two key properties: (i) it displays a subgaussian concentration rate; (ii) it preserves the differentiability in the target distribution. Finally, we provide numerical simulations on both synthetic examples and contextual bandits, in comparison with off-policy evaluation and learning baselines.",
    "authors": [
      "Metelli, Alberto Maria",
      "Russo, Alessio",
      "Restelli, Marcello"
    ]
  },
  {
    "id": "447b0408b80078338810051bb38b177f",
    "title": "Rethinking gradient sparsification as total error minimization",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/447b0408b80078338810051bb38b177f-Paper.pdf",
    "abstract": "Gradient compression is a widely-established remedy to tackle the communication bottleneck in distributed training of large deep neural networks (DNNs). Under the error-feedback framework, Top-$k$ sparsification, sometimes with $k$ as little as 0.1% of the gradient size, enables training to the same model quality as the uncompressed case for a similar iteration count. From the optimization perspective, we find that Top-$k$ is the communication-optimal sparsifier given a per-iteration $k$ element budget.We argue that to further the benefits of gradient sparsification, especially for DNNs, a different perspective is necessary \u2014 one that moves from per-iteration optimality to consider optimality for the entire training.We identify that the total error \u2014 the sum of the compression errors for all iterations \u2014 encapsulates sparsification throughout training. Then, we propose a communication complexity model that minimizes the total error under a communication budget for the entire training. We find that the hard-threshold sparsifier, a variant of the Top-$k$ sparsifier with $k$ determined by a constant hard-threshold, is the optimal sparsifier for this model. Motivated by this, we provide convex and non-convex convergence analyses for the hard-threshold sparsifier with error-feedback. We show that hard-threshold has the same asymptotic convergence and linear speedup property as SGD in both the case, and unlike with Top-$k$ sparsifier, has no impact due to data-heterogeneity. Our diverse experiments on various DNNs and a logistic regression model demonstrate that the hard-threshold sparsifier is more communication-efficient than Top-$k$.",
    "authors": [
      "Sahu, Atal",
      "Dutta, Aritra",
      "M. Abdelmoniem, Ahmed",
      "Banerjee, Trambak",
      "Canini, Marco",
      "Kalnis, Panos"
    ]
  },
  {
    "id": "44b422a6d1df1d47db5d50a8d0aaca5d",
    "title": "Approximate optimization of convex functions with outlier noise",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/44b422a6d1df1d47db5d50a8d0aaca5d-Paper.pdf",
    "abstract": "We study the problem of minimizing a convex function given by a zeroth order oracle that is possibly corrupted by {\\em outlier noise}. Specifically, we assume the function values at some points of the domain are corrupted arbitrarily by an adversary, with the only restriction being that the total volume of corrupted points is bounded. The goal then is to find a point close to the function's minimizer using access to the corrupted oracle.We first prove a lower bound result showing that, somewhat surprisingly, one cannot hope to approximate the minimizer {\\em nearly as well} as one might expect, even if one is allowed {\\em an unbounded number} of queries to the oracle. Complementing this negative result, we then develop an efficient algorithm that outputs a point close to the minimizer of the convex function, where the specific distance matches {\\em exactly}, up to constant factors, the distance bound shown in our lower bound result.",
    "authors": [
      "De, Anindya",
      "Khanna, Sanjeev",
      "Li, Huan",
      "NikpeySalekde, MohammadHesam"
    ]
  },
  {
    "id": "44e207aecc63505eb828d442de03f2e9",
    "title": "Fair Classification with Adversarial Perturbations",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/44e207aecc63505eb828d442de03f2e9-Paper.pdf",
    "abstract": "We study fair classification in the presence of an omniscient adversary that, given an $\\eta$, is allowed to choose an arbitrary $\\eta$-fraction of the training samples and arbitrarily perturb their protected attributes. The motivation comes from settings in which protected attributes can be incorrect due to strategic misreporting, malicious actors, or errors in imputation; and prior approaches that make stochastic or independence assumptions on errors may not satisfy their guarantees in this adversarial setting. Our main contribution is an optimization framework to learn fair classifiers in this adversarial setting that comes with provable guarantees on accuracy and fairness. Our framework works with multiple and non-binary protected attributes, is designed for the large class of linear-fractional fairness metrics, and can also handle perturbations besides protected attributes. We prove near-tightness of our framework's guarantees for natural hypothesis classes: no algorithm can have significantly better accuracy and any algorithm with better fairness must have lower accuracy. Empirically, we evaluate the classifiers produced by our framework for statistical rate on real-world and synthetic datasets for a family of adversaries.",
    "authors": [
      "Celis, L. Elisa",
      "Mehrotra, Anay",
      "Vishnoi, Nisheeth"
    ]
  },
  {
    "id": "44e65d3e9bc2f88b2b3d566de51a5381",
    "title": "Distributed Saddle-Point Problems Under Data Similarity",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/44e65d3e9bc2f88b2b3d566de51a5381-Paper.pdf",
    "abstract": "We study solution methods for (strongly-)convex-(strongly)-concave Saddle-Point Problems (SPPs) over networks of two type--master/workers (thus centralized) architectures  and  mesh (thus decentralized) networks. The local functions at each node are assumed to be \\textit{similar}, due to statistical data similarity or otherwise. We establish lower complexity bounds for a fairly general  class of algorithms solving the SPP. We show that   a given suboptimality $\\epsilon>0$ is achieved over master/workers networks in $\\Omega\\big(\\Delta\\cdot  \\delta/\\mu\\cdot \\log (1/\\varepsilon)\\big)$ rounds of communications, where $\\delta>0$ measures the degree of similarity of the local functions, $\\mu$ is their strong convexity constant, and $\\Delta$ is the diameter of the network. The lower communication complexity bound over mesh networks reads     $\\Omega\\big(1/{\\sqrt{\\rho}} \\cdot  {\\delta}/{\\mu}\\cdot\\log (1/\\varepsilon)\\big)$, where $\\rho$ is the (normalized) eigengap of the gossip matrix used for the communication between neighbouring nodes.  We then propose algorithms matching the lower bounds over either  types of networks (up to log-factors). We assess the effectiveness of the proposed algorithms on a robust regression  problem.",
    "authors": [
      "Beznosikov, Aleksandr",
      "Scutari, Gesualdo",
      "Rogozin, Alexander",
      "Gasnikov, Alexander"
    ]
  },
  {
    "id": "44e76e99b5e194377e955b13fb12f630",
    "title": "Combining Latent Space and Structured Kernels for Bayesian Optimization over Combinatorial Spaces",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/44e76e99b5e194377e955b13fb12f630-Paper.pdf",
    "abstract": "We consider the problem of optimizing combinatorial spaces (e.g., sequences, trees, and graphs) using expensive black-box function evaluations. For example, optimizing molecules for drug design using physical lab experiments. Bayesian optimization (BO) is an efficient framework for solving such problems by intelligently selecting the inputs with high utility guided by a learned surrogate model. A recent BO approach for combinatorial spaces is through a reduction to BO over continuous spaces by learning a latent representation of structures using deep generative models (DGMs). The selected input from the continuous space is decoded into a discrete structure for performing function evaluation. However, the surrogate model over the latent space only uses the information learned by the DGM, which may not have the desired inductive bias to approximate the target black-box function. To overcome this drawback, this paper proposes a principled approach referred as LADDER. The key idea is to define a novel structure-coupled kernel that explicitly integrates the structural information from decoded structures with the learned latent space representation for better surrogate modeling. Our experiments on real-world benchmarks show that LADDER significantly improves over the BO over latent space method, and performs better or similar to state-of-the-art methods.",
    "authors": [
      "Deshwal, Aryan",
      "Doppa, Jana"
    ]
  },
  {
    "id": "45017f6511f91be700fda3d118034994",
    "title": "Gradual Domain Adaptation without Indexed Intermediate Domains",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/45017f6511f91be700fda3d118034994-Paper.pdf",
    "abstract": "The effectiveness of unsupervised domain adaptation degrades when there is a large discrepancy between the source and target domains. Gradual domain adaption (GDA) is one promising way to mitigate such an issue, by leveraging additional unlabeled data that gradually shift from the source to the target. Through sequentially adapting the model along the \"indexed\" intermediate domains, GDA substantially improves the overall adaptation performance. In practice, however, the extra unlabeled data may not be separated into intermediate domains and indexed properly, limiting the applicability of GDA. In this paper, we investigate how to discover the sequence of intermediate domains when it is not already available. Concretely, we propose a coarse-to-fine framework, which starts with a coarse domain discovery step via progressive domain discriminator training. This coarse domain sequence then undergoes a fine indexing step via a novel cycle-consistency loss, which encourages the next intermediate domain to preserve sufficient discriminative knowledge of the current intermediate domain. The resulting domain sequence can then be used by a GDA algorithm. On benchmark data sets of GDA, we show that our approach, which we name Intermediate DOmain Labeler (IDOL), can lead to comparable or even better adaptation performance compared to the pre-defined domain sequence, making GDA more applicable and robust to the quality of domain sequences. Codes are available at https://github.com/hongyouc/IDOL.",
    "authors": [
      "Chen, Hong-You",
      "Chao, Wei-Lun"
    ]
  },
  {
    "id": "4547dff5fd7604f18c8ee32cf3da41d7",
    "title": "K-level Reasoning for Zero-Shot Coordination in Hanabi",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4547dff5fd7604f18c8ee32cf3da41d7-Paper.pdf",
    "abstract": "Requests for name changes in the electronic proceedings will be accepted with no questions asked.  However name changes may cause bibliographic tracking issues.  Authors are asked to consider this carefully and discuss it with their co-authors prior to requesting a name change in the electronic proceedings.",
    "authors": [
      "Cui, Brandon",
      "Hu, Hengyuan",
      "Pineda, Luis",
      "Foerster, Jakob"
    ]
  },
  {
    "id": "454cecc4829279e64d624cd8a8c9ddf1",
    "title": "Learning Markov State Abstractions for Deep Reinforcement Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/454cecc4829279e64d624cd8a8c9ddf1-Paper.pdf",
    "abstract": "A fundamental assumption of reinforcement learning in Markov decision processes (MDPs) is that the relevant decision process is, in fact, Markov. However, when MDPs have rich observations, agents typically learn by way of an abstract state representation, and such representations are not guaranteed to preserve the Markov property. We introduce a novel set of conditions and prove that they are sufficient for learning a Markov abstract state representation. We then describe a practical training procedure that combines inverse model estimation and temporal contrastive learning to learn an abstraction that approximately satisfies these conditions. Our novel training objective is compatible with both online and offline training: it does not require a reward signal, but agents can capitalize on reward information when available. We empirically evaluate our approach on a visual gridworld domain and a set of continuous control benchmarks. Our approach learns representations that capture the underlying structure of the domain and lead to improved sample efficiency over state-of-the-art deep reinforcement learning with visual features---often matching or exceeding the performance achieved with hand-designed compact state information.",
    "authors": [
      "Allen, Cameron",
      "Parikh, Neev",
      "Gottesman, Omer",
      "Konidaris, George"
    ]
  },
  {
    "id": "4588e674d3f0faf985047d4c3f13ed0d",
    "title": "Towards Deeper Deep Reinforcement Learning with Spectral Normalization",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4588e674d3f0faf985047d4c3f13ed0d-Paper.pdf",
    "abstract": "In computer vision and natural language processing, innovations in model architecture that increase model capacity have reliably translated into gains in performance. In stark contrast with this trend, state-of-the-art reinforcement learning (RL) algorithms often use small MLPs, and gains in performance typically originate from algorithmic innovations. It is natural to hypothesize that small datasets in RL necessitate simple models to avoid overfitting; however, this hypothesis is untested. In this paper we investigate how RL agents are affected by exchanging the small MLPs with larger modern networks with skip connections and normalization, focusing specifically on actor-critic algorithms. We empirically verify that naively adopting such architectures leads to instabilities and poor performance, likely contributing to the popularity of simple models in practice. However, we show that dataset size is not the limiting factor, and instead argue that instability from taking gradients through the critic is the culprit. We demonstrate that spectral normalization (SN) can mitigate this issue and enable stable training with large modern architectures. After smoothing with SN, larger models yield significant performance improvements --- suggesting that more ``easy'' gains may be had by focusing on model architectures in addition to algorithmic innovations. ",
    "authors": [
      "Bjorck, Nils",
      "Gomes, Carla P.",
      "Weinberger, Kilian Q."
    ]
  },
  {
    "id": "459a4ddcb586f24efd9395aa7662bc7c",
    "title": "Functionally Regionalized Knowledge Transfer for Low-resource Drug Discovery",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/459a4ddcb586f24efd9395aa7662bc7c-Paper.pdf",
    "abstract": "More recently, there has been a surge of interest in employing machine learning approaches to expedite the drug discovery process where virtual screening for hit discovery and ADMET prediction for lead optimization play essential roles. One of the main obstacles to the wide success of machine learning approaches in these two tasks is that the number of compounds labeled with activities or ADMET properties is too small to build an effective predictive model. This paper seeks to remedy the problem by transferring the knowledge from previous assays, namely in-vivo experiments, by different laboratories and against various target proteins. To accommodate these wildly different assays and capture the similarity between assays, we propose a functional rationalized meta-learning algorithm FRML for such knowledge transfer. FRML constructs the predictive model with layers of neural sub-networks or so-called functional regions. Building on this, FRML shares an initialization for the weights of the predictive model across all assays, while customizes it to each assay with a region localization network choosing the pertinent regions. The compositionality of the model improves the capacity of generalization to various and even out-of-distribution tasks. Empirical results on both virtual screening and ADMET prediction validate the superiority of FRML over state-of-the-art baselines powered with interpretability in assay relationship.",
    "authors": [
      "Yao, Huaxiu",
      "Wei, Ying",
      "Huang, Long-Kai",
      "Xue, Ding",
      "Huang, Junzhou",
      "Li, Zhenhui (Jessie)"
    ]
  },
  {
    "id": "45c166d697d65080d54501403b433256",
    "title": "Memory-Efficient Approximation Algorithms for Max-k-Cut and Correlation Clustering",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/45c166d697d65080d54501403b433256-Paper.pdf",
    "abstract": "Max-k-Cut and correlation clustering are fundamental graph partitioning problems. For a graph $G=(V,E)$ with $n$ vertices, the methods with the best approximation guarantees for Max-k-Cut and the Max-Agree variant of correlation clustering involve solving SDPs with $\\mathcal{O}(n^2)$ constraints and variables. Large-scale instances of SDPs, thus, present a memory bottleneck. In this paper, we develop simple polynomial-time Gaussian sampling-based algorithms for these two problems that use $\\mathcal{O}(n+|E|)$ memory and nearly achieve the best existing approximation guarantees. For dense graphs arriving in a stream, we eliminate the dependence on $|E|$ in the storage complexity at the cost of a slightly worse approximation ratio by combining our approach with sparsification.",
    "authors": [
      "Shinde, Nimita",
      "Narayanan, Vishnu",
      "Saunderson, James"
    ]
  },
  {
    "id": "46031b3d04dc90994ca317a7c55c4289",
    "title": "Panoptic 3D Scene Reconstruction From a Single RGB Image",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/46031b3d04dc90994ca317a7c55c4289-Paper.pdf",
    "abstract": "Richly segmented 3D scene reconstructions are an integral basis for many high-level scene understanding tasks, such as for robotics, motion planning, or augmented reality. Existing works in 3D perception from a single RGB image tend to focus on geometric reconstruction only, or geometric reconstruction with semantic  segmentation or instance segmentation.Inspired by 2D panoptic segmentation, we propose to unify the tasks of geometric reconstruction, 3D semantic segmentation, and 3D instance segmentation into the task of panoptic 3D scene reconstruction -- from a single RGB image, predicting the complete geometric reconstruction of the scene in the camera frustum of the image, along with semantic and instance segmentations.We propose a new approach for holistic 3D scene understanding from a single RGB image which learns to lift and propagate 2D features from an input image to a 3D volumetric scene representation.Our panoptic 3D reconstruction metric evaluates both geometric reconstruction quality as well as panoptic segmentation.Our experiments demonstrate that our approach for panoptic 3D scene reconstruction outperforms alternative approaches for this task.",
    "authors": [
      "Dahnert, Manuel",
      "Hou, Ji",
      "Niessner, Matthias",
      "Dai, Angela"
    ]
  },
  {
    "id": "4607f7fff0dce694258e1c637512aa9d",
    "title": "Measuring Generalization with Optimal Transport",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4607f7fff0dce694258e1c637512aa9d-Paper.pdf",
    "abstract": "Understanding the generalization of deep neural networks is one of the most important tasks in deep learning. Although much progress has been made, theoretical error bounds still often behave disparately from empirical observations. In this work, we develop margin-based generalization bounds, where the margins are normalized with optimal transport costs between independent random subsets sampled from the training distribution. In particular, the optimal transport cost can be interpreted as a generalization of variance which captures the structural properties of the learned feature space. Our bounds robustly predict the generalization error, given training data and network parameters, on large scale datasets. Theoretically, we demonstrate that the concentration and separation of features play crucial roles in generalization, supporting empirical results in the literature.",
    "authors": [
      "Chuang, Ching-Yao",
      "Mroueh, Youssef",
      "Greenewald, Kristjan",
      "Torralba, Antonio",
      "Jegelka, Stefanie"
    ]
  },
  {
    "id": "460b491b917d4185ed1f5be97229721a",
    "title": "Uniform Concentration Bounds toward a Unified  Framework for Robust Clustering",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/460b491b917d4185ed1f5be97229721a-Paper.pdf",
    "abstract": "Recent advances in center-based clustering continue to improve upon the drawbacks of Lloyd's celebrated $k$-means algorithm over $60$ years after its introduction. Various methods seek to address poor local minima, sensitivity to outliers, and data that are not well-suited to Euclidean measures of fit, but many are supported largely empirically. Moreover, combining such approaches in a piecemeal manner can result in ad hoc methods, and the limited theoretical results supporting each individual contribution may no longer hold. Toward addressing these issues in a principled way, this paper proposes a cohesive robust framework for center-based clustering under a general class of dissimilarity measures. In particular, we present a rigorous theoretical treatment within a Median-of-Means (MoM) estimation framework, showing that it subsumes several popular $k$-means variants. In addition to unifying existing methods, we derive uniform concentration bounds that complete their analyses, and bridge these results to the MoM framework via Dudley's chaining arguments. Importantly, we neither require any assumptions on the distribution of the outlying observations nor on the relative number of observations $n$ to features $p$. We establish strong consistency and an error rate of $O(n^{-1/2})$ under mild conditions, surpassing the best-known results in the literature. The methods are empirically validated thoroughly on real and synthetic datasets. ",
    "authors": [
      "Paul, Debolina",
      "Chakraborty, Saptarshi",
      "Das, Swagatam",
      "Xu, Jason"
    ]
  },
  {
    "id": "4639475d6782a08c1e964f9a4329a254",
    "title": "Learning Signal-Agnostic Manifolds of Neural Fields",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4639475d6782a08c1e964f9a4329a254-Paper.pdf",
    "abstract": "Deep neural networks have been used widely to learn the latent structure of datasets, across modalities such as images, shapes, and audio signals. However, existing models are generally modality-dependent, requiring custom architectures and objectives to process different classes of signals. We leverage neural fields to capture the underlying structure in image, shape, audio and cross-modal audiovisual domains in a modality-independent manner. We cast our task as one of learning a manifold, where we aim to infer a low-dimensional, locally linear subspace in which our data resides. By enforcing coverage of the manifold, local linearity, and local isometry, our model -- dubbed GEM -- learns to capture the underlying structure of datasets across modalities. We can then travel along linear regions of our manifold to obtain perceptually consistent interpolations between samples, and can further use GEM to recover points on our manifold and glean not only diverse completions of input images, but cross-modal hallucinations of audio or image signals.  Finally, we show that by walking across the underlying manifold of GEM, we may generate new samples in our signal domains.",
    "authors": [
      "Du, Yilun",
      "Collins, Katie",
      "Tenenbaum, Josh",
      "Sitzmann, Vincent"
    ]
  },
  {
    "id": "464074179972cbbd75a39abc6954cd12",
    "title": "Low-dimensional Structure in the Space of Language Representations is Reflected in Brain Responses",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/464074179972cbbd75a39abc6954cd12-Paper.pdf",
    "abstract": "How related are the representations learned by neural language models, translation models, and language tagging tasks? We answer this question by adapting an encoder-decoder transfer learning method from computer vision to investigate the structure among 100 different feature spaces extracted from hidden representations of various networks trained on language tasks.This method reveals a low-dimensional structure where language models and translation models smoothly interpolate between word embeddings, syntactic and semantic tasks, and future word embeddings. We call this low-dimensional structure a language representation embedding because it encodes the relationships between representations needed to process language for a variety of NLP tasks. We find that this representation embedding can predict how well each individual feature space maps to human brain responses to natural language stimuli recorded using fMRI. Additionally, we find that the principal dimension of this structure can be used to create a metric which highlights the brain's natural language processing hierarchy. This suggests that the embedding captures some part of the brain's natural language representation structure.",
    "authors": [
      "Antonello, Richard",
      "Turek, Javier S.",
      "Vo, Vy",
      "Huth, Alexander"
    ]
  },
  {
    "id": "46489c17893dfdcf028883202cefd6d1",
    "title": "On the Suboptimality  of Thompson Sampling in High Dimensions",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/46489c17893dfdcf028883202cefd6d1-Paper.pdf",
    "abstract": "In this paper we consider Thompson Sampling for combinatorial semi-bandits. We demonstrate that, perhaps surprisingly, Thompson Sampling is sub-optimal for this problem in the sense that its regret scales exponentially in the ambient dimension, and its minimax regret scales almost linearly. This phenomenon occurs under a wide variety of assumptions including both non-linear and linear reward functions in the Bernoulli distribution setting. We also show that including a fixed amount of forced exploration to Thompson Sampling does not alleviate the problem. We complement our theoretical results with numerical results and show that in practice Thompson Sampling indeed can perform very poorly in some high dimension situations.",
    "authors": [
      "Zhang, Raymond",
      "Combes, Richard"
    ]
  },
  {
    "id": "465636eb4a7ff4b267f3b765d07a02da",
    "title": "Learning Debiased and Disentangled Representations for Semantic Segmentation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/465636eb4a7ff4b267f3b765d07a02da-Paper.pdf",
    "abstract": "Deep neural networks are susceptible to learn biased models with entangled feature representations, which may lead to subpar performances on various downstream tasks. This is particularly true for under-represented classes, where a lack of diversity in the data exacerbates the tendency. This limitation has been addressed mostly in classification tasks, but there is little study on additional challenges that may appear in more complex dense prediction problems including semantic segmentation. To this end, we propose a model-agnostic and stochastic training scheme for semantic segmentation, which facilitates the learning of debiased and disentangled representations. For each class, we first extract class-specific information from the highly entangled feature map. Then, information related to a randomly sampled class is suppressed by a feature selection process in the feature space. By randomly eliminating certain class information in each training iteration, we effectively reduce feature dependencies among classes, and the model is able to learn more debiased and disentangled feature representations. Models trained with our approach demonstrate strong results on multiple semantic segmentation benchmarks, with especially notable performance gains on under-represented classes.",
    "authors": [
      "Chu, Sanghyeok",
      "Kim, Dongwan",
      "Han, Bohyung"
    ]
  },
  {
    "id": "466473650870501e3600d9a1b4ee5d44",
    "title": "Diversity Matters When Learning From Ensembles",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/466473650870501e3600d9a1b4ee5d44-Paper.pdf",
    "abstract": "Deep ensembles excel in large-scale image classification tasks both in terms of prediction accuracy and calibration. Despite being simple to train, the computation and memory cost of deep ensembles limits their practicability. While some recent works propose to distill an ensemble model into a single model to reduce such costs, there is still a performance gap between the ensemble and distilled models. We propose a simple approach for reducing this gap, i.e., making the distilled performance close to the full ensemble. Our key assumption is that a distilled model should absorb as much function diversity inside the ensemble as possible. We first empirically show that the typical distillation procedure does not effectively transfer such diversity, especially for complex models that achieve near-zero training error. To fix this, we propose a perturbation strategy for distillation that reveals diversity by seeking inputs for which ensemble member outputs disagree. We empirically show that a model distilled with such perturbed samples indeed exhibits enhanced diversity, leading to improved performance.",
    "authors": [
      "Nam, Giung",
      "Yoon, Jongmin",
      "Lee, Yoonho",
      "Lee, Juho"
    ]
  },
  {
    "id": "46c7cb50b373877fb2f8d5c4517bb969",
    "title": "Locally Valid and Discriminative Prediction Intervals for Deep Learning Models",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/46c7cb50b373877fb2f8d5c4517bb969-Paper.pdf",
    "abstract": "Crucial for building trust in deep learning models for critical real-world applications is efficient and theoretically sound uncertainty quantification, a task that continues to be challenging. Useful uncertainty information is expected to have two key properties: It should be valid (guaranteeing coverage) and discriminative (more uncertain when the expected risk is high). Moreover, when combined with deep learning (DL) methods, it should be scalable and affect the DL model performance minimally. Most existing Bayesian methods lack frequentist coverage guarantees and usually affect model performance. The few available frequentist methods are rarely discriminative and/or violate coverage guarantees due to unrealistic assumptions. Moreover, many methods are expensive or require substantial modifications to the base neural network. Building upon recent advances in conformal prediction [13, 33] and leveraging the classical idea of kernel regression, we propose Locally Valid and Discriminative prediction intervals (LVD), a simple, efficient, and lightweight method to construct discriminative prediction intervals (PIs) for almost any DL model. With no assumptions on the data distribution, such PIs also offer finite-sample local coverage guarantees (contrasted to the simpler marginal coverage). We empirically verify, using diverse datasets, that besides being the only locally valid method for DL, LVD also exceeds or matches the performance (including coverage rate and prediction accuracy) of existing uncertainty quantification methods, while offering additional benefits in scalability and flexibility.",
    "authors": [
      "Lin, Zhen",
      "Trivedi, Shubhendu",
      "Sun, Jimeng"
    ]
  },
  {
    "id": "46d0671dd4117ea366031f87f3aa0093",
    "title": "Personalized Federated Learning With Gaussian Processes",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/46d0671dd4117ea366031f87f3aa0093-Paper.pdf",
    "abstract": "Federated learning aims to learn a global model that performs well on client devices with limited cross-client communication. Personalized federated learning (PFL) further extends this setup to handle data heterogeneity between clients by learning personalized models. A key challenge in this setting is to learn effectively across clients even though each client has unique data that is often limited in size. Here we present pFedGP, a solution to PFL that is based on Gaussian processes (GPs) with deep kernel learning. GPs are highly expressive models that work well in the low data regime due to their Bayesian nature.However, applying GPs to PFL raises multiple challenges. Mainly, GPs performance depends heavily on access to a good kernel function, and learning a kernel requires a large training set. Therefore, we propose learning a shared kernel function across all clients, parameterized by a neural network, with a personal GP classifier for each client. We further extend pFedGP to include inducing points using two novel methods, the first helps to improve generalization in the low data regime and the second reduces the computational cost. We derive a PAC-Bayes generalization bound on novel clients and empirically show that it gives non-vacuous guarantees. Extensive experiments on standard PFL benchmarks with CIFAR-10, CIFAR-100, and CINIC-10, and on a new setup of learning under input noise show that pFedGP achieves well-calibrated predictions while significantly outperforming baseline methods, reaching up to 21% in accuracy gain.",
    "authors": [
      "Achituve, Idan",
      "Shamsian, Aviv",
      "Navon, Aviv",
      "Chechik, Gal",
      "Fetaya, Ethan"
    ]
  },
  {
    "id": "46e0eae7d5217c79c3ef6b4c212b8c6f",
    "title": "Risk Bounds for Over-parameterized Maximum Margin Classification on Sub-Gaussian Mixtures",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/46e0eae7d5217c79c3ef6b4c212b8c6f-Paper.pdf",
    "abstract": "Modern machine learning systems such as deep neural networks are often highly over-parameterized so that they can fit the noisy training data exactly, yet they can still achieve small test errors in practice. In this paper, we study this \"benign overfitting\" phenomenon of the maximum margin classifier for linear classification problems. Specifically, we consider data generated from sub-Gaussian mixtures, and provide a tight risk bound for the maximum margin linear classifier in the over-parameterized setting. Our results precisely characterize the condition under which benign overfitting can occur in linear classification problems, and improve on previous work. They also have direct implications for over-parameterized logistic regression. ",
    "authors": [
      "Cao, Yuan",
      "Gu, Quanquan",
      "Belkin, Mikhail"
    ]
  },
  {
    "id": "46fc943ecd56441056a560ba37d0b9e8",
    "title": "Implicit SVD for Graph Representation Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/46fc943ecd56441056a560ba37d0b9e8-Paper.pdf",
    "abstract": "Recent improvements in the performance of state-of-the-art (SOTA) methods for Graph Representational Learning (GRL) have come at the cost of significant computational resource requirements for training, e.g., for calculating gradients via backprop over many data epochs. Meanwhile, Singular Value Decomposition (SVD) can find closed-form solutions to convex problems, using merely a handful of epochs. In this paper, we make GRL more computationally tractable for those with modest hardware. We design a framework that computes SVD of *implicitly* defined matrices, and apply this framework to several GRL tasks. For each task, we derive first-order approximation of a SOTA model, where we design (expensive-to-store) matrix $\\mathbf{M}$ and train the model, in closed-form, via SVD of $\\mathbf{M}$, without calculating entries of $\\mathbf{M}$. By converging to a unique point in one step, and without calculating gradients, our models show competitive empirical test performance over various graphs such as article citation and biological interaction networks. More importantly, SVD can initialize a deeper model, that is architected to be non-linear almost everywhere, though behaves linearly when its parameters reside on a hyperplane, onto which SVD initializes. The deeper model can then be fine-tuned within only a few epochs. Overall, our algorithm trains hundreds of times faster than state-of-the-art methods, while competing on test empirical performance. We open-source our implementation at: https://github.com/samihaija/isvd",
    "authors": [
      "Abu-El-Haija, Sami",
      "Mostafa, Hesham",
      "Nassar, Marcel",
      "Crespi, Valentino",
      "Ver Steeg, Greg",
      "Galstyan, Aram"
    ]
  },
  {
    "id": "470e7a4f017a5476afb7eeb3f8b96f9b",
    "title": "Offline Model-based Adaptable Policy Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/470e7a4f017a5476afb7eeb3f8b96f9b-Paper.pdf",
    "abstract": "In reinforcement learning, a promising direction to avoid online trial-and-error costs is learning from an offline dataset. Current offline reinforcement learning methods commonly learn in the policy space constrained to in-support regions by the offline dataset, in order to ensure the robustness of the outcome policies. Such constraints, however, also limit the potential of the outcome policies. In this paper, to release the potential of offline policy learning, we investigate the decision-making problems in out-of-support regions directly and propose offline Model-based Adaptable Policy LEarning (MAPLE). By this approach, instead of learning in in-support regions, we learn an adaptable policy that can adapt its behavior in out-of-support regions when deployed. We conduct experiments on MuJoCo controlling tasks with offline datasets. The results show that the proposed method can make robust decisions in out-of-support regions and achieve better performance than SOTA algorithms.",
    "authors": [
      "Chen, Xiong-Hui",
      "Yu, Yang",
      "Li, Qingyang",
      "Luo, Fan-Ming",
      "Qin, Zhiwei",
      "Shang, Wenjie",
      "Ye, Jieping"
    ]
  },
  {
    "id": "473803f0f2ebd77d83ee60daaa61f381",
    "title": "Multilingual Pre-training with Universal Dependency Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/473803f0f2ebd77d83ee60daaa61f381-Paper.pdf",
    "abstract": "The pre-trained language model (PrLM) demonstrates domination in downstream natural language processing tasks, in which multilingual PrLM takes advantage of language universality to alleviate the issue of limited resources for low-resource languages. Despite its successes, the performance of multilingual PrLM is still unsatisfactory, when multilingual PrLMs only focus on plain text and ignore obvious universal linguistic structure clues. Existing PrLMs have shown that monolingual linguistic structure knowledge may bring about better performance. Thus we propose a novel multilingual PrLM that supports both explicit universal dependency parsing and implicit language modeling. Syntax in terms of universal dependency parse serves as not only pre-training objective but also learned representation in our model, which brings unprecedented PrLM interpretability and convenience in downstream task use. Our model outperforms two popular multilingual PrLM, multilingual-BERT and XLM-R, on cross-lingual natural language understanding (NLU) benchmarks and linguistic structure parsing datasets, demonstrating the effectiveness and stronger cross-lingual modeling capabilities of our approach.",
    "authors": [
      "Sun, Kailai",
      "Li, Zuchao",
      "Zhao, Hai"
    ]
  },
  {
    "id": "477bdb55b231264bb53a7942fd84254d",
    "title": "Parameter-free HE-friendly Logistic Regression",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/477bdb55b231264bb53a7942fd84254d-Paper.pdf",
    "abstract": "Privacy in machine learning has been widely recognized as an essential ethical and legal issue, because the data used for machine learning may contain sensitive information. Homomorphic encryption has recently attracted attention as a key solution to preserve privacy in machine learning applications. However, current approaches on the training of encrypted machine learning have relied heavily on hyperparameter selection, which should be avoided owing to the extreme difficulty of conducting validation on encrypted data. In this study, we propose an effective privacy-preserving logistic regression method that is free from the approximation of the sigmoid function and hyperparameter selection. In our framework, a logistic regression model can be transformed into the corresponding ridge regression for the logit function. We provide a theoretical background for our framework by suggesting a new generalization error bound on the encrypted data. Experiments on various real-world data show that our framework achieves better classification results while reducing latency by $\\sim68\\%$, compared to the previous models.",
    "authors": [
      "Byun, Junyoung",
      "Lee, Woojin",
      "Lee, Jaewook"
    ]
  },
  {
    "id": "47841cc9e552bd5c40164db7073b817b",
    "title": "Active clustering for labeling training data",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/47841cc9e552bd5c40164db7073b817b-Paper.pdf",
    "abstract": "Gathering training data is a key step of any supervised learning task, and it is both critical and expensive. Critical, because the quantity and quality of the training data has a high impact on the performance of the learned function. Expensive, because most practical cases rely on humans-in-the-loop to label the data. The process of determining the correct labels is much more expensive than comparing two items to see whether they belong to the same class. Thus motivated, we propose a setting for training data gathering where the human experts perform the comparatively cheap task of answering pairwise queries, and the computer groups the items into classes (which can be labeled cheaply at the very end of the process). Given the items, we consider two random models for the classes: one where the set partition they form is drawn uniformly, the other one where each item chooses its class independently following a fixed distribution. In the first model, we characterize the algorithms that minimize the average number of queries required to cluster the items and analyze their complexity. In the second model, we analyze a specific algorithm family, propose as a conjecture that they reach the minimum average number of queries and compare their performance to a random approach. We also propose solutions to handle errors or inconsistencies in the experts' answers.",
    "authors": [
      "Lutz, Quentin",
      "de Panafieu, Elie",
      "Stein, Maya",
      "Scott, Alex"
    ]
  },
  {
    "id": "47951a40efc0d2f7da8ff1ecbfde80f4",
    "title": "Exploring Social Posterior Collapse in Variational Autoencoder for Interaction Modeling",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/47951a40efc0d2f7da8ff1ecbfde80f4-Paper.pdf",
    "abstract": "Multi-agent behavior modeling and trajectory forecasting are crucial for the safe navigation of autonomous agents in interactive scenarios. Variational Autoencoder (VAE) has been widely applied in multi-agent interaction modeling to generate diverse behavior and learn a low-dimensional representation for interacting systems. However, existing literature did not formally discuss if a VAE-based model can properly encode interaction into its latent space. In this work, we argue that one of the typical formulations of VAEs in multi-agent modeling suffers from an issue we refer to as social posterior collapse, i.e., the model is prone to ignoring historical social context when predicting the future trajectory of an agent. It could cause significant prediction errors and poor generalization performance. We analyze the reason behind this under-explored phenomenon and propose several measures to tackle it. Afterward, we implement the proposed framework and experiment on real-world datasets for multi-agent trajectory prediction. In particular, we propose a novel sparse graph attention message-passing (sparse-GAMP) layer, which helps us detect social posterior collapse in our experiments. In the experiments, we verify that social posterior collapse indeed occurs. Also, the proposed measures are effective in alleviating the issue. As a result, the model attains better generalization performance when historical social context is informative for prediction. ",
    "authors": [
      "Tang, Chen",
      "Zhan, Wei",
      "Tomizuka, Masayoshi"
    ]
  },
  {
    "id": "479b4864e55e12e0fb411eadb115c095",
    "title": "Ensembling Graph Predictions for AMR Parsing",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/479b4864e55e12e0fb411eadb115c095-Paper.pdf",
    "abstract": "In many machine learning tasks, models are trained to predict structure data such as graphs. For example, in natural language processing, it is very common to parse texts into dependency trees or abstract meaning representation (AMR) graphs. On the other hand, ensemble methods combine predictions from multiple models to create a new one that is more robust and accurate than individual predictions. In the literature, there are many ensembling techniques proposed for classification or regression problems, however, ensemble graph prediction has not been studied thoroughly. In this work, we formalize this problem as mining the largest graph that is the most supported by a collection of graph predictions. As the problem is NP-Hard, we propose an efficient heuristic algorithm to approximate the optimal solution. To validate our approach, we carried out experiments in AMR parsing problems. The experimental results demonstrate that the proposed approach can combine the strength of state-of-the-art AMR parsers to create new predictions that are more accurate than any individual models in five standard benchmark datasets. ",
    "authors": [
      "Hoang, Thanh Lam",
      "Picco, Gabriele",
      "Hou, Yufang",
      "Lee, Young-Suk",
      "Nguyen, Lam",
      "Phan, Dzung",
      "Lopez, Vanessa",
      "Fernandez Astudillo, Ramon"
    ]
  },
  {
    "id": "47a5feca4ce02883a5643e295c7ce6cd",
    "title": "On the interplay between data structure and loss function in classification problems",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/47a5feca4ce02883a5643e295c7ce6cd-Paper.pdf",
    "abstract": "One of the central features of modern machine learning models, including deep neural networks, is their generalization ability on structured data in the over-parametrized regime. In this work, we consider an analytically solvable setup to investigate how properties of data impact learning in classification problems, and compare the results obtained for quadratic loss and logistic loss. Using methods from statistical physics, we obtain a precise asymptotic expression for the train and test errors of random feature models trained on a simple model of structured data. The input covariance is built from independent blocks allowing us to tune the saliency of low-dimensional structures and their alignment with respect to the target function.Our results show in particular that in the over-parametrized regime, the impact of data structure on both train and test error curves is greater for logistic loss than for mean-squared loss: the easier the task, the wider the gap in performance between the two losses at the advantage of the logistic. Numerical experiments on MNIST and CIFAR10 confirm our insights.",
    "authors": [
      "d'Ascoli, St\u00e9phane",
      "Gabri\u00e9, Marylou",
      "Sagun, Levent",
      "Biroli, Giulio"
    ]
  },
  {
    "id": "47a658229eb2368a99f1d032c8848542",
    "title": "Near-optimal Offline and Streaming Algorithms for Learning Non-Linear Dynamical Systems",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/47a658229eb2368a99f1d032c8848542-Paper.pdf",
    "abstract": "We consider the setting of vector valued non-linear dynamical systems $X_{t+1} = \\phi(A^{*} X_t) + \\eta_t$, where $\\eta_t$ is unbiased noise and $\\phi : \\mathbb{R} \\to \\mathbb{R}$ is a known link function that satisfies certain {\\em expansivity property}. The goal is to learn $A^{*}$ from a single trajectory $X_1,\\cdots , X_T$ of {\\em dependent or correlated} samples.While the problem is well-studied in the linear case, where $\\phi$ is identity, with optimal error rates even for non-mixing systems, existing results in the non-linear case hold only for mixing systems. In this work, we improve existing results for learning nonlinear systems in a number of ways: a) we provide the first offline algorithm that can learn non-linear dynamical systems without the mixing assumption, b) we significantly improve upon the sample complexity of existing results for mixing systems, c) in the much harder one-pass, streaming setting we study a SGD with Reverse Experience Replay (SGD-RER) method, and demonstrate that for mixing systems, it achieves the same sample complexity as our offline algorithm, d) we justify the expansivity assumption by showing that for the popular ReLU  link function --- a non-expansive but easy to learn link function with i.i.d. samples --- any method would require exponentially many samples (with respect to dimension of $X_t$) from the dynamical system. We validate our results via. simulations and  demonstrate that a naive application of SGD can be highly sub-optimal. Indeed, our work demonstrates that for correlated data, specialized  methods designed for the dependency structure in data can  significantly outperform  standard SGD based methods. ",
    "authors": [
      "Kowshik, Suhas",
      "Nagaraj, Dheeraj",
      "Jain, Prateek",
      "Netrapalli, Praneeth"
    ]
  },
  {
    "id": "47b4f1bfdf6d298682e610ad74b37dca",
    "title": "Mixture Proportion Estimation and PU Learning:A Modern Approach",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/47b4f1bfdf6d298682e610ad74b37dca-Paper.pdf",
    "abstract": "Given only positive examples and unlabeled examples (from both positive and negative classes), we might hope nevertheless to estimate an accurate positive-versus-negative classifier. Formally, this task is broken down into two subtasks: (i) Mixture Proportion Estimation (MPE)---determining the fraction of positive examples in the unlabeled data; and (ii) PU-learning---given such an estimate, learning the desired positive-versus-negative classifier. Unfortunately, classical methods for both problems break down in high-dimensional settings. Meanwhile, recently proposed heuristics lack theoretical coherence and depend precariously on hyperparameter tuning. In this paper, we propose two simple techniques: Best Bin Estimation (BBE) (for MPE); and Conditional Value Ignoring Risk (CVIR), a simple objective for PU-learning. Both methods dominate previous approaches empirically, and for BBE, we establish formal guarantees that hold whenever we can train a model to cleanly separate out a small subset of positive examples. Our final algorithm (TED)$^n$, alternates between the two procedures, significantly improving both our mixture proportion estimator and classifier",
    "authors": [
      "Garg, Saurabh",
      "Wu, Yifan",
      "Smola, Alexander J.",
      "Balakrishnan, Sivaraman",
      "Lipton, Zachary"
    ]
  },
  {
    "id": "47bd8ac1becf213f155a82244b4a696a",
    "title": "Escape saddle points by a simple gradient-descent based algorithm",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/47bd8ac1becf213f155a82244b4a696a-Paper.pdf",
    "abstract": "Escaping saddle points is a central research topic in nonconvex optimization. In this paper, we propose a simple gradient-based algorithm such that for a smooth function $f\\colon\\mathbb{R}^n\\to\\mathbb{R}$, it outputs an $\\epsilon$-approximate second-order stationary point in $\\tilde{O}(\\log n/\\epsilon^{1.75})$ iterations. Compared to the previous state-of-the-art algorithms by Jin et al. with $\\tilde{O}(\\log^4 n/\\epsilon^{2})$ or $\\tilde{O}(\\log^6 n/\\epsilon^{1.75})$ iterations, our algorithm is polynomially better in terms of $\\log n$ and matches their complexities in terms of $1/\\epsilon$. For the stochastic setting, our algorithm outputs an $\\epsilon$-approximate second-order stationary point in $\\tilde{O}(\\log^{2} n/\\epsilon^{4})$ iterations. Technically, our main contribution is an idea of implementing a robust Hessian power method using only gradients, which can find negative curvature near saddle points and achieve the polynomial speedup in $\\log n$ compared to the perturbed gradient descent methods. Finally, we also perform numerical experiments that support our results.",
    "authors": [
      "Zhang, Chenyi",
      "Li, Tongyang"
    ]
  },
  {
    "id": "48000647b315f6f00f913caa757a70b3",
    "title": "AC/DC: Alternating Compressed/DeCompressed Training of Deep Neural Networks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/48000647b315f6f00f913caa757a70b3-Paper.pdf",
    "abstract": "The increasing computational requirements of deep neural networks (DNNs) have led to significant interest in obtaining DNN models that are sparse, yet accurate. Recent work has investigated the even harder case of sparse training, where the DNN weights are, for as much as possible, already sparse to reduce computational costs during training. Existing sparse training methods are often empirical and can have lower accuracy relative to the dense baseline. In this paper, we present a general approach called Alternating Compressed/DeCompressed (AC/DC) training of DNNs, demonstrate convergence for a variant of the algorithm,  and show that AC/DC outperforms existing sparse training methods in accuracy at similar computational budgets; at high sparsity levels, AC/DC even outperforms existing methods that rely on accurate pre-trained dense models. An important property of AC/DC is that it allows co-training of dense and sparse models, yielding accurate sparse-dense model pairs at the end of the training process. This is useful in practice, where compressed variants may be desirable for deployment in resource-constrained settings without re-doing the entire training flow, and also provides us with insights into the accuracy gap between dense and compressed models.",
    "authors": [
      "Peste, Alexandra",
      "Iofinova, Eugenia",
      "Vladu, Adrian",
      "Alistarh, Dan"
    ]
  },
  {
    "id": "481fbfa59da2581098e841b7afc122f1",
    "title": "HyperSPNs: Compact and Expressive Probabilistic Circuits",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/481fbfa59da2581098e841b7afc122f1-Paper.pdf",
    "abstract": "Probabilistic circuits (PCs) are a family of generative models which allows for the computation of exact likelihoods and marginals of its probability distributions. PCs are both expressive and tractable, and serve as popular choices for discrete density estimation tasks. However, large PCs are susceptible to overfitting, and only a few regularization strategies (e.g., dropout, weight-decay) have been explored. We propose HyperSPNs: a new paradigm of generating the mixture weights of large PCs using a small-scale neural network. Our framework can be viewed as a soft weight-sharing strategy, which combines the greater expressiveness of large models with the better generalization and memory-footprint properties of small models.  We show the merits of our regularization strategy on two state-of-the-art PC families introduced in recent literature -- RAT-SPNs and EiNETs -- and demonstrate generalization improvements in both models on a suite of density estimation benchmarks in both discrete and continuous domains.",
    "authors": [
      "Shih, Andy",
      "Sadigh, Dorsa",
      "Ermon, Stefano"
    ]
  },
  {
    "id": "48237d9f2dea8c74c2a72126cf63d933",
    "title": "Scaling Vision with Sparse Mixture of Experts",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/48237d9f2dea8c74c2a72126cf63d933-Paper.pdf",
    "abstract": "Sparsely-gated Mixture of Experts networks (MoEs) have demonstrated excellent scalability in Natural Language Processing. In Computer Vision, however, almost all performant networks are \"dense\", that is, every input is processed by every parameter. We present a Vision MoE (V-MoE), a sparse version of the Vision Transformer, that is scalable and competitive with the largest dense networks. When applied to image recognition, V-MoE matches the performance of state-of-the-art networks, while requiring as little as half of the compute at inference time. Further, we propose an extension to the routing algorithm that can prioritize subsets of each input across the entire batch, leading to adaptive per-image compute. This allows V-MoE to trade-off performance and compute smoothly at test-time. Finally, we demonstrate the potential of V-MoE to scale vision models, and train a 15B parameter model that attains 90.35% on ImageNet.",
    "authors": [
      "Riquelme, Carlos",
      "Puigcerver, Joan",
      "Mustafa, Basil",
      "Neumann, Maxim",
      "Jenatton, Rodolphe",
      "Susano Pinto, Andr\u00e9",
      "Keysers, Daniel",
      "Houlsby, Neil"
    ]
  },
  {
    "id": "48259990138bc03361556fb3f94c5d45",
    "title": "Two-sided fairness in rankings via Lorenz dominance",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/48259990138bc03361556fb3f94c5d45-Paper.pdf",
    "abstract": "We consider the problem of generating rankings that are fair towards both users and item producers in recommender systems. We address both usual recommendation (e.g., of music or movies) and reciprocal recommendation (e.g., dating). Following concepts of distributive justice in welfare economics, our notion of fairness aims at increasing the utility of the worse-off individuals, which we formalize using the criterion of Lorenz efficiency. It guarantees that rankings are Pareto efficient, and that they maximally redistribute utility from better-off to worse-off, at a given level of overall utility. We propose to generate rankings by maximizing concave welfare functions, and develop an efficient inference procedure based on the Frank-Wolfe algorithm. We prove that unlike existing approaches based on fairness constraints, our approach always produces fair rankings. Our experiments also show that it increases the utility of the worse-off at lower costs in terms of overall utility.",
    "authors": [
      "Do, Virginie",
      "Corbett-Davies, Sam",
      "Atif, Jamal",
      "Usunier, Nicolas"
    ]
  },
  {
    "id": "483101a6bc4e6c46a86222eb65fbcb6a",
    "title": "Stability & Generalisation of Gradient Descent for Shallow Neural Networks without the Neural Tangent Kernel",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/483101a6bc4e6c46a86222eb65fbcb6a-Paper.pdf",
    "abstract": "We revisit on-average algorithmic stability of Gradient Descent (GD) for training  overparameterised  shallow  neural  networks  and prove  new  generalisation and  excess  risk  bounds  without  the  Neural  Tangent  Kernel  (NTK)  or  Polyak-\u0141ojasiewicz (PL) assumptions. In particular, we show oracle type bounds which reveal that the generalisation and excess risk of GD is controlled by an interpolating network with the shortest GD path from initialisation (in a sense, an interpolating network with the smallest relative norm).  While this was known for kernelised interpolants, our proof applies directly to networks trained by GD without intermediate kernelisation. At the same time, by relaxing oracle inequalities developed here we recover existing NTK-based risk bounds in a straightforward way, which demonstrates that our analysis is tighter. Finally, unlike most of the NTK-based analyses we focus on regression with label noise and show that GD with early stopping is consistent",
    "authors": [
      "Richards, Dominic",
      "Kuzborskij, Ilja"
    ]
  },
  {
    "id": "486c0401c56bf7ec2daa9eba58907da9",
    "title": "Adversarial Intrinsic Motivation for Reinforcement Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/486c0401c56bf7ec2daa9eba58907da9-Paper.pdf",
    "abstract": "Learning with an objective to minimize the mismatch with a reference distribution has been shown to be useful for generative modeling and imitation learning. In this paper, we investigate whether one such objective, the Wasserstein-1 distance between a policy's state visitation distribution and a target distribution, can be utilized effectively for reinforcement learning (RL) tasks. Specifically, this paper focuses on goal-conditioned reinforcement learning where the idealized (unachievable) target distribution has full measure at the goal. This paper introduces a quasimetric specific to Markov Decision Processes (MDPs) and uses this quasimetric to estimate the above Wasserstein-1 distance. It further shows that the policy that minimizes this Wasserstein-1 distance is the policy that reaches the goal in as few steps as possible. Our approach, termed Adversarial Intrinsic Motivation (AIM), estimates this Wasserstein-1 distance through its dual objective and uses it to compute a supplemental reward function. Our experiments show that this reward function changes smoothly with respect to transitions in the MDP and directs the agent's exploration to find the goal efficiently. Additionally, we combine AIM with Hindsight Experience Replay (HER) and show that the resulting algorithm accelerates learning significantly on several simulated robotics tasks when compared to other rewards that encourage exploration or accelerate learning.",
    "authors": [
      "Durugkar, Ishan",
      "Tec, Mauricio",
      "Niekum, Scott",
      "Stone, Peter"
    ]
  },
  {
    "id": "488b084119a1c7a4950f00706ec7ea16",
    "title": "Machine Learning for Variance Reduction in Online Experiments",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/488b084119a1c7a4950f00706ec7ea16-Paper.pdf",
    "abstract": "We consider the problem of variance reduction in randomized controlled trials, through the use of covariates correlated with the outcome but independent of the treatment. We propose a machine learning regression-adjusted treatment effect estimator, which we call MLRATE. MLRATE uses machine learning predictors of the outcome to reduce estimator variance. It employs cross-fitting to avoid overfitting biases, and we prove consistency and asymptotic normality under general conditions. MLRATE is robust to poor predictions from the machine learning step: if the predictions are uncorrelated with the outcomes, the estimator performs asymptotically no worse than the standard difference-in-means estimator, while if predictions are highly correlated with outcomes, the efficiency gains are large. In A/A tests, for a set of 48 outcome metrics commonly monitored in Facebook experiments, the estimator has over $70\\%$ lower variance than the simple difference-in-means estimator, and about $19\\%$ lower variance than the common univariate procedure which adjusts only for pre-experiment values of the outcome.",
    "authors": [
      "Guo, Yongyi",
      "Coey, Dominic",
      "Konutgan, Mikael",
      "Li, Wenting",
      "Schoener, Chris",
      "Goldman, Matt"
    ]
  },
  {
    "id": "48aedb8880cab8c45637abc7493ecddd",
    "title": "L2ight: Enabling On-Chip Learning for Optical Neural Networks via Efficient in-situ Subspace Optimization",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/48aedb8880cab8c45637abc7493ecddd-Paper.pdf",
    "abstract": "Silicon-photonics-based optical neural network (ONN) is a promising hardware platform that could represent a paradigm shift in efficient AI with its CMOS-compatibility, flexibility, ultra-low execution latency, and high energy efficiency. In-situ training on the online programmable photonic chips is appealing but still encounters challenging issues in on-chip implementability, scalability, and efficiency. In this work, we propose a closed-loop ONN on-chip learning framework L2ight to enable scalable ONN mapping and efficient in-situ learning. L2ight adopts a three-stage learning flow that first calibrates the complicated photonic circuit states under challenging physical constraints, then performs photonic core mapping via combined analytical solving and zeroth-order optimization. A subspace learning procedure with multi-level sparsity is integrated into L2ight to enable in-situ gradient evaluation and fast adaptation, unleashing the power of optics for real on-chip intelligence. Extensive experiments demonstrate our proposed L2ight outperforms prior ONN training protocols with 3-order-of-magnitude higher scalability and over 30x better efficiency, when benchmarked on various models and learning tasks. This synergistic framework is the first scalable on-chip learning solution that pushes this emerging field from intractable to scalable and further to efficient for next-generation self-learnable photonic neural chips. From a co-design perspective, L2ight also provides essential insights for hardware-restricted unitary subspace optimization and efficient sparse training. We open-source our framework at the link.",
    "authors": [
      "Gu, Jiaqi",
      "Zhu, Hanqing",
      "Feng, Chenghao",
      "Jiang, Zixuan",
      "Chen, Ray",
      "Pan, David"
    ]
  },
  {
    "id": "48bea99c85bcbaaba618ba10a6f69e44",
    "title": "Towards Gradient-based Bilevel Optimization with Non-convex Followers and Beyond",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/48bea99c85bcbaaba618ba10a6f69e44-Paper.pdf",
    "abstract": "In recent years, Bi-Level Optimization (BLO) techniques have received extensive attentions from both learning and vision communities. A variety of BLO models in complex and practical tasks are of non-convex follower structure in nature (a.k.a., without Lower-Level Convexity, LLC for short). However, this challenging class of BLOs is lack of developments on both efficient solution strategies and solid theoretical guarantees. In this work, we propose a new algorithmic framework, named Initialization Auxiliary and Pessimistic Trajectory Truncated Gradient Method (IAPTT-GM), to partially address the above issues. In particular, by introducing an auxiliary as initialization to guide the optimization dynamics and designing a pessimistic trajectory truncation operation, we construct a reliable approximate version of the original BLO in the absence of LLC hypothesis. Our theoretical investigations establish the convergence of solutions returned by IAPTT-GM towards those of the original BLO without LLC. As an additional bonus, we also theoretically justify the quality of our IAPTT-GM embedded with Nesterov's accelerated dynamics under LLC. The experimental results confirm both the convergence of our algorithm without LLC, and the theoretical findings under LLC.",
    "authors": [
      "Liu, Risheng",
      "Liu, Yaohua",
      "Zeng, Shangzhi",
      "Zhang, Jin"
    ]
  },
  {
    "id": "48cb136b65a69e8c2aa22913a0d91b2f",
    "title": "Multi-Facet Clustering Variational Autoencoders",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/48cb136b65a69e8c2aa22913a0d91b2f-Paper.pdf",
    "abstract": "Work in deep clustering focuses on finding a single partition of data. However, high-dimensional data, such as images, typically feature multiple interesting characteristics one could cluster over. For example, images of objects against a background could be clustered over the shape of the object and separately by the colour of the background. In this paper, we introduce Multi-Facet Clustering Variational Autoencoders (MFCVAE), a novel class of variational autoencoders with a hierarchy of latent variables, each with a Mixture-of-Gaussians prior, that learns multiple clusterings simultaneously, and is trained fully unsupervised and end-to-end. MFCVAE uses a progressively-trained ladder architecture which leads to highly stable performance. We provide novel theoretical results for optimising the ELBO analytically with respect to the categorical variational posterior distribution, correcting earlier influential theoretical work. On image benchmarks, we demonstrate that our approach separates out and clusters over different aspects of the data in a disentangled manner. We also show other advantages of our model: the compositionality of its latent space and that it provides controlled generation of samples.",
    "authors": [
      "Falck, Fabian",
      "Zhang, Haoting",
      "Willetts, Matthew",
      "Nicholson, George",
      "Yau, Christopher",
      "Holmes, Chris C"
    ]
  },
  {
    "id": "48d23e87eb98cc2227b5a8c33fa00680",
    "title": "Synthetic Design: An Optimization Approach to Experimental Design with Synthetic Controls",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/48d23e87eb98cc2227b5a8c33fa00680-Paper.pdf",
    "abstract": "We investigate the optimal design of experimental studies that have pre-treatment outcome data available.  The average treatment effect is estimated as the difference between the weighted average outcomes of the treated and control units. A number of commonly used approaches fit this formulation, including the difference-in-means estimator and a variety of synthetic-control techniques. We propose several methods for choosing the set of treated units in conjunction with the weights. Observing the NP-hardness of the problem, we introduce a mixed-integer programming formulation which selects both the treatment and control sets and unit weightings. We prove that these proposed approaches lead to qualitatively different experimental units being selected for treatment. We use simulations based on publicly available data from the US Bureau of Labor Statistics that show improvements in terms of mean squared error and statistical power when compared to simple and commonly used alternatives such as randomized trials.",
    "authors": [
      "Doudchenko, Nick",
      "Khosravi, Khashayar",
      "Pouget-Abadie, Jean",
      "Lahaie, S\u00e9bastien",
      "Lubin, Miles",
      "Mirrokni, Vahab",
      "Spiess, Jann",
      "imbens, guido"
    ]
  },
  {
    "id": "48db71587df6c7c442e5b76cc723169a",
    "title": "Ranking Policy Decisions",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/48db71587df6c7c442e5b76cc723169a-Paper.pdf",
    "abstract": "Policies trained via Reinforcement Learning (RL) without human intervention are often needlessly complex, making them difficult to analyse and interpret. In a run with $n$ time steps, a policy will make $n$ decisions on actions to take; we conjecture that only a small subset of these decisions delivers value over selecting a simple default action. Given a trained policy, we propose a novel black-box method based on statistical fault localisation that ranks the states of the environment according to the importance of decisions made in those states. We argue that among other things, the ranked list of states can help explain and understand the policy. As the ranking method is statistical, a direct evaluation of its quality is hard. As a proxy for quality, we use the ranking to create new, simpler policies from the original ones by pruning decisions identified as unimportant (that is, replacing them by default actions) and measuring the impact on performance. Our experimental results on a diverse set of standard benchmarks demonstrate that pruned policies can perform on a level comparable to the original policies. We show that naive approaches for ranking policies, e.g. ranking based on the frequency of visiting a state, do not result in high-performing pruned policies. To the best of our knowledge, there are no similar techniques for ranking RL policies' decisions. ",
    "authors": [
      "Pouget, Hadrien",
      "Chockler, Hana",
      "Sun, Youcheng",
      "Kroening, Daniel"
    ]
  },
  {
    "id": "48e95c45c8217961bf6cd7696d80d238",
    "title": "Searching the Search Space of Vision Transformer",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/48e95c45c8217961bf6cd7696d80d238-Paper.pdf",
    "abstract": "Vision Transformer has shown great visual representation power in substantial vision tasks such as recognition and detection, and thus been attracting fast-growing efforts on manually designing more effective architectures. In this paper, we propose to use neural architecture search to automate this process, by searching not only the architecture but also the search space. The central idea is to gradually evolve different search dimensions guided by their E-T Error computed using a weight-sharing supernet. Moreover, we provide design guidelines of general vision transformers with extensive analysis according to the space searching process, which could promote the understanding of vision transformer. Remarkably, the searched models, named S3 (short for Searching the Search Space), from the searched space achieve superior performance to recently proposed models, such as Swin, DeiT and ViT, when evaluated on ImageNet. The effectiveness of S3 is also illustrated on object detection, semantic segmentation and visual question answering, demonstrating its generality to downstream vision and vision-language tasks. Code and models will be available at https://github.com/microsoft/Cream.",
    "authors": [
      "Chen, Minghao",
      "Wu, Kan",
      "Ni, Bolin",
      "Peng, Houwen",
      "Liu, Bei",
      "Fu, Jianlong",
      "Chao, Hongyang",
      "Ling, Haibin"
    ]
  },
  {
    "id": "497476fe61816251905e8baafdf54c23",
    "title": "Relative stability toward diffeomorphisms indicates performance in deep nets",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/497476fe61816251905e8baafdf54c23-Paper.pdf",
    "abstract": "Understanding why deep nets can classify data in large dimensions remains a challenge. It has been proposed that they do so by becoming stable to diffeomorphisms, yet existing empirical measurements support that it is often not the case. We revisit this question by defining a maximum-entropy distribution on diffeomorphisms, that allows to study typical diffeomorphisms of a given norm. We confirm that stability toward diffeomorphisms does not strongly correlate to performance on benchmark data sets of images. By contrast, we find that the stability toward diffeomorphisms relative to that of generic transformations $R_f$ correlates remarkably with the test error $\\epsilon_t$. It is of order unity at initialization but decreases by several decades during training for state-of-the-art architectures. For CIFAR10 and 15 known architectures, we find $\\epsilon_t\\approx 0.2\\sqrt{R_f}$, suggesting that obtaining a small $R_f$ is important to achieve good performance. We study how $R_f$ depends on the size of the training set and compare it to a simple model of invariant learning.",
    "authors": [
      "Petrini, Leonardo",
      "Favero, Alessandro",
      "Geiger, Mario",
      "Wyart, Matthieu"
    ]
  },
  {
    "id": "498f2c21688f6451d9f5fd09d53edda7",
    "title": "Raw Nav-merge Seismic Data to Subsurface Properties with MLP based Multi-Modal Information Unscrambler",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/498f2c21688f6451d9f5fd09d53edda7-Paper.pdf",
    "abstract": "Traditional seismic inversion (SI) maps the hundreds of terabytes of raw-field data to subsurface properties in gigabytes.  This inversion process is expensive, requiring over a year of human and computational effort. Recently, data-driven approaches equipped with Deep learning (DL) are envisioned to improve SI efficiency.  However, these improvements are restricted to data with highly reduced scale and complexity. To extend these approaches to real-scale seismic data, researchers need to process raw nav-merge seismic data into an image and perform convolution. We argue that this convolution-based way of SI is not only computationally expensive but also conceptually problematic. Seismic data is not naturally an image and need not be processed as images. In this work, we go beyond convolution and propose a novel SI method. We solve the scalability of SI by proposing a new auxiliary learning paradigm for SI (Aux-SI). This paradigm breaks the SI into local inversion tasks, which predicts each small chunk of subsurface properties using surrounding seismic data. Aux-SI combines these local predictions to obtain the entire subsurface model. However, even this local inversion is still challenging due to: (1) high-dimensional, spatially irregular multi-modal seismic data, (2) there is no concrete spatial mapping (or alignment) between subsurface properties and raw data. To handle these challenges, we propose an all-MLP architecture,  Multi-Modal Information Unscrambler (MMI-Unscrambler), that unscrambles seismic information by ingesting all available multi-modal data. The experiment shows that MMI-Unscrambler outperforms both SOTA U-Net and Transformer models on simulation data. We also scale MMI-Unscrambler to raw-field nav-merge data on Gulf-of-Mexico to obtain a geologically sound velocity model with an SSIM score of 0.8. To the best of our knowledge, this is the first successful demonstration of the DL approach on SI for real, large-scale, and complicated raw field data. ",
    "authors": [
      "Desai, Aditya",
      "Xu, Zhaozhuo",
      "Gupta, Menal",
      "Chandran, Anu",
      "Vial-Aussavy, Antoine",
      "Shrivastava, Anshumali"
    ]
  },
  {
    "id": "498f940d9b933c529b06aa96d18f7eda",
    "title": "Inverse Problems Leveraging Pre-trained Contrastive Representations",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/498f940d9b933c529b06aa96d18f7eda-Paper.pdf",
    "abstract": "We study a new family of inverse problems for recovering representations of corrupted data. We assume access to a pre-trained representation learning network R(x) that operates on clean images, like CLIP. The problem is to recover the representation of an image R(x), if we are only given a corrupted version A(x), for some known forward operator A. We propose a supervised inversion method that uses a contrastive objective to obtain excellent representations for highly corrupted images. Using a linear probe on our robust representations, we achieve a higher accuracy than end-to-end supervised baselines when classifying images with various types of distortions, including blurring, additive noise, and random pixel masking. We evaluate on a subset of ImageNet and observe that our method is robust to varying levels of distortion. Our method outperforms end-to-end baselines even with a fraction of the labeled data in a wide range of forward operators. ",
    "authors": [
      "Ravula, Sriram",
      "Smyrnis, Georgios",
      "Jordan, Matt",
      "Dimakis, Alexandros G."
    ]
  },
  {
    "id": "4990974d150d0de5e6e15a1454fe6b0f",
    "title": "The Unbalanced Gromov Wasserstein Distance: Conic Formulation and Relaxation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4990974d150d0de5e6e15a1454fe6b0f-Paper.pdf",
    "abstract": "Comparing metric measure spaces (i.e. a metric space endowed with a probability distribution) is at the heart of many machine learning problems. The most popular distance between such metric measure spaces is the Gromov-Wasserstein (GW) distance, which is the solution of a quadratic assignment problem. The GW distance is however limited to the comparison of metric measure spaces endowed with a \\emph{probability} distribution. To alleviate this issue, we introduce two Unbalanced Gromov-Wasserstein formulations: a distance and a more tractable upper-bounding relaxation.  They both allow the comparison of metric spaces equipped with arbitrary positive measures up to isometries. The first formulation is a positive and definite divergence based on a relaxation of the mass conservation constraint using a novel type of quadratically-homogeneous divergence. This divergence works hand in hand with the entropic regularization approach which is popular to solve large scale optimal transport problems. We show that the underlying non-convex optimization problem can be efficiently tackled using a highly parallelizable and GPU-friendly iterative scheme. The second formulation is a distance between mm-spaces up to isometries based on a conic lifting.  Lastly, we provide numerical experiments on synthetic and domain adaptation data with a Positive-Unlabeled learning task to highlight the salient features of the unbalanced divergence and its potential applications in ML.",
    "authors": [
      "Sejourne, Thibault",
      "Vialard, Francois-Xavier",
      "Peyr\u00e9, Gabriel"
    ]
  },
  {
    "id": "49ad23d1ec9fa4bd8d77d02681df5cfa",
    "title": "Diffusion Models Beat GANs on Image Synthesis",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/49ad23d1ec9fa4bd8d77d02681df5cfa-Paper.pdf",
    "abstract": "We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet 256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512.",
    "authors": [
      "Dhariwal, Prafulla",
      "Nichol, Alexander"
    ]
  },
  {
    "id": "49e863b146f3b5470ee222ee84669b1c",
    "title": "Learning MDPs from Features: Predict-Then-Optimize for Sequential Decision Making by Reinforcement Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/49e863b146f3b5470ee222ee84669b1c-Paper.pdf",
    "abstract": "In the predict-then-optimize framework, the objective is to train a predictive model, mapping from environment features to parameters of an optimization problem, which maximizes decision quality when the optimization is subsequently solved. Recent work on decision-focused learning shows that embedding the optimization problem in the training pipeline can improve decision quality and help generalize better to unseen tasks compared to relying on an intermediate loss function for evaluating prediction quality. We study the predict-then-optimize framework in the context of sequential decision problems (formulated as MDPs) that are solved via reinforcement learning. In particular, we are given environment features and a set of trajectories from training MDPs, which we use to train a predictive model that generalizes to unseen test MDPs without trajectories. Two significant computational challenges arise in applying decision-focused learning to MDPs: (i) large state and action spaces make it infeasible for existing techniques to differentiate through MDP problems, and (ii) the high-dimensional policy space, as parameterized by a neural network, makes differentiating through a policy expensive. We resolve the first challenge by sampling provably unbiased derivatives to approximate and differentiate through optimality conditions, and the second challenge by using a low-rank approximation to the high-dimensional sample-based derivatives. We implement both Bellman-based and policy gradient-based decision-focused learning on three different MDP problems with missing parameters, and show that decision-focused learning performs better in generalization to unseen tasks.",
    "authors": [
      "Wang, Kai",
      "Shah, Sanket",
      "Chen, Haipeng",
      "Perrault, Andrew",
      "Doshi-Velez, Finale",
      "Tambe, Milind"
    ]
  },
  {
    "id": "49ef08ad6e7f26d7f200e1b2b9e6e4ac",
    "title": "A Closer Look at the Worst-case Behavior of Multi-armed Bandit Algorithms",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/49ef08ad6e7f26d7f200e1b2b9e6e4ac-Paper.pdf",
    "abstract": "One of the key drivers of complexity in the classical (stochastic) multi-armed bandit (MAB) problem is the difference between mean rewards in the top two arms, also known as the instance gap. The celebrated Upper Confidence Bound (UCB) policy is among the simplest optimism-based MAB algorithms that naturally adapts to this gap: for a horizon of play n, it achieves optimal O(log n) regret in instances with \"large\" gaps, and a near-optimal O(\\sqrt{n log n}) minimax regret when the gap can be arbitrarily \"small.\" This paper provides new results on the arm-sampling behavior of UCB, leading to several important insights. Among these, it is shown that arm-sampling rates under UCB are asymptotically deterministic, regardless of the problem complexity. This discovery facilitates new sharp asymptotics and a novel alternative proof for the O(\\sqrt{n log n}) minimax regret of UCB. Furthermore, the paper also provides the first complete process-level characterization of the MAB problem in the conventional diffusion scaling. Among other things, the \"small\" gap worst-case lens adopted in this paper also reveals profound distinctions between the behavior of UCB and Thompson Sampling, such as an \"incomplete learning\" phenomenon characteristic of the latter.",
    "authors": [
      "Kalvit, Anand",
      "Zeevi, Assaf"
    ]
  },
  {
    "id": "4a06d868d044c50af0cf9bc82d2fc19f",
    "title": "SAPE: Spatially-Adaptive Progressive Encoding for Neural Optimization",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4a06d868d044c50af0cf9bc82d2fc19f-Paper.pdf",
    "abstract": "Multilayer-perceptrons (MLP)  are known to struggle learning functions of high-frequencies, and in particular, instances of wide frequency bands.We present a progressive mapping scheme for input signals of MLP networks, enabling them to better fit a wide range of frequencies without sacrificing training stability or requiring any domain specific preprocessing. We introduce Spatially Adaptive Progressive Encoding (SAPE) layers, which gradually unmask signal components with increasing frequencies as a function of time and space. The progressive exposure of frequencies is monitored by a feedback loop throughout the neural optimization process, allowing changes to propagate at different rates among local spatial portions of the signal space. We demonstrate the advantage of our method on variety of domains and applications: regression of low dimensional signals and images, representation learning of occupancy networks, and a geometric task of mesh transfer between 3D shapes.",
    "authors": [
      "Hertz, Amir",
      "Perel, Or",
      "Giryes, Raja",
      "Sorkine-hornung, Olga",
      "Cohen-or, Daniel"
    ]
  },
  {
    "id": "4a08142c38dbe374195d41c04562d9f8",
    "title": "A Biased Graph Neural Network Sampler with Near-Optimal Regret",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4a08142c38dbe374195d41c04562d9f8-Paper.pdf",
    "abstract": "Graph neural networks (GNN) have recently emerged as a vehicle for applying deep network architectures to graph and relational data.  However, given the increasing size of industrial datasets, in many practical situations, the message passing computations required for sharing information across GNN layers are no longer scalable. Although various sampling methods have been introduced to approximate full-graph training within a tractable budget, there remain unresolved complications such as high variances and limited theoretical guarantees.  To address these issues, we build upon existing work and treat GNN neighbor sampling as a multi-armed bandit problem but with a newly-designed reward function that introduces some degree of bias designed to reduce variance and avoid unstable, possibly-unbounded pay outs.  And unlike prior bandit-GNN use cases, the resulting policy leads to near-optimal regret while accounting for the GNN training dynamics introduced by SGD. From a practical standpoint, this translates into lower variance estimates and competitive or superior test accuracy across several benchmarks. ",
    "authors": [
      "Zhang, Qingru",
      "Wipf, David",
      "Gan, Quan",
      "Song, Le"
    ]
  },
  {
    "id": "4a3050ae2c77da4f9c90e2e58e8e520f",
    "title": "Equilibrium Refinement for the Age of Machines: The One-Sided Quasi-Perfect Equilibrium",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4a3050ae2c77da4f9c90e2e58e8e520f-Paper.pdf",
    "abstract": "In two-player zero-sum extensive-form games, Nash equilibrium prescribes optimal strategies against perfectly rational opponents. However, it does not guarantee rational play in parts of the game tree that can only be reached by the players making mistakes. This can be problematic when operationalizing equilibria in the real world among imperfect players. Trembling-hand refinements are a sound remedy to this issue, and are subsets of Nash equilibria that are designed to handle the possibility that any of the players may make mistakes. In this paper, we initiate the study of equilibrium refinements for settings where one of the players is perfectly rational (the ``machine'') and the other may make mistakes. As we show, this endeavor has many pitfalls: many intuitively appealing approaches to refinement fail in various ways. On the positive side, we introduce a modification of the classical quasi-perfect equilibrium (QPE) refinement, which we call the one-sided quasi-perfect equilibrium. Unlike QPE, one-sided QPE only accounts for mistakes from one player and assumes that no mistakes will be made by the machine. We present experiments on standard benchmark games and an endgame from the famous man-machine match where the AI Libratus was the first to beat top human specialist professionals in heads-up no-limit Texas hold'em poker. We show that one-sided QPE can be computed more efficiently than all known prior refinements, paving the way to wider adoption of Nash equilibrium refinements in settings with perfectly rational machines (or humans perfectly actuating machine-generated strategies) that interact with players prone to mistakes. We also show that one-sided QPE tends to play better than a Nash equilibrium strategy against imperfect opponents. ",
    "authors": [
      "Farina, Gabriele",
      "Sandholm, Tuomas"
    ]
  },
  {
    "id": "4a3e00961a08879c34f91ca0070ea2f5",
    "title": "Interpreting Representation Quality of DNNs for 3D Point Cloud Processing",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4a3e00961a08879c34f91ca0070ea2f5-Paper.pdf",
    "abstract": "In this paper, we evaluate the quality of knowledge representations encoded in deep neural networks (DNNs) for 3D point cloud processing. We propose a method to disentangle the overall model vulnerability into the sensitivity to the rotation, the translation, the scale, and local 3D structures. Besides, we also propose metrics to evaluate the spatial smoothness of encoding 3D structures, and the representation complexity of the DNN. Based on such analysis, experiments expose representation problems with classic DNNs, and explain the utility of the adversarial training. The code will be released when this paper is accepted.",
    "authors": [
      "Shen, Wen",
      "Ren, Qihan",
      "Liu, Dongrui",
      "Zhang, Quanshi"
    ]
  },
  {
    "id": "4a533591763dfa743a13affab1a85793",
    "title": "How Fine-Tuning Allows for Effective Meta-Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4a533591763dfa743a13affab1a85793-Paper.pdf",
    "abstract": "Representation learning has served as a key tool for meta-learning, enabling rapid learning of new tasks. Recent works like MAML learn task-specific representations by finding an initial representation requiring minimal per-task adaptation (i.e. a fine-tuning-based objective). We present a theoretical framework for analyzing a MAML-like algorithm, assuming all available tasks require approximately the same representation. We then provide risk bounds on predictors found by fine-tuning via gradient descent, demonstrating that the method provably leverages the shared structure. We illustrate these bounds in the logistic regression and neural network settings. In contrast, we establish settings where learning one representation for all tasks (i.e. using a \"frozen representation\" objective) fails. Notably, any such algorithm cannot outperform directly learning the target task with no other information, in the worst case. This separation underscores the benefit of fine-tuning-based over \u201cfrozen representation\u201d objectives in few-shot learning.",
    "authors": [
      "Chua, Kurtland",
      "Lei, Qi",
      "Lee, Jason D."
    ]
  },
  {
    "id": "4a5876b450b45371f6cfe5047ac8cd45",
    "title": "Cooperative Stochastic Bandits with Asynchronous Agents and Constrained Feedback",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4a5876b450b45371f6cfe5047ac8cd45-Paper.pdf",
    "abstract": "This paper studies a cooperative multi-armed bandit problem with $M$ agents cooperating together to solve the same instance of a $K$-armed stochastic bandit problem with the goal of maximizing the cumulative reward of agents. The agents are heterogeneous in (i) their limited access to a local subset of arms; and (ii) their decision-making rounds, i.e., agents are asynchronous with different decision-making gaps. The goal is to find the global optimal arm and agents are able to pull any arm, however, they observe the reward only when the selected arm is local.The challenge is a tradeoff for agents between pulling a local arm with the possibility of observing the feedback, or relying on the observations of other agents that might occur at different rates. Naive extensions of traditional algorithms lead to an arbitrarily poor regret as a function of aggregate action frequency of any $\\textit{suboptimal}$ arm located in slow agents. We resolve this issue by proposing a novel two-stage learning algorithm, called $\\texttt{CO-LCB}$ algorithm, whose regret is a function of aggregate action frequency of agents containing the $\\textit{optimal}$ arm. We also show that the regret of $\\texttt{CO-LCB}$ matches the regret lower bound up to a small factor.",
    "authors": [
      "Yang, Lin",
      "Chen, Yu-Zhen Janice",
      "Pasteris, Stephen",
      "Hajiesmaili, Mohammad",
      "Lui, John C. S.",
      "Towsley, Don"
    ]
  },
  {
    "id": "4ae67a7dd7e491f8fb6f9ea0cf25dfdb",
    "title": "Multiple Descent: Design Your Own Generalization Curve",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4ae67a7dd7e491f8fb6f9ea0cf25dfdb-Paper.pdf",
    "abstract": "This paper explores the generalization loss of linear regression in variably parameterized families of models, both under-parameterized and over-parameterized. We show that the generalization curve can have an arbitrary number of peaks, and moreover, the locations of those peaks can be explicitly controlled. Our results highlight the fact that both the classical U-shaped generalization curve and the recently observed double descent curve are not intrinsic properties of the model family. Instead, their emergence is due to the interaction between the properties of the data and the inductive biases of learning algorithms. ",
    "authors": [
      "Chen, Lin",
      "Min, Yifei",
      "Belkin, Mikhail",
      "Karbasi, Amin"
    ]
  },
  {
    "id": "4afa19649ae378da31a423bcd78a97c8",
    "title": "On Empirical Risk Minimization with Dependent and Heavy-Tailed Data",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4afa19649ae378da31a423bcd78a97c8-Paper.pdf",
    "abstract": "In this work, we establish risk bounds for Empirical Risk Minimization (ERM) with both dependent and heavy-tailed data-generating processes. We do so by extending the seminal works~\\cite{pmlr-v35-mendelson14, mendelson2018learning} on the analysis of ERM with heavy-tailed but independent and identically distributed observations, to the strictly stationary exponentially $\\beta$-mixing case. We allow for the interaction between the noise and inputs to be even polynomially heavy-tailed, which covers a significantly large class of heavy-tailed models beyond what is analyzed in the learning theory literature. We illustrate our theoretical results by obtaining rates of convergence for high-dimensional linear regression with dependent and heavy-tailed data.",
    "authors": [
      "Roy, Abhishek",
      "Balasubramanian, Krishnakumar",
      "Erdogdu, Murat A."
    ]
  },
  {
    "id": "4afe044911ed2c247005912512ace23b",
    "title": "Gone Fishing: Neural Active Learning with Fisher Embeddings",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4afe044911ed2c247005912512ace23b-Paper.pdf",
    "abstract": "There is an increasing need for effective active learning algorithms that are compatible with deep neural networks. This paper motivates and revisits a classic, Fisher-based active selection objective, and proposes BAIT, a practical, tractable, and high-performing algorithm that makes it viable for use with neural models. BAIT draws inspiration from the theoretical analysis of maximum likelihood estimators (MLE) for parametric models. It selects batches of samples by optimizing a bound on the MLE error in terms of the Fisher information, which we show can be implemented efficiently at scale by exploiting linear-algebraic structure especially amenable to execution on modern hardware. Our experiments demonstrate that BAIT outperforms the previous state of the art on both classification and regression problems, and is flexible enough to be used with a variety of model architectures.",
    "authors": [
      "Ash, Jordan",
      "Goel, Surbhi",
      "Krishnamurthy, Akshay",
      "Kakade, Sham"
    ]
  },
  {
    "id": "4b04b0dcd2ade339a3d7ce13252a29d4",
    "title": "On Riemannian Optimization over Positive Definite Matrices with the Bures-Wasserstein Geometry",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4b04b0dcd2ade339a3d7ce13252a29d4-Paper.pdf",
    "abstract": "In this paper, we comparatively analyze the Bures-Wasserstein (BW) geometry with the popular Affine-Invariant (AI) geometry for Riemannian optimization on the symmetric positive definite (SPD) matrix manifold. Our study begins with an observation that the BW metric has a linear dependence on SPD matrices in contrast to the quadratic dependence of the AI metric. We build on this to show that the BW metric is a more suitable and robust choice for several Riemannian optimization problems over ill-conditioned SPD matrices. We show that the BW geometry has a non-negative curvature, which further improves convergence rates of algorithms over the non-positively curved AI geometry. Finally, we verify that several popular cost functions, which are known to be geodesic convex under the AI geometry, are also geodesic convex under the BW geometry. Extensive experiments on various applications support our findings. ",
    "authors": [
      "Han, Andi",
      "Mishra, Bamdev",
      "Jawanpuria, Pratik Kumar",
      "Gao, Junbin"
    ]
  },
  {
    "id": "4b26dc4663ccf960c8538d595d0a1d3a",
    "title": "Refining Language Models with Compositional Explanations",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4b26dc4663ccf960c8538d595d0a1d3a-Paper.pdf",
    "abstract": "Pre-trained language models have been successful on text classification tasks, but are prone to learning spurious correlations from biased datasets, and are thus vulnerable when making inferences in a new domain. Prior work reveals such spurious patterns via post-hoc explanation algorithms which compute the importance of input features. Further, the model is regularized to align the importance scores with human knowledge, so that the unintended model behaviors are eliminated. However, such a regularization technique lacks flexibility and coverage, since only importance scores towards a pre-defined list of features are adjusted, while more complex human knowledge such as feature interaction and pattern generalization can hardly be incorporated. In this work, we propose to refine a learned language model for a target domain by collecting human-provided compositional explanations regarding observed biases. By parsing these explanations into executable logic rules, the human-specified refinement advice from a small set of explanations can be generalized to more training examples. We additionally introduce a regularization term allowing adjustments for both importance and interaction of features to better rectify model behavior. We demonstrate the effectiveness of the proposed approach on two text classification tasks by showing improved performance in target domain as well as improved model fairness after refinement.",
    "authors": [
      "Yao, Huihan",
      "Chen, Ying",
      "Ye, Qinyuan",
      "Jin, Xisen",
      "Ren, Xiang"
    ]
  },
  {
    "id": "4b4edc2630fe75800ddc29a7b4070add",
    "title": "Going Beyond Linear RL: Sample Efficient Neural Function Approximation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4b4edc2630fe75800ddc29a7b4070add-Paper.pdf",
    "abstract": "Deep Reinforcement Learning (RL) powered by neural net approximation of the Q function has had enormous empirical success. While the theory of RL has traditionally focused on linear function approximation (or eluder dimension) approaches, little is known about nonlinear RL with neural net approximations of the Q functions. This is the focus of this work, where we study function approximation with two-layer neural networks (considering both ReLU and polynomial activation functions).  Our first result is a computationally and statistically efficient algorithm in the generative model setting under completeness for two-layer neural networks. Our second result considers this setting but under only realizability of the neural net function class.  Here, assuming deterministic dynamics, the sample complexity scales linearly in the algebraic dimension. In all cases, our results significantly improve upon what can be attained with linear (or eluder dimension) methods.",
    "authors": [
      "Huang, Baihe",
      "Huang, Kaixuan",
      "Kakade, Sham",
      "Lee, Jason D.",
      "Lei, Qi",
      "Wang, Runzhe",
      "Yang, Jiaqi"
    ]
  },
  {
    "id": "4b55df75e2e804bab559aa885be40310",
    "title": "Scalable Neural Data Server: A Data Recommender for Transfer Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4b55df75e2e804bab559aa885be40310-Paper.pdf",
    "abstract": "Absence of large-scale labeled data in the practitioner's target domain can be a bottleneck to applying machine learning algorithms in practice. Transfer learning is a popular strategy for leveraging additional data to improve the downstream performance, but finding the most relevant data to transfer from can be challenging. Neural Data Server (NDS), a search engine that recommends relevant data for a given downstream task, has been previously proposed to address this problem (Yan et al., 2020). NDS uses a mixture of experts trained on data sources to estimate similarity between each source and the downstream task. Thus, the computational cost to each user grows with the number of sources and requires an expensive training step for each data provider.To address these issues, we propose Scalable Neural Data Server (SNDS), a large-scale search engine that can theoretically index thousands of datasets to serve relevant ML data to end users. SNDS trains the mixture of experts on intermediary datasets during initialization, and represents both data sources and downstream tasks by their proximity to the intermediary datasets. As such, computational cost incurred by users of SNDS remains fixed as new datasets are added to the server, without pre-training for the data providers.We validate SNDS on a plethora of real world tasks and find that data recommended by SNDS improves downstream task performance over baselines. We also demonstrate the scalability of our system by demonstrating its ability to select relevant data for transfer outside of the natural image setting.",
    "authors": [
      "Cao, Tianshi",
      "Doubov, Sasha (Alexandre)",
      "Acuna, David",
      "Fidler, Sanja"
    ]
  },
  {
    "id": "4b5deb9a14d66ab0acc3b8a2360cde7c",
    "title": "What can linearized neural networks actually say about generalization?",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4b5deb9a14d66ab0acc3b8a2360cde7c-Paper.pdf",
    "abstract": "For certain infinitely-wide neural networks, the neural tangent kernel (NTK) theory fully characterizes generalization, but for the networks used in practice, the empirical NTK only provides a rough first-order approximation. Still, a growing body of work keeps leveraging this approximation to successfully analyze important deep learning phenomena and design algorithms for new applications. In our work, we provide strong empirical evidence to determine the practical validity of such approximation by conducting a systematic comparison of the behavior of different neural networks and their linear approximations on different tasks. We show that the linear approximations can indeed rank the learning complexity of certain tasks for neural networks, even when they achieve very different performances. However, in contrast to what was previously reported, we discover that neural networks do not always perform better than their kernel approximations, and reveal that the performance gap heavily depends on architecture, dataset size and training task. We discover that networks overfit to these tasks mostly due to the evolution of their kernel during training, thus, revealing a new type of implicit bias.",
    "authors": [
      "Ortiz-Jimenez, Guillermo",
      "Moosavi-Dezfooli, Seyed-Mohsen",
      "Frossard, Pascal"
    ]
  },
  {
    "id": "4b6538a44a1dfdc2b83477cd76dee98e",
    "title": "CATs: Cost Aggregation Transformers for Visual Correspondence",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4b6538a44a1dfdc2b83477cd76dee98e-Paper.pdf",
    "abstract": "We propose a novel cost aggregation network, called Cost Aggregation Transformers (CATs), to find dense correspondences between semantically similar images with additional challenges posed by large intra-class appearance and geometric variations. Cost aggregation is a highly important process in matching tasks, which the matching accuracy depends on the quality of its output. Compared to hand-crafted or CNN-based methods addressing the cost aggregation, in that either lacks robustness to severe deformations or inherit the limitation of CNNs that fail to discriminate incorrect matches due to limited receptive fields, CATs explore global consensus among initial correlation map with the help of some architectural designs that allow us to fully leverage self-attention mechanism. Specifically, we include appearance affinity modeling to aid the cost aggregation process in order to disambiguate the noisy initial correlation maps and propose multi-level aggregation to efficiently capture different semantics from hierarchical feature representations. We then combine with swapping self-attention technique and residual connections not only to enforce consistent matching, but also to ease the learning process, which we find that these result in an apparent performance boost. We conduct experiments to demonstrate the effectiveness of the proposed model over the latest methods and provide extensive ablation studies. Code and trained models are available at https://sunghwanhong.github.io/CATs/.",
    "authors": [
      "Cho, Seokju",
      "Hong, Sunghwan",
      "Jeon, Sangryul",
      "Lee, Yunsung",
      "Sohn, Kwanghoon",
      "Kim, Seungryong"
    ]
  },
  {
    "id": "4b85256c4881edb6c0776df5d81f6236",
    "title": "Asynchronous Stochastic Optimization Robust to Arbitrary Delays",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4b85256c4881edb6c0776df5d81f6236-Paper.pdf",
    "abstract": "We consider the problem of stochastic optimization with delayed gradients in which, at each time step $t$, the algorithm makes an update using a stale stochastic gradient from step $t - d_t$ for some arbitrary delay $d_t$.   This setting abstracts asynchronous distributed optimization where a central server receives gradient updates computed by worker machines. These machines can experience computation and communication loads that might vary significantly over time.   In the general non-convex smooth optimization setting, we give a simple and efficient algorithm that requires $O( \\sigma^2/\\epsilon^4 + \\tau/\\epsilon^2 )$ steps for finding an $\\epsilon$-stationary point $x$.   Here, $\\tau$ is the \\emph{average} delay $\\frac{1}{T}\\sum_{t=1}^T d_t$ and $\\sigma^2$ is the variance of the stochastic gradients.   This improves over previous work, which showed that stochastic gradient decent achieves the same rate but with respect to the \\emph{maximal} delay $\\max_{t} d_t$, that can be significantly larger than the average delay especially in heterogeneous distributed systems.   Our experiments demonstrate the efficacy and robustness of our algorithm in cases where the delay distribution is skewed or heavy-tailed.",
    "authors": [
      "Cohen, Alon",
      "Daniely, Amit",
      "Drori, Yoel",
      "Koren, Tomer",
      "Schain, Mariano"
    ]
  },
  {
    "id": "4bb236de7787ceedafdff83bb8ea4710",
    "title": "Consistent Non-Parametric Methods for Maximizing Robustness",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4bb236de7787ceedafdff83bb8ea4710-Paper.pdf",
    "abstract": "Learning classifiers that are robust to adversarial examples has received a great deal of recent attention. A major drawback of the standard robust learning framework is the imposition of an artificial robustness radius $r$ that applies to all inputs, and ignores the fact that data may be highly heterogeneous. In particular, it is plausible that robustness regions should be larger in some regions of data, and smaller in other. In this paper, we address this limitation by proposing a new limit classifier, called the neighborhood optimal classifier, that extends the Bayes optimal classifier outside its support by using the label of the closest in-support point. We then argue that this classifier maximizes the size of its robustness regions subject to the constraint of having accuracy equal to the Bayes optimal. We then present sufficient conditions under which general non-parametric methods that can be represented as weight functions converge towards this limit object, and show that both nearest neighbors and kernel classifiers (under certain assumptions) suffice.",
    "authors": [
      "Bhattacharjee, Robi",
      "Chaudhuri, Kamalika"
    ]
  },
  {
    "id": "4bbdcc0e821637155ac4217bdab70d2e",
    "title": "Generalizable Multi-linear Attention Network",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4bbdcc0e821637155ac4217bdab70d2e-Paper.pdf",
    "abstract": "The majority of existing multimodal sequential learning methods focus on how to obtain effective representations and ignore the importance of multimodal fusion. Bilinear attention network (BAN) is a commonly used fusion method, which leverages tensor operations to associate the features of different modalities. However, BAN has a poor compatibility for more modalities, since the computational complexity of the attention map increases exponentially with the number of modalities. Based on this concern, we propose a new method called generalizable multi-linear attention network (MAN), which can associate as many modalities as possible in linear complexity with hierarchical approximation decomposition (HAD). Besides, considering the fact that softmax attention kernels cannot be decomposed as linear operation directly, we adopt the addition random features (ARF) mechanism to approximate the non-linear softmax functions with enough theoretical analysis. We conduct extensive experiments on four datasets of three tasks (multimodal sentiment analysis, multimodal speaker traits recognition, and video retrieval), the experimental results show that MAN could achieve competitive results compared with the state-of-the-art methods, showcasing the effectiveness of the approximation decomposition and addition random features mechanism.",
    "authors": [
      "Jin, Tao",
      "Zhao, Zhou"
    ]
  },
  {
    "id": "4be49c79f233b4f4070794825c323733",
    "title": "Labeling Trick: A Theory of Using Graph Neural Networks for Multi-Node Representation Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4be49c79f233b4f4070794825c323733-Paper.pdf",
    "abstract": "In this paper, we provide a theory of using graph neural networks (GNNs) for multi-node representation learning (where we are interested in learning a representation for a set of more than one node, such as link). We know that GNN is designed to learn single-node representations. When we want to learn a node set representation involving multiple nodes, a common practice in previous works is to directly aggregate the single-node representations obtained by a GNN into a joint node set representation. In this paper, we show a fundamental constraint of such an approach, namely the inability to capture the dependence between nodes in the node set, and argue that directly aggregating individual node representations does not lead to an effective joint representation for multiple nodes. Then, we notice that a few previous successful works for multi-node representation learning, including SEAL, Distance Encoding, and ID-GNN, all used node labeling. These methods first label nodes in the graph according to their relationships with the target node set before applying a GNN. Then, the node representations obtained in the labeled graph are aggregated into a node set representation. By investigating their inner mechanisms, we unify these node labeling techniques into a single and most general form---labeling trick. We prove that with labeling trick a sufficiently expressive GNN learns the most expressive node set representations, thus in principle solves any joint learning tasks over node sets. Experiments on one important two-node representation learning task, link prediction, verified our theory. Our work explains the superior performance of previous node-labeling-based methods, and establishes a theoretical foundation of using GNNs for multi-node representation learning.",
    "authors": [
      "Zhang, Muhan",
      "Li, Pan",
      "Xia, Yinglong",
      "Wang, Kai",
      "Jin, Long"
    ]
  },
  {
    "id": "4be5a36cbaca8ab9d2066debfe4e65c1",
    "title": "SUPER-ADAM: Faster and Universal Framework of Adaptive Gradients",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4be5a36cbaca8ab9d2066debfe4e65c1-Paper.pdf",
    "abstract": "Adaptive gradient methods have shown excellent performances for solving many machine learning problems. Although multiple adaptive gradient  methods were recently studied, they mainly focus on either empirical or theoretical aspects and also only work for specific problems by using some  specific adaptive learning rates. Thus, it is desired to design  a  universal  framework for practical algorithms of adaptive gradients with theoretical guarantee to solve general problems. To fill this gap, we propose a faster and universal framework of adaptive gradients (i.e., SUPER-ADAM) by introducing a universal adaptive matrix that includes most existing adaptive gradient forms. Moreover, our framework can flexibly integrate the momentum and variance reduced techniques. In particular, our novel framework provides the convergence analysis support for adaptive gradient methods under the nonconvex setting. In theoretical analysis, we prove that our SUPER-ADAM algorithm can achieve the best known gradient (i.e., stochastic first-order oracle (SFO)) complexity of $\\tilde{O}(\\epsilon^{-3})$ for finding an $\\epsilon$-stationary point of nonconvex optimization, which matches the lower bound for stochastic smooth nonconvex optimization. In numerical experiments, we employ various deep learning tasks to validate that our algorithm consistently outperforms the existing adaptive algorithms. Code is available at https://github.com/LIJUNYI95/SuperAdam",
    "authors": [
      "Huang, Feihu",
      "Li, Junyi",
      "Huang, Heng"
    ]
  },
  {
    "id": "4bfbd52f4e8466dc12aaf30b7e057b66",
    "title": "General Nonlinearities in SO(2)-Equivariant CNNs",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4bfbd52f4e8466dc12aaf30b7e057b66-Paper.pdf",
    "abstract": "Invariance under symmetry is an important problem in machine learning. Our paper looks specifically at equivariant neural networks where transformations of inputs yield homomorphic transformations of outputs. Here, steerable CNNs have emerged as the standard solution. An inherent problem of steerable representations is that general nonlinear layers break equivariance, thus restricting architectural choices. Our paper applies harmonic distortion analysis to illuminate the effect of nonlinearities on Fourier representations of SO(2). We develop a novel FFT-based algorithm for computing representations of non-linearly transformed activations while maintaining band-limitation. It yields exact equivariance for polynomial (approximations of) nonlinearities, as well as approximate solutions with tunable accuracy for general functions. We apply the approach to build a fully E(3)-equivariant network for sampled 3D surface data. In experiments with 2D and 3D data, we obtain results that compare favorably to the state-of-the-art in terms of accuracy while permitting continuous symmetry and exact equivariance.",
    "authors": [
      "Franzen, Daniel",
      "Wand, Michael"
    ]
  },
  {
    "id": "4c07fe24771249c343e70c32289c1192",
    "title": "Denoising Normalizing Flow",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4c07fe24771249c343e70c32289c1192-Paper.pdf",
    "abstract": "Normalizing flows (NF) are expressive as well as tractable density estimation methods whenever the support of the density is diffeomorphic to the entire data-space. However, real-world data sets typically live on (or very close to) low-dimensional manifolds thereby challenging the applicability of standard NF on real-world problems. Here we propose a novel method - called Denoising Normalizing Flow (DNF) - that estimates the density on the low-dimensional manifold while learning the manifold as well. The DNF works in 3 steps. First, it inflates the manifold - making it diffeomorphic to the entire data-space. Secondly, it learns an NF on the inflated manifold and finally it learns a denoising mapping - similarly to denoising autoencoders. The DNF relies on a single cost function and does not require to alternate between a density estimation phase and a manifold learning phase - as it is the case with other recent methods. Furthermore, we show that the DNF can learn meaningful low-dimensional representations from naturalistic images as well as generate high-quality samples.",
    "authors": [
      "Horvat, Christian",
      "Pfister, Jean-Pascal"
    ]
  },
  {
    "id": "4c26774d852f62440fc746ea4cdd57f6",
    "title": "Attention over Learned Object Embeddings Enables Complex Visual Reasoning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4c26774d852f62440fc746ea4cdd57f6-Paper.pdf",
    "abstract": "Neural networks have achieved success in a wide array of perceptual tasks but often fail at tasks involving both perception and higher-level reasoning. On these more challenging tasks, bespoke approaches (such as modular symbolic components, independent dynamics models or semantic parsers) targeted towards that specific type of task have typically performed better. The downside to these targeted approaches, however, is that they can be more brittle than general-purpose neural networks, requiring significant modification or even redesign according to the particular task at hand. Here, we propose a more general neural-network-based approach to dynamic visual reasoning problems that obtains state-of-the-art performance on three different domains, in each case outperforming bespoke modular approaches tailored specifically to the task. Our method relies on learned object-centric representations, self-attention and self-supervised dynamics learning, and all three elements together are required for strong performance to emerge. The success of this combination suggests that there may be no need to trade off flexibility for performance on problems involving spatio-temporal or causal-style reasoning. With the right soft biases and learning objectives in a neural network we may be able to attain the best of both worlds.    ",
    "authors": [
      "Ding, David",
      "Hill, Felix",
      "Santoro, Adam",
      "Reynolds, Malcolm",
      "Botvinick, Matt"
    ]
  },
  {
    "id": "4c27cea8526af8cfee3be5e183ac9605",
    "title": "Differentially Private Federated Bayesian Optimization with Distributed Exploration",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4c27cea8526af8cfee3be5e183ac9605-Paper.pdf",
    "abstract": "Bayesian optimization (BO) has recently been extended to the federated learning (FL) setting by the federated Thompson sampling (FTS) algorithm, which has promising applications such as federated hyperparameter tuning. However, FTS is not equipped with a rigorous privacy guarantee which is an important consideration in FL. Recent works have incorporated differential privacy (DP) into the training of deep neural networks through a general framework for adding DP to iterative algorithms. Following this general DP framework, our work here integrates DP into FTS to preserve user-level privacy. We also leverage the ability of this general DP framework to handle different parameter vectors, as well as the technique of local modeling for BO, to further improve the utility of our algorithm through distributed exploration (DE). The resulting differentially private FTS with DE (DP-FTS-DE) algorithm is endowed with theoretical guarantees for both the privacy and utility and is amenable to interesting theoretical insights about the privacy-utility trade-off. We also use real-world experiments to show that DP-FTS-DE achieves high utility (competitive performance) with a strong privacy guarantee (small privacy loss) and induces a trade-off between privacy and utility.",
    "authors": [
      "Dai, Zhongxiang",
      "Low, Bryan Kian Hsiang",
      "Jaillet, Patrick"
    ]
  },
  {
    "id": "4c4c937b67cc8d785cea1e42ccea185c",
    "title": "Differentiable Learning Under Triage",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4c4c937b67cc8d785cea1e42ccea185c-Paper.pdf",
    "abstract": "Multiple lines of evidence suggest that predictive models may benefit from algorithmic triage. Under algorithmic triage, a predictive model does not predict all instances but instead defers some of them to human experts. However, the interplay between the prediction accuracy of the model and the human experts under algorithmic triage is not well understood. In this work, we start by formally characterizing under which circumstances a predictive model may benefit from algorithmic triage. In doing so, we also demonstrate that models trained for full automation may be suboptimal under triage. Then, given any model and desired level of triage, we show that the optimal triage policy is a deterministic threshold rule in which triage decisions are derived deterministically by thresholding the difference between the model and human errors on a per-instance level. Building upon these results, we introduce a practical gradient-based algorithm that is guaranteed to find a sequence of predictive models and triage policies of increasing performance. Experiments on a wide variety of supervised learning tasks using synthetic and real data from two important applications---content moderation and scientific discovery---illustrate our theoretical results and show that the models and triage policies provided by our gradient-based algorithm outperform those provided by several competitive baselines.",
    "authors": [
      "Okati, Nastaran",
      "De, Abir",
      "Rodriguez, Manuel"
    ]
  },
  {
    "id": "4c4ea5258ef3fb3fb1fc48fee9b4408c",
    "title": "ROI Maximization in Stochastic Online Decision-Making",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4c4ea5258ef3fb3fb1fc48fee9b4408c-Paper.pdf",
    "abstract": "We introduce a novel theoretical framework for Return On Investment (ROI) maximization in repeated decision-making. Our setting is motivated by the use case of companies that regularly receive proposals for technological innovations and want to quickly decide whether they are worth implementing. We design an algorithm for learning ROI-maximizing decision-making policies over a sequence of innovation proposals. Our algorithm provably converges to an optimal policy in class $\\Pi$ at a rate of order $\\min\\big\\{1/(N\\Delta^2),N^{-1/3}\\}$, where $N$ is the number of innovations and $\\Delta$ is the suboptimality gap in $\\Pi$. A significant hurdle of our formulation, which sets it aside from other online learning problems such as bandits, is that running a policy does not provide an unbiased estimate of its performance.",
    "authors": [
      "Cesa-Bianchi, Nicol\u00f2",
      "Cesari, Tom",
      "Mansour, Yishay",
      "Perchet, Vianney"
    ]
  },
  {
    "id": "4c7a167bb329bd92580a99ce422d6fa6",
    "title": "When Expressivity Meets Trainability: Fewer than $n$ Neurons Can Work",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4c7a167bb329bd92580a99ce422d6fa6-Paper.pdf",
    "abstract": "Modern neural networks are often quite wide, causing large memory and computation costs. It is thus of great interest to train a narrower network. However, training narrow neural nets remains a challenging task. We ask two theoretical questions: Can narrow networks have as strong expressivity as wide ones? If so, does the loss function exhibit a  benign optimization landscape? In this work, we provide partially affirmative answers to both questions for 1-hidden-layer networks with fewer than $n$ (sample size) neurons when the activation is smooth.  First, we prove that as long as the width $m \\geq 2n/d$ (where $d$ is the input dimension), its expressivity is strong, i.e., there exists at least one global minimizer with zero training loss. Second, we identify a nice local region with no local-min or saddle points. Nevertheless, it is not clear whether gradient descent can stay in this nice region. Third, we consider a constrained optimization formulation where the feasible region is the nice local region, and prove that every KKT point is a nearly global minimizer. It is expected that projected gradient methods converge to KKT points under mild technical conditions, but we leave the rigorous convergence analysis to future work. Thorough numerical results show that projected gradient methods on this constrained formulation significantly outperform SGD for training narrow neural nets. ",
    "authors": [
      "Zhang, Jiawei",
      "Zhang, Yushun",
      "Hong, Mingyi",
      "Sun, Ruoyu",
      "Luo, Zhi-Quan"
    ]
  },
  {
    "id": "4ca82782c5372a547c104929f03fe7a9",
    "title": "Analyzing the Confidentiality of Undistillable Teachers in Knowledge Distillation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4ca82782c5372a547c104929f03fe7a9-Paper.pdf",
    "abstract": "Knowledge distillation (KD) has recently been identified as a method that can unintentionally leak private information regarding the details of a teacher model to an unauthorized student. Recent research in developing undistillable nasty teachers that can protect model confidentiality has gained significant attention. However, the level of protection these nasty models offer has been largely untested. In this paper, we show that transferring knowledge to a shallow sub-section of a student can largely reduce a teacher\u2019s influence. By exploring the depth of the shallow subsection, we then present a distillation technique that enables a skeptical student model to learn even from a nasty teacher. To evaluate the efficacy of our skeptical students, we conducted experiments with several models with KD on both training data-available and data-free scenarios for various datasets. While distilling from nasty teachers, compared to the normal student models, skeptical students consistently provide superior classification performance of up to \u223c59.5%. Moreover, similar to normal students, skeptical students maintain high classification accuracy when distilled from a normal teacher, showing their efficacy irrespective of the teacher being nasty or not. We believe the ability of skeptical students to largely diminish the KD-immunity of potentially nasty teachers will motivate the research community to create more robust mechanisms for model confidentiality. We have open-sourced the code at https://github.com/ksouvik52/Skeptical2021",
    "authors": [
      "Kundu, Souvik",
      "Sun, Qirui",
      "Fu, Yao",
      "Pedram, Massoud",
      "Beerel, Peter"
    ]
  },
  {
    "id": "4cb811134b9d39fc3104bd06ce75abad",
    "title": "High Probability Complexity Bounds for Line Search Based on Stochastic Oracles",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4cb811134b9d39fc3104bd06ce75abad-Paper.pdf",
    "abstract": "We consider a line-search method for continuous optimization under a stochastic setting where the function values and gradients are available only through inexact probabilistic zeroth and first-order oracles. These oracles capture multiple standard settings including expected loss minimization and zeroth-order optimization. Moreover, our framework is very general and allows the function and gradient estimates to be biased.  The proposed algorithm is simple to describe, easy to implement, and uses these oracles in a similar way as the standard deterministic line search uses exact function and gradient values.  Under fairly general conditions on the oracles, we derive a high probability tail bound on the iteration complexity of the algorithm when applied to non-convex smooth functions. These results are stronger than those for other existing stochastic line search methods and apply in more general settings. ",
    "authors": [
      "Jin, Billy",
      "Scheinberg, Katya",
      "Xie, Miaolan"
    ]
  },
  {
    "id": "4cc05b35c2f937c5bd9e7d41d3686fff",
    "title": "Pay Attention to MLPs",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4cc05b35c2f937c5bd9e7d41d3686fff-Paper.pdf",
    "abstract": "Transformers have become one of the most important architectural innovations in deep learning and have enabled many breakthroughs over the past few years. Here we propose a simple network architecture, gMLP, based solely on MLPs with gating, and show that it can perform as well as Transformers in key language and vision applications. Our comparisons show that self-attention is not critical for Vision Transformers, as gMLP can achieve the same accuracy. For BERT, our model achieves parity with Transformers on pretraining perplexity and is better on some downstream NLP tasks. On finetuning tasks where gMLP performs worse, making the gMLP model substantially larger can close the gap with Transformers. In general, our experiments show that gMLP can scale as well as Transformers over increased data and compute.",
    "authors": [
      "Liu, Hanxiao",
      "Dai, Zihang",
      "So, David",
      "Le, Quoc V"
    ]
  },
  {
    "id": "4cef5b5e6ff1b3445db4c013f1d452e0",
    "title": "An Image is Worth More Than a Thousand Words: Towards Disentanglement in The Wild",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4cef5b5e6ff1b3445db4c013f1d452e0-Paper.pdf",
    "abstract": "Unsupervised disentanglement has been shown to be theoretically impossible without inductive biases on the models and the data. As an alternative approach, recent methods rely on limited supervision to disentangle the factors of variation and allow their identifiability. While annotating the true generative factors is only required for a limited number of observations, we argue that it is infeasible to enumerate all the factors of variation that describe a real-world image distribution. To this end, we propose a method for disentangling a set of factors which are only partially labeled, as well as separating the complementary set of residual factors that are never explicitly specified. Our success in this challenging setting, demonstrated on synthetic benchmarks, gives rise to leveraging off-the-shelf image descriptors to partially annotate a subset of attributes in real image domains (e.g. of human faces) with minimal manual effort. Specifically, we use a recent language-image embedding model (CLIP) to annotate a set of attributes of interest in a zero-shot manner and demonstrate state-of-the-art disentangled image manipulation results.",
    "authors": [
      "Gabbay, Aviv",
      "Cohen, Niv",
      "Hoshen, Yedid"
    ]
  },
  {
    "id": "4cf0ed8641cfcbbf46784e620a0316fb",
    "title": "Dynamics of Stochastic Momentum Methods on Large-scale, Quadratic Models",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4cf0ed8641cfcbbf46784e620a0316fb-Paper.pdf",
    "abstract": "We analyze a class of stochastic gradient algorithms with momentum on a high-dimensional random least squares problem. Our framework, inspired by random matrix theory, provides an exact (deterministic) characterization for the sequence of function values produced by these algorithms which is expressed only in terms of the eigenvalues of the Hessian. This leads to simple expressions for nearly-optimal hyperparameters, a description of the limiting neighborhood, and average-case complexity. As a consequence, we show that (small-batch) stochastic heavy-ball momentum with a fixed momentum parameter provides no actual performance improvement over SGD when step sizes are adjusted correctly. For contrast, in the non-strongly convex setting, it is possible to get a large improvement over SGD using momentum. By introducing hyperparameters that depend on the number of samples, we propose a new algorithm sDANA (stochastic dimension adjusted Nesterov acceleration) which obtains an asymptotically optimal average-case complexity while remaining linearly convergent in the strongly convex setting without adjusting parameters.",
    "authors": [
      "Paquette, Courtney",
      "Paquette, Elliot"
    ]
  },
  {
    "id": "4d19b37a2c399deace9082d464930022",
    "title": "Adversarial Examples in Multi-Layer Random ReLU Networks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4d19b37a2c399deace9082d464930022-Paper.pdf",
    "abstract": "We consider the phenomenon of adversarial examples in ReLU networks with independent Gaussian parameters.  For networks of constant depth and with a large range of widths (for instance, it suffices if the width of each layer is polynomial in that of any other layer), small perturbations of input vectors lead to large changes of outputs.  This generalizes results of Daniely and Schacham (2020) for networks of rapidly decreasing width and of Bubeck et al (2021) for two-layer networks. Our proof shows that adversarial examples arise in these networks because the functions they compute are \\emph{locally} very similar to random linear functions. Bottleneck layers play a key role: the minimal width up to some point in the network determines scales and sensitivities of mappings computed up to that point.  The main result is for networks with constant depth, but we also show that some constraint on depth is necessary for a result of this kind, because there are suitably deep networks that, with constant probability, compute a function that is close to constant.",
    "authors": [
      "Bartlett, Peter",
      "Bubeck, Sebastien",
      "Cherapanamjeri, Yeshwanth"
    ]
  },
  {
    "id": "4d215ab7508a3e089af43fb605dd27d1",
    "title": "Efficient Statistical Assessment of Neural Network Corruption Robustness",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4d215ab7508a3e089af43fb605dd27d1-Paper.pdf",
    "abstract": "We quantify the robustness of a trained network to input uncertainties with a stochastic simulation inspired by the field of Statistical Reliability Engineering. The robustness assessment is cast as a statistical hypothesis test: the network is deemed as locally robust if the estimated probability of failure is lower than a critical level.The procedure is based on an Importance Splitting simulation generating samples of rare events. We derive theoretical guarantees that are non-asymptotic w.r.t. sample size. Experiments tackling large scale networks outline the efficiency of our method making a low number of calls to the network function. ",
    "authors": [
      "TIT, Karim",
      "Furon, Teddy",
      "ROUSSET, Mathias"
    ]
  },
  {
    "id": "4d410063822cd9be28f86701c0bc3a31",
    "title": "A Highly-Efficient Group Elastic Net Algorithm with an Application to Function-On-Scalar Regression",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4d410063822cd9be28f86701c0bc3a31-Paper.pdf",
    "abstract": "Feature Selection and Functional Data Analysis are two dynamic areas of research, with important applications in the analysis of large and complex data sets. Straddling these two areas, we propose a new highly efficient algorithm to perform Group Elastic Net with application to function-on-scalar feature selection, where a functional response is modeled against a very large number of potential scalar predictors. First, we introduce a new algorithm to solve Group Elastic Net in ultra-high dimensional settings, which exploits the sparsity structure of the Augmented Lagrangian to greatly reduce the computational burden. Next, taking advantage of the properties of Functional Principal Components, we extend our algorithm to the function-on-scalar regression framework. We use simulations to demonstrate the CPU time gains afforded by our approach compared to its best existing competitors, and present an application to data from a Genome-Wide Association Study on childhood obesity.",
    "authors": [
      "Boschi, Tobia",
      "Reimherr, Matthew",
      "Chiaromonte, Francesca"
    ]
  },
  {
    "id": "4d68e143defa221fead61c84de7527a3",
    "title": "Hierarchical Clustering: $O(1)$-Approximation for Well-Clustered Graphs",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4d68e143defa221fead61c84de7527a3-Paper.pdf",
    "abstract": "Hierarchical clustering  studies a recursive partition of a data set into clusters of successively smaller size, and is a fundamental problem in data analysis. In this work we study the cost function for hierarchical clustering introduced by Dasgupta, and present two polynomial-time approximation algorithms: Our first result is an $O(1)$-approximation algorithm for graphs of high conductance. Our simple construction bypasses complicated recursive routines of finding sparse cuts known in the literature. Our second and main result is an $O(1)$-approximation algorithm for a wide family of graphs that exhibit a well-defined structure of clusters. This result generalises the previous state-of-the-art, which holds only for graphs generated from stochastic models. The significance of our work is demonstrated by the empirical analysis on both synthetic and real-world data sets, on which our presented algorithm outperforms  the previously proposed algorithm for graphs with a well-defined cluster structure.",
    "authors": [
      "Manghiuc, Bogdan-Adrian",
      "Sun, He"
    ]
  },
  {
    "id": "4d7a968bb636e25818ff2a3941db08c1",
    "title": "Realistic evaluation of transductive few-shot learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4d7a968bb636e25818ff2a3941db08c1-Paper.pdf",
    "abstract": "Transductive inference is widely used in few-shot learning, as it leverages the statistics of the unlabeled query set of a few-shot task, typically yielding substantially better performances than its inductive counterpart.  The current few-shot benchmarks use perfectly class-balanced tasks at inference. We argue that such an artificial regularity is unrealistic, as it assumes that the marginal label probability of the testing samples is known and fixed to the uniform distribution. In fact, in realistic scenarios, the unlabeled query sets come with arbitrary and unknown label marginals.   We introduce and study the effect of arbitrary class distributions within the query sets of few-shot tasks at inference,  removing the class-balance artefact. Specifically, we model the marginal probabilities of the classes as Dirichlet-distributed random variables, which yields a principled and realistic sampling within the simplex.  This leverages the current few-shot benchmarks, building testing tasks with arbitrary class distributions. We evaluate experimentally state-of-the-art transductive methods over 3 widely used data sets, and observe, surprisingly, substantial performance drops, even below inductive methods in some cases. Furthermore, we propose a generalization of the mutual-information loss, based on \u03b1-divergences, which can handle effectively class-distribution variations. Empirically, we show that our transductive \u03b1-divergence optimization outperforms state-of-the-art methods across several data sets, models and few-shot settings.",
    "authors": [
      "Veilleux, Olivier",
      "Boudiaf, Malik",
      "Piantanida, Pablo",
      "Ben Ayed, Ismail"
    ]
  },
  {
    "id": "4d8bd3f7351f4fee76ba17594f070ddd",
    "title": "Qu-ANTI-zation: Exploiting Quantization Artifacts for Achieving Adversarial Outcomes",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4d8bd3f7351f4fee76ba17594f070ddd-Paper.pdf",
    "abstract": "Quantization is a popular technique that transforms the parameter representation of a neural network from floating-point numbers into lower-precision ones (e.g., 8-bit integers). It reduces the memory footprint and the computational cost at inference, facilitating the deployment of resource-hungry models. However, the parameter perturbations caused by this transformation result in behavioral disparities between the model before and after quantization. For example, a quantized model can misclassify some test-time samples that are otherwise classified correctly. It is not known whether such differences lead to a new security vulnerability. We hypothesize that an adversary may control this disparity to introduce specific behaviors that activate upon quantization. To study this hypothesis, we weaponize quantization-aware training and propose a new training framework to implement adversarial quantization outcomes. Following this framework, we present three attacks we carry out with quantization: (i) an indiscriminate attack for significant accuracy loss; (ii) a targeted attack against specific samples; and (iii) a backdoor attack for controlling the model with an input trigger. We further show that a single compromised model defeats multiple quantization schemes, including robust quantization techniques. Moreover, in a federated learning scenario, we demonstrate that a set of malicious participants who conspire can inject our quantization-activated backdoor. Lastly, we discuss potential counter-measures and show that only re-training consistently removes the attack artifacts. Our code is available at https://github.com/Secure-AI-Systems-Group/Qu-ANTI-zation",
    "authors": [
      "Hong, Sanghyun",
      "Panaitescu-Liess, Michael-Andrei",
      "Kaya, Yigitcan",
      "Dumitras, Tudor"
    ]
  },
  {
    "id": "4ddb5b8d603f88e9de689f3230234b47",
    "title": "Differentially Private Stochastic Optimization: New Results in Convex and Non-Convex Settings",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4ddb5b8d603f88e9de689f3230234b47-Paper.pdf",
    "abstract": "  We study differentially private stochastic optimization in convex and non-convex settings. For the convex case, we focus on the family of non-smooth generalized linear losses (GLLs). Our algorithm for the $\\ell_2$ setting achieves optimal excess population risk in near-linear time, while the best known differentially private algorithms for general convex losses run in super-linear time. Our algorithm for the $\\ell_1$ setting has nearly-optimal excess population risk $\\tilde{O}\\big(\\sqrt{\\frac{\\log{d}}{n}}\\big)$, and circumvents the dimension dependent lower bound of \\cite{Asi:2021} for general non-smooth convex losses. In the differentially private non-convex setting, we provide several new algorithms for approximating stationary points of the population risk. For the $\\ell_1$-case with smooth losses and polyhedral constraint, we provide the first nearly dimension independent rate, $\\tilde O\\big(\\frac{\\log^{2/3}{d}}{{n^{1/3}}}\\big)$ in linear time. For the constrained $\\ell_2$-case, with smooth losses, we obtain a linear-time algorithm with rate $\\tilde O\\big(\\frac{1}{n^{3/10}d^{1/10}}+\\big(\\frac{d}{n^2}\\big)^{1/5}\\big)$.   Finally, for the $\\ell_2$-case we provide the first method  for {\\em non-smooth weakly convex} stochastic optimization with rate $\\tilde O\\big(\\frac{1}{n^{1/4}}+\\big(\\frac{d}{n^2}\\big)^{1/6}\\big)$ which matches the best existing non-private algorithm when $d= O(\\sqrt{n})$. We also extend all our results above for the non-convex $\\ell_2$ setting to the $\\ell_p$ setting, where $1 < p \\leq 2$, with only polylogarithmic (in the dimension) overhead in the rates.",
    "authors": [
      "Bassily, Raef",
      "Guzm\u00e1n, Crist\u00f3bal",
      "Menart, Michael"
    ]
  },
  {
    "id": "4dea382d82666332fb564f2e711cbc71",
    "title": "TacticZero: Learning to Prove Theorems from Scratch with Deep Reinforcement Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4dea382d82666332fb564f2e711cbc71-Paper.pdf",
    "abstract": "We propose a novel approach to interactive theorem-proving (ITP) using deep reinforcement learning. The proposed framework is able to learn proof search strategies as well as tactic and arguments prediction in an end-to-end manner. We formulate the process of ITP as a Markov decision process (MDP) in which each state represents a set of potential derivation paths. This structure allows us to introduce a novel backtracking mechanism which enables the agent to efficiently discard (predicted) dead-end derivations and restart the derivation from promising alternatives. We implement the framework in the HOL theorem prover. Experimental results show that the framework using learned search strategies outperforms existing automated theorem provers (i.e., hammers) available in HOL when evaluated on unseen problems. We further elaborate the role of key components of the framework using ablation studies.",
    "authors": [
      "Wu, Minchao",
      "Norrish, Michael",
      "Walder, Christian",
      "Dezfouli, Amir"
    ]
  },
  {
    "id": "4e0223a87610176ef0d24ef6d2dcde3a",
    "title": "Integrating Tree Path in Transformer for Code Representation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4e0223a87610176ef0d24ef6d2dcde3a-Paper.pdf",
    "abstract": "Learning distributed representation of source code requires modelling its syntax and semantics. Recent state-of-the-art models leverage highly structured source code representations, such as the syntax trees and paths therein. In this paper, we investigate two representative path encoding methods shown in previous research work and integrate them into the attention module of Transformer. We draw inspiration from the ideas of positional encoding and modify them to incorporate these path encoding. Specifically, we encode both the pairwise path between tokens of source code and the path from the leaf node to the tree root for each token in the syntax tree. We explore the interaction between these two kinds of paths by integrating them into the unified Transformer framework. The detailed empirical study for path encoding methods also leads to our novel state-of-the-art representation model TPTrans, which finally outperforms strong baselines. Extensive experiments and ablation studies on code summarization across four different languages demonstrate the effectiveness of our approaches. We release our code at \\url{https://github.com/AwdHanPeng/TPTrans}.",
    "authors": [
      "Peng, Han",
      "Li, Ge",
      "Wang, Wenhan",
      "Zhao, YunFei",
      "Jin, Zhi"
    ]
  },
  {
    "id": "4e0928de075538c593fbdabb0c5ef2c3",
    "title": "Twins: Revisiting the Design of Spatial Attention in Vision Transformers",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4e0928de075538c593fbdabb0c5ef2c3-Paper.pdf",
    "abstract": "Very recently, a variety of vision transformer architectures for dense prediction tasks have been proposed and they show that the design of spatial attention is critical to their success in these tasks. In this work, we revisit the design of the spatial attention and demonstrate that a carefully devised yet simple spatial attention mechanism performs favorably against the state-of-the-art schemes. As a result, we propose two vision transformer architectures, namely, Twins- PCPVT and Twins-SVT. Our proposed architectures are highly efficient and easy to implement, only involving matrix multiplications that are highly optimized in modern deep learning frameworks. More importantly, the proposed architectures achieve excellent performance on a wide range of visual tasks including image-level classification as well as dense detection and segmentation. The simplicity and strong performance suggest that our proposed architectures may serve as stronger backbones for many vision tasks. ",
    "authors": [
      "Chu, Xiangxiang",
      "Tian, Zhi",
      "Wang, Yuqing",
      "Zhang, Bo",
      "Ren, Haibing",
      "Wei, Xiaolin",
      "Xia, Huaxia",
      "Shen, Chunhua"
    ]
  },
  {
    "id": "4e0ccd2b894f717df5ebc12f4282ee70",
    "title": "Evaluating State-of-the-Art Classification Models Against Bayes Optimality",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4e0ccd2b894f717df5ebc12f4282ee70-Paper.pdf",
    "abstract": "Evaluating the inherent difficulty of a given data-driven classification problem is important for establishing absolute benchmarks and evaluating progress in the field. To this end, a natural quantity to consider is the \\emph{Bayes error}, which measures the optimal classification error theoretically achievable for a given data distribution.  While generally an intractable quantity, we show that we can compute the exact Bayes error of generative models learned using normalizing flows. Our technique relies on a fundamental result, which states that the Bayes error is invariant under invertible transformation. Therefore, we can compute the exact Bayes error of the learned flow models by computing it for Gaussian base distributions, which can be done efficiently using Holmes-Diaconis-Ross integration. Moreover, we show that by varying the temperature of the learned flow models, we can generate synthetic datasets that closely resemble standard benchmark datasets, but with almost any desired Bayes error. We use our approach to conduct a thorough investigation of state-of-the-art classification models, and find that in some --- but not all --- cases, these models are capable of obtaining accuracy very near optimal. Finally, we use our method to evaluate the intrinsic \"hardness\" of standard benchmark datasets.",
    "authors": [
      "Theisen, Ryan",
      "Wang, Huan",
      "Varshney, Lav R.",
      "Xiong, Caiming",
      "Socher, Richard"
    ]
  },
  {
    "id": "4e0d67e54ad6626e957d15b08ae128a6",
    "title": "Data-Efficient Instance Generation from Instance Discrimination",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4e0d67e54ad6626e957d15b08ae128a6-Paper.pdf",
    "abstract": "Generative Adversarial Networks (GANs) have significantly advanced image synthesis, however, the synthesis quality drops significantly given a limited amount of training data. To improve the data efficiency of GAN training, prior work typically employs data augmentation to mitigate the overfitting of the discriminator yet still learn the discriminator with a bi-classification ($\\textit{i.e.}$, real $\\textit{vs.}$ fake) task. In this work, we propose a data-efficient Instance Generation ($\\textit{InsGen}$) method based on instance discrimination. Concretely, besides differentiating the real domain from the fake domain, the discriminator is required to distinguish every individual image, no matter it comes from the training set or from the generator. In this way, the discriminator can benefit from the infinite synthesized samples for training, alleviating the overfitting problem caused by insufficient training data. A noise perturbation strategy is further introduced to improve its discriminative power. Meanwhile, the learned instance discrimination capability from the discriminator is in turn exploited to encourage the generator for diverse generation. Extensive experiments demonstrate the effectiveness of our method on a variety of datasets and training settings. Noticeably, on the setting of $2K$ training images from the FFHQ dataset, we outperform the state-of-the-art approach with 23.5\\% FID improvement.",
    "authors": [
      "Yang, Ceyuan",
      "Shen, Yujun",
      "Xu, Yinghao",
      "Zhou, Bolei"
    ]
  },
  {
    "id": "4e246a381baf2ce038b3b0f82c7d6fb4",
    "title": "Reliable Post hoc Explanations: Modeling Uncertainty in Explainability",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4e246a381baf2ce038b3b0f82c7d6fb4-Paper.pdf",
    "abstract": "As black box explanations are increasingly being employed to establish model credibility in high stakes settings, it is important to ensure that these explanations are accurate and reliable. However, prior work demonstrates that explanations generated by state-of-the-art techniques are inconsistent, unstable, and provide very little insight into their correctness and reliability. In addition, these methods are also computationally inefficient, and require significant hyper-parameter tuning. In this paper, we address the aforementioned challenges by developing a novel Bayesian framework for generating local explanations along with their associated uncertainty. We instantiate this framework to obtain Bayesian versions of LIME and KernelSHAP which  output credible intervals for the feature importances, capturing the associated uncertainty. The resulting explanations not only enable us to make concrete inferences about their quality (e.g., there is a 95% chance that the feature importance lies within the given range), but are also highly consistent and stable. We carry out a detailed theoretical analysis that leverages the aforementioned uncertainty to estimate how many perturbations to sample, and how to sample for faster convergence.  This work makes the first attempt at addressing several critical issues with popular explanation methods in one shot, thereby generating consistent, stable, and reliable explanations with guarantees in a computationally efficient manner. Experimental evaluation with multiple real world datasets and user studies demonstrate that the efficacy of the proposed framework.",
    "authors": [
      "Slack, Dylan",
      "Hilgard, Anna",
      "Singh, Sameer",
      "Lakkaraju, Himabindu"
    ]
  },
  {
    "id": "4e2a6330465c8ffcaa696a5a16639176",
    "title": "Learning Graph Models for Retrosynthesis Prediction",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4e2a6330465c8ffcaa696a5a16639176-Paper.pdf",
    "abstract": "Retrosynthesis prediction is a fundamental problem in organic synthesis, where the task is to identify precursor molecules that can be used to synthesize a target molecule. A key consideration in building neural models for this task is aligning model design with strategies adopted by chemists. Building on this viewpoint, this paper introduces a graph-based approach that capitalizes on the idea that the graph topology of precursor molecules is largely unaltered during a chemical reaction. The model first predicts the set of graph edits transforming the target into incomplete molecules called synthons. Next, the model learns to expand synthons into complete molecules by attaching relevant leaving groups. This decomposition simplifies the architecture, making its predictions more interpretable, and also amenable to manual correction. Our model achieves a top-1 accuracy of 53.7%, outperforming previous template-free and semi-template-based methods.",
    "authors": [
      "Somnath, Vignesh Ram",
      "Bunne, Charlotte",
      "Coley, Connor",
      "Krause, Andreas",
      "Barzilay, Regina"
    ]
  },
  {
    "id": "4e4b5fbbbb602b6d35bea8460aa8f8e5",
    "title": "Differentiable Equilibrium Computation with Decision Diagrams for Stackelberg Models of Combinatorial Congestion Games",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4e4b5fbbbb602b6d35bea8460aa8f8e5-Paper.pdf",
    "abstract": "We address Stackelberg models of combinatorial congestion games (CCGs); we aim to optimize the parameters of CCGs so that the selfish behavior of non-atomic players attains desirable equilibria. This model is essential for designing such social infrastructures as traffic and communication networks. Nevertheless, computational approaches to the model have not been thoroughly studied due to two difficulties: (I) bilevel-programming structures and (II) the combinatorial nature of CCGs. We tackle them by carefully combining (I) the idea of \\textit{differentiable} optimization and (II) data structures called \\textit{zero-suppressed binary decision diagrams} (ZDDs), which can compactly represent sets of combinatorial strategies. Our algorithm numerically approximates the equilibria of CCGs, which we can differentiate with respect to parameters of CCGs by automatic differentiation. With the resulting derivatives, we can apply gradient-based methods to Stackelberg models of CCGs. Our method is tailored to induce Nesterov's acceleration and can fully utilize the empirical compactness of ZDDs. These technical advantages enable us to deal with CCGs with a vast number of combinatorial strategies. Experiments on real-world network design instances demonstrate the practicality of our method.",
    "authors": [
      "Sakaue, Shinsaku",
      "Nakamura, Kengo"
    ]
  },
  {
    "id": "4e55139e019a58e0084f194f758ffdea",
    "title": "Inverse Optimal Control Adapted to the Noise Characteristics of the Human Sensorimotor System",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4e55139e019a58e0084f194f758ffdea-Paper.pdf",
    "abstract": "Computational level explanations based on optimal feedback control with signal-dependent noise have been able to account for a vast array of phenomena in human sensorimotor behavior. However, commonly a cost function needs to be assumed for a task and the optimality of human behavior is evaluated by comparing observed and predicted trajectories. Here, we introduce inverse optimal control with signal-dependent noise, which allows inferring the cost function from observed behavior. To do so, we formalize the problem as a partially observable Markov decision process and distinguish between the agent\u2019s and the experimenter\u2019s inference problems.  Specifically, we derive a probabilistic formulation of the evolution of states and belief states and an approximation to the propagation equation in the linear-quadratic Gaussian problem with signal-dependent noise. We extend the model to the case of partial observability of state variables from the point of view of the experimenter. We show the feasibility of the approach through validation on synthetic data and application to experimental data. Our approach enables recovering the costs and benefits implicit in human sequential sensorimotor behavior, thereby reconciling normative and descriptive approaches in a computational framework.",
    "authors": [
      "Schultheis, Matthias",
      "Straub, Dominik",
      "Rothkopf, Constantin A."
    ]
  },
  {
    "id": "4e6cd95227cb0c280e99a195be5f6615",
    "title": "Deep Neural Networks as Point Estimates for Deep Gaussian Processes",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4e6cd95227cb0c280e99a195be5f6615-Paper.pdf",
    "abstract": "Neural networks and Gaussian processes are complementary in their strengths and weaknesses. Having a better understanding of their relationship comes with the promise to make each method benefit from the strengths of the other. In this work, we establish an equivalence between the forward passes of neural networks and (deep) sparse Gaussian process models. The theory we develop is based on interpreting activation functions as interdomain inducing features through a rigorous analysis of the interplay between activation functions and kernels. This results in models that can either be seen as neural networks with improved uncertainty prediction or deep Gaussian processes with increased prediction accuracy. These claims are supported by experimental results on regression and classification datasets.",
    "authors": [
      "Dutordoir, Vincent",
      "Hensman, James",
      "van der Wilk, Mark",
      "Ek, Carl Henrik",
      "Ghahramani, Zoubin",
      "Durrande, Nicolas"
    ]
  },
  {
    "id": "4e8eaf897c638d519710b1691121f8cb",
    "title": "Locality defeats the curse of dimensionality in convolutional teacher-student scenarios",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4e8eaf897c638d519710b1691121f8cb-Paper.pdf",
    "abstract": "Convolutional neural networks perform a local and translationally-invariant treatment of the data: quantifying which of these two aspects is central to their success remains a challenge. We study this problem within a teacher-student framework for kernel regression, using 'convolutional' kernels inspired by the neural tangent kernel of simple convolutional architectures of given filter size. Using heuristic methods from physics, we find in the ridgeless case that locality is key in determining the learning curve exponent $\\beta$ (that relates the test error $\\epsilon_t\\sim P^{-\\beta}$ to the size of the training set $P$), whereas translational invariance is not. In particular, if the filter size of the teacher $t$ is smaller than that of the student $s$, $\\beta$ is a function of $s$ only and does not depend on the input dimension. We confirm our predictions on $\\beta$ empirically. We conclude by proving, under a natural universality assumption, that performing kernel regression with a ridge that decreases with the size of the training set leads to similar learning curve exponents to those we obtain in the ridgeless case.",
    "authors": [
      "Favero, Alessandro",
      "Cagnetta, Francesco",
      "Wyart, Matthieu"
    ]
  },
  {
    "id": "4ea06fbc83cdd0a06020c35d50e1e89a",
    "title": "Causal Identification with Matrix Equations",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4ea06fbc83cdd0a06020c35d50e1e89a-Paper.pdf",
    "abstract": "Causal effect identification is concerned with determining whether a causal effect is computable from a combination of qualitative assumptions about the underlying system (e.g., a causal graph) and distributions collected from this system. Many identification algorithms exclusively rely on graphical criteria made of a non-trivial combination of probability axioms, do-calculus, and refined c-factorization (e.g., Lee & Bareinboim, 2020). In a sequence of increasingly sophisticated results, it has been shown how proxy variables can be used to identify certain effects that would not be otherwise recoverable in challenging scenarios through solving matrix equations (e.g., Kuroki & Pearl, 2014; Miao et al., 2018). In this paper, we develop a new causal identification algorithm which utilizes both graphical criteria and matrix equations. Specifically, we first characterize the relationships between certain graphically-driven formulae and matrix multiplications. With such characterizations, we broaden the spectrum of proxy variable based identification conditions and further propose novel intermediary criteria based on the pseudoinverse of a matrix. Finally, we devise a causal effect identification algorithm, which accepts as input a collection of marginal, conditional, and interventional distributions, integrating enriched matrix-based criteria into a graphical identification approach.",
    "authors": [
      "Lee, Sanghack",
      "Bareinboim, Elias"
    ]
  },
  {
    "id": "4eb0194ddf4d6c7a72dca4fd3149e92e",
    "title": "Private and Non-private Uniformity Testing for Ranking Data",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4eb0194ddf4d6c7a72dca4fd3149e92e-Paper.pdf",
    "abstract": "We study the problem of uniformity testing for statistical data that consists of rankings over $m$ items where the alternative class is restricted to Mallows models with single parameter. Testing ranking data is challenging because of the size of the large domain that is factorial in $m$, therefore the tester needs to take advantage of some structure of the alternative class. We show that uniform distribution can be distinguished from Mallows model with $O(m^{-1/2})$ samples based on simple pairwise statistics, which allows us to test uniformity using only two samples, if $m$ is large enough. We also consider uniformity testing with central and locally differential private (DP) constraints. We present a central DP algorithm that requires $O\\left(\\max \\{ 1/\\epsilon_0, 1/\\sqrt{m} \\} \\right)$ where $\\epsilon_0$ is the privacy budget parameter. Interestingly, our uniformity testing algorithm is  straightforward to apply in the local DP scenario by its nature, since it works with binary statistics that is extracted from the ranking data. We carry out large-scale experiments, including $m=10000$, to show that these testing algorithms scales very gracefully with the number of items.",
    "authors": [
      "Busa-Fekete, R\u00f3bert",
      "Fotakis, Dimitris",
      "Zampetakis, Emmanouil"
    ]
  },
  {
    "id": "4ebccfb3e317c7789f04f7a558df4537",
    "title": "Model-Based Reinforcement Learning via Imagination with Derived Memory",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4ebccfb3e317c7789f04f7a558df4537-Paper.pdf",
    "abstract": "Model-based reinforcement learning aims to improve the sample efficiency of policy learning by modeling the dynamics of the environment. Recently, the latent dynamics model is further developed to enable fast planning in a compact space. It summarizes the high-dimensional experiences of an agent, which mimics the memory function of humans.  Learning policies via imagination with the latent model shows great potential for solving complex tasks. However, only considering memories from the true experiences in the process of imagination could limit its advantages. Inspired by the memory prosthesis proposed by neuroscientists, we present a novel model-based reinforcement learning framework called Imagining with Derived Memory (IDM). It enables the agent to learn policy from enriched diverse imagination with prediction-reliability weight, thus improving sample efficiency and policy robustness. Experiments on various high-dimensional visual control tasks in the DMControl benchmark demonstrate that IDM outperforms previous state-of-the-art methods in terms of policy robustness and further improves the sample efficiency of the model-based method. ",
    "authors": [
      "Mu, Yao",
      "Zhuang, Yuzheng",
      "Wang, Bin",
      "Zhu, Guangxiang",
      "Liu, Wulong",
      "Chen, Jianyu",
      "Luo, Ping",
      "Li, Shengbo",
      "Zhang, Chongjie",
      "Hao, Jianye"
    ]
  },
  {
    "id": "4eff0720836a198b6174eecf02cbfdbf",
    "title": "Compositional Transformers for Scene Generation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4eff0720836a198b6174eecf02cbfdbf-Paper.pdf",
    "abstract": "We introduce the GANformer2 model, an iterative object-oriented transformer, explored for the task of generative modeling. The network incorporates strong and explicit structural priors, to reflect the compositional nature of visual scenes, and synthesizes images through a sequential process. It operates in two stages: a fast and lightweight planning phase, where we draft a high-level scene layout, followed by an attention-based execution phase, where the layout is being refined, evolving into a rich and detailed picture. Our model moves away from conventional black-box GAN architectures that feature a flat and monolithic latent space towards a transparent design that encourages efficiency, controllability and interpretability. We demonstrate GANformer2's strengths and qualities through a careful evaluation over a range of datasets, from multi-object CLEVR scenes to the challenging COCO images, showing it successfully achieves state-of-the-art performance in terms of visual quality, diversity and consistency. Further experiments demonstrate the model's disentanglement and provide a deeper insight into its generative process, as it proceeds step-by-step from a rough initial sketch, to a detailed layout that accounts for objects' depths and dependencies, and up to the final high-resolution depiction of vibrant and intricate real-world scenes. See https://github.com/dorarad/gansformer for model implementation.",
    "authors": [
      "Arad Hudson, Dor",
      "Zitnick, Larry"
    ]
  },
  {
    "id": "4f00921114932db3f8662a41b44ee68f",
    "title": "An Exponential Lower Bound for Linearly Realizable MDP with Constant Suboptimality Gap",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4f00921114932db3f8662a41b44ee68f-Paper.pdf",
    "abstract": "A fundamental question in the theory of reinforcement learning is: suppose the optimal $Q$-function lies in the linear span of a given $d$ dimensional feature mapping, is sample-efficient reinforcement learning (RL) possible? The recent and remarkable result of Weisz et al. (2020) resolves this question in the negative, providing an exponential (in $d$) sample size lower bound, which holds even if the agent has access to a generative model of the environment. One may hope that such a lower can be circumvented with an even stronger assumption that there is a \\emph{constant gap} between the optimal $Q$-value of the best action and that of the second-best action (for all states); indeed, the construction in Weisz et al. (2020) relies on having an exponentially small gap. This work resolves this subsequent question, showing that an exponential sample complexity lower bound still holds even if a constant gap is assumed.  Perhaps surprisingly, this result implies an exponential separation between the online RL setting and the generative model setting, where sample-efficient RL is in fact possible in the latter setting with a constant gap. Complementing our negative hardness result, we give two positive results showing that provably sample-efficient RL is possible either under an additional low-variance assumption or under a novel hypercontractivity assumption.",
    "authors": [
      "Wang, Yuanhao",
      "Wang, Ruosong",
      "Kakade, Sham"
    ]
  },
  {
    "id": "4f16c818875d9fcb6867c7bdc89be7eb",
    "title": "Combating Noise: Semi-supervised Learning by Region Uncertainty Quantification",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4f16c818875d9fcb6867c7bdc89be7eb-Paper.pdf",
    "abstract": "Semi-supervised learning aims to leverage a large amount of unlabeled data for performance boosting. Existing works primarily focus on image classification. In this paper, we delve into semi-supervised learning for object detection, where labeled data are more labor-intensive to collect. Current methods are easily distracted by noisy regions generated by pseudo labels. To combat the noisy labeling, we propose noise-resistant semi-supervised learning by quantifying the region uncertainty. We first investigate the adverse effects brought by different forms of noise associated with pseudo labels. Then we propose to quantify the uncertainty of regions by identifying the noise-resistant properties of regions over different strengths. By importing the region uncertainty quantification and promoting multi-peak probability distribution output, we introduce uncertainty into training and further achieve noise-resistant learning. Experiments on both PASCAL VOC and MS COCO demonstrate the extraordinary performance of our method.",
    "authors": [
      "Wang, Zhenyu",
      "Li, Ya-Li",
      "Guo, Ye",
      "Wang, Shengjin"
    ]
  },
  {
    "id": "4f284803bd0966cc24fa8683a34afc6e",
    "title": "Reducing the Covariate Shift by Mirror Samples in Cross Domain Alignment",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4f284803bd0966cc24fa8683a34afc6e-Paper.pdf",
    "abstract": "Eliminating the covariate shift cross domains is one of the common methods to deal with the issue of domain shift in visual unsupervised domain adaptation. However, current alignment methods, especially the prototype based or sample-level based methods neglect the structural properties of the underlying distribution and even break the condition of covariate shift. To relieve the limitations and conflicts, we introduce a novel concept named (virtual) mirror, which represents the equivalent sample in another domain. The equivalent sample pairs, named mirror pairs reflect the natural correspondence of the empirical distributions. Then a mirror loss, which aligns the mirror pairs cross domains, is constructed to enhance the alignment of the domains. The proposed method does not distort the internal structure of the underlying distribution. We also provide theoretical proof that the mirror samples and mirror loss have better asymptotic properties in reducing the domain shift. By applying the virtual mirror and mirror loss to the generic unsupervised domain adaptation model, we achieved consistently superior performance on several mainstream benchmarks.",
    "authors": [
      "Zhao, Yin",
      "wang, minquan",
      "Cai, Longjun"
    ]
  },
  {
    "id": "4f3d7d38d24b740c95da2b03dc3a2333",
    "title": "Permutation-Invariant Variational Autoencoder for Graph-Level Representation Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4f3d7d38d24b740c95da2b03dc3a2333-Paper.pdf",
    "abstract": "Recently, there has been great success in applying deep neural networks on graph structured data. Most work, however, focuses on either node- or graph-level supervised learning, such as node, link or graph classification or node-level unsupervised learning (e.g. node clustering). Despite its wide range of possible applications, graph-level unsupervised learning has not received much attention yet. This might be mainly attributed to the high representation complexity of graphs, which can be represented by $n!$ equivalent adjacency matrices, where $n$ is the number of nodes.In this work we address this issue by proposing a permutation-invariant variational autoencoder for graph structured data. Our proposed model indirectly learns to match the node ordering of input and output graph, without imposing a particular node ordering or performing expensive graph matching. We demonstrate the effectiveness of our proposed model for graph reconstruction, generation and interpolation and evaluate the expressive power of extracted representations for downstream graph-level classification and regression. ",
    "authors": [
      "Winter, Robin",
      "Noe, Frank",
      "Clevert, Djork-Arn\u00e9"
    ]
  },
  {
    "id": "4f5c422f4d49a5a807eda27434231040",
    "title": "Causal Abstractions of Neural Networks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4f5c422f4d49a5a807eda27434231040-Paper.pdf",
    "abstract": "Structural analysis methods (e.g., probing and feature attribution) are increasingly important tools for neural network analysis. We propose a new structural analysis method grounded in a formal theory of causal abstraction that provides rich characterizations of model-internal representations and their roles in input/output behavior. In this method, neural representations are aligned with variables in interpretable causal models, and then interchange interventions are used to experimentally verify that the neural representations have the causal properties of their aligned variables. We apply this method in a case study to analyze neural models trained on Multiply Quantified Natural Language Inference (MQNLI) corpus, a highly complex NLI dataset that was constructed with a tree-structured natural logic causal model. We discover that a BERT-based model with state-of-the-art performance successfully realizes parts of the natural logic model\u2019s causal structure, whereas a simpler baseline model fails to show any such structure, demonstrating that neural representations encode the compositional structure of MQNLI examples.",
    "authors": [
      "Geiger, Atticus",
      "Lu, Hanson",
      "Icard, Thomas",
      "Potts, Christopher"
    ]
  },
  {
    "id": "4f87658ef0de194413056248a00ce009",
    "title": "Conic Blackwell Algorithm: Parameter-Free Convex-Concave Saddle-Point Solving",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4f87658ef0de194413056248a00ce009-Paper.pdf",
    "abstract": "We develop new parameter-free and scale-free algorithms for solving convex-concave saddle-point problems. Our results are based on a new simple regret minimizer, the Conic Blackwell Algorithm$^+$ (CBA$^+$), which attains $O(1/\\sqrt{T})$ average regret. Intuitively, our approach generalizes to other decision sets of interest ideas from the Counterfactual Regret minimization (CFR$^+$) algorithm, which has very strong practical performance for solving sequential games on simplexes.We show how to implement CBA$^+$ for the simplex, $\\ell_{p}$ norm balls, and ellipsoidal confidence regions in the simplex, and we present numerical experiments for solving matrix games and distributionally robust optimization problems.Our empirical results show that CBA$^+$ is a simple algorithm that outperforms state-of-the-art methods on synthetic data and real data instances, without the need for any choice of step sizes or other algorithmic parameters.",
    "authors": [
      "Grand-Cl\u00e9ment, Julien",
      "Kroer, Christian"
    ]
  },
  {
    "id": "4fc66104f8ada6257fa55f29a2a567c7",
    "title": "3DP3: 3D Scene Perception via Probabilistic Programming",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4fc66104f8ada6257fa55f29a2a567c7-Paper.pdf",
    "abstract": "We present 3DP3, a framework for inverse graphics that uses inference in a structured generative model of objects, scenes, and images. 3DP3 uses (i) voxel models to represent the 3D shape of objects, (ii) hierarchical scene graphs to decompose scenes into objects and the contacts between them, and (iii) depth image likelihoods based on real-time graphics. Given an observed RGB-D image, 3DP3's inference algorithm infers the underlying latent 3D scene, including the object poses and a parsimonious joint parametrization of these poses, using fast bottom-up pose proposals, novel involutive MCMC updates of the scene graph structure, and, optionally, neural object detectors and pose estimators. We show that 3DP3 enables scene understanding that is aware of 3D shape, occlusion, and contact structure. Our results demonstrate that 3DP3 is more accurate at 6DoF object pose estimation from real images than deep learning baselines and shows better generalization to challenging scenes with novel viewpoints, contact, and partial observability.",
    "authors": [
      "Gothoskar, Nishad",
      "Cusumano-Towner, Marco",
      "Zinberg, Ben",
      "Ghavamizadeh, Matin",
      "Pollok, Falk",
      "Garrett, Austin",
      "Tenenbaum, Josh",
      "Gutfreund, Dan",
      "Mansinghka, Vikash"
    ]
  },
  {
    "id": "4fc7e9c4df30aafd8b7e1ab324f27712",
    "title": "Novel Upper Bounds for the Constrained Most Probable Explanation Task",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4fc7e9c4df30aafd8b7e1ab324f27712-Paper.pdf",
    "abstract": " We propose several schemes for upper bounding the optimal value of the constrained most probable explanation (CMPE) problem. Given a set of discrete random variables, two probabilistic graphical models defined over them and a real number $q$, this problem involves finding an assignment of values to all the variables such that the probability of the assignment is maximized according to the first model and is bounded by $q$ w.r.t. the second model. In prior work, it was shown that CMPE is a unifying problem with several applications and special cases including the nearest assignment problem, the decision preserving most probable explanation task and robust estimation. It was also shown that CMPE is NP-hard even on tractable models such as bounded treewidth networks and is hard for integer linear programming methods because it includes a dense global constraint. The main idea in our approach is to simplify the problem via Lagrange relaxation and decomposition to yield either a knapsack problem or the unconstrained most probable explanation (MPE) problem, and then solving the two problems, respectively using specialized knapsack algorithms and mini-buckets based upper bounding schemes. We evaluate our proposed scheme along several dimensions including quality of the bounds and computation time required on various benchmark graphical models and how it can be used to find heuristic, near-optimal feasible solutions in an example application pertaining to robust estimation and adversarial attacks on classifiers.",
    "authors": [
      "Rahman, Tahrima",
      "Rouhani, Sara",
      "Gogate, Vibhav"
    ]
  },
  {
    "id": "4ffb0d2ba92f664c2281970110a2e071",
    "title": "Why Spectral Normalization Stabilizes GANs: Analysis and Improvements",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4ffb0d2ba92f664c2281970110a2e071-Paper.pdf",
    "abstract": "Spectral normalization (SN) is a widely-used technique for improving the stability and sample quality of Generative Adversarial Networks (GANs). However, current understanding of SN's efficacy is limited. In this work, we show that SN controls two important failure modes of GAN training: exploding and vanishing gradients. Our proofs illustrate a (perhaps unintentional) connection with the successful LeCun initialization. This connection helps to explain why the most popular implementation of SN for GANs requires no hyper-parameter tuning, whereas stricter implementations of SN have poor empirical performance out-of-the-box. Unlike LeCun initialization which only controls gradient vanishing at the beginning of training, SN preserves this property throughout training. Building on this theoretical understanding, we propose a new spectral normalization technique: Bidirectional Scaled Spectral Normalization (BSSN), which incorporates insights from later improvements to LeCun initialization: Xavier initialization and Kaiming initialization. Theoretically, we show that BSSN gives better gradient control than SN. Empirically, we demonstrate that it outperforms SN in sample quality and training stability on several benchmark datasets.",
    "authors": [
      "Lin, Zinan",
      "Sekar, Vyas",
      "Fanti, Giulia"
    ]
  },
  {
    "id": "4ffbd5c8221d7c147f8363ccdc9a2a37",
    "title": "$(\\textrm{Implicit})^2$: Implicit Layers for Implicit Representations",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4ffbd5c8221d7c147f8363ccdc9a2a37-Paper.pdf",
    "abstract": "Recent research in deep learning has investigated two very different forms of ''implicitness'': implicit representations model high-frequency data such as images or 3D shapes directly via a low-dimensional neural network (often using e.g., sinusoidal bases or nonlinearities); implicit layers, in contrast, refer to techniques where the forward pass of a network is computed via non-linear dynamical systems, such as fixed-point or differential equation solutions, with the backward pass computed via the implicit function theorem.  In this work, we demonstrate that these two seemingly orthogonal concepts are remarkably well-suited for each other. In particular, we show that by exploiting fixed-point implicit layer to model implicit representations, we can substantially improve upon the performance of the conventional explicit-layer-based approach. Additionally, as implicit representation networks are typically trained in large-batch settings, we propose to leverage the property of implicit layers to amortize the cost of fixed-point forward/backward passes over training steps -- thereby addressing one of the primary challenges with implicit layers (that many iterations are required for the black-box fixed-point solvers). We empirically evaluated our method on learning multiple implicit representations for images, videos and audios, showing that our $(\\textrm{Implicit})^2$ approach substantially improve upon existing models while being both faster to train and much more memory efficient.",
    "authors": [
      "Huang, Zhichun",
      "Bai, Shaojie",
      "Kolter, J. Zico"
    ]
  },
  {
    "id": "500ee9106e0e4d8f769fadfdf9f2837e",
    "title": "Mean-based Best Arm Identification in Stochastic Bandits under Reward Contamination",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/500ee9106e0e4d8f769fadfdf9f2837e-Paper.pdf",
    "abstract": "This paper investigates the problem of best arm identification in {\\sl contaminated} stochastic multi-arm bandits. In this setting, the rewards obtained from any arm are replaced by samples from an adversarial model with probability $\\varepsilon$. A fixed confidence (infinite-horizon) setting is considered, where the goal of the learner is to identify the arm with the largest mean. Owing to the adversarial contamination of the rewards, each arm's mean is only partially identifiable. This paper proposes two algorithms, a gap-based algorithm and one based on the successive elimination, for best arm identification in sub-Gaussian bandits. These algorithms involve mean estimates that achieve the optimal error guarantee on the deviation of the true mean from the estimate asymptotically. Furthermore, these algorithms asymptotically achieve the optimal sample complexity. Specifically, for the gap-based algorithm, the sample complexity is asymptotically optimal up to constant factors, while for the successive elimination-based algorithm, it is optimal up to logarithmic factors. Finally, numerical experiments are provided to illustrate the gains of the algorithms compared to the existing baselines.",
    "authors": [
      "Mukherjee, Arpan",
      "Tajer, Ali",
      "Chen, Pin-Yu",
      "Das, Payel"
    ]
  },
  {
    "id": "5011bf6d8a37692913fce3a15a51f070",
    "title": "MADE: Exploration via Maximizing Deviation from Explored Regions",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5011bf6d8a37692913fce3a15a51f070-Paper.pdf",
    "abstract": "In online reinforcement learning (RL), efficient exploration remains particularly challenging in high-dimensional environments with sparse rewards. In low-dimensional environments, where tabular parameterization is possible, count-based upper confidence bound (UCB) exploration methods achieve minimax near-optimal rates. However, it remains unclear how to efficiently implement UCB in realistic RL tasks that involve non-linear function approximation. To address this, we propose a new exploration approach via maximizing the deviation of the occupancy of the next policy from the explored regions. We add this term as an adaptive regularizer to the standard RL objective to balance exploration vs. exploitation.  We pair the new objective with a provably convergent algorithm, giving rise to a new intrinsic reward that adjusts existing bonuses. The proposed intrinsic reward is easy to implement and combine with other existing RL algorithms to conduct exploration. As a proof of concept, we evaluate the new intrinsic reward on tabular examples across a variety of model-based and model-free algorithms, showing improvements over count-only exploration strategies. When tested on navigation and locomotion tasks from MiniGrid and DeepMind Control Suite benchmarks, our approach significantly improves sample efficiency over state-of-the-art methods.",
    "authors": [
      "Zhang, Tianjun",
      "Rashidinejad, Paria",
      "Jiao, Jiantao",
      "Tian, Yuandong",
      "Gonzalez, Joseph E.",
      "Russell, Stuart"
    ]
  },
  {
    "id": "503e7dbbd6217b9a591f3322f39b5a6c",
    "title": "Variational Automatic Curriculum Learning for Sparse-Reward Cooperative Multi-Agent Problems",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/503e7dbbd6217b9a591f3322f39b5a6c-Paper.pdf",
    "abstract": "We introduce an automatic curriculum algorithm, Variational Automatic Curriculum Learning (VACL), for solving challenging goal-conditioned cooperative multi-agent reinforcement learning problems. We motivate our curriculum learning paradigm through a variational perspective, where the learning objective can be decomposed into two terms: task learning on the current curriculum, and curriculum update to a new task distribution. Local optimization over the second term suggests that the curriculum should gradually expand the training tasks from easy to hard. Our VACL algorithm implements this variational paradigm with two practical components, task expansion and entity curriculum, which produces a series of training tasks over both the task configurations as well as the number of entities in the task. Experiment results show that VACL solves a collection of sparse-reward problems with a large number of agents. Particularly, using a single desktop machine, VACL achieves 98% coverage rate with 100 agents in the simple-spread benchmark and reproduces the ramp-use behavior originally shown in OpenAI\u2019s hide-and-seek project.",
    "authors": [
      "Chen, Jiayu",
      "Zhang, Yuanxin",
      "Xu, Yuanfan",
      "Ma, Huimin",
      "Yang, Huazhong",
      "Song, Jiaming",
      "Wang, Yu",
      "Wu, Yi"
    ]
  },
  {
    "id": "505259756244493872b7709a8a01b536",
    "title": "Align before Fuse: Vision and Language Representation Learning with Momentum Distillation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/505259756244493872b7709a8a01b536-Paper.pdf",
    "abstract": "Large-scale vision and language representation learning has shown promising improvements on various vision-language tasks. Most existing methods employ a transformer-based multimodal encoder to jointly model visual tokens (region-based image features) and word tokens. Because the visual tokens and word tokens are unaligned, it is challenging for the multimodal encoder to learn image-text interactions. In this paper, we introduce a contrastive loss to ALign the image and text representations BEfore Fusing (ALBEF) them through cross-modal attention, which enables more grounded vision and language representation learning. Unlike most existing methods, our method does not require bounding box annotations nor high-resolution images. In order to improve learning from noisy web data, we propose momentum distillation, a self-training method which learns from pseudo-targets produced by a momentum model. We provide a theoretical analysis of ALBEF from a mutual information maximization perspective, showing that different training tasks can be interpreted as different ways to generate views for an image-text pair. ALBEF achieves state-of-the-art performance on multiple downstream vision-language tasks. On image-text retrieval, ALBEF outperforms methods that are pre-trained on orders of magnitude larger datasets. On VQA and NLVR$^2$, ALBEF achieves absolute improvements of 2.37% and 3.84% compared to the state-of-the-art, while enjoying faster inference speed. Code and models are available at https://github.com/salesforce/ALBEF.",
    "authors": [
      "Li, Junnan",
      "Selvaraju, Ramprasaath",
      "Gotmare, Akhilesh",
      "Joty, Shafiq",
      "Xiong, Caiming",
      "Hoi, Steven Chu Hong"
    ]
  },
  {
    "id": "50a074e6a8da4662ae0a29edde722179",
    "title": "Variational Model Inversion Attacks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/50a074e6a8da4662ae0a29edde722179-Paper.pdf",
    "abstract": "Given the ubiquity of deep neural networks, it is important that these models do not reveal information about sensitive data that they have been trained on. In model inversion attacks, a malicious user attempts to recover the private dataset used to train a supervised neural network. A successful model inversion attack should generate realistic and diverse samples that accurately describe each of the classes in the private dataset. In this work, we provide a probabilistic interpretation of model inversion attacks, and formulate a variational objective that accounts for both diversity and accuracy. In order to optimize this variational objective, we choose a variational family defined in the code space of a deep generative model, trained on a public auxiliary dataset that shares some structural similarity with the target dataset.  Empirically, our method substantially improves performance in terms of target attack accuracy, sample realism, and diversity on datasets of faces and chest X-ray images. ",
    "authors": [
      "Wang, Kuan-Chieh",
      "FU, YAN",
      "Li, Ke",
      "Khisti, Ashish",
      "Zemel, Richard",
      "Makhzani, Alireza"
    ]
  },
  {
    "id": "50abc3e730e36b387ca8e02c26dc0a22",
    "title": "Graph Neural Networks with Adaptive Residual",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/50abc3e730e36b387ca8e02c26dc0a22-Paper.pdf",
    "abstract": "Graph neural networks (GNNs) have shown the power in graph representation learning for numerous tasks. In this work, we discover an interesting phenomenon that although residual connections in the message passing of GNNs help improve the performance, they immensely amplify GNNs' vulnerability against abnormal node features. This is undesirable because in real-world applications, node features in graphs could often be abnormal such as being naturally noisy or adversarially manipulated. We analyze possible reasons to understand this phenomenon and aim to design GNNs with stronger resilience to abnormal features. Our understandings motivate us to propose and derive a simple, efficient, interpretable, and adaptive message passing scheme, leading to a novel GNN with Adaptive Residual, AirGNN. Extensive experiments under various abnormal feature scenarios demonstrate the effectiveness of the proposed algorithm. ",
    "authors": [
      "Liu, Xiaorui",
      "Ding, Jiayuan",
      "Jin, Wei",
      "Xu, Han",
      "Ma, Yao",
      "Liu, Zitao",
      "Tang, Jiliang"
    ]
  },
  {
    "id": "50d2e70cdf7dd05be85e1b8df3f8ced4",
    "title": "Efficient Active Learning for Gaussian Process Classification by Error Reduction",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/50d2e70cdf7dd05be85e1b8df3f8ced4-Paper.pdf",
    "abstract": "Active learning sequentially selects the best instance for labeling by optimizing an acquisition function to enhance data/label efficiency. The selection can be either from a discrete instance set (pool-based scenario) or a continuous instance space (query synthesis scenario). In this work, we study both active learning scenarios for Gaussian Process Classification (GPC). The existing active learning strategies that  maximize the Estimated Error Reduction (EER) aim at reducing the classification error after training with the new acquired instance in a one-step-look-ahead manner. The computation of EER-based acquisition functions is typically prohibitive as it requires retraining the GPC with every new query. Moreover, as the EER is not smooth, it can not be combined with gradient-based optimization techniques to efficiently explore the continuous instance space for query synthesis. To overcome these critical limitations, we develop computationally efficient algorithms for EER-based active learning with GPC. We derive the joint predictive distribution of label pairs as a one-dimensional integral, as a result of which the computation of the  acquisition function avoids retraining the GPC for each query, remarkably reducing the computational overhead. We also derive the gradient chain rule to efficiently calculate the gradient of the acquisition function, which leads to the first query synthesis active learning algorithm implementing EER-based strategies. Our experiments clearly demonstrate the computational efficiency of the proposed algorithms. We also benchmark our algorithms on both synthetic and real-world datasets, which show superior performance in terms of sampling efficiency compared to the existing state-of-the-art algorithms. ",
    "authors": [
      "Zhao, Guang",
      "Dougherty, Edward",
      "Yoon, Byung-Jun",
      "Alexander, Francis",
      "Qian, Xiaoning"
    ]
  },
  {
    "id": "50e207ab6946b5d78b377ae0144b9e07",
    "title": "Non-Asymptotic Analysis for Two Time-scale TDC with General Smooth Function Approximation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/50e207ab6946b5d78b377ae0144b9e07-Paper.pdf",
    "abstract": "Temporal-difference learning with gradient correction (TDC) is a two time-scale algorithm for policy evaluation in reinforcement learning. This algorithm was initially proposed with linear function approximation, and was later extended to the one with general smooth function approximation. The asymptotic convergence for the on-policy setting with general smooth function approximation was established in [Bhatnagar et al., 2009], however, the non-asymptotic convergence analysis remains unsolved due to challenges in the non-linear and two-time-scale update structure, non-convex objective function and the projection onto a time-varying tangent plane. In this paper, we develop novel techniques to address the above challenges and explicitly characterize the non-asymptotic error bound for the general off-policy setting with i.i.d. or Markovian samples, and show that it converges as fast as $\\mathcal O(1/\\sqrt T)$ (up to a factor of $\\mathcal O(\\log T)$). Our approach can be applied to a wide range of value-based reinforcement learning algorithms with general smooth function approximation.",
    "authors": [
      "Wang, Yue",
      "Zou, Shaofeng",
      "Zhou, Yi"
    ]
  },
  {
    "id": "50f3f8c42b998a48057e9d33f4144b8b",
    "title": "A Little Robustness Goes a Long Way: Leveraging Robust Features for Targeted Transfer Attacks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/50f3f8c42b998a48057e9d33f4144b8b-Paper.pdf",
    "abstract": "Adversarial examples for neural network image classifiers are known to be transferable: examples optimized to be misclassified by a source classifier are often misclassified as well by classifiers with different architectures. However, targeted adversarial examples\u2014optimized to be classified as a chosen target class\u2014tend to be less transferable between architectures. While prior research on constructing transferable targeted attacks has focused on improving the optimization procedure, in this work we examine the role of the source classifier. Here, we show that training the source classifier to be \"slightly robust\"\u2014that is, robust to small-magnitude adversarial examples\u2014substantially improves the transferability of class-targeted and representation-targeted adversarial attacks, even between architectures as different as convolutional neural networks and transformers. The results we present provide insight into the nature of adversarial examples as well as the mechanisms underlying so-called \"robust\" classifiers.",
    "authors": [
      "Springer, Jacob",
      "Mitchell, Melanie",
      "Kenyon, Garrett"
    ]
  },
  {
    "id": "51200d29d1fc15f5a71c1dab4bb54f7c",
    "title": "TriBERT: Human-centric Audio-visual Representation Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/51200d29d1fc15f5a71c1dab4bb54f7c-Paper.pdf",
    "abstract": "The recent success of transformer models in language, such as BERT, has motivated the use of such architectures for multi-modal feature learning and tasks. However, most multi-modal variants (e.g., ViLBERT) have limited themselves to visual-linguistic data. Relatively few have explored its use in audio-visual modalities, and none, to our knowledge, illustrate them in the context of granular audio-visual detection or segmentation tasks such as sound source separation and localization. In this work, we introduce TriBERT -- a transformer-based architecture, inspired by ViLBERT, which enables contextual feature learning across three modalities: vision, pose, and audio, with the use of flexible co-attention. The use of pose keypoints is inspired by recent works that illustrate that such representations can significantly boost performance in many audio-visual scenarios where often one or more persons are responsible for the sound explicitly (e.g., talking) or implicitly (e.g., sound produced as a function of human manipulating an object). From a technical perspective, as part of the TriBERT architecture, we introduce a learned visual tokenization scheme based on spatial attention and leverage weak-supervision to allow granular cross-modal interactions for visual and pose modalities. Further, we supplement learning with sound-source separation loss formulated across all three streams. We pre-train our model on the large MUSIC21 dataset and demonstrate improved performance in audio-visual sound source separation on that dataset as well as other datasets through fine-tuning. In addition, we show that the learned TriBERT representations are generic and significantly improve performance on other audio-visual tasks such as cross-modal audio-visual-pose retrieval by as much as 66.7% in top-1 accuracy. ",
    "authors": [
      "Rahman, Tanzila",
      "Yang, Mengyu",
      "Sigal, Leonid"
    ]
  },
  {
    "id": "51311013e51adebc3c34d2cc591fefee",
    "title": "How does a Neural Network's Architecture Impact its Robustness to Noisy Labels?",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/51311013e51adebc3c34d2cc591fefee-Paper.pdf",
    "abstract": "Noisy labels are inevitable in large real-world datasets. In this work, we explore an area understudied by previous works --- how the network's architecture impacts its robustness to noisy labels. We provide a formal framework connecting the robustness of a network to the alignments between its architecture and target/noise functions. Our framework measures a network's robustness via the predictive power in its representations --- the test performance of a linear model trained on the learned representations using a small set of clean labels. We hypothesize that a network is more robust to noisy labels if its architecture is more aligned with the target function than the noise. To support our hypothesis, we provide both theoretical and empirical evidence across various neural network architectures and different domains. We also find that when the network is well-aligned with the target function, its predictive power in representations could improve upon state-of-the-art (SOTA) noisy-label-training methods in terms of test accuracy and even outperform sophisticated methods that use clean labels.",
    "authors": [
      "Li, Jingling",
      "Zhang, Mozhi",
      "Xu, Keyulu",
      "Dickerson, John",
      "Ba, Jimmy"
    ]
  },
  {
    "id": "514a70448c235ccb8b6842ef5e02ad3b",
    "title": "Calibration and Consistency of Adversarial Surrogate Losses",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/514a70448c235ccb8b6842ef5e02ad3b-Paper.pdf",
    "abstract": "Adversarial robustness is an increasingly critical property of classifiers in applications. The design of robust algorithms relies on surrogate losses since the optimization of the adversarial loss with most hypothesis sets is NP-hard. But, which surrogate losses should be used and when do they benefit from theoretical guarantees? We present an extensive study of this question, including a detailed analysis of the $\\mathcal{H}$-calibration and $\\mathcal{H}$-consistency of adversarial surrogate losses. We show that convex loss functions, or the supremum-based convex losses often used in applications, are not $\\mathcal{H}$-calibrated for common hypothesis sets used in machine learning. We then give a characterization of $\\mathcal{H}$-calibration and prove that some surrogate losses are indeed $\\mathcal{H}$-calibrated for the adversarial zero-one loss, with common hypothesis sets. In particular, we fix some calibration results presented in prior work for a family of linear models and significantly generalize the results to the nonlinear hypothesis sets. Next, we show that $\\mathcal{H}$-calibration is not sufficient to guarantee consistency and prove that, in the absence of any distributional assumption, no continuous surrogate loss is consistent in the adversarial setting. This, in particular, proves that a claim made in prior work is inaccurate. Next, we identify natural conditions under which some surrogate losses that we describe in detail are $\\mathcal{H}$-consistent. We also report a series of empirical results which show that many $\\mathcal{H}$-calibrated surrogate losses are indeed not $\\mathcal{H}$-consistent, and validate our theoretical assumptions. Our adversarial $\\mathcal{H}$-consistency results are novel, even for the case where $\\mathcal{H}$ is the family of all measurable functions.",
    "authors": [
      "Awasthi, Pranjal",
      "Frank, Natalie",
      "Mao, Anqi",
      "Mohri, Mehryar",
      "Zhong, Yutao"
    ]
  },
  {
    "id": "517da335fd0ec2f4a25ea139d5494163",
    "title": "The Value of Information When Deciding What to Learn",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/517da335fd0ec2f4a25ea139d5494163-Paper.pdf",
    "abstract": "All sequential decision-making agents explore so as to acquire knowledge about a particular target. It is often the responsibility of the agent designer to construct this target which, in rich and complex environments, constitutes a onerous burden; without full knowledge of the environment itself, a designer may forge a sub-optimal learning target that poorly balances the amount of information an agent must acquire to identify the target against the target's associated performance shortfall. While recent work has developed a connection between learning targets and rate-distortion theory to address this challenge and empower agents that decide what to learn in an automated fashion, the proposed algorithm does not optimally tackle the equally important challenge of efficient information acquisition. In this work, building upon the seminal design principle of information-directed sampling (Russo & Van Roy, 2014), we address this shortcoming directly to couple optimal information acquisition with the optimal design of learning targets. Along the way, we offer new insights into learning targets from the literature on rate-distortion theory before turning to empirical results that confirm the value of information when deciding what to learn.",
    "authors": [
      "Arumugam, Dilip",
      "Van Roy, Benjamin"
    ]
  },
  {
    "id": "517f24c02e620d5a4dac1db388664a63",
    "title": "Co-Adaptation of Algorithmic and Implementational Innovations in Inference-based Deep Reinforcement Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/517f24c02e620d5a4dac1db388664a63-Paper.pdf",
    "abstract": "Recently many algorithms were devised for reinforcement learning (RL) with function approximation. While they have clear algorithmic distinctions, they also have many implementation differences that are algorithm-independent and sometimes under-emphasized. Such mixing of algorithmic novelty and implementation craftsmanship makes rigorous analyses of the sources of performance improvements across algorithms difficult. In this work, we focus on a series of off-policy inference-based actor-critic algorithms -- MPO, AWR, and SAC -- to decouple their algorithmic innovations and implementation decisions. We present unified derivations through a single control-as-inference objective, where we can categorize each algorithm as based on either Expectation-Maximization (EM) or direct Kullback-Leibler (KL) divergence minimization and treat the rest of specifications as implementation details. We performed extensive ablation studies, and identified substantial performance drops whenever implementation details are mismatched for algorithmic choices. These results show which implementation or code details are co-adapted and co-evolved with algorithms, and which are transferable across algorithms: as examples, we identified that tanh Gaussian policy and network sizes are highly adapted to algorithmic types, while layer normalization and ELU are critical for MPO's performances but also transfer to noticeable gains in SAC. We hope our work can inspire future work to further demystify sources of performance improvements across multiple algorithms and allow researchers to build on one another's both algorithmic and implementational innovations.",
    "authors": [
      "Furuta, Hiroki",
      "Kozuno, Tadashi",
      "Matsushima, Tatsuya",
      "Matsuo, Yutaka",
      "Gu, Shixiang (Shane)"
    ]
  },
  {
    "id": "51a472c08e21aef54ed749806e3e6490",
    "title": "Can fMRI reveal the representation of syntactic structure in the brain?",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/51a472c08e21aef54ed749806e3e6490-Paper.pdf",
    "abstract": "While studying semantics in the brain, neuroscientists use two approaches. One is to identify areas that are correlated with semantic processing load. Another is to find areas that are predicted by the semantic representation of the stimulus words. However, most studies of syntax have focused only on identifying areas correlated with syntactic processing load.  One possible reason for this discrepancy is that representing syntactic structure in an embedding space such that it can be used to model brain activity is a non-trivial computational problem. Another possible reason is that it is unclear if the low signal-to-noise ratio of neuroimaging tools such as functional Magnetic Resonance Imaging (fMRI) can allow us to reveal the correlates of complex (and perhaps subtle) syntactic representations. In this study, we propose novel multi-dimensional features that encode information about the syntactic structure of sentences. Using these features and fMRI recordings of participants reading a natural text, we model the brain representation of syntax. First, we find that our syntactic structure-based features explain additional variance in the brain activity of various parts of the language system, even after controlling for complexity metrics that capture processing load. At the same time, we see that regions well-predicted by syntactic features are distributed in the language system and are not distinguishable from those processing semantics. Our code and data will be available at https://github.com/anikethjr/brainsyntacticrepresentations.",
    "authors": [
      "Reddy, Aniketh Janardhan",
      "Wehbe, Leila"
    ]
  },
  {
    "id": "51a6ce0252d8fa6e913524bdce8db490",
    "title": "Robust Implicit Networks via Non-Euclidean Contractions",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/51a6ce0252d8fa6e913524bdce8db490-Paper.pdf",
    "abstract": "Implicit neural networks, a.k.a., deep equilibrium networks, are a class of implicit-depth learning models where function evaluation is performed by solving a fixed point equation. They generalize classic feedforward models and are equivalent to infinite-depth weight-tied feedforward networks. While implicit models show improved accuracy and significant reduction in memory consumption, they can suffer from ill-posedness and convergence instability.This paper provides a new framework, which we call Non-Euclidean Monotone Operator Network (NEMON), to design well-posed and robust implicit neural networks based upon contraction theory for the non-Euclidean norm $\\ell_\\infty$. Our framework includes (i) a novel condition for well-posedness based on one-sided Lipschitz constants, (ii) an average iteration for computing fixed-points, and (iii) explicit estimates on input-output Lipschitz constants. Additionally, we design a training problem with the well-posedness condition and the average iteration as constraints and, to achieve robust models, with the input-output Lipschitz constant as a regularizer. Our $\\ell_\\infty$ well-posedness condition leads to a larger polytopic training search space than existing conditions and our average iteration enjoys accelerated convergence. Finally, we evaluate our framework in image classification through the MNIST and the CIFAR-10 datasets. Our numerical results demonstrate improved accuracy and robustness of the implicit models with smaller input-output Lipschitz bounds. Code is available at https://github.com/davydovalexander/Non-Euclidean_Mon_Op_Net.",
    "authors": [
      "Jafarpour, Saber",
      "Davydov, Alexander",
      "Proskurnikov, Anton",
      "Bullo, Francesco"
    ]
  },
  {
    "id": "51be2fed6c55f5aa0c16ff14c140b187",
    "title": "A Kernel-based Test of Independence for Cluster-correlated Data",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/51be2fed6c55f5aa0c16ff14c140b187-Paper.pdf",
    "abstract": "The Hilbert-Schmidt Independence Criterion (HSIC) is a powerful kernel-based statistic for assessing the generalized dependence between two multivariate variables. However, independence testing based on the HSIC is not directly possible for cluster-correlated data. Such a correlation pattern among the observations arises in many practical situations, e.g., family-based and longitudinal data, and requires proper accommodation. Therefore, we propose a novel HSIC-based independence test to evaluate the dependence between two multivariate variables based on cluster-correlated data. Using the previously proposed empirical HSIC as our test statistic, we derive its asymptotic distribution under the null hypothesis of independence between the two variables but in the presence of sample correlation. Based on both simulation studies and real data analysis, we show that, with clustered data, our approach effectively controls type I error and has a higher statistical power than competing methods.",
    "authors": [
      "Liu, Hongjiao",
      "Plantinga, Anna",
      "Xiang, Yunhua",
      "Wu, Michael"
    ]
  },
  {
    "id": "51e6d6e679953c6311757004d8cbbba9",
    "title": "Efficient methods for Gaussian Markov random fields under sparse linear constraints",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/51e6d6e679953c6311757004d8cbbba9-Paper.pdf",
    "abstract": "Methods for inference and simulation of linearly constrained Gaussian Markov Random Fields (GMRF) are computationally prohibitive when the number of constraints is large. In some cases, such as for intrinsic GMRFs, they may even be unfeasible. We propose a new class of methods to overcome these challenges in the common case of sparse constraints, where one has a large number of constraints and each only involves a few elements. Our methods rely on a basis transformation into blocks of constrained versus non-constrained subspaces, and we show that the methods greatly outperform existing alternatives in terms of computational cost. By combining the proposed methods with the stochastic partial differential equation approach for Gaussian random fields, we also show how to formulate Gaussian process regression with linear constraints in a GMRF setting to reduce computational cost. This is illustrated in two applications with simulated data.",
    "authors": [
      "Bolin, David",
      "Wallin, Jonas"
    ]
  },
  {
    "id": "51f15efdd170e6043fa02a74882f0470",
    "title": "Sparse is Enough in Scaling Transformers",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/51f15efdd170e6043fa02a74882f0470-Paper.pdf",
    "abstract": "Large Transformer models yield impressive results on many tasks, but are expensive to train, or even fine-tune, and so slow at decoding that their use and study becomes out of reach. We address this problem by leveraging sparsity. We study sparse variants for all layers in the Transformer and propose Scaling Transformers, a family of next generation Transformer models that use sparse layers to scale efficiently and perform unbatched decoding much faster than the standard Transformer as we scale up the model size. Surprisingly, the sparse layers are enough to obtain the same perplexity as the standard Transformer with the same number of parameters. We also integrate with prior sparsity approaches to attention and enable fast inference on long sequences even with limited memory. This results in performance competitive to the state-of-the-art on long text summarization.",
    "authors": [
      "Jaszczur, Sebastian",
      "Chowdhery, Aakanksha",
      "Mohiuddin, Afroz",
      "KAISER, LUKASZ",
      "Gajewski, Wojciech",
      "Michalewski, Henryk",
      "Kanerva, Jonni"
    ]
  },
  {
    "id": "5227b6aaf294f5f027273aebf16015f2",
    "title": "Sparse Training via Boosting Pruning Plasticity  with Neuroregeneration",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5227b6aaf294f5f027273aebf16015f2-Paper.pdf",
    "abstract": "Works on lottery ticket hypothesis (LTH) and single-shot network pruning (SNIP) have raised a lot of attention currently on post-training pruning (iterative magnitude pruning), and before-training pruning (pruning at initialization). The former method suffers from an extremely large computation cost and the latter usually struggles with insufficient performance. In comparison, during-training pruning, a class of pruning methods that simultaneously enjoys the training/inference efficiency and the comparable performance, temporarily, has been less explored. To better understand during-training pruning, we quantitatively study the effect of pruning throughout training from the perspective of pruning plasticity (the ability of the pruned networks to recover the original performance). Pruning plasticity can help explain several other empirical observations about neural network pruning in literature. We further find that pruning plasticity can be substantially improved by injecting a brain-inspired mechanism called neuroregeneration, i.e., to regenerate the same number of connections as pruned. We design a novel gradual magnitude pruning (GMP) method, named gradual pruning with zero-cost neuroregeneration (GraNet), that advances state of the art. Perhaps most impressively, its sparse-to-sparse version for the first time boosts the sparse-to-sparse training performance over various dense-to-sparse methods with ResNet-50 on ImageNet without extending the training time. We release all codes in https://github.com/Shiweiliuiiiiiii/GraNet. ",
    "authors": [
      "Liu, Shiwei",
      "Chen, Tianlong",
      "Chen, Xiaohan",
      "Atashgahi, Zahra",
      "Yin, Lu",
      "Kou, Huanyu",
      "Shen, Li",
      "Pechenizkiy, Mykola",
      "Wang, Zhangyang",
      "Mocanu, Decebal Constantin"
    ]
  },
  {
    "id": "522a9ae9a99880d39e5daec35375e999",
    "title": "Low-Fidelity Video Encoder Optimization for Temporal Action Localization",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/522a9ae9a99880d39e5daec35375e999-Paper.pdf",
    "abstract": "Most existing temporal action localization (TAL) methods rely on a transfer learning pipeline: by first optimizing a video encoder on a large action classification dataset (i.e., source domain), followed by freezing the encoder and training a TAL head on the action localization dataset (i.e., target domain). This results in a task discrepancy problem for the video encoder \u2013 trained for action classification, but used for TAL. Intuitively, joint optimization with both the video encoder and TAL head is a strong baseline solution to this discrepancy. However, this is not operable for TAL subject to the GPU memory constraints, due to the prohibitive computational cost in processing long untrimmed videos. In this paper, we resolve this challenge by introducing a novel low-fidelity (LoFi) video encoder optimization method. Instead of always using the full training configurations in TAL learning, we propose to reduce the mini-batch composition in terms of temporal, spatial, or spatio-temporal resolution so that jointly optimizing the video encoder and TAL head becomes operable under the same memory conditions of a mid-range hardware budget. Crucially, this enables the gradients to flow backwards through the video encoder conditioned on a TAL supervision loss, favourably solving the task discrepancy problem and providing more effective feature representations. Extensive experiments show that the proposed LoFi optimization approach can significantly enhance the performance of existing TAL methods. Encouragingly, even with a lightweight ResNet18 based video encoder in a single RGB stream, our method surpasses two-stream (RGB + optical-flow) ResNet50 based alternatives, often by a good margin. Our code is publicly available at https://github.com/saic-fi/lofiactionlocalization.",
    "authors": [
      "Xu, Mengmeng",
      "Perez Rua, Juan Manuel",
      "Zhu, Xiatian",
      "Ghanem, Bernard",
      "Martinez, Brais"
    ]
  },
  {
    "id": "524265e8b942930fbbe8a5d979d29205",
    "title": "On Provable Benefits of Depth in Training Graph Convolutional Networks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/524265e8b942930fbbe8a5d979d29205-Paper.pdf",
    "abstract": "Graph Convolutional Networks (GCNs) are known to suffer from performance degradation as the number of layers increases, which is usually attributed to over-smoothing. Despite the apparent consensus, we observe that there exists a discrepancy between the theoretical understanding of over-smoothing and the practical capabilities of GCNs. Specifically, we argue that over-smoothing does not necessarily happen in practice, a deeper model is provably expressive, can converge to global optimum with linear convergence rate, and achieve very high training accuracy as long as properly trained. Despite being capable of achieving high training accuracy, empirical results show that the deeper models generalize poorly on the testing stage and existing theoretical understanding of such behavior remains elusive. To achieve better understanding, we carefully analyze the generalization capability of GCNs, and show that the training strategies to achieve high training accuracy significantly deteriorate the generalization capability of GCNs. Motivated by these findings, we propose a decoupled structure for GCNs that detaches weight matrices from feature propagation to preserve the expressive power and ensure good generalization performance. We conduct empirical evaluations on various synthetic and real-world datasets to validate the correctness of our theory.",
    "authors": [
      "Cong, Weilin",
      "Ramezani, Morteza",
      "Mahdavi, Mehrdad"
    ]
  },
  {
    "id": "5248e5118c84beea359b6ea385393661",
    "title": " Practical Near Neighbor Search via Group Testing",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5248e5118c84beea359b6ea385393661-Paper.pdf",
    "abstract": "We present a new algorithm for the approximate near neighbor problem that combines classical ideas from group testing with locality-sensitive hashing (LSH). We reduce the near neighbor search problem to a group testing problem by designating neighbors as \"positives,\" non-neighbors as \"negatives,\" and approximate membership queries as group tests. We instantiate this framework using distance-sensitive Bloom Filters to Identify Near-Neighbor Groups (FLINNG). We prove that FLINNG has sub-linear query time and show that our algorithm comes with a variety of practical advantages. For example, FLINNG can be constructed in a single pass through the data, consists entirely of efficient integer operations, and does not require any distance computations. We conduct large-scale experiments on high-dimensional search tasks such as genome search, URL similarity search, and embedding search over the massive YFCC100M dataset. In our comparison with leading algorithms such as HNSW and FAISS, we find that FLINNG can provide up to a 10x query speedup with substantially smaller indexing time and memory.",
    "authors": [
      "Engels, Joshua",
      "Coleman, Benjamin",
      "Shrivastava, Anshumali"
    ]
  },
  {
    "id": "525b8410cc8612283c9ecaf9a319f8ed",
    "title": "Baby Intuitions Benchmark (BIB):  Discerning the goals, preferences, and actions of others",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/525b8410cc8612283c9ecaf9a319f8ed-Paper.pdf",
    "abstract": "To achieve human-like common sense about everyday life, machine learning systems must understand and reason about the goals, preferences, and actions of other agents in the environment. By the end of their first year of life, human infants intuitively achieve such common sense, and these cognitive achievements lay the foundation for humans' rich and complex understanding of the mental states of others. Can machines achieve generalizable, commonsense reasoning about other agents like human infants? The Baby Intuitions Benchmark (BIB) challenges machines to predict the plausibility of an agent's behavior based on the underlying causes of its actions. Because BIB's content and paradigm are adopted from developmental cognitive science, BIB allows for direct comparison between human and machine performance. Nevertheless, recently proposed, deep-learning-based agency reasoning models fail to show infant-like reasoning, leaving BIB an open challenge.",
    "authors": [
      "Gandhi, Kanishk",
      "Stojnic, Gala",
      "Lake, Brenden M.",
      "Dillon, Moira R"
    ]
  },
  {
    "id": "5291822d0636dc429e80e953c58b6a76",
    "title": "Neural Hybrid Automata: Learning Dynamics With Multiple Modes and Stochastic Transitions",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5291822d0636dc429e80e953c58b6a76-Paper.pdf",
    "abstract": "Effective control and prediction of dynamical systems require appropriate handling of continuous-time and discrete, event-triggered processes. Stochastic hybrid systems (SHSs), common across engineering domains, provide a formalism for dynamical systems subject to discrete, possibly stochastic, state jumps and multi-modal continuous-time flows. Despite the versatility and importance of SHSs across applications, a general procedure for the explicit learning of both discrete events and multi-mode continuous dynamics remains an open problem. This work introduces Neural Hybrid Automata (NHAs), a recipe for learning SHS dynamics without a priori knowledge on the number, mode parameters, and inter-modal transition dynamics. NHAs provide a systematic inference method based on normalizing flows, neural differential equations, and self-supervision. We showcase NHAs on several tasks, including mode recovery and flow learning in systems with stochastic transitions, and end-to-end learning of hierarchical robot controllers.",
    "authors": [
      "Poli, Michael",
      "Massaroli, Stefano",
      "Scimeca, Luca",
      "Chun, Sanghyuk",
      "Oh, Seong Joon",
      "Yamashita, Atsushi",
      "Asama, Hajime",
      "Park, Jinkyoo",
      "Garg, Animesh"
    ]
  },
  {
    "id": "52aaa62e71f829d41d74892a18a11d59",
    "title": "Fast Projection onto the Capped Simplex with Applications to Sparse Regression in Bioinformatics",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/52aaa62e71f829d41d74892a18a11d59-Paper.pdf",
    "abstract": "We consider the problem of projecting a vector onto the so-called k-capped simplex, which is a hyper-cube cut by a hyperplane.For an n-dimensional input vector with bounded elements, we found that a simple algorithm based on Newton's method is able to solve the projection problem to high precision with a complexity roughly about O(n), which has a much lower computational cost compared with the existing sorting-based methods proposed in the literature.We provide a theory for partial explanation and justification of the method.We demonstrate that the proposed algorithm can produce a solution of the projection problem with high precision on large scale datasets, and the algorithm is able to significantly outperform the state-of-the-art methods in terms of runtime (about 6-8 times faster than a commercial software with respect to CPU time for input vector with 1 million variables or more).We further illustrate the effectiveness of the proposed algorithm on solving sparse regression in a bioinformatics problem.Empirical results on the GWAS dataset (with 1,500,000 single-nucleotide polymorphisms) show that, when using the proposed method to accelerate the Projected Quasi-Newton (PQN) method, the accelerated PQN algorithm is able to handle huge-scale regression problem and it is more efficient (about 3-6 times faster) than the current state-of-the-art methods.",
    "authors": [
      "Ang, Man Shun",
      "Ma, Jianzhu",
      "Liu, Nianjun",
      "Huang, Kun",
      "Wang, Yijie"
    ]
  },
  {
    "id": "52c4608c2f126708211b9e0a60eaf050",
    "title": "The Many Faces of Adversarial Risk",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/52c4608c2f126708211b9e0a60eaf050-Paper.pdf",
    "abstract": "Adversarial risk quantifies the performance of classifiers on adversarially perturbed data. Numerous definitions of adversarial risk---not all mathematically rigorous and differing subtly in the details---have appeared in the literature. In this paper, we revisit these definitions, make them rigorous, and critically examine their similarities and differences. Our technical tools derive from optimal transport, robust statistics, functional analysis, and game theory. Our contributions include the following: generalizing Strassen\u2019s theorem to the unbalanced optimal transport setting with applications to adversarial classification with unequal priors; showing an equivalence between adversarial robustness and robust hypothesis testing with $\\infty$-Wasserstein uncertainty sets; proving the existence of a pure Nash equilibrium in the two-player game between the adversary and the algorithm; and characterizing adversarial risk by the minimum Bayes error between distributions belonging to the $\\infty$-Wasserstein uncertainty sets. Our results generalize and deepen recently discovered connections between optimal transport and adversarial robustness and reveal new connections to Choquet capacities and game theory.",
    "authors": [
      "Pydi, Muni Sreenivas",
      "Jog, Varun"
    ]
  },
  {
    "id": "52fc2aee802efbad698503d28ebd3a1f",
    "title": "Meta-Adaptive Nonlinear Control: Theory and Algorithms",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/52fc2aee802efbad698503d28ebd3a1f-Paper.pdf",
    "abstract": "We present an online multi-task learning approach for adaptive nonlinear control, which we call Online Meta-Adaptive Control (OMAC). The goal is to control a nonlinear system subject to adversarial disturbance and unknown \\emph{environment-dependent} nonlinear dynamics, under the assumption that the environment-dependent dynamics can be well captured with some shared representation. Our approach is motivated by robot control, where a robotic system encounters a sequence of new environmental conditions that it must quickly adapt to. A key emphasis is to integrate online representation learning with established methods from control theory, in order to arrive at a unified framework that yields both control-theoretic and learning-theoretic guarantees. We provide instantiations of our approach under varying conditions, leading to the first non-asymptotic end-to-end convergence guarantee for multi-task nonlinear control. OMAC can also be integrated with deep representation learning. Experiments show that OMAC significantly outperforms conventional adaptive control approaches which do not learn the shared representation, in inverted pendulum and 6-DoF drone control tasks under varying wind conditions.",
    "authors": [
      "Shi, Guanya",
      "Azizzadenesheli, Kamyar",
      "O'Connell, Michael",
      "Chung, Soon-Jo",
      "Yue, Yisong"
    ]
  },
  {
    "id": "531db99cb00833bcd414459069dc7387",
    "title": "Compositional Reinforcement Learning from Logical Specifications",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/531db99cb00833bcd414459069dc7387-Paper.pdf",
    "abstract": "We study the problem of learning control policies for complex tasks given by logical specifications. Recent approaches automatically generate a reward function from a given specification and use a suitable reinforcement learning algorithm to learn a policy that maximizes the expected reward. These approaches, however, scale poorly to complex tasks that require high-level planning. In this work, we develop a compositional learning approach, called DIRL, that interleaves high-level planning and reinforcement learning. First, DIRL encodes the specification as an abstract graph; intuitively, vertices and edges of the graph correspond to regions of the state space and simpler sub-tasks, respectively. Our approach then incorporates reinforcement learning to learn neural network policies for each edge (sub-task) within a Dijkstra-style planning algorithm to compute a high-level plan in the graph. An evaluation of the proposed approach on a set of challenging control benchmarks with continuous state and action spaces demonstrates that it outperforms state-of-the-art baselines.",
    "authors": [
      "Jothimurugan, Kishor",
      "Bansal, Suguman",
      "Bastani, Osbert",
      "Alur, Rajeev"
    ]
  },
  {
    "id": "532923f11ac97d3e7cb0130315b067dc",
    "title": "Differentiable Quality Diversity",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/532923f11ac97d3e7cb0130315b067dc-Paper.pdf",
    "abstract": "Quality diversity (QD) is a growing branch of stochastic optimization research that studies the problem of generating an archive of solutions that maximize a given objective function but are also diverse with respect to a set of specified measure functions. However, even when these functions are differentiable, QD algorithms treat them as \"black boxes\", ignoring gradient information. We present the differentiable quality diversity (DQD) problem, a special case of QD, where both the objective and measure functions are first order differentiable. We then present MAP-Elites via a Gradient Arborescence (MEGA), a DQD algorithm that leverages gradient information to efficiently explore the joint range of the objective and measure functions. Results in two QD benchmark domains and in searching the latent space of a StyleGAN show that MEGA significantly outperforms state-of-the-art QD algorithms, highlighting DQD's promise for efficient quality diversity optimization when gradient information is available. Source code is available at https://github.com/icaros-usc/dqd.",
    "authors": [
      "Fontaine, Matthew",
      "Nikolaidis, Stefanos"
    ]
  },
  {
    "id": "532b81fa223a1b1ec74139a5b8151d12",
    "title": "Credit Assignment Through Broadcasting a Global Error Vector",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/532b81fa223a1b1ec74139a5b8151d12-Paper.pdf",
    "abstract": "Backpropagation (BP) uses detailed, unit-specific feedback to train deep neural networks (DNNs) with remarkable success. That biological neural circuits appear to perform credit assignment, but cannot implement BP, implies the existence of other powerful learning algorithms. Here, we explore the extent to which a globally broadcast learning signal, coupled with local weight updates, enables training of DNNs. We present both a learning rule, called global error-vector broadcasting (GEVB), and a class of DNNs, called vectorized nonnegative networks (VNNs), in which this learning rule operates. VNNs have vector-valued units and nonnegative weights past the first layer. The GEVB learning rule generalizes three-factor Hebbian learning, updating each weight by an amount proportional to the inner product of the presynaptic activation and a globally broadcast error vector when the postsynaptic unit is active. We prove that these weight updates are matched in sign to the gradient, enabling accurate credit assignment. Moreover, at initialization, these updates are exactly proportional to the gradient in the limit of infinite network width. GEVB matches the performance of BP in VNNs, and in some cases outperforms direct feedback alignment (DFA) applied in conventional networks. Unlike DFA, GEVB successfully trains convolutional layers. Altogether, our theoretical and empirical results point to a surprisingly powerful role for a global learning signal in training DNNs.",
    "authors": [
      "Clark, David",
      "Abbott, L F",
      "Chung, Sueyeon"
    ]
  },
  {
    "id": "533fa796b43291fc61a9e812a50c3fb6",
    "title": "An Online Method for A Class of Distributionally Robust Optimization with Non-convex Objectives",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/533fa796b43291fc61a9e812a50c3fb6-Paper.pdf",
    "abstract": "In this paper, we propose a practical online method for solving a class of distributional robust optimization (DRO) with non-convex objectives, which has important applications in machine learning for improving the robustness of neural networks. In the literature, most methods for solving DRO are based on stochastic primal-dual methods. However, primal-dual methods for DRO suffer from several drawbacks: (1) manipulating a high-dimensional dual variable corresponding to the size of data is time expensive; (2) they are not friendly to online learning where data is coming sequentially. To address these issues, we consider a class of DRO with an KL divergence regularization on the dual variables, transform the min-max problem into a compositional minimization problem, and propose practical duality-free online stochastic methods without requiring a large mini-batch size. We establish the state-of-the-art complexities of the proposed methods with and without a Polyak-\u0141ojasiewicz (PL) condition of the objective. Empirical studies on large-scale deep learning tasks (i) demonstrate that our method can speed up the training by more than 2 times than baseline methods and save days of training time on a large-scale dataset with \u223c 265K images, and (ii) verify the supreme performance of DRO over Empirical Risk Minimization (ERM) on imbalanced datasets. Of independent interest, the proposed method can be also used for solving a family of stochastic compositional problems with state-of-the-art complexities.",
    "authors": [
      "Qi, Qi",
      "Guo, Zhishuai",
      "Xu, Yi",
      "Jin, Rong",
      "Yang, Tianbao"
    ]
  },
  {
    "id": "536eecee295b92db6b32194e269541f8",
    "title": "A single gradient step finds adversarial examples on random two-layers neural networks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/536eecee295b92db6b32194e269541f8-Paper.pdf",
    "abstract": "Daniely and Schacham recently showed that gradient descent finds adversarial examples on random undercomplete two-layers ReLU neural networks. The term \u201cundercomplete\u201d refers to the fact that their proof only holds when the number of neurons is a vanishing fraction of the ambient dimension. We extend their result to the overcomplete case, where the number of neurons is larger than the dimension (yet also subexponential in the dimension). In fact we prove that a single step of gradient descent suffices. We also show this result for any subexponential width random neural network with smooth activation function.",
    "authors": [
      "Bubeck, Sebastien",
      "Cherapanamjeri, Yeshwanth",
      "Gidel, Gauthier",
      "Tachet des Combes, Remi"
    ]
  },
  {
    "id": "5383c7318a3158b9bc261d0b6996f7c2",
    "title": "Parameterized Knowledge Transfer for Personalized Federated Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5383c7318a3158b9bc261d0b6996f7c2-Paper.pdf",
    "abstract": "In recent years, personalized federated learning (pFL) has attracted increasing attention for its potential in dealing with statistical heterogeneity among clients. However, the state-of-the-art pFL methods rely on model parameters aggregation at the server side, which require all models to have the same structure and size, and thus limits the application for more heterogeneous scenarios. To deal with such model constraints, we exploit the potentials of heterogeneous model settings and propose a novel training framework to employ personalized models for different clients. Specifically, we formulate the aggregation procedure in original pFL into a personalized group knowledge transfer training algorithm, namely, KT-pFL, which enables each client to maintain a personalized soft prediction at the server side to guide the others' local training.  KT-pFL updates the personalized soft prediction of each client by a linear combination of all local soft predictions using a knowledge coefficient matrix, which can adaptively reinforce the collaboration among clients who own similar data distribution. Furthermore, to quantify the contributions of each client to others' personalized training, the knowledge coefficient matrix is parameterized so that it can be trained simultaneously with the models.  The knowledge coefficient matrix and the model parameters are alternatively updated in each round following the gradient descent way. Extensive experiments on various datasets (EMNIST, Fashion_MNIST, CIFAR-10) are conducted under different settings (heterogeneous models and data distributions). It is demonstrated that the proposed framework is the first federated learning paradigm that realizes personalized model training via parameterized group knowledge transfer while achieving significant performance gain comparing with state-of-the-art algorithms.",
    "authors": [
      "Zhang, Jie",
      "Guo, Song",
      "Ma, Xiaosong",
      "Wang, Haozhao",
      "Xu, Wenchao",
      "Wu, Feijie"
    ]
  },
  {
    "id": "53c5b2affa12eed84dfec9bfd83550b1",
    "title": "Contrastively Disentangled Sequential  Variational Autoencoder",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/53c5b2affa12eed84dfec9bfd83550b1-Paper.pdf",
    "abstract": "Self-supervised disentangled representation learning is a critical task in sequence modeling. The learnt representations contribute to better model interpretability as well as the data generation, and improve the sample efficiency for downstream tasks. We propose a novel sequence representation learning method, named Contrastively Disentangled Sequential Variational Autoencoder (C-DSVAE), to extract and separate the static (time-invariant) and dynamic (time-variant) factors in the latent space. Different from previous sequential variational autoencoder methods, we use a novel evidence lower bound which maximizes the mutual information between the input and the latent factors, while penalizes the mutual information between the static and dynamic factors. We leverage contrastive estimations of the mutual information terms in training, together with simple yet effective augmentation techniques, to introduce additional inductive biases. Our experiments show that C-DSVAE significantly outperforms the previous state-of-the-art methods on multiple metrics. ",
    "authors": [
      "Bai, Junwen",
      "Wang, Weiran",
      "Gomes, Carla P."
    ]
  },
  {
    "id": "53edebc543333dfbf7c5933af792c9c4",
    "title": "Recursive Causal Structure Learning in the Presence of Latent Variables and Selection Bias",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/53edebc543333dfbf7c5933af792c9c4-Paper.pdf",
    "abstract": "We consider the problem of learning the causal MAG of a system from observational data in the presence of latent variables and selection bias. Constraint-based methods are one of the main approaches for solving this problem, but the existing methods are either computationally impractical when dealing with large graphs or lacking completeness guarantees. We propose a novel computationally efficient recursive constraint-based method that is sound and complete. The key idea of our approach is that at each iteration a specific type of variable is identified and removed. This allows us to learn the structure efficiently and recursively, as this technique reduces both the number of required conditional independence (CI) tests and the size of the conditioning sets. The former substantially reduces the computational complexity, while the latter results in more reliable CI tests. We provide an upper bound on the number of required CI tests in the worst case. To the best of our knowledge, this is the tightest bound in the literature. We further provide a lower bound on the number of CI tests required by any constraint-based method.  The upper bound of our proposed approach and the lower bound at most differ by a factor equal to the number of variables in the worst case. We provide experimental results to compare the proposed approach with the state of the art on both synthetic and real-world structures.",
    "authors": [
      "Akbari, Sina",
      "Mokhtarian, Ehsan",
      "Ghassami, AmirEmad",
      "Kiyavash, Negar"
    ]
  },
  {
    "id": "543bec10c8325987595fcdc492a525f4",
    "title": "Generalization Error Rates in Kernel Regression: The Crossover from the Noiseless to Noisy Regime",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/543bec10c8325987595fcdc492a525f4-Paper.pdf",
    "abstract": "In this manuscript we consider Kernel Ridge Regression (KRR) under the Gaussian design. Exponents for the decay of the excess generalization error of KRR have been reported in various works under the assumption of power-law decay of eigenvalues of the features co-variance. These decays were, however, provided for sizeably different setups, namely in the noiseless case with constant regularization and in the noisy optimally regularized case. Intermediary settings have been left substantially uncharted. In this work, we unify and extend this line of work, providing characterization of all regimes and excess error decay rates that can be observed in terms of the interplay of noise and regularization. In particular, we show the existence of a transition in the noisy setting between the noiseless exponents to its noisy values as the sample complexity is increased. Finally, we illustrate how this crossover can also be observed on real data sets.",
    "authors": [
      "Cui, Hugo",
      "Loureiro, Bruno",
      "Krzakala, Florent",
      "Zdeborov\u00e1, Lenka"
    ]
  },
  {
    "id": "543e83748234f7cbab21aa0ade66565f",
    "title": "Learning Gaussian Mixtures with Generalized Linear Models: Precise Asymptotics in High-dimensions",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/543e83748234f7cbab21aa0ade66565f-Paper.pdf",
    "abstract": "Generalised linear models for multi-class classification problems are one of the fundamental building blocks of modern machine learning tasks. In this manuscript, we characterise the learning of a mixture of $K$ Gaussians with generic means and covariances via empirical risk minimisation (ERM) with any convex loss and regularisation. In particular, we prove exact asymptotics characterising the ERM estimator in high-dimensions, extending several previous results about Gaussian mixture classification in the literature. We exemplify our result in two tasks of interest in statistical learning: a) classification for a mixture with sparse means, where we study the efficiency of $\\ell_1$ penalty with respect to $\\ell_2$; b) max-margin multi-class classification, where we characterise the phase transition on the existence of the multi-class logistic maximum likelihood estimator for $K>2$. Finally, we discuss how our theory can be applied beyond the scope of synthetic data, showing that in different cases Gaussian mixtures capture closely the learning curve of classification tasks in real data sets.",
    "authors": [
      "Loureiro, Bruno",
      "Sicuro, Gabriele",
      "Gerbelot, Cedric",
      "Pacco, Alessandro",
      "Krzakala, Florent",
      "Zdeborov\u00e1, Lenka"
    ]
  },
  {
    "id": "5446f217e9504bc593ad9dcf2ec88dda",
    "title": "Spectral embedding for dynamic networks with stability guarantees",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5446f217e9504bc593ad9dcf2ec88dda-Paper.pdf",
    "abstract": "We consider the problem of embedding a dynamic network, to obtain time-evolving vector representations of each node, which can then be used to describe changes in behaviour of individual nodes, communities, or the entire graph. Given this open-ended remit, we argue that two types of stability in the spatio-temporal positioning of nodes are desirable: to assign the same position, up to noise, to nodes behaving similarly at a given time (cross-sectional stability) and a constant position, up to noise, to a single node behaving similarly across different times (longitudinal stability). Similarity in behaviour is defined formally using notions of exchangeability under a dynamic latent position network model. By showing how this model can be recast as a multilayer random dot product graph, we demonstrate that unfolded adjacency spectral embedding satisfies both stability conditions. We also show how two alternative methods, omnibus and independent spectral embedding, alternately lack one or the other form of stability.",
    "authors": [
      "Gallagher, Ian",
      "Jones, Andrew",
      "Rubin-Delanchy, Patrick"
    ]
  },
  {
    "id": "544defa9fddff50c53b71c43e0da72be",
    "title": "Infinite Time Horizon Safety of Bayesian Neural Networks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/544defa9fddff50c53b71c43e0da72be-Paper.pdf",
    "abstract": "Bayesian neural networks (BNNs) place distributions over the weights of a neural network to model uncertainty in the data and the network's prediction.We consider the problem of verifying safety when running a Bayesian neural network policy in a feedback loop with infinite time horizon systems.Compared to the existing sampling-based approaches, which are inapplicable to the infinite time horizon setting, we train a separate deterministic neural network that serves as an infinite time horizon safety certificate.In particular, we show that the certificate network guarantees the safety of the system over a subset of the BNN weight posterior's support. Our method first computes a safe weight set and then alters the BNN's weight posterior to reject samples outside this set. Moreover, we show how to extend our approach to a safe-exploration reinforcement learning setting, in order to avoid unsafe trajectories during the training of the policy. We evaluate our approach on a series of reinforcement learning benchmarks, including non-Lyapunovian safety specifications.",
    "authors": [
      "Lechner, Mathias",
      "\u017dikeli\u0107, \u0110or\u0111e",
      "Chatterjee, Krishnendu",
      "Henzinger, Thomas"
    ]
  },
  {
    "id": "5470abe68052c72afb19be45bb418d02",
    "title": "Towards understanding retrosynthesis by energy-based models",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5470abe68052c72afb19be45bb418d02-Paper.pdf",
    "abstract": "Retrosynthesis is the process of identifying a set of reactants to synthesize a target molecule. It is of vital importance to material design and drug discovery. Existing machine learning approaches based on language models and graph neural networks have achieved encouraging results. However, the inner connections of these models are rarely discussed, and rigorous evaluations of these models are largely in need. In this paper, we propose a framework that unifies sequence- and graph-based methods as energy-based models (EBMs) with different energy functions. This unified view establishes connections and reveals the differences between models, thereby enhancing our understanding of model design. We also provide a comprehensive assessment of performance to the community. Moreover, we present a novel dual variant within the framework that performs consistent training to induce the agreement between forward- and backward-prediction. This model improves the state-of-the-art of template-free methods with or without reaction types.  ",
    "authors": [
      "Sun, Ruoxi",
      "Dai, Hanjun",
      "Li, Li",
      "Kearnes, Steven",
      "Dai, Bo"
    ]
  },
  {
    "id": "547b85f3fafdf30856386753dc21c4e1",
    "title": "List-Decodable Mean Estimation in Nearly-PCA Time",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/547b85f3fafdf30856386753dc21c4e1-Paper.pdf",
    "abstract": "Robust statistics has traditionally focused on designing estimators tolerant to a minority of contaminated data. {\\em List-decodable learning}~\\cite{CharikarSV17} studies the more challenging regime where only a minority $\\tfrac 1 k$ fraction of the dataset, $k \\geq 2$, is drawn from the distribution of interest, and no assumptions are made on the remaining data. We study the fundamental task of list-decodable mean estimation in high dimensions. Our main result is a new algorithm for bounded covariance distributions with optimal sample complexity and near-optimal error guarantee, running in {\\em nearly-PCA time}. Assuming the ground truth distribution on $\\mathbb{R}^d$ has identity-bounded covariance, our algorithm outputs $O(k)$ candidate means, one of which is within distance $O(\\sqrt{k\\log k})$ from the truth.    Our algorithm runs in time $\\widetilde{O}(ndk)$, where $n$ is the dataset size. This runtime nearly matches the cost of performing $k$-PCA on the data, a natural bottleneck of known algorithms for (very) special cases of our problem, such as clustering well-separated mixtures. Prior to our work, the fastest runtimes were $\\widetilde{O}(n^2 d k^2)$~\\cite{DiakonikolasKK20}, and $\\widetilde{O}(nd k^C)$ \\cite{CherapanamjeriMY20} for an unspecified constant $C \\geq 6$. Our approach builds on a novel soft downweighting method we term SIFT, arguably the simplest known polynomial-time mean estimator in the list-decodable setting. To develop our fast algorithms, we boost the computational cost of SIFT via a careful ``win-win-win'' analysis of an approximate Ky Fan matrix multiplicative weights procedure we develop, which may be of independent interest.",
    "authors": [
      "Diakonikolas, Ilias",
      "Kane, Daniel",
      "Kongsgaard, Daniel",
      "Li, Jerry",
      "Tian, Kevin"
    ]
  },
  {
    "id": "5487e79fa0ccd0b79e5d4a4c8ced005d",
    "title": "Distributed Zero-Order Optimization under Adversarial Noise",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5487e79fa0ccd0b79e5d4a4c8ced005d-Paper.pdf",
    "abstract": "We study the problem of distributed zero-order optimization for a class of strongly convex functions. They are formed by the average of local objectives, associated to different nodes in a prescribed network. We propose a distributed zero-order projected gradient descent algorithm to solve the problem. Exchange of information within the network is permitted only between neighbouring nodes. An important feature of our procedure is that it can query only function values, subject to a general noise model, that does not require zero mean or independent errors.  We derive upper bounds for the average cumulative regret and optimization error of the algorithm  which highlight the role played by a network connectivity parameter, the number of variables, the noise level, the strong convexity parameter, and smoothness properties of the local objectives. The bounds indicate some key improvements of our method over the state-of-the-art, both in the distributed and standard zero-order optimization settings.",
    "authors": [
      "Akhavan, Arya",
      "Pontil, Massimiliano",
      "Tsybakov, Alexandre"
    ]
  },
  {
    "id": "54a367d629152b720749e187b3eaa11b",
    "title": "Reliable Estimation of KL Divergence using a Discriminator in Reproducing Kernel Hilbert Space",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/54a367d629152b720749e187b3eaa11b-Paper.pdf",
    "abstract": "Estimating Kullback\u2013Leibler (KL) divergence from samples of two distributions is essential in many machine learning problems. Variational methods using neural network discriminator have been proposed to achieve this task in a scalable manner. However, we noticed that most of these methods using neural network discriminators suffer from high fluctuations (variance) in estimates and instability in training. In this paper, we look at this issue from statistical learning theory and function space complexity perspective to understand why this happens and how to solve it. We argue that the cause of these pathologies is lack of control over the complexity of the neural network discriminator function and could be mitigated by controlling it. To achieve this objective, we 1) present a novel construction of the discriminator in the Reproducing Kernel Hilbert Space (RKHS), 2) theoretically relate the error probability bound of the KL estimates to the complexity of the discriminator in the RKHS space, 3) present a scalable way to control the complexity (RKHS norm) of the discriminator for a reliable estimation of KL divergence, and 4) prove the consistency of the proposed estimator. In three different applications of KL divergence -- estimation of KL, estimation of mutual information and Variational Bayes -- we show that by controlling the complexity as developed in the theory, we are able to reduce the variance of KL estimates and stabilize the training.",
    "authors": [
      "Ghimire, Sandesh",
      "Masoomi, Aria",
      "Dy, Jennifer"
    ]
  },
  {
    "id": "54b2b21af94108d83c2a909d5b0a6a50",
    "title": "Latent Matters: Learning Deep State-Space Models",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/54b2b21af94108d83c2a909d5b0a6a50-Paper.pdf",
    "abstract": "Deep state-space models (DSSMs) enable temporal predictions by learning the underlying dynamics of observed sequence data. They are often trained by maximising the evidence lower bound. However, as we show, this does not ensure the model actually learns the underlying dynamics. We therefore propose a constrained optimisation framework as a general approach for training DSSMs. Building upon this, we introduce the extended Kalman VAE (EKVAE), which combines amortised variational inference with classic Bayesian filtering/smoothing to model dynamics more accurately than RNN-based DSSMs. Our results show that the constrained optimisation framework significantly improves system identification and prediction accuracy on the example of established state-of-the-art DSSMs. The EKVAE outperforms previous models w.r.t. prediction accuracy, achieves remarkable results in identifying dynamical systems, and can furthermore successfully learn state-space representations where static and dynamic features are disentangled.",
    "authors": [
      "Klushyn, Alexej",
      "Kurle, Richard",
      "Soelch, Maximilian",
      "Cseke, Botond",
      "van der Smagt, Patrick"
    ]
  },
  {
    "id": "54e8912427a8d007ece906c577fdca60",
    "title": "On the Estimation Bias in Double Q-Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/54e8912427a8d007ece906c577fdca60-Paper.pdf",
    "abstract": "Double Q-learning is a classical method for reducing overestimation bias, which is caused by taking maximum estimated values in the Bellman operation. Its variants in the deep Q-learning paradigm have shown great promise in producing reliable value prediction and improving learning performance. However, as shown by prior work, double Q-learning is not fully unbiased and suffers from underestimation bias. In this paper, we show that such underestimation bias may lead to multiple non-optimal fixed points under an approximate Bellman operator. To address the concerns of converging to non-optimal stationary solutions, we propose a simple but effective approach as a partial fix for the underestimation bias in double Q-learning. This approach leverages an approximate dynamic programming to bound the target value. We extensively evaluate our proposed method in the Atari benchmark tasks and demonstrate its significant improvement over baseline algorithms.",
    "authors": [
      "Ren, Zhizhou",
      "Zhu, Guangxiang",
      "Hu, Hao",
      "Han, Beining",
      "Chen, Jianglun",
      "Zhang, Chongjie"
    ]
  },
  {
    "id": "54ee290e80589a2a1225c338a71839f5",
    "title": "Mitigating Forgetting in Online Continual Learning with  Neuron Calibration",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/54ee290e80589a2a1225c338a71839f5-Paper.pdf",
    "abstract": "Inspired by human intelligence, the research on online continual learning aims to push the limits of the machine learning models to constantly learn from sequentially encountered tasks, with the data from each task being observed in an online fashion. Though recent studies have achieved remarkable progress in improving the online continual learning performance empowered by the deep neural networks-based models, many of today's approaches still suffer a lot from catastrophic forgetting, a persistent challenge for continual learning. In this paper, we present a novel method which attempts to mitigate catastrophic forgetting in online continual learning from a new perspective, i.e., neuron calibration. In particular, we model the neurons in the deep neural networks-based models as calibrated units under a general formulation. Then we formalize a learning framework to effectively train the calibrated model, where neuron calibration could give ubiquitous benefit to balance the stability and plasticity of online continual learning algorithms through influencing both their forward inference path and backward optimization path.  Our proposed formulation for neuron calibration is lightweight and applicable to general feed-forward neural networks-based models. We perform extensive experiments to evaluate our method on four benchmark continual learning datasets. The results show that neuron calibration plays a vital role in improving online continual learning performance and our method could substantially improve the state-of-the-art performance on all~the~evaluated~datasets.",
    "authors": [
      "Yin, Haiyan",
      "yang, peng",
      "Li, Ping"
    ]
  },
  {
    "id": "54eea69746513c0b90bbe6227b6f46c3",
    "title": "Escaping Saddle Points with Compressed SGD",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/54eea69746513c0b90bbe6227b6f46c3-Paper.pdf",
    "abstract": "Stochastic gradient descent (SGD) is a prevalent optimization technique for large-scale distributed machine learning. While SGD computation can be efficiently divided between multiple machines, communication typically becomes a bottleneck in the distributed setting. Gradient compression methods can be used to alleviate this problem, and a recent line of work shows that SGD augmented with gradient compression converges to an $\\varepsilon$-first-order stationary point. In this paper we extend these results to convergence to an $\\varepsilon$-second-order stationary point ($\\varepsilon$-SOSP), which is to the best of our knowledge the first result of this type. In addition, we show that, when the stochastic gradient is not Lipschitz, compressed SGD with RandomK compressor converges to an $\\varepsilon$-SOSP with the same number of iterations as uncompressed SGD [Jin et al.,2021] (JACM), while improving the total communication by a factor of $\\tilde \\Theta(\\sqrt{d} \\varepsilon^{-3/4})$, where $d$ is the dimension of the optimization problem. We present additional results for the cases when the compressor is arbitrary and when the stochastic gradient is Lipschitz.",
    "authors": [
      "Avdiukhin, Dmitrii",
      "Yaroslavtsev, Grigory"
    ]
  },
  {
    "id": "54f3bc04830d762a3b56a789b6ff62df",
    "title": "Non-Gaussian Gaussian Processes for Few-Shot Regression",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/54f3bc04830d762a3b56a789b6ff62df-Paper.pdf",
    "abstract": "Gaussian Processes (GPs) have been widely used in machine learning to model distributions over functions, with applications including multi-modal regression, time-series prediction, and few-shot learning. GPs are particularly useful in the last application since they rely on Normal distributions and enable closed-form computation of the posterior probability function. Unfortunately, because the resulting posterior is not flexible enough to capture complex distributions, GPs assume high similarity between subsequent tasks - a requirement rarely met in real-world conditions. In this work, we address this limitation by leveraging the flexibility of Normalizing Flows to modulate the posterior predictive distribution of the GP. This makes the GP posterior locally non-Gaussian, therefore we name our method Non-Gaussian Gaussian Processes (NGGPs). More precisely, we propose an invertible ODE-based mapping that operates on each component of the random variable vectors and shares the parameters across all of them. We empirically tested the flexibility of NGGPs on various few-shot learning regression datasets, showing that the mapping can incorporate context embedding information to model different noise levels for periodic functions. As a result, our method shares the structure of the problem between subsequent tasks, but the contextualization allows for adaptation to dissimilarities. NGGPs outperform the competing state-of-the-art approaches on a diversified set of benchmarks and applications.",
    "authors": [
      "Sendera, Marcin",
      "Tabor, Jacek",
      "Nowak, Aleksandra",
      "Bedychaj, Andrzej",
      "Patacchiola, Massimiliano",
      "Trzcinski, Tomasz",
      "Spurek, Przemys\u0142aw",
      "Zieba, Maciej"
    ]
  },
  {
    "id": "550a141f12de6341fba65b0ad0433500",
    "title": "Believe What You See: Implicit Constraint Approach for Offline Multi-Agent Reinforcement Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/550a141f12de6341fba65b0ad0433500-Paper.pdf",
    "abstract": "Learning from datasets without interaction with environments (Offline Learning) is an essential step to apply Reinforcement Learning (RL) algorithms in real-world scenarios.However, compared with the single-agent counterpart, offline multi-agent RL introduces more agents with the larger state and action space, which is more challenging but attracts little attention. We demonstrate current offline RL algorithms are ineffective in multi-agent systems due to the accumulated extrapolation error. In this paper, we propose a novel offline RL algorithm, named Implicit Constraint Q-learning (ICQ), which effectively alleviates the extrapolation error by only trusting the state-action pairs given in the dataset for value estimation.  Moreover, we extend ICQ to multi-agent tasks by decomposing the joint-policy under the implicit constraint.  Experimental results demonstrate that the extrapolation error is successfully controlled within a reasonable range and insensitive to the number of agents. We further show that ICQ achieves the state-of-the-art performance in the challenging multi-agent offline tasks (StarCraft II). Our code is public online at https://github.com/YiqinYang/ICQ.",
    "authors": [
      "Yang, Yiqin",
      "Ma, Xiaoteng",
      "Li, Chenghao",
      "Zheng, Zewu",
      "Zhang, Qiyuan",
      "Huang, Gao",
      "Yang, Jun",
      "Zhao, Qianchuan"
    ]
  },
  {
    "id": "55563844bcd4bba067fe86ac1f008c7e",
    "title": "Online Learning in Periodic Zero-Sum Games",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/55563844bcd4bba067fe86ac1f008c7e-Paper.pdf",
    "abstract": "A seminal result in game theory is von Neumann's minmax theorem, which states that zero-sum games admit an essentially unique equilibrium solution. Classical learning results build on this theorem to show that online no-regret dynamics converge to an equilibrium in a time-average sense in zero-sum games. In the past several years, a key research direction has focused on characterizing the transient behavior of such dynamics. General results in this direction show that broad classes of online learning dynamics are cyclic, and formally Poincar\\'{e} recurrent, in zero-sum games. We analyze the robustness of these online learning behaviors in the case of periodic zero-sum games with a time-invariant equilibrium. This model generalizes the usual repeated game formulation while also being a realistic and natural model of a repeated competition between players that depends on exogenous environmental variations such as time-of-day effects, week-to-week trends, and seasonality. Interestingly, time-average convergence may fail even in the simplest such settings, in spite of the equilibrium being fixed. In contrast, using novel analysis methods, we show that Poincar\\'{e} recurrence provably generalizes despite the complex, non-autonomous nature of these dynamical systems.",
    "authors": [
      "Fiez, Tanner",
      "Sim, Ryann",
      "Skoulakis, Stratis",
      "Piliouras, Georgios",
      "Ratliff, Lillian"
    ]
  },
  {
    "id": "55a7cf9c71f1c9c495413f934dd1a158",
    "title": "K-Net: Towards Unified Image Segmentation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/55a7cf9c71f1c9c495413f934dd1a158-Paper.pdf",
    "abstract": "Semantic, instance, and panoptic segmentations have been addressed using different and specialized frameworks despite their underlying connections. This paper presents a unified, simple, and effective framework for these essentially similar tasks. The framework, named K-Net, segments both instances and semantic categories consistently by a group of learnable kernels, where each kernel is responsible for generating a mask for either a potential instance or a stuff class. To remedy the difficulties of distinguishing various instances, we propose a kernel update strategy that enables each kernel dynamic and conditional on its meaningful group in the input image. K-Net can be trained in an end-to-end manner with bipartite matching, and its training and inference are naturally NMS-free and box-free. Without bells and whistles, K-Net surpasses all previous published state-of-the-art single-model results of panoptic segmentation on MS COCO test-dev split and semantic segmentation on ADE20K val split with 55.2% PQ and 54.3% mIoU, respectively. Its instance segmentation performance is also on par with Cascade Mask R-CNN on MS COCO with 60%-90% faster inference speeds. Code and models will be released at https://github.com/ZwwWayne/K-Net/.",
    "authors": [
      "Zhang, Wenwei",
      "Pang, Jiangmiao",
      "Chen, Kai",
      "Loy, Chen Change"
    ]
  },
  {
    "id": "55a988dfb00a914717b3000a3374694c",
    "title": "Pareto-Optimal Learning-Augmented Algorithms for Online Conversion Problems",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/55a988dfb00a914717b3000a3374694c-Paper.pdf",
    "abstract": "This paper leverages machine-learned predictions to design competitive algorithms for online conversion problems with the goal of improving the competitive ratio when predictions are accurate (i.e., consistency), while also guaranteeing a worst-case competitive ratio regardless of the prediction quality (i.e., robustness). We unify the algorithmic design of both integral and fractional conversion problems, which are also known as the 1-max-search and one-way trading problems, into a class of online threshold-based algorithms (OTA). By incorporating predictions into design of OTA, we achieve the Pareto-optimal trade-off of consistency and robustness, i.e., no online algorithm can achieve a better consistency guarantee given for a robustness guarantee. We demonstrate the performance of OTA using numerical experiments on Bitcoin conversion. ",
    "authors": [
      "Sun, Bo",
      "Lee, Russell",
      "Hajiesmaili, Mohammad",
      "Wierman, Adam",
      "Tsang, Danny"
    ]
  },
  {
    "id": "55b1927fdafef39c48e5b73b5d61ea60",
    "title": "Dynaboard: An Evaluation-As-A-Service Platform for Holistic Next-Generation Benchmarking",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/55b1927fdafef39c48e5b73b5d61ea60-Paper.pdf",
    "abstract": "We introduce Dynaboard, an evaluation-as-a-service framework for hosting benchmarks and conducting holistic model comparison, integrated with the Dynabench platform. Our platform evaluates NLP models directly instead of relying on self-reported metrics or predictions on a single dataset. Under this paradigm, models are submitted to be evaluated in the cloud, circumventing the issues of reproducibility, accessibility, and backwards compatibility that often hinder benchmarking in NLP. This allows users to interact with uploaded models in real time to assess their quality, and permits the collection of additional metrics such as memory use, throughput, and robustness, which -- despite their importance to practitioners -- have traditionally been absent from leaderboards. On each task, models are ranked according to the Dynascore, a novel utility-based aggregation of these statistics, which users can customize to better reflect their preferences, placing more/less weight on a particular axis of evaluation or dataset. As state-of-the-art NLP models push the limits of traditional benchmarks, Dynaboard offers a standardized solution for a more diverse and comprehensive evaluation of model quality.",
    "authors": [
      "Ma, Zhiyi",
      "Ethayarajh, Kawin",
      "Thrush, Tristan",
      "Jain, Somya",
      "Wu, Ledell",
      "Jia, Robin",
      "Potts, Christopher",
      "Williams, Adina",
      "Kiela, Douwe"
    ]
  },
  {
    "id": "55d99a37b2e1badba7c8df4ccd506a88",
    "title": "NTopo: Mesh-free Topology Optimization using Implicit Neural Representations",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/55d99a37b2e1badba7c8df4ccd506a88-Paper.pdf",
    "abstract": "Recent advances in implicit neural representations show great promise when it comes to generating numerical solutions to partial differential equations. Compared to conventional alternatives, such representations employ parameterized neural networks to define, in a mesh-free manner, signals that are highly-detailed, continuous, and fully differentiable. In this work, we present a novel machine learning approach for topology optimization---an important class of inverse problems with high-dimensional parameter spaces and highly nonlinear objective landscapes. To effectively leverage neural representations in the context of mesh-free topology optimization, we use multilayer perceptrons to parameterize both density and displacement fields. Our experiments indicate that our method is highly competitive for minimizing  structural compliance objectives, and it enables self-supervised learning of continuous solution spaces for topology optimization problems.",
    "authors": [
      "Zehnder, Jonas",
      "Li, Yue",
      "Coros, Stelian",
      "Thomaszewski, Bernhard"
    ]
  },
  {
    "id": "55fd1368113e5a675e868c5653a7bb9e",
    "title": "Generalization Bounds for (Wasserstein) Robust Optimization",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/55fd1368113e5a675e868c5653a7bb9e-Paper.pdf",
    "abstract": "(Distributionally) robust optimization has gained momentum in machine learning community recently, due to its promising applications in developing generalizable learning paradigms. In this paper, we derive generalization bounds for robust optimization and Wasserstein robust optimization for Lipschitz and piecewise H\u00f6lder smooth loss functions under both stochastic and adversarial setting, assuming that the underlying data distribution satisfies transportation-information inequalities. The proofs are built on new generalization bounds for variation regularization (such as Lipschitz or gradient regularization) and its connection with robustness.",
    "authors": [
      "An, Yang",
      "Gao, Rui"
    ]
  },
  {
    "id": "5616060fb8ae85d93f334e7267307664",
    "title": "Faster Matchings via Learned Duals",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5616060fb8ae85d93f334e7267307664-Paper.pdf",
    "abstract": "A recent line of research investigates how algorithms can be augmented with machine-learned predictions to overcome worst case lower bounds.  This area has revealed interesting algorithmic insights into problems, with particular success in the design of competitive online algorithms.  However, the question of improving algorithm running times with predictions has largely been unexplored.  We take a first step in this direction by combining the idea of machine-learned predictions with the idea of ``warm-starting\" primal-dual algorithms. We consider one of the most important primitives in combinatorial optimization: weighted bipartite matching and its generalization to $b$-matching. We identify three key challenges when using learned dual variables in a primal-dual algorithm.  First, predicted duals may be infeasible, so we give an algorithm that efficiently maps predicted infeasible duals to nearby feasible solutions.  Second, once the duals are feasible, they may not be optimal, so we show that they can be used to quickly find an optimal solution. Finally, such predictions are useful only if they can be learned, so we show that the problem of learning duals for matching has low sample complexity.  We validate our theoretical findings through experiments on both real and synthetic data.  As a result we give a rigorous, practical, and empirically effective method to compute bipartite matchings.",
    "authors": [
      "Dinitz, Michael",
      "Im, Sungjin",
      "Lavastida, Thomas",
      "Moseley, Benjamin",
      "Vassilvitskii, Sergei"
    ]
  },
  {
    "id": "5631e6ee59a4175cd06c305840562ff3",
    "title": " Online learning in MDPs with linear function approximation and bandit feedback. ",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5631e6ee59a4175cd06c305840562ff3-Paper.pdf",
    "abstract": "We consider the problem of online learning in an episodic Markov decision process, where the reward function is allowed to change between episodes in an adversarial manner and the learner only observes the rewards associated with its actions. We assume that rewards and the transition function can be represented as linear functions in terms of a known low-dimensional feature map, which allows us to consider the setting where  the state space is arbitrarily large. We also assume that the learner has a perfect knowledge of the MDP dynamics. Our main contribution is developing an algorithm whose expected regret after $T$ episodes is bounded by $\\widetilde{\\mathcal{O}}(\\sqrt{dHT})$, where $H$ is the number of steps in each episode and $d$ is the dimensionality of the feature map.",
    "authors": [
      "Neu, Gergely",
      "Olkhovskaya, Julia"
    ]
  },
  {
    "id": "564127c03caab942e503ee6f810f54fd",
    "title": "Learning Collaborative Policies to Solve NP-hard Routing Problems",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/564127c03caab942e503ee6f810f54fd-Paper.pdf",
    "abstract": "Recently, deep reinforcement learning (DRL) frameworks have shown potential for solving NP-hard routing problems such as the traveling salesman problem (TSP) without problem-specific expert knowledge. Although DRL can be used to solve complex problems, DRL frameworks still struggle to compete with state-of-the-art heuristics showing a substantial performance gap. This paper proposes a novel hierarchical problem-solving strategy, termed learning collaborative policies (LCP), which can effectively find the near-optimum solution using two iterative DRL policies: the seeder and reviser. The seeder generates as diversified candidate solutions as possible (seeds) while being dedicated to exploring over the full combinatorial action space (i.e., sequence of assignment action). To this end, we train the seeder's policy using a simple yet effective entropy regularization reward to encourage the seeder to find diverse solutions. On the other hand, the reviser modifies each candidate solution generated by the seeder; it partitions the full trajectory into sub-tours and simultaneously revises each sub-tour to minimize its traveling distance. Thus, the reviser is trained to improve the candidate solution's quality, focusing on the reduced solution space (which is beneficial for exploitation). Extensive experiments demonstrate that the proposed two-policies collaboration scheme improves over single-policy DRL framework on various NP-hard routing problems, including TSP, prize collecting TSP (PCTSP), and capacitated vehicle routing problem (CVRP).",
    "authors": [
      "Kim, Minsu",
      "Park, Jinkyoo",
      "kim, joungho"
    ]
  },
  {
    "id": "56503192b14190d3826780d47c0d3bf3",
    "title": "Efficient Mirror Descent Ascent Methods for Nonsmooth Minimax Problems",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/56503192b14190d3826780d47c0d3bf3-Paper.pdf",
    "abstract": "In the paper, we propose a class of  efficient mirror descent ascent methods to solve the nonsmooth nonconvex-strongly-concave minimax problems by using dynamic mirror functions, and introduce a convergence analysis framework to conduct rigorous theoretical analysis for our mirror descent ascent methods. For our stochastic algorithms, we first prove that the mini-batch stochastic mirror descent ascent (SMDA) method obtains a gradient  complexity of $O(\\kappa^3\\epsilon^{-4})$ for finding an $\\epsilon$-stationary point, where $\\kappa$ denotes the condition number. Further, we propose an accelerated stochastic mirror descent ascent (VR-SMDA) method based on  the variance reduced technique. We prove that our VR-SMDA method achieves a lower gradient complexity of  $O(\\kappa^3\\epsilon^{-3})$. For our deterministic algorithm, we prove that our deterministic mirror descent ascent (MDA) achieves a lower gradient complexity of $O(\\sqrt{\\kappa}\\epsilon^{-2})$ under mild conditions, which matches the best known complexity in solving smooth nonconvex-strongly-concave minimax optimization. We conduct the experiments on fair classifier and robust neural network training tasks to demonstrate the efficiency of our new algorithms.",
    "authors": [
      "Huang, Feihu",
      "Wu, Xidong",
      "Huang, Heng"
    ]
  },
  {
    "id": "56577889b3c1cd083b6d7b32d32f99d5",
    "title": "CO-PILOT: COllaborative Planning and reInforcement Learning On sub-Task curriculum",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/56577889b3c1cd083b6d7b32d32f99d5-Paper.pdf",
    "abstract": "Goal-conditioned reinforcement learning (RL) usually suffers from sparse reward and inefficient exploration in long-horizon tasks. Planning can find the shortest path to a distant goal that provides dense reward/guidance but is inaccurate without a precise environment model. We show that RL and planning can collaboratively learn from each other to overcome their own drawbacks. In ''CO-PILOT'', a learnable path-planner and an RL agent produce dense feedback to train each other on a curriculum of tree-structured sub-tasks. Firstly, the planner recursively decomposes a long-horizon task to a tree of sub-tasks in a top-down manner, whose layers construct coarse-to-fine sub-task sequences as plans to complete the original task. The planning policy is trained to minimize the RL agent's cost of completing the sequence in each layer from top to bottom layers, which gradually increases the sub-tasks and thus forms an easy-to-hard curriculum for the planner.  Next, a bottom-up traversal of the tree trains the RL agent from easier sub-tasks with denser rewards on bottom layers to harder ones on top layers and collects its cost on each sub-task train the planner in the next episode. CO-PILOT repeats this mutual training for multiple episodes before switching to a new task, so the RL agent and planner are fully optimized to facilitate each other's training. We compare CO-PILOT with RL (SAC, HER, PPO), planning (RRT*, NEXT, SGT), and their combination (SoRB) on navigation and continuous control tasks. CO-PILOT significantly improves the success rate and sample efficiency.",
    "authors": [
      "Ao, Shuang",
      "Zhou, Tianyi",
      "Long, Guodong",
      "Lu, Qinghua",
      "Zhu, Liming",
      "Jiang, Jing"
    ]
  },
  {
    "id": "569ff987c643b4bedf504efda8f786c2",
    "title": "Modality-Agnostic Topology Aware Localization",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/569ff987c643b4bedf504efda8f786c2-Paper.pdf",
    "abstract": "This work presents a data-driven approach for the indoor localization of an observer on a 2D topological map of the environment. State-of-the-art techniques may yield accurate estimates only when they are tailor-made for a specific data modality like camera-based system that prevents their applicability to broader domains. Here, we establish a modality-agnostic framework (called OT-Isomap) and formulate the localization problem in the context of parametric manifold learning while leveraging optimal transportation. This framework allows jointly learning a low-dimensional embedding as well as correspondences with a topological map. We examine the generalizability of the proposed algorithm by applying it to data from diverse modalities such as image sequences and radio frequency signals. The experimental results demonstrate decimeter-level accuracy for localization using different sensory inputs.",
    "authors": [
      "Ghazvinian Zanjani, Farhad",
      "Karmanov, Ilia",
      "Ackermann, Hanno",
      "Dijkman, Daniel",
      "Merlin, Simone",
      "Welling, Max",
      "Porikli, Fatih"
    ]
  },
  {
    "id": "56a3107cad6611c8337ee36d178ca129",
    "title": "Scalable Quasi-Bayesian Inference for Instrumental Variable Regression",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/56a3107cad6611c8337ee36d178ca129-Paper.pdf",
    "abstract": "Recent years have witnessed an upsurge of interest in employing flexible machine learning models for instrumental variable (IV) regression, but the development of uncertainty quantification methodology is still lacking.  In this work we present a scalable quasi-Bayesian procedure for IV regression, building upon the recently developed kernelized IV models.  Contrary to Bayesian modeling for IV, our approach does not require additional assumptions on the data generating process, and leads to a scalable approximate inference algorithm with time cost comparable to the corresponding point estimation methods.  Our algorithm can be further extended to work with neural network models.  We analyze the theoretical properties of the proposed quasi-posterior, and demonstrate through empirical evaluation the competitive performance of our method.",
    "authors": [
      "Wang, Ziyu",
      "Zhou, Yuhao",
      "Ren, Tongzheng",
      "Zhu, Jun"
    ]
  },
  {
    "id": "56c3b2c6ea3a83aaeeff35eeb45d700d",
    "title": "Kernel Identification Through Transformers",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/56c3b2c6ea3a83aaeeff35eeb45d700d-Paper.pdf",
    "abstract": "Kernel selection plays a central role in determining the performance of Gaussian Process (GP) models, as the chosen kernel determines both the inductive biases and prior support of functions under the GP prior. This work addresses the challenge of constructing custom kernel functions for high-dimensional GP regression models. Drawing inspiration from recent progress in deep learning, we introduce a novel approach named KITT: Kernel Identification Through Transformers. KITT exploits a transformer-based architecture to generate kernel recommendations in under 0.1 seconds, which is several orders of magnitude faster than conventional kernel search algorithms. We train our model using synthetic data generated from priors over a vocabulary of known kernels. By exploiting the nature of the self-attention mechanism, KITT is able to process datasets with inputs of arbitrary dimension. We demonstrate that kernels chosen by KITT yield strong performance over a diverse collection of regression benchmarks. ",
    "authors": [
      "Simpson, Fergus",
      "Davies, Ian",
      "Lalchand, Vidhi",
      "Vullo, Alessandro",
      "Durrande, Nicolas",
      "Rasmussen, Carl Edward"
    ]
  },
  {
    "id": "56c51a39a7c77d8084838cc920585bd0",
    "title": "Curriculum Design for Teaching via Demonstrations: Theory and Applications",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/56c51a39a7c77d8084838cc920585bd0-Paper.pdf",
    "abstract": "We consider the problem of teaching via demonstrations in sequential decision-making settings. In particular, we study how to design a personalized curriculum over demonstrations to speed up the learner's convergence. We provide a unified curriculum strategy for two popular learner models: Maximum Causal Entropy Inverse Reinforcement Learning (MaxEnt-IRL) and Cross-Entropy Behavioral Cloning (CrossEnt-BC). Our unified strategy induces a ranking over demonstrations based on a notion of difficulty scores computed w.r.t. the teacher's optimal policy and the learner's current policy. Compared to the state of the art, our strategy doesn't require access to the learner's internal dynamics and still enjoys similar convergence guarantees under mild technical conditions. Furthermore, we adapt our curriculum strategy to the setting where no teacher agent is present using task-specific difficulty scores. Experiments on a synthetic car driving environment and navigation-based environments demonstrate the effectiveness of our curriculum strategy.",
    "authors": [
      "Yengera, Gaurav",
      "Devidze, Rati",
      "Kamalaruban, Parameswaran",
      "Singla, Adish"
    ]
  },
  {
    "id": "56d33021e640f5d64a611a71b5dc30a3",
    "title": "Revenue maximization via machine learning with noisy data",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/56d33021e640f5d64a611a71b5dc30a3-Paper.pdf",
    "abstract": "Increasingly, copious amounts of consumer data are used to learn high-revenue mechanisms via machine learning. Existing research on mechanism design via machine learning assumes that there is a distribution over the buyers' values for the items for sale and that the learning algorithm's input is a training set sampled from this distribution. This setup makes the strong assumption that no noise is introduced during data collection. In order to help place mechanism design via machine learning on firm foundations, we investigate the extent to which this learning process is robust to noise. Optimizing revenue using noisy data is challenging because revenue functions are extremely volatile: an infinitesimal change in the buyers' values can cause a steep drop in revenue. Nonetheless, we provide guarantees when arbitrarily correlated noise is added to the training set; we only require that the noise has bounded magnitude or is sub-Gaussian. We conclude with an application of our guarantees to multi-task mechanism design, where there are multiple distributions over buyers' values and the goal is to learn a high-revenue mechanism per distribution. To our knowledge, we are the first to study mechanism design via machine learning with noisy data as well as multi-task mechanism design.",
    "authors": [
      "Vitercik, Ellen",
      "Yan, Tom"
    ]
  },
  {
    "id": "56db57b4db0a6fcb7f9e0c0b504f6472",
    "title": "Exploiting Data Sparsity in Secure Cross-Platform Social Recommendation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/56db57b4db0a6fcb7f9e0c0b504f6472-Paper.pdf",
    "abstract": "Social recommendation has shown promising improvements over traditional systems since it leverages social correlation data as an additional input. Most existing work assumes that all data are available to the recommendation platform. However, in practice, user-item interaction data (e.g.,rating) and user-user social data are usually generated by different platforms, and both of which contain sensitive information.  Therefore, \"How to perform secure and efficient social recommendation across different platforms, where the data are highly-sparse in nature\" remains an important challenge. In this work, we bring secure computation techniques into social recommendation, and propose S3Rec, a sparsity-aware secure cross-platform social recommendation framework. As a result, our model can not only improve the recommendation performance of the rating platform by incorporating the sparse social data on the social platform, but also protect data privacy of both platforms. Moreover, to further improve model training efficiency, we propose two secure sparse matrix multiplication protocols based on homomorphic encryption and private information retrieval. Our experiments on two benchmark datasets demonstrate the effectiveness of S3Rec.",
    "authors": [
      "Cui, Jinming",
      "Chen, Chaochao",
      "Lyu, Lingjuan",
      "Yang, Carl",
      "Li, Wang"
    ]
  },
  {
    "id": "56f0b515214a7ec9f08a4bbf9a56f7ba",
    "title": "Parallelizing Thompson Sampling",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/56f0b515214a7ec9f08a4bbf9a56f7ba-Paper.pdf",
    "abstract": "How can we make use of information parallelism in online decision-making problems while efficiently balancing the exploration-exploitation trade-off? In this paper, we introduce a batch Thompson Sampling framework for two canonical online decision-making problems with partial feedback, namely,  stochastic multi-arm bandit and linear contextual bandit. Over a time horizon $T$,  our batch Thompson Sampling policy achieves the same  (asymptotic) regret bound of a fully sequential one while carrying out only   $O(\\log T)$ batch queries.  To achieve this exponential reduction, i.e., reducing the number of interactions from $T$ to $O(\\log T)$, our batch policy dynamically determines the duration of each batch in order to balance the exploration-exploitation trade-off. We also demonstrate experimentally that dynamic batch allocation outperforms natural baselines. ",
    "authors": [
      "Karbasi, Amin",
      "Mirrokni, Vahab",
      "Shadravan, Mohammad"
    ]
  },
  {
    "id": "577bcc914f9e55d5e4e4f82f9f00e7d4",
    "title": "Dynamic Causal Bayesian Optimization",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/577bcc914f9e55d5e4e4f82f9f00e7d4-Paper.pdf",
    "abstract": "We study the problem of performing a sequence of optimal interventions in a dynamic causal system where both the target variable of interest, and the inputs, evolve over time. This problem arises in a variety of domains including healthcare, operational research and policy design. Our approach, which we call Dynamic Causal Bayesian Optimisation (DCBO), brings together ideas from decision making, causal inference and Gaussian process (GP) emulation. DCBO is useful in scenarios where the causal effects are changing over time. Indeed, at every time step, DCBO identifies a local optimal intervention by integrating both observational and past interventional data collected from the system. We give theoretical results detailing how one can transfer interventional information across time steps and define a dynamic causal GP model which can be used to find optimal interventions in practice. Finally, we demonstrate how DCBO identifies optimal interventions faster than competing approaches in multiple settings and applications.",
    "authors": [
      "Aglietti, Virginia",
      "Dhir, Neil",
      "Gonz\u00e1lez, Javier",
      "Damoulas, Theodoros"
    ]
  },
  {
    "id": "580760fb5def6e2ca8eaf601236d5b08",
    "title": "Local Differential Privacy for Regret Minimization in Reinforcement Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/580760fb5def6e2ca8eaf601236d5b08-Paper.pdf",
    "abstract": "Reinforcement learning algorithms are widely used in domains where it is desirable to provide a personalized service. In these domains it is common that user data contains sensitive information that needs to be protected from third parties. Motivated by this, we study privacy in the context of finite-horizon Markov Decision Processes (MDPs) by requiring information to be obfuscated on the user side. We formulate this notion of privacy for RL by leveraging the local differential privacy (LDP) framework. We establish a lower bound for regret minimization in finite-horizon MDPs with LDP guarantees which shows that guaranteeing privacy has a multiplicative effect on the regret. This result shows that while LDP is an appealing notion of privacy, it makes the learning problem significantly more complex. Finally, we present an optimistic algorithm that simultaneously satisfies $\\varepsilon$-LDP requirements, and achieves $\\sqrt{K}/\\varepsilon$ regret in any finite-horizon MDP after $K$ episodes,  matching the lower bound dependency on the number of episodes $K$.",
    "authors": [
      "Garcelon, Evrard",
      "Perchet, Vianney",
      "Pike-Burke, Ciara",
      "Pirotta, Matteo"
    ]
  },
  {
    "id": "5812f92450ccaf17275500841c70924a",
    "title": "Emergent Discrete Communication in Semantic Spaces",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5812f92450ccaf17275500841c70924a-Paper.pdf",
    "abstract": "Neural agents trained in reinforcement learning settings can learn to communicate among themselves via discrete tokens, accomplishing as a team what agents would be unable to do alone. However, the current standard of using one-hot vectors as discrete communication tokens prevents agents from acquiring more desirable aspects of communication such as zero-shot understanding. Inspired by word embedding techniques from natural language processing, we propose neural agent architectures that enables them to communicate via discrete tokens derived from a learned, continuous space. We show in a decision theoretic framework that our technique optimizes communication over a wide range of scenarios, whereas one-hot tokens are only optimal under restrictive assumptions. In self-play experiments, we validate that our trained agents learn to cluster tokens in semantically-meaningful ways, allowing them communicate in noisy environments where other techniques fail. Lastly, we demonstrate both that agents using our method can effectively respond to novel human communication and that humans can understand unlabeled emergent agent communication, outperforming the use of one-hot communication.",
    "authors": [
      "Tucker, Mycal",
      "Li, Huao",
      "Agrawal, Siddharth",
      "Hughes, Dana",
      "Sycara, Katia ",
      "Lewis, Michael",
      "Shah, Julie A"
    ]
  },
  {
    "id": "58182b82110146887c02dbd78719e3d5",
    "title": "Drop, Swap, and Generate: A Self-Supervised Approach for Generating Neural Activity",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/58182b82110146887c02dbd78719e3d5-Paper.pdf",
    "abstract": "Meaningful and simplified representations of neural activity can yield insights into how and what information is being processed within a neural circuit. However, without labels, finding representations that reveal the link between the brain and behavior can be challenging. Here, we introduce a novel unsupervised approach for learning disentangled representations of neural activity called Swap-VAE. Our approach combines a generative modeling framework with an instance-specific alignment loss that tries to maximize the representational similarity between transformed views of the input (brain state). These transformed (or augmented) views are created by dropping out neurons and jittering samples in time, which intuitively should lead the network to a representation that maintains both temporal consistency and invariance to the specific neurons used to represent the neural state. Through evaluations on both synthetic data and neural recordings from hundreds of neurons in different primate brains, we show that it is possible to build representations that disentangle neural datasets along relevant latent dimensions linked to behavior.",
    "authors": [
      "Liu, Ran",
      "Azabou, Mehdi",
      "Dabagia, Max",
      "Lin, Chi-Heng",
      "Gheshlaghi Azar, Mohammad",
      "Hengen, Keith",
      "Valko, Michal",
      "Dyer, Eva"
    ]
  },
  {
    "id": "581b41df0cd50ace849e061ef74827fc",
    "title": "Equivariant Manifold Flows",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/581b41df0cd50ace849e061ef74827fc-Paper.pdf",
    "abstract": "Tractably modelling distributions over manifolds has long been an important goal in the natural sciences. Recent work has focused on developing general machine learning models to learn such distributions. However, for many applications these distributions must respect manifold symmetries\u2014a trait which most previous models disregard. In this paper, we lay the theoretical foundations for learning symmetry-invariant distributions on arbitrary manifolds via equivariant manifold flows. We demonstrate the utility of our approach by learning quantum field theory-motivated invariant SU(n) densities and by correcting meteor impact dataset bias.",
    "authors": [
      "Katsman, Isay",
      "Lou, Aaron",
      "Lim, Derek",
      "Jiang, Qingxuan",
      "Lim, Ser Nam",
      "De Sa, Christopher M."
    ]
  },
  {
    "id": "58238e9ae2dd305d79c2ebc8c1883422",
    "title": "Scalable Bayesian GPFA with automatic relevance determination and discrete noise models",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/58238e9ae2dd305d79c2ebc8c1883422-Paper.pdf",
    "abstract": "Latent variable models are ubiquitous in the exploratory analysis of neural population recordings, where they allow researchers to summarize the activity of large populations of neurons in lower dimensional \u2018latent\u2019 spaces. Existing methods can generally be categorized into (i) Bayesian methods that facilitate flexible incorporation of prior knowledge and uncertainty estimation, but which typically do not scale to large datasets; and (ii) highly parameterized methods without explicit priors that scale better but often struggle in the low-data regime. Here, we bridge this gap by developing a fully Bayesian yet scalable version of Gaussian process factor analysis (bGPFA), which models neural data as arising from a set of inferred latent processes with a prior that encourages smoothness over time. Additionally, bGPFA uses automatic relevance determination to infer the dimensionality of neural activity directly from the training data during optimization. To enable the analysis of continuous recordings without trial structure, we introduce a novel variational inference strategy that scales near-linearly in time and also allows for non-Gaussian noise models appropriate for electrophysiological recordings. We apply bGPFA to continuous recordings spanning 30 minutes with over 14 million data points from primate motor and somatosensory cortices during a self-paced reaching task. We show that neural activity progresses from an initial state at target onset to a reach- specific preparatory state well before movement onset. The distance between these initial and preparatory latent states is predictive of reaction times across reaches, suggesting that such preparatory dynamics have behavioral relevance despite the lack of externally imposed delay periods. Additionally, bGPFA discovers latent processes that evolve over slow timescales on the order of several seconds and contain complementary information about reaction time. These timescales are longer than those revealed by methods which focus on individual movement epochs and may reflect fluctuations in e.g. task engagement.",
    "authors": [
      "Jensen, Kristopher",
      "Kao, Ta-Chu",
      "Stone, Jasmine",
      "Hennequin, Guillaume"
    ]
  },
  {
    "id": "582967e09f1b30ca2539968da0a174fa",
    "title": "Recurrence along Depth: Deep Convolutional Neural Networks with Recurrent Layer Aggregation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/582967e09f1b30ca2539968da0a174fa-Paper.pdf",
    "abstract": "This paper introduces a concept of layer aggregation to describe how information from previous layers can be reused to better extract features at the current layer. While DenseNet is a typical example of the layer aggregation mechanism, its redundancy has been commonly criticized in the literature. This motivates us to propose a very light-weighted module, called recurrent layer aggregation (RLA), by making use of the sequential structure of layers in a deep CNN. Our RLA module is compatible with many mainstream deep CNNs, including ResNets, Xception and MobileNetV2, and its effectiveness is verified by our extensive experiments on image classification, object detection and instance segmentation tasks. Specifically, improvements can be uniformly observed on CIFAR, ImageNet and MS COCO datasets, and the corresponding RLA-Nets can surprisingly boost the performances by 2-3% on the object detection task. This evidences the power of our RLA module in helping main CNNs better learn structural information in images.",
    "authors": [
      "Zhao, Jingyu",
      "Fang, Yanwen",
      "Li, Guodong"
    ]
  },
  {
    "id": "584b98aac2dddf59ee2cf19ca4ccb75e",
    "title": "Independent Prototype Propagation for Zero-Shot Compositionality",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/584b98aac2dddf59ee2cf19ca4ccb75e-Paper.pdf",
    "abstract": "Humans are good at compositional zero-shot reasoning; someone who has never seen a zebra before could nevertheless recognize one when we tell them it looks like a horse with black and white stripes. Machine learning systems, on the other hand, usually leverage spurious correlations in the training data, and while such correlations can help recognize objects in context, they hurt generalization. To be able to deal with underspecified datasets while still leveraging contextual clues during classification, we propose ProtoProp, a novel prototype propagation graph method. First we learn prototypical representations of objects (e.g., zebra) that are independent w.r.t. their attribute labels (e.g., stripes) and vice versa. Next we propagate the independent prototypes through a compositional graph, to learn compositional prototypes of novel attribute-object combinations that reflect the dependencies of the target distribution. The method does not rely on any external data, such as class hierarchy graphs or pretrained word embeddings. We evaluate our approach on AO-Clevr, a synthetic and strongly visual dataset with clean labels, UT-Zappos, a noisy real-world dataset of fine-grained shoe types, and C-GQA, a large-scale object detection dataset modified for compositional zero-shot learning. We show that in the generalized compositional zero-shot setting we outperform state-of-the-art results, and through ablations we show the importance of each part of the method and their contribution to the final results. The code is available on github.",
    "authors": [
      "Ruis, Frank",
      "Burghouts, Gertjan",
      "Bucur, Doina"
    ]
  },
  {
    "id": "5857d68cd9280bc98d079fa912fd6740",
    "title": "Universal Graph Convolutional Networks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5857d68cd9280bc98d079fa912fd6740-Paper.pdf",
    "abstract": "Graph Convolutional Networks (GCNs), aiming to obtain the representation of a node by aggregating its neighbors, have demonstrated great power in tackling various analytics tasks on graph (network) data. The remarkable performance of GCNs typically relies on the homophily assumption of networks, while such assumption cannot always be satisfied, since the heterophily or randomness are also widespread in real-world. This gives rise to one fundamental question: whether networks with different structural properties should adopt different propagation mechanisms? In this paper, we first conduct an experimental investigation. Surprisingly, we discover that there are actually segmentation rules for the propagation mechanism, i.e., 1-hop, 2-hop and $k$-nearest neighbor ($k$NN) neighbors are more suitable as neighborhoods of network with complete homophily, complete heterophily and randomness, respectively. However, the real-world networks are complex, and may present diverse structural properties, e.g., the network dominated by homophily may contain a small amount of randomness. So can we reasonably utilize these segmentation rules to design a universal propagation mechanism independent of the network structural assumption? To tackle this challenge, we develop a new universal GCN framework, namely U-GCN. It first introduces a multi-type convolution to extract information from 1-hop, 2-hop and $k$NN networks simultaneously, and then designs a discriminative aggregation to sufficiently fuse them aiming to given learning objectives. Extensive experiments demonstrate the superiority of U-GCN over state-of-the-arts. The code and data are available at https://github.com/jindi-tju.",
    "authors": [
      "Jin, Di",
      "Yu, Zhizhi",
      "Huo, Cuiying",
      "Wang, Rui",
      "Wang, Xiao",
      "He, Dongxiao",
      "Han, Jiawei"
    ]
  },
  {
    "id": "587b7b833034299fdd5f4b10e7dc9fca",
    "title": "Adversarial Feature Desensitization",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/587b7b833034299fdd5f4b10e7dc9fca-Paper.pdf",
    "abstract": "Neural networks are known to be vulnerable to adversarial attacks -- slight but carefully constructed perturbations of the inputs which can drastically impair the network's performance. Many defense methods have been proposed for improving robustness of  deep networks by training them on adversarially perturbed inputs. However, these models often remain vulnerable to new types of attacks not seen during training, and even to slightly stronger versions of previously seen  attacks. In this work, we propose a novel approach to  adversarial robustness, which builds upon the insights from the domain adaptation field. Our method, called Adversarial Feature Desensitization (AFD), aims at learning  features that are invariant towards adversarial perturbations of the inputs. This is achieved through a game where we learn features that are both predictive and robust (insensitive to adversarial attacks), i.e. cannot be used to discriminate between natural and adversarial data. Empirical results on several benchmarks  demonstrate the effectiveness of the proposed approach against a wide range of attack types and attack strengths. Our code is available at https://github.com/BashivanLab/afd.",
    "authors": [
      "Bashivan, Pouya",
      "Bayat, Reza",
      "Ibrahim, Adam",
      "Ahuja, Kartik",
      "Faramarzi, Mojtaba",
      "Laleh, Touraj",
      "Richards, Blake",
      "Rish, Irina"
    ]
  },
  {
    "id": "588da7a73a2e919a23cb9a419c4c6d44",
    "title": "Few-Shot Data-Driven Algorithms for Low Rank Approximation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/588da7a73a2e919a23cb9a419c4c6d44-Paper.pdf",
    "abstract": "Recently, data-driven and learning-based algorithms for low rank matrix approximation were shown to outperform classical data-oblivious algorithms by wide margins in terms of accuracy.  Those algorithms are based on the optimization of sparse sketching matrices, which lead to large savings in time and memory during testing. However, they require long training times on a large amount of existing data, and rely on access to specialized hardware and software. In this work, we develop new data-driven low rank approximation algorithms with better computational efficiency in the training phase, alleviating these drawbacks. Furthermore, our methods are interpretable: while previous algorithms choose the sketching matrix either at random or by black-box learning, we show that it can be set (or initialized) to clearly interpretable values extracted from the dataset. Our experiments show that our algorithms, either by themselves or in combination with previous methods, achieve significant empirical advantage over previous work, improving training times by up to an order of magnitude toward achieving the same target accuracy.",
    "authors": [
      "Indyk, Piotr",
      "Wagner, Tal",
      "Woodruff, David"
    ]
  },
  {
    "id": "58ae749f25eded36f486bc85feb3f0ab",
    "title": "Neural-PIL: Neural Pre-Integrated Lighting for Reflectance Decomposition",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/58ae749f25eded36f486bc85feb3f0ab-Paper.pdf",
    "abstract": "Decomposing a scene into its shape, reflectance and illumination is a fundamental problem in computer vision and graphics. Neural approaches such as NeRF have achieved remarkable success in view synthesis, but do not explicitly perform decomposition and instead operate exclusively on radiance (the product of reflectance and illumination). Extensions to NeRF, such as NeRD, can perform decomposition but struggle to accurately recover detailed illumination, thereby significantly limiting realism. We propose a novel reflectance decomposition network that can estimate shape, BRDF, and per-image illumination given a set of object images captured under varying illumination. Our key technique is a novel illumination integration network called Neural-PIL that replaces a costly illumination integral operation in the rendering with a simple network query. In addition, we also learn deep low-dimensional priors on BRDF and illumination representations using novel smooth manifold auto-encoders. Our decompositions can result in considerably better BRDF and light estimates enabling more accurate novel view-synthesis and relighting compared to prior art. Project page: https://markboss.me/publication/2021-neural-pil/",
    "authors": [
      "Boss, Mark",
      "Jampani, Varun",
      "Braun, Raphael",
      "Liu, Ce",
      "Barron, Jonathan",
      "Lensch, Hendrik PA"
    ]
  },
  {
    "id": "58b7483ba899e0ce4d97ac5eecf6fa99",
    "title": "Asymptotics of the Bootstrap via Stability with Applications to Inference with Model Selection",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/58b7483ba899e0ce4d97ac5eecf6fa99-Paper.pdf",
    "abstract": "One of the most commonly used methods for forming confidence intervals is the empirical bootstrap, which is especially expedient when the limiting distribution of the estimator is unknown. However, despite its ubiquitous role in machine learning, its theoretical properties are still not well understood. Recent developments in probability have provided new tools to study the bootstrap method. However, they have been applied only to specific applications and contexts, and it is unclear whether these techniques are applicable to the understanding of the consistency of the bootstrap in machine learning pipelines. In this paper, we derive general stability conditions under which the empirical bootstrap estimator is consistent and quantify the speed of convergence. Moreover, we propose alternative ways to use the bootstrap method to build confidence intervals with coverage guarantees. Finally, we illustrate the generality and tightness of our results by examples of interest for machine learning including for two-sample kernel tests after kernel selection and the empirical risk of stacked estimators.",
    "authors": [
      "Austern, Morgane",
      "Syrgkanis, Vasilis"
    ]
  },
  {
    "id": "58ec72df0caca51df569d0b497c33805",
    "title": "Dynamic influence maximization",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/58ec72df0caca51df569d0b497c33805-Paper.pdf",
    "abstract": "We initiate a systematic study on {\\em dynamic influence maximization} (DIM). In the DIM problem, one maintains a seed set $S$ of at most $k$ nodes in a dynamically involving social network, with the goal of maximizing the expected influence spread while minimizing the amortized updating cost. We consider two evolution models. In the {\\em incremental model}, the social network gets enlarged over time and one only introduces new users and establishes new social links, we design an algorithm that achieves $(1-1/e-\\epsilon)$-approximation to the optimal solution and has $k \\cdot\\mathsf{poly}(\\log n, \\epsilon^{-1})$ amortized running time, which matches the state-of-art offline algorithm with only poly-logarithmic overhead. In the fully dynamic model, users join in and leave, influence propagation gets strengthened or weakened in real time, we prove that under the Strong Exponential Time Hypothesis (SETH), no algorithm can achieve $2^{-(\\log n)^{1-o(1)}}$-approximation unless the amortized running time is $n^{1-o(1)}$.  On the technical side, we exploit novel adaptive sampling approaches that reduce DIM to the dynamic MAX-k coverage problem, and design an efficient $(1-1/e-\\epsilon)$-approximation algorithm for it. Our lower bound leverages the recent developed distributed PCP framework.",
    "authors": [
      "Peng, Binghui"
    ]
  },
  {
    "id": "5907c88df2965e500c98e948dfae20c0",
    "title": "Risk Monotonicity in Statistical Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5907c88df2965e500c98e948dfae20c0-Paper.pdf",
    "abstract": "Acquisition of data is a difficult task in many applications of machine learning, and it is only natural that one hopes and expects the population risk to decrease (better performance) monotonically with increasing data points. It turns out, somewhat surprisingly, that this is not the case even for the most standard algorithms that minimize the empirical risk. Non-monotonic behavior of the risk and instability in training have manifested and appeared in the popular deep learning paradigm under the description of double descent. These problems highlight the current lack of understanding of learning algorithms and generalization. It is, therefore, crucial to pursue this concern and provide a characterization of such behavior. In this paper, we derive the first consistent and risk-monotonic (in high probability) algorithms for a general statistical learning setting under weak assumptions, consequently answering some questions posed by Viering et. al. 2019 on how to avoid non-monotonic behavior of risk curves. We further show that risk monotonicity need not necessarily come at the price of worse excess risk rates. To achieve this, we derive new empirical Bernstein-like concentration inequalities of independent interest that hold for certain non-i.i.d.~processes such as Martingale Difference Sequences. ",
    "authors": [
      "Mhammedi, Zakaria"
    ]
  },
  {
    "id": "59112692262234e3fad47fa8eabf03a4",
    "title": "Information is Power: Intrinsic Control via Information Capture",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/59112692262234e3fad47fa8eabf03a4-Paper.pdf",
    "abstract": "Humans and animals explore their environment and acquire useful skills even in the absence of clear goals, exhibiting intrinsic motivation. The study of intrinsic motivation in artificial agents is concerned with the following question: what is a good general-purpose objective for an agent? We study this question in dynamic partially-observed environments, and argue that a compact and general learning objective is to minimize the entropy of the agent's state visitation estimated using a latent state-space model. This objective induces an agent to both gather information about its environment, corresponding to reducing uncertainty, and to gain control over its environment, corresponding to reducing the unpredictability of future world states. We instantiate this approach as a deep reinforcement learning agent equipped with a deep variational Bayes filter. We find that our agent learns to discover, represent, and exercise control of dynamic objects in a variety of partially-observed environments sensed with visual observations without extrinsic reward.",
    "authors": [
      "Rhinehart, Nicholas",
      "Wang, Jenny",
      "Berseth, Glen",
      "Co-Reyes, John",
      "Hafner, Danijar",
      "Finn, Chelsea",
      "Levine, Sergey"
    ]
  },
  {
    "id": "5934c1ec0cd31e12bd9084d106bc2e32",
    "title": "Extracting Deformation-Aware Local Features by Learning to Deform",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5934c1ec0cd31e12bd9084d106bc2e32-Paper.pdf",
    "abstract": "Despite the advances in extracting local features achieved by handcrafted and learning-based descriptors, they are still limited by the lack of invariance to non-rigid transformations. In this paper, we present a new approach to compute features from still images that are robust to non-rigid deformations to circumvent the problem of matching deformable surfaces and objects. Our deformation-aware local descriptor, named DEAL, leverages a polar sampling and a spatial transformer warping to provide invariance to rotation, scale, and image deformations. We train the model architecture end-to-end by applying isometric non-rigid deformations to objects in a simulated environment as guidance to provide highly discriminative local features. The experiments show that our method outperforms state-of-the-art handcrafted, learning-based image, and RGB-D descriptors in different datasets with both real and realistic synthetic deformable objects in still images. The source code and trained model of the descriptor are publicly available at https://www.verlab.dcc.ufmg.br/descriptors/neurips2021.",
    "authors": [
      "Potje, Guilherme",
      "Martins, Renato",
      "Chamone, Felipe",
      "Nascimento, Erickson"
    ]
  },
  {
    "id": "593906af0d138e69f49d251d3e7cbed0",
    "title": "Object-Centric Representation Learning with Generative Spatial-Temporal Factorization",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/593906af0d138e69f49d251d3e7cbed0-Paper.pdf",
    "abstract": "Learning object-centric scene representations is essential for attaining structural understanding and abstraction of complex scenes. Yet, as current approaches for unsupervised object-centric representation learning are built upon either a stationary observer assumption or a static scene assumption, they often: i) suffer single-view spatial ambiguities, or ii) infer incorrectly or inaccurately object representations from dynamic scenes. To address this, we propose Dynamics-aware Multi-Object Network (DyMON), a method that broadens the scope of multi-view object-centric representation learning to dynamic scenes. We train DyMON on multi-view-dynamic-scene data and show that DyMON learns---without supervision---to factorize the entangled effects of observer motions and scene object dynamics from a sequence of observations, and constructs scene object spatial representations suitable for rendering at arbitrary times (querying across time) and from arbitrary viewpoints (querying across space). We also show that the factorized scene representations (w.r.t. objects) support querying about a single object by space and time independently. ",
    "authors": [
      "Li, Nanbo",
      "Raza, Muhammad Ahmed",
      "Hu, Wenbin",
      "Sun, Zhaole",
      "Fisher, Robert"
    ]
  },
  {
    "id": "594ca7adb3277c51a998252e2d4c906e",
    "title": "Learning to Simulate Self-driven Particles System with Coordinated Policy Optimization",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/594ca7adb3277c51a998252e2d4c906e-Paper.pdf",
    "abstract": "Self-Driven Particles (SDP) describe a category of multi-agent systems common in everyday life, such as flocking birds and traffic flows. In a SDP system, each agent pursues its own goal and constantly changes its cooperative or competitive behaviors with its nearby agents. Manually designing the controllers for such SDP system is time-consuming, while the resulting emergent behaviors are often not realistic nor generalizable. Thus the realistic simulation of SDP systems remains challenging. Reinforcement learning provides an appealing alternative for automating the development of the controller for SDP. However, previous multi-agent reinforcement learning (MARL) methods define the agents to be teammates or enemies before hand, which fail to capture the essence of SDP where the role of each agent varies to be cooperative or competitive even within one episode. To simulate SDP with MARL, a key challenge is to coordinate agents' behaviors while still maximizing individual objectives. Taking traffic simulation as the testing bed, in this work we develop a novel MARL method called Coordinated Policy Optimization (CoPO), which incorporates social psychology principle to learn neural controller for SDP. Experiments show that the proposed method can achieve superior performance compared to MARL baselines in various metrics. Noticeably the trained vehicles exhibit complex and diverse social behaviors that improve performance and safety of the population as a whole. Demo video and source code are available at: https://decisionforce.github.io/CoPO/",
    "authors": [
      "Peng, Zhenghao",
      "Li, Quanyi",
      "Hui, Ka Ming",
      "Liu, Chunxiao",
      "Zhou, Bolei"
    ]
  },
  {
    "id": "596dedf4498e258e4bdc9fd70df9a859",
    "title": "Gradient-based Hyperparameter Optimization Over Long Horizons",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/596dedf4498e258e4bdc9fd70df9a859-Paper.pdf",
    "abstract": "Gradient-based hyperparameter optimization has earned a widespread popularity in the context of few-shot meta-learning, but remains broadly impractical for tasks with long horizons (many gradient steps), due to memory scaling and gradient degradation issues. A common workaround is to learn hyperparameters online, but this introduces greediness which comes with a significant performance drop. We propose forward-mode differentiation with sharing (FDS), a simple and efficient algorithm which tackles memory scaling issues with forward-mode differentiation, and gradient degradation issues by sharing hyperparameters that are contiguous in time. We provide theoretical guarantees about the noise reduction properties of our algorithm, and demonstrate its efficiency empirically by differentiating through $\\sim 10^4$ gradient steps of unrolled optimization. We consider large hyperparameter search ranges on CIFAR-10 where we significantly outperform greedy gradient-based alternatives, while achieving $\\times 20$ speedups compared to the state-of-the-art black-box methods.",
    "authors": [
      "Micaelli, Paul",
      "Storkey, Amos J."
    ]
  },
  {
    "id": "597c7b407a02cc0a92167e7a371eca25",
    "title": "Stochastic Bias-Reduced Gradient Methods",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/597c7b407a02cc0a92167e7a371eca25-Paper.pdf",
    "abstract": "We develop a new primitive for stochastic optimization: a low-bias, low-cost  estimator of the minimizer $x_\\star$ of any Lipschitz strongly-convex function $f$. In particular, we use a multilevel Monte-Carlo approach due to Blanchet and Glynn to turn any optimal stochastic gradient method into an estimator of $x_\\star$ with bias $\\delta$, variance $O(\\log(1/\\delta))$, and an expected sampling cost of $O(\\log(1/\\delta))$ stochastic gradient evaluations. As an immediate consequence, we obtain cheap and nearly unbiased gradient estimators for the Moreau envelope of any Lipschitz convex function. We demonstrate the potential of our estimator through four applications. First, we develop a method for minimizing the maximum of $N$ functions, improving on recent results and matching a lower bound up to logarithmic factors. Second and third, we recover state-of-the-art rates for projection-efficient and gradient-efficient optimization using simple algorithms with a transparent analysis. Finally, we show that an improved version of our estimator would yield a nearly linear-time, optimal-utility, differentially-private non-smooth stochastic optimization method.",
    "authors": [
      "Asi, Hilal",
      "Carmon, Yair",
      "Jambulapati, Arun",
      "Jin, Yujia",
      "Sidford, Aaron"
    ]
  },
  {
    "id": "5989add1703e4b0480f75e2390739f34",
    "title": "The Causal-Neural Connection: Expressiveness, Learnability, and Inference",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5989add1703e4b0480f75e2390739f34-Paper.pdf",
    "abstract": "One of the central elements of any causal inference is an object called structural causal model (SCM), which represents a collection of mechanisms and exogenous sources of random variation of the system under investigation (Pearl, 2000). An important property of many kinds of neural networks is universal approximability: the ability to approximate any function to arbitrary precision. Given this property, one may be tempted to surmise that a collection of neural nets is capable of learning any SCM by training on data generated by that SCM. In this paper, we show this is not the case by disentangling the notions of expressivity and learnability. Specifically, we show that the causal hierarchy theorem (Thm. 1, Bareinboim et al., 2020), which describes the limits of what can be learned from data, still holds for neural models. For instance, an arbitrarily complex and expressive neural net is unable to predict the effects of interventions given observational data alone. Given this result, we introduce a special type of SCM called a neural causal model (NCM), and formalize a new type of inductive bias to encode structural constraints necessary for performing causal inferences. Building on this new class of models, we focus on solving two canonical tasks found in the literature known as causal  identification and estimation. Leveraging the neural toolbox, we develop an algorithm that is both sufficient and necessary to determine whether a causal effect can be learned from data (i.e., causal identifiability); it then estimates the effect whenever identifiability holds (causal estimation). Simulations corroborate the proposed approach.",
    "authors": [
      "Xia, Kevin",
      "Lee, Kai-Zhan",
      "Bengio, Yoshua",
      "Bareinboim, Elias"
    ]
  },
  {
    "id": "59a3adea76fadcb6dd9e54c96fc155d1",
    "title": "Validation Free and Replication Robust Volume-based Data Valuation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/59a3adea76fadcb6dd9e54c96fc155d1-Paper.pdf",
    "abstract": "Data valuation arises as a non-trivial challenge in real-world use cases such as collaborative machine learning, federated learning, trusted data sharing, data marketplaces. The value of data is often associated with the learning performance (e.g., validation accuracy) of a model trained on the data, which introduces a close coupling between data valuation and validation.  However, a validation set may notbe available in practice and it can be challenging for the data providers to reach an agreement on the choice of the validation set. Another practical issue is that of data replication: Given the value of some data points, a dishonest data provider may replicate these data points to exploit the valuation for a larger reward/payment. We observe that the diversity of the data points is an inherent property of a dataset that is independent of validation. We formalize diversity via the volume of the data matrix (i.e., determinant of its left Gram), which allows us to establish a formal connection between the diversity of data and learning performance without requiring validation. Furthermore, we propose a robust volume measure with a theoretical guarantee on the replication robustness by following the intuition that copying the same data points does not increase the diversity of data.  We perform extensive experiments to demonstrate its consistency in valuation and practical advantages over existing baselines and show that our method is model- and task-agnostic and can be flexibly adapted to handle various neural networks.",
    "authors": [
      "Xu, Xinyi",
      "Wu, Zhaoxuan",
      "Foo, Chuan Sheng",
      "Low, Bryan Kian Hsiang"
    ]
  },
  {
    "id": "59b1deff341edb0b76ace57820cef237",
    "title": "Implicit Finite-Horizon Approximation and Efficient Optimal Algorithms for Stochastic Shortest Path",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/59b1deff341edb0b76ace57820cef237-Paper.pdf",
    "abstract": "We introduce a generic template for developing regret minimization algorithms in the Stochastic Shortest Path (SSP) model, which achieves minimax optimal regret as long as certain properties are ensured. The key of our analysis is a new technique called implicit finite-horizon approximation, which approximates the SSP model by a finite-horizon counterpart only in the analysis without explicit implementation. Using this template, we develop two new algorithms: the first one is model-free (the first in the literature to our knowledge) and minimax optimal under strictly positive costs; the second one is model-based and minimax optimal even with zero-cost state-action pairs, matching the best existing result from [Tarbouriech et al., 2021b]. Importantly, both algorithms admit highly sparse updates, making them  computationally more efficient than all existing algorithms. Moreover, both can be made completely parameter-free.",
    "authors": [
      "Chen, Liyu",
      "Jafarnia-Jahromi, Mehdi",
      "Jain, Rahul",
      "Luo, Haipeng"
    ]
  },
  {
    "id": "5a499f6e26313e19bd4049009bbed5bd",
    "title": "A Separation Result Between Data-oblivious and Data-aware Poisoning Attacks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5a499f6e26313e19bd4049009bbed5bd-Paper.pdf",
    "abstract": "Poisoning attacks have emerged as a significant security threat to machine learning algorithms. It has been demonstrated that adversaries who make small changes to the training set, such as adding specially crafted data points, can hurt the performance of the output model. Most of these attacks require the full knowledge of training data. This leaves open the possibility of achieving the same attack results using poisoning attacks that do not have the full knowledge of the clean training set.In this work, we initiate a theoretical study of the problem above. Specifically, for the case of feature selection with LASSO, we show that \\emph{full information} adversaries (that craft poisoning examples based on the rest of the training data) are provably much more devastating compared to the optimal attacker that is \\emph{oblivious} to the training set yet has access to the distribution of the data.  Our separation result shows that the two settings of data-aware and data-oblivious are fundamentally different and we cannot hope to achieve the same attack or defense results in these scenarios. ",
    "authors": [
      "Deng, Samuel",
      "Garg, Sanjam",
      "Jha, Somesh",
      "Mahloujifar, Saeed",
      "Mahmoody, Mohammad",
      "Guha Thakurta, Abhradeep"
    ]
  },
  {
    "id": "5a4b25aaed25c2ee1b74de72dc03c14e",
    "title": "Deep Learning Through the Lens of Example Difficulty",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5a4b25aaed25c2ee1b74de72dc03c14e-Paper.pdf",
    "abstract": "Existing work on understanding deep learning often employs measures that compress all data-dependent information into a few numbers. In this work, we adopt a perspective based on the role of individual examples. We introduce a measure of the computational difficulty of making a prediction for a given input: the (effective) prediction depth. Our extensive investigation reveals surprising yet simple relationships between the prediction depth of a given input and the model\u2019s uncertainty, confidence, accuracy and speed of learning for that data point. We further categorize difficult examples into three interpretable groups, demonstrate how these groups are processed differently inside deep models and showcase how this understanding allows us to improve prediction accuracy. Insights from our study lead to a coherent view of a number of separately reported phenomena in the literature: early layers generalize while later layers memorize; early layers converge faster and networks learn easy data and simple functions first.",
    "authors": [
      "Baldock, Robert",
      "Maennel, Hartmut",
      "Neyshabur, Behnam"
    ]
  },
  {
    "id": "5a66b9200f29ac3fa0ae244cc2a51b39",
    "title": "R-Drop: Regularized Dropout for Neural Networks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5a66b9200f29ac3fa0ae244cc2a51b39-Paper.pdf",
    "abstract": "Dropout is a powerful and widely used technique to regularize the training of deep neural networks. Though effective and performing well, the randomness introduced by dropout causes unnegligible inconsistency between training and inference. In this paper, we introduce a simple consistency training strategy to regularize dropout, namely R-Drop, which forces the output distributions of different sub models generated by dropout to be consistent with each other. Specifically, for each training sample, R-Drop minimizes the bidirectional KL-divergence between the output distributions of two sub models sampled by dropout. Theoretical analysis reveals that R-Drop reduces the above inconsistency. Experiments on $\\bf{5}$ widely used deep learning tasks ($\\bf{18}$ datasets in total), including neural machine translation, abstractive summarization, language understanding, language modeling, and image classification, show that R-Drop is universally effective. In particular, it yields substantial improvements when applied to fine-tune large-scale pre-trained models, e.g., ViT, RoBERTa-large, and BART, and achieves state-of-the-art (SOTA) performances with the vanilla Transformer model on WMT14 English$\\to$German translation ($\\bf{30.91}$ BLEU) and WMT14 English$\\to$French translation ($\\bf{43.95}$ BLEU), even surpassing models trained with extra large-scale data and expert-designed advanced variants of Transformer models. Our code is available at GitHub\\footnote{\\url{https://github.com/dropreg/R-Drop}}.",
    "authors": [
      "liang, xiaobo",
      "Wu, Lijun",
      "Li, Juntao",
      "Wang, Yue",
      "Meng, Qi",
      "Qin, Tao",
      "Chen, Wei",
      "Zhang, Min",
      "Liu, Tie-Yan"
    ]
  },
  {
    "id": "5a7b238ba0f6502e5d6be14424b20ded",
    "title": "Diversity Enhanced Active Learning with Strictly Proper Scoring Rules",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5a7b238ba0f6502e5d6be14424b20ded-Paper.pdf",
    "abstract": "We study acquisition functions for active learning (AL) for text classification. The Expected Loss Reduction (ELR) method focuses on a Bayesian estimate of the reduction in classification error, recently updated with Mean Objective Cost of Uncertainty (MOCU).  We convert the ELR framework to estimate the increase in (strictly proper) scores like log probability or negative mean square error, which we call Bayesian Estimate of Mean Proper Scores (BEMPS). We also prove convergence results borrowing techniques used with MOCU. In order to allow better experimentation with the new acquisition functions,  we develop a complementary batch AL algorithm, which encourages diversity in the vector of expected changes in scores for unlabelled data. To allow high performance text classifiers, we combine ensembling and dynamic validation set construction on pretrained language models.  Extensive experimental evaluation then explores how these different acquisition functions perform. The results show that the use of mean square error and log probability with BEMPS yields robust acquisition functions, which consistently outperform the others tested.",
    "authors": [
      "Tan, Wei",
      "Du, Lan",
      "Buntine, Wray"
    ]
  },
  {
    "id": "5a9542c773018268fc6271f7afeea969",
    "title": "SSUL: Semantic Segmentation with Unknown Label for Exemplar-based Class-Incremental Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5a9542c773018268fc6271f7afeea969-Paper.pdf",
    "abstract": "We consider a class-incremental semantic segmentation (CISS) problem. While some recently proposed algorithms utilized variants of knowledge distillation (KD) technique to tackle the problem, they only partially addressed the key additional challenges in CISS that causes the catastrophic forgetting; \\textit{i.e.}, the semantic drift of the background class and multi-label prediction issue. To better address these challenges, we propose a new method, dubbed as SSUL-M (Semantic Segmentation with Unknown Label with Memory), by carefully combining several techniques tailored for semantic segmentation. More specifically, we make three main contributions; (1) modeling \\textit{unknown} class within the background class to help learning future classes (help plasticity), (2) \\textit{freezing} backbone network and past classifiers with binary cross-entropy loss and pseudo-labeling to overcome catastrophic forgetting (help stability), and (3) utilizing \\textit{tiny exemplar memory} for the first time in CISS to improve \\textit{both} plasticity and stability. As a result, we show our method achieves significantly better performance than the recent state-of-the-art baselines on the standard benchmark datasets. Furthermore, we justify our contributions with thorough and extensive ablation analyses and discuss different natures of the CISS problem compared to the standard class-incremental learning for classification. The official code is available at https://github.com/clovaai/SSUL.",
    "authors": [
      "Cha, Sungmin",
      "kim, beomyoung",
      "Yoo, YoungJoon",
      "Moon, Taesup"
    ]
  },
  {
    "id": "5a9d8bf5b7a4b35f3110dde8673bdda2",
    "title": "Lower and Upper Bounds on the Pseudo-Dimension of Tensor Network Models",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5a9d8bf5b7a4b35f3110dde8673bdda2-Paper.pdf",
    "abstract": "Tensor network methods have been a key ingredient of advances in condensed matter physics and have recently sparked interest in the machine learning community for their ability to compactly represent very high-dimensional objects. Tensor network methods can for example be used to efficiently learn linear models in exponentially large feature spaces [Stoudenmire and Schwab, 2016]. In this work, we derive upper and lower bounds on the VC dimension and pseudo-dimension of a large class of tensor network models for classification, regression and completion. Our upper bounds hold for linear models parameterized by arbitrary tensor network structures, and we derive lower bounds for common  tensor decomposition models~(CP, Tensor Train, Tensor Ring and Tucker) showing the tightness of our general upper bound. These results are used to derive a generalization bound which can be applied to classification with low rank matrices as well as linear classifiers based on any of the commonly used tensor decomposition models. As a corollary of our results, we obtain a bound on the VC dimension of the matrix product state classifier introduced in [Stoudenmire and Schwab, 2016] as a function of the so-called bond dimension~(i.e. tensor train rank), which  answers an open problem listed by Cirac, Garre-Rubio and P\u00e9rez-Garc\u00eda in [Cirac et al., 2019].",
    "authors": [
      "Khavari, Behnoush",
      "Rabusseau, Guillaume"
    ]
  },
  {
    "id": "5aa3405a3f865c10f420a4a7b55cbff3",
    "title": "What Makes Multi-Modal Learning Better than Single (Provably)",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5aa3405a3f865c10f420a4a7b55cbff3-Paper.pdf",
    "abstract": "The world provides us with data of multiple modalities. Intuitively, models fusing data from different modalities outperform their uni-modal counterparts, since more information is aggregated. Recently, joining the success of deep learning, there is an influential line of work on deep multi-modal learning, which has remarkable empirical results on various applications. However, theoretical justifications in this field are notably lacking.                        Can multi-modal learning provably perform better than uni-modal?In this paper, we answer this question under a most popular multi-modal fusion framework, which firstly encodes features from different modalities into a common latent space and seamlessly maps the latent representations into the task space. We prove that learning with multiple modalities achieves a  smaller population risk than only using its subset of modalities. The main intuition is that the former has a more accurate estimate of the latent space representation. To the best of our knowledge, this is the first theoretical treatment to capture important qualitative phenomena observed in real multi-modal applications from the generalization perspective. Combining with experiment results, we show that multi-modal learning does possess an appealing formal guarantee.",
    "authors": [
      "Huang, Yu",
      "Du, Chenzhuang",
      "Xue, Zihui",
      "Chen, Xuanyao",
      "Zhao, Hang",
      "Huang, Longbo"
    ]
  },
  {
    "id": "5adaacd4531b78ff8b5cedfe3f4d5212",
    "title": "Quantifying and Improving Transferability in Domain Generalization",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5adaacd4531b78ff8b5cedfe3f4d5212-Paper.pdf",
    "abstract": "Out-of-distribution generalization is one of the key challenges when transferring a model from the lab to the real world.  Existing efforts mostly focus on building invariant features among source and target domains. Based on invariant features, a high-performing classifier on source domains could hopefully behave equally well on a target domain. In other words, we hope the invariant features to be \\emph{transferable}. However, in practice, there are no perfectly transferable features, and some algorithms seem to learn ``more transferable'' features than others. How can we understand and quantify such \\emph{transferability}? In this paper, we formally define transferability that one can quantify and compute in domain generalization. We point out the difference and connection with common discrepancy measures between domains, such as total variation and Wasserstein distance. We then prove that our transferability can be estimated with enough samples and give a new upper bound for the target error based on our transferability. Empirically, we evaluate the transferability of the feature embeddings learned by existing algorithms for domain generalization. Surprisingly, we find that many algorithms are not quite learning transferable features, although few could still survive. In light of this, we propose a new algorithm for learning transferable features and test it over various benchmark datasets, including RotatedMNIST, PACS, Office-Home and WILDS-FMoW. Experimental results show that the proposed algorithm achieves consistent improvement over many state-of-the-art algorithms, corroborating our theoretical findings.",
    "authors": [
      "Zhang, Guojun",
      "Zhao, Han",
      "Yu, Yaoliang",
      "Poupart, Pascal"
    ]
  },
  {
    "id": "5b168fdba5ee5ea262cc2d4c0b457697",
    "title": "Beyond Pinball Loss: Quantile Methods for Calibrated Uncertainty Quantification",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5b168fdba5ee5ea262cc2d4c0b457697-Paper.pdf",
    "abstract": "Among the many ways of quantifying uncertainty in a regression setting, specifying the full quantile function is attractive, as quantiles are amenable to interpretation and evaluation. A model that predicts the true conditional quantiles for each input, at all quantile levels, presents a correct and efficient representation of the underlying uncertainty. To achieve this, many current quantile-based methods focus on optimizing the pinball loss. However, this loss restricts the scope of applicable regression models, limits the ability to target many desirable properties (e.g. calibration, sharpness, centered intervals), and may produce poor conditional quantiles. In this work, we develop new quantile methods that address these shortcomings. In particular, we propose methods that can apply to any class of regression model, select an explicit balance between calibration and sharpness, optimize for calibration of centered intervals, and produce more accurate conditional quantiles. We provide a thorough experimental evaluation of our methods, which includes a high dimensional uncertainty quantification task in nuclear fusion.",
    "authors": [
      "Chung, Youngseog",
      "Neiswanger, Willie",
      "Char, Ian",
      "Schneider, Jeff"
    ]
  },
  {
    "id": "5b4e9aa703d0bfa11041debaa2d1b633",
    "title": "Dynamic Inference with Neural Interpreters",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5b4e9aa703d0bfa11041debaa2d1b633-Paper.pdf",
    "abstract": "Modern neural network architectures can leverage large amounts of data to generalize well within the training distribution. However, they are less capable of systematic generalization to data drawn from unseen but related distributions, a feat that is hypothesized to require compositional reasoning and reuse of knowledge. In this work, we present Neural Interpreters, an architecture that factorizes inference in a self-attention network as a system of modules, which we call functions. Inputs to the model are routed through a sequence of functions in a way that is end-to-end learned. The proposed architecture can flexibly compose computation along width and depth, and lends itself well to capacity extension after training. To demonstrate the versatility of Neural Interpreters, we evaluate it in two distinct settings: image classification and visual abstract reasoning on Raven Progressive Matrices. In the former, we show that Neural Interpreters perform on par with the vision transformer using fewer parameters, while being transferrable to a new task in a sample efficient manner. In the latter, we find that Neural Interpreters are competitive with respect to the state-of-the-art in terms of systematic generalization. ",
    "authors": [
      "Rahaman, Nasim",
      "Gondal, Muhammad Waleed",
      "Joshi, Shruti",
      "Gehler, Peter",
      "Bengio, Yoshua",
      "Locatello, Francesco",
      "Sch\u00f6lkopf, Bernhard"
    ]
  },
  {
    "id": "5b658d2a925565f0755e035597f8d22f",
    "title": "Leveraging Recursive Gumbel-Max Trick for Approximate Inference in Combinatorial Spaces",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5b658d2a925565f0755e035597f8d22f-Paper.pdf",
    "abstract": "Structured latent variables allow incorporating meaningful prior knowledge into deep learning models. However, learning with such variables remains challenging because of their discrete nature. Nowadays, the standard learning approach is to define a latent variable as a perturbed algorithm output and to use a differentiable surrogate for training. In general, the surrogate puts additional constraints on the model and inevitably leads to biased gradients. To alleviate these shortcomings, we extend the Gumbel-Max trick to define distributions over structured domains. We avoid the differentiable surrogates by leveraging the score function estimators for optimization. In particular, we highlight a family of recursive algorithms with a common feature we call stochastic invariant. The feature allows us to construct reliable gradient estimates and control variates without additional constraints on the model. In our experiments, we consider various structured latent variable models and achieve results competitive with relaxation-based counterparts.",
    "authors": [
      "Struminsky, Kirill",
      "Gadetsky, Artyom",
      "Rakitin, Denis",
      "Karpushkin, Danil",
      "Vetrov, Dmitry P."
    ]
  },
  {
    "id": "5b970a1d9be0fd100063fd6cd688b73e",
    "title": "Hamiltonian Dynamics with Non-Newtonian Momentum for Rapid Sampling",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5b970a1d9be0fd100063fd6cd688b73e-Paper.pdf",
    "abstract": "Sampling from an unnormalized probability distribution is a fundamental problem in machine learning with applications including Bayesian modeling, latent factor inference, and energy-based model training. After decades of research, variations of MCMC remain the default approach to sampling despite slow convergence. Auxiliary neural models can learn to speed up MCMC, but the overhead for training the extra model can be prohibitive. We propose a fundamentally different approach to this problem via a new Hamiltonian dynamics with a non-Newtonian momentum. In contrast to MCMC approaches like Hamiltonian Monte Carlo, no stochastic step is required. Instead, the proposed deterministic dynamics in an extended state space exactly sample the target distribution, specified by an energy function, under an assumption of ergodicity. Alternatively, the dynamics can be interpreted as a normalizing flow that samples a specified energy model without training. The proposed Energy Sampling Hamiltonian (ESH) dynamics have a simple form that can be solved with existing ODE solvers, but we derive a specialized solver that exhibits much better performance. ESH dynamics converge faster than their MCMC competitors enabling faster, more stable training of neural network energy models. ",
    "authors": [
      "Ver Steeg, Greg",
      "Galstyan, Aram"
    ]
  },
  {
    "id": "5bd529d5b07b647a8863cf71e98d651a",
    "title": "Dynamic Normalization and Relay for Video Action Recognition",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5bd529d5b07b647a8863cf71e98d651a-Paper.pdf",
    "abstract": "Convolutional Neural Networks (CNNs) have been the dominant model for video action recognition. Due to the huge memory and compute demand, popular action recognition networks need to be trained with small batch sizes, which makes learning discriminative spatial-temporal representations for videos become a challenging problem. In this paper, we present Dynamic Normalization and Relay (DNR), an improved normalization design, to augment the spatial-temporal representation learning of any deep action recognition model, adapting to small batch size training settings. We observe that state-of-the-art action recognition networks usually apply the same normalization parameters to all video data, and ignore the dependencies of the estimated normalization parameters between neighboring frames (at the same layer) and between neighboring layers (with all frames of a video clip). Inspired by this, DNR introduces two dynamic normalization relay modules to explore the potentials of cross-temporal and cross-layer feature distribution dependencies for estimating accurate layer-wise normalization parameters. These two DNR modules are instantiated as a light-weight recurrent structure conditioned on the current input features, and the normalization parameters estimated from the neighboring frames based features at the same layer or from the whole video clip based features at the preceding layers. We first plug DNR into prevailing 2D CNN backbones and test its performance on public action recognition datasets including Kinetics and Something-Something. Experimental results show that DNR brings large performance improvements to the baselines, achieving over 4.4% absolute margins in top-1 accuracy without training bells and whistles. More experiments on 3D backbones and several latest 2D spatial-temporal networks further validate its effectiveness. Code will be available at https://github.com/caidonkey/dnr.",
    "authors": [
      "Cai, Dongqi",
      "Yao, Anbang",
      "Chen, Yurong"
    ]
  },
  {
    "id": "5bd53571b97884635d13910db49626bc",
    "title": "Robust Visual Reasoning via Language Guided Neural Module Networks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5bd53571b97884635d13910db49626bc-Paper.pdf",
    "abstract": "Neural module networks (NMN) are a popular approach for solving multi-modal tasks such as visual question answering (VQA) and visual referring expression recognition (REF). A key limitation in prior implementations of NMN is that the neural modules do not effectively capture the association between the visual input and the relevant neighbourhood context of the textual input. This limits their generalizability. For instance, NMN fail to understand new concepts such as \u201cyellow sphere to the left\" even when it is a combination of known concepts from train data: \u201cblue sphere\", \u201cyellow cube\", and \u201cmetallic cube to the left\". In this paper, we address this limitation by introducing a language-guided adaptive convolution layer (LG-Conv) into NMN, in which the filter weights of convolutions are explicitly multiplied with a spatially varying language-guided kernel. Our model allows the neural module to adaptively co-attend over potential objects of interest from the visual and textual inputs. Extensive experiments on VQA and REF tasks demonstrate the effectiveness of our approach. Additionally, we propose a new challenging out-of-distribution test split for REF task, which we call C3-Ref+, for explicitly evaluating the NMN\u2019s ability to generalize well to adversarial perturbations and unseen combinations of known concepts. Experiments on C3-Ref+ further demonstrate the generalization capabilities of our approach.",
    "authors": [
      "Akula, Arjun",
      "Jampani, Varun",
      "Changpinyo, Soravit",
      "Zhu, Song-Chun"
    ]
  },
  {
    "id": "5c04925674920eb58467fb52ce4ef728",
    "title": "True Few-Shot Learning with Language Models",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5c04925674920eb58467fb52ce4ef728-Paper.pdf",
    "abstract": "Pretrained language models (LMs) perform well on many tasks even when learning from a few examples, but prior work uses many held-out examples to tune various aspects of learning, such as hyperparameters, training objectives, and natural language templates (\"prompts\"). Here, we evaluate the few-shot ability of LMs when such held-out examples are unavailable, a setting we call true few-shot learning. We test two model selection criteria, cross-validation and minimum description length, for choosing LM prompts and hyperparameters in the true few-shot setting. On average, both marginally outperform random selection and greatly underperform selection based on held-out examples. Moreover, selection criteria often prefer models that perform significantly worse than randomly-selected ones. We find similar results even when taking into account our uncertainty in a model's true performance during selection, as well as when varying the amount of computation and number of examples used for selection. Overall, our findings suggest that prior work significantly overestimated the true few-shot ability of LMs given the difficulty of few-shot model selection.",
    "authors": [
      "Perez, Ethan",
      "Kiela, Douwe",
      "Cho, Kyunghyun"
    ]
  },
  {
    "id": "5c333c4ffd55c7a3576e6a614d81af82",
    "title": "Selective Sampling for Online Best-arm Identification",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5c333c4ffd55c7a3576e6a614d81af82-Paper.pdf",
    "abstract": "This work considers the problem of selective-sampling for best-arm identification. Given a set of potential options $\\mathcal{Z}\\subset\\mathbb{R}^d$, a learner aims to compute with probability greater than $1-\\delta$, $\\arg\\max_{z\\in \\mathcal{Z}} z^{\\top}\\theta_{\\ast}$ where $\\theta_{\\ast}$ is unknown. At each time step, a potential measurement $x_t\\in \\mathcal{X}\\subset\\mathbb{R}^d$ is drawn IID and the learner can either choose to take the measurement, in which case they observe a noisy measurement of $x^{\\top}\\theta_{\\ast}$, or to abstain from taking the measurement and wait for a potentially more informative point to arrive in the stream. Hence the learner faces a fundamental trade-off between the number of labeled samples they take and when they have collected enough evidence to declare the best arm and stop sampling. The main results of this work precisely characterize this trade-off between labeled samples and stopping time and provide an algorithm that nearly-optimally achieves the minimal label complexity given a desired stopping time. In addition, we show that the optimal decision rule has a simple geometric form based on deciding whether a point is in an ellipse or not. Finally, our framework is general enough to capture binary classification improving upon previous works. ",
    "authors": [
      "Camilleri, Romain",
      "Xiong, Zhihan",
      "Fazel, Maryam",
      "Jain, Lalit",
      "Jamieson, Kevin G."
    ]
  },
  {
    "id": "5c3a3b139a11689e0bc55abd95e20e39",
    "title": "Multi-task Learning of Order-Consistent Causal Graphs",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5c3a3b139a11689e0bc55abd95e20e39-Paper.pdf",
    "abstract": "We consider the problem of discovering $K$ related Gaussian directed acyclic graphs (DAGs), where the involved graph structures share a consistent causal order and sparse unions of supports. Under the multi-task learning setting, we propose a $l_1/l_2$-regularized maximum likelihood estimator (MLE) for learning $K$ linear structural equation models. We theoretically show that the joint estimator, by leveraging data across related tasks, can achieve a better sample complexity for recovering the causal order (or topological order) than separate estimations. Moreover, the joint estimator is able to recover non-identifiable DAGs, by estimating them together with some identifiable DAGs.  Lastly, our analysis also shows the consistency of union support recovery of the structures. To allow practical implementation, we design a continuous optimization problem whose optimizer is the same as the joint estimator and can be approximated efficiently by an iterative algorithm. We validate the theoretical analysis and the effectiveness of the joint estimator in experiments.",
    "authors": [
      "Chen, Xinshi",
      "Sun, Haoran",
      "Ellington, Caleb",
      "Xing, Eric",
      "Song, Le"
    ]
  },
  {
    "id": "5c53292c032b6cb8510041c54274e65f",
    "title": "Learning to Iteratively Solve Routing Problems with Dual-Aspect Collaborative Transformer",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5c53292c032b6cb8510041c54274e65f-Paper.pdf",
    "abstract": "Recently, Transformer has become a prevailing deep architecture for solving vehicle routing problems (VRPs). However, it is less effective in learning improvement models for VRP because its positional encoding (PE) method is not suitable in representing VRP solutions. This paper presents a novel Dual-Aspect Collaborative Transformer (DACT) to learn embeddings for the node and positional features separately, instead of fusing them together as done in existing ones, so as to avoid potential noises and incompatible correlations. Moreover, the positional features are embedded through a novel cyclic positional encoding (CPE) method to allow Transformer to effectively capture the circularity and symmetry of VRP solutions (i.e., cyclic sequences). We train DACT using Proximal Policy Optimization and design a curriculum learning strategy for better sample efficiency. We apply DACT to solve the traveling salesman problem (TSP) and capacitated vehicle routing problem (CVRP). Results show that our DACT outperforms existing Transformer based improvement models, and exhibits much better generalization performance across different problem sizes on synthetic and benchmark instances, respectively.",
    "authors": [
      "Ma, Yining",
      "Li, Jingwen",
      "Cao, Zhiguang",
      "Song, Wen",
      "Zhang, Le",
      "Chen, Zhenghua",
      "Tang, Jing"
    ]
  },
  {
    "id": "5c572eca050594c7bc3c36e7e8ab9550",
    "title": "Learning interaction rules from multi-animal trajectories via augmented behavioral models",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5c572eca050594c7bc3c36e7e8ab9550-Paper.pdf",
    "abstract": "Extracting the interaction rules of biological agents from movement sequences pose challenges in various domains. Granger causality is a practical framework for analyzing the interactions from observed time-series data; however, this framework ignores the structures and assumptions of the generative process in animal behaviors, which may lead to interpretational problems and sometimes erroneous assessments of causality. In this paper, we propose a new framework for learning Granger causality from multi-animal trajectories via augmented theory-based behavioral models with interpretable data-driven models. We adopt an approach for augmenting incomplete multi-agent behavioral models described by time-varying dynamical systems with neural networks. For efficient and interpretable learning, our model leverages theory-based architectures separating navigation and motion processes, and the theory-guided regularization for reliable behavioral modeling. This can provide interpretable signs of Granger-causal effects over time, i.e., when specific others cause the approach or separation. In experiments using synthetic datasets, our method achieved better performance than various baselines. We then analyzed multi-animal datasets of mice, flies, birds, and bats, which verified our method and obtained novel biological insights.",
    "authors": [
      "Fujii, Keisuke",
      "Takeishi, Naoya",
      "Tsutsui, Kazushi",
      "Fujioka, Emyo",
      "Nishiumi, Nozomi",
      "Tanaka, Ryoya",
      "Fukushiro, Mika",
      "Ide, Kaoru",
      "Kohno, Hiroyoshi",
      "Yoda, Ken",
      "Takahashi, Susumu",
      "Hiryu, Shizuko",
      "Kawahara, Yoshinobu"
    ]
  },
  {
    "id": "5c5a93a042235058b1ef7b0ac1e11b67",
    "title": "Differentiable Synthesis of Program Architectures",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5c5a93a042235058b1ef7b0ac1e11b67-Paper.pdf",
    "abstract": "Differentiable programs have recently attracted much interest due to their interpretability, compositionality, and their efficiency to leverage differentiable training. However, synthesizing differentiable programs requires optimizing over a combinatorial, rapidly exploded space of program architectures. Despite the development of effective pruning heuristics, previous works essentially enumerate the discrete search space of program architectures, which is inefficient. We propose to encode program architecture search as learning the probability distribution over all possible program derivations induced by a context-free grammar. This allows the search algorithm to efficiently prune away unlikely program derivations to synthesize optimal program architectures. To this end, an efficient gradient-descent based method is developed to conduct program architecture search in a continuous relaxation of the discrete space of grammar rules. Experiment results on four sequence classification tasks demonstrate that our program synthesizer excels in discovering program architectures that lead to differentiable programs with higher F1 scores, while being more efficient than state-of-the-art program synthesis methods.",
    "authors": [
      "Cui, Guofeng",
      "Zhu, He"
    ]
  },
  {
    "id": "5c5bc7df3d37b2a7ea29e1b47b2bd4ab",
    "title": "Make Sure You're Unsure: A Framework for Verifying Probabilistic Specifications",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5c5bc7df3d37b2a7ea29e1b47b2bd4ab-Paper.pdf",
    "abstract": "Most real world applications require dealing with stochasticity like sensor noise or predictive uncertainty, where formal specifications of desired behavior are inherently probabilistic.  Despite the promise of formal verification in ensuring the reliability of neural networks, progress in the direction of probabilistic specifications has been limited. In this direction, we first introduce a general formulation of probabilistic specifications for neural networks, which captures both probabilistic networks (e.g., Bayesian neural networks, MC-Dropout networks) and uncertain inputs (distributions over inputs arising from sensor noise or other perturbations). We then propose a general technique to verify such specifications by generalizing the notion of Lagrangian duality, replacing standard Lagrangian multipliers with \"functional multipliers\" that can be arbitrary functions of the activations at a given layer. We show that an optimal choice of functional multipliers leads to exact verification (i.e.,  sound and complete verification),  and for specific forms of multipliers, we develop tractable practical verification algorithms. We empirically validate our algorithms by applying them to Bayesian Neural Networks (BNNs) and MC Dropout Networks, and certifying properties such as adversarial robustness and robust detection of out-of-distribution (OOD) data. On these tasks we are able to provide significantly stronger guarantees when compared to prior work -- for instance, for a VGG-64 MC-Dropout CNN trained on CIFAR-10 in a verification-agnostic manner,  we improve the certified AUC (a verified lower bound on the true AUC) for robust OOD detection (on CIFAR-100) from $0 \\% \\rightarrow 29\\%$. Similarly, for a BNN trained on MNIST, we improve on the $\\ell_\\infty$ robust accuracy from $60.2 \\% \\rightarrow 74.6\\%$. Further, on a novel specification -- distributionally robust OOD detection -- we improve on the certified AUC from $5\\% \\rightarrow 23\\%$.",
    "authors": [
      "Berrada, Leonard",
      "Dathathri, Sumanth",
      "Dvijotham, Krishnamurthy",
      "Stanforth, Robert",
      "Bunel, Rudy R.",
      "Uesato, Jonathan",
      "Gowal, Sven",
      "Kumar, M. Pawan"
    ]
  },
  {
    "id": "5c936263f3428a40227908d5a3847c0b",
    "title": "Oracle-Efficient Regret Minimization in Factored MDPs with Unknown Structure",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5c936263f3428a40227908d5a3847c0b-Paper.pdf",
    "abstract": "We study regret minimization in non-episodic factored Markov decision processes (FMDPs), where all existing algorithms make the strong assumption that the factored structure of the FMDP is known to the learner in advance. In this paper, we provide the first algorithm that learns the structure of the FMDP while minimizing the regret. Our algorithm is based on the optimism in face of uncertainty principle, combined with a simple statistical method for structure learning, and can be implemented efficiently given oracle-access to an FMDP planner. Moreover, we give a variant of our algorithm that remains efficient even when the oracle is limited to non-factored actions, which is the case with almost all existing approximate planners. Finally, we leverage our techniques to prove a novel lower bound for the known structure case, closing the gap to the regret bound of Chen et al. [2021].",
    "authors": [
      "Rosenberg, Aviv",
      "Mansour, Yishay"
    ]
  },
  {
    "id": "5ca3e9b122f61f8f06494c97b1afccf3",
    "title": "Linear-Time Probabilistic Solution of Boundary Value Problems",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf",
    "abstract": "We propose a fast algorithm for the probabilistic solution of boundary value problems (BVPs), which are ordinary differential equations subject to boundary conditions.  In contrast to previous work, we introduce a Gauss-Markov prior and tailor it specifically to BVPs, which allows computing a posterior distribution over the solution in linear time, at a quality and cost comparable to that of well-established, non-probabilistic methods.  Our model further delivers uncertainty quantification, mesh refinement, and hyperparameter adaptation. We demonstrate how these practical considerations positively impact the efficiency of the scheme. Altogether, this results in a practically usable probabilistic BVP solver that is (in contrast to non-probabilistic algorithms) natively compatible with other parts of the statistical modelling tool-chain.",
    "authors": [
      "Kr\u00e4mer, Nicholas",
      "Hennig, Philipp"
    ]
  },
  {
    "id": "5caf41d62364d5b41a893adc1a9dd5d4",
    "title": "Lifelong Domain Adaptation via Consolidated Internal Distribution",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5caf41d62364d5b41a893adc1a9dd5d4-Paper.pdf",
    "abstract": "We develop an algorithm to address unsupervised domain adaptation (UDA) in continual learning (CL) settings.  The goal is to update a model continually to learn distributional shifts across sequentially arriving tasks with unlabeled data while retaining the knowledge about the past learned tasks. Existing  UDA  algorithms address the challenge of domain shift, but they require simultaneous access to the datasets of the source and the target domains. On the other hand, existing works on CL can handle tasks with labeled data.  Our solution is based on consolidating the learned internal distribution for improved model generalization on new domains and benefitting from experience replay to overcome catastrophic forgetting.",
    "authors": [
      "Rostami, Mohammad"
    ]
  },
  {
    "id": "5cc3749a6e56ef6d656735dff9176074",
    "title": "Counterbalancing Learning and Strategic Incentives in Allocation Markets",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5cc3749a6e56ef6d656735dff9176074-Paper.pdf",
    "abstract": "Motivated by the high discard rate of donated organs in the United States, we study an allocation problem in the presence of learning and strategic incentives. We consider a setting where a benevolent social planner decides whether and how to allocate a single indivisible object to a queue of strategic agents.  The object has a common true quality, good or bad,  which is ex-ante unknown to everyone. Each agent holds an informative, yet noisy, private signal about the quality. To make a correct allocation decision the planner attempts to learn the object quality by truthfully eliciting agents' signals. Under the commonly applied sequential offering mechanism, we show that learning is hampered by the presence of strategic incentives as herding may emerge. This can result in incorrect allocation and welfare loss. To overcome these issues, we propose a novel class of incentive-compatible mechanisms. Our mechanism involves a batch-by-batch, dynamic voting process using a majority rule. We prove that the proposed voting mechanisms improve the probability of correct allocation whenever agents are sufficiently well informed. Particularly, we show that such an improvement can be achieved via a simple greedy algorithm. We quantify the improvement using simulations.",
    "authors": [
      "Kang, Jamie",
      "Monachou, Faidra",
      "Koren, Moran",
      "Ashlagi, Itai"
    ]
  },
  {
    "id": "5cd5058bca53951ffa7801bcdf421651",
    "title": "Controlling Neural Networks with Rule Representations",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5cd5058bca53951ffa7801bcdf421651-Paper.pdf",
    "abstract": "We propose a novel training method that integrates rules into deep learning, in a way the strengths of the rules are controllable at inference. Deep Neural Networks with Controllable Rule Representations (DeepCTRL) incorporates a rule encoder into the model coupled with a rule-based objective, enabling a shared representation for decision making. DeepCTRL is agnostic to data type and model architecture. It can be applied to any kind of rule defined for inputs and outputs. The key aspect of DeepCTRL is that it does not require retraining to adapt the rule strength -- at inference, the user can adjust it based on the desired operation point on accuracy vs. rule verification ratio. In real-world domains where incorporating rules is critical -- such as Physics, Retail and Healthcare -- we show the effectiveness of DeepCTRL in teaching rules for deep learning. DeepCTRL improves the trust and reliability of the trained models by significantly increasing their rule verification ratio, while also providing accuracy gains at downstream tasks. Additionally, DeepCTRL enables novel use cases such as hypothesis testing of the rules on data samples, and unsupervised adaptation based on shared rules between datasets.",
    "authors": [
      "Seo, Sungyong",
      "Arik, Sercan",
      "Yoon, Jinsung",
      "Zhang, Xiang",
      "Sohn, Kihyuk",
      "Pfister, Tomas"
    ]
  },
  {
    "id": "5d2c2cee8ab0b9a36bd1ed7196bd6c4a",
    "title": "Making the most of your day: online learning for optimal allocation of time",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5d2c2cee8ab0b9a36bd1ed7196bd6c4a-Paper.pdf",
    "abstract": "We study online learning for optimal allocation when the resource to be allocated is time. An agent receives task proposals sequentially according to a Poisson process and can either accept or reject a proposed task. If she accepts the proposal, she is busy for the duration of the task and obtains a reward that depends on the task duration. If she rejects it, she remains on hold until a new task proposal arrives. We study the regret incurred by the agent first when she knows her reward function but does not know the distribution of the task duration, and then when she does not know her reward function, either. Faster rates are finally obtained by adding structural assumptions on the distribution of rides or on the reward function. This natural setting bears similarities with contextual (one-armed) bandits, but with the crucial difference that the normalized reward associated to a context depends on the whole distribution of contexts.",
    "authors": [
      "Boursier, Etienne",
      "Garrec, Tristan",
      "Perchet, Vianney",
      "Scarsini, Marco"
    ]
  },
  {
    "id": "5d44a2b0d85aa1a4dd3f218be6422c66",
    "title": "Federated Reconstruction: Partially Local Federated Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5d44a2b0d85aa1a4dd3f218be6422c66-Paper.pdf",
    "abstract": "Personalization methods in federated learning aim to balance the benefits of federated and local training for data availability, communication cost, and robustness to client heterogeneity. Approaches that require clients to communicate all model parameters can be undesirable due to privacy and communication constraints. Other approaches require always-available or stateful clients, impractical in large-scale cross-device settings. We introduce Federated Reconstruction, the first model-agnostic framework for partially local federated learning suitable for training and inference at scale. We motivate the framework via a connection to model-agnostic meta learning, empirically demonstrate its performance over existing approaches for collaborative filtering and next word prediction, and release an open-source library for evaluating approaches in this setting. We also describe the successful deployment of this approach at scale for federated collaborative filtering in a mobile keyboard application.",
    "authors": [
      "Singhal, Karan",
      "Sidahmed, Hakim",
      "Garrett, Zachary",
      "Wu, Shanshan",
      "Rush, John",
      "Prakash, Sushant"
    ]
  },
  {
    "id": "5d69dc892ba6e79fda0c6a1e286f24c5",
    "title": "Optimal prediction of Markov chains with and without spectral gap",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5d69dc892ba6e79fda0c6a1e286f24c5-Paper.pdf",
    "abstract": "We study the following learning problem with dependent data: Given a trajectory of length $n$ from a stationary Markov chain with $k$ states, the goal is to predict the distribution of the next state. For $3 \\leq k \\leq O(\\sqrt{n})$, the optimal prediction risk in the Kullback-Leibler divergence is shown to be $\\Theta(\\frac{k^2}{n}\\log \\frac{n}{k^2})$, in contrast to the optimal rate of $\\Theta(\\frac{\\log \\log n}{n})$ for $k=2$ previously shown in Falahatgar et al in 2016. These nonparametric rates can be attributed to the memory in the data, as the spectral gap of the Markov chain can be arbitrarily small. To quantify the memory effect, we study irreducible reversible chains with a prescribed spectral gap. In addition to characterizing the optimal prediction risk for two states, we show that, as long as the spectral gap is not excessively small, the prediction risk in the Markov model is $O(\\frac{k^2}{n})$, which coincides with that of an iid model with the same number of parameters. ",
    "authors": [
      "Han, Yanjun",
      "Jana, Soham",
      "Wu, Yihong"
    ]
  },
  {
    "id": "5d9e4a04afb9f3608ccc76c1ffa7573e",
    "title": "Subquadratic Overparameterization for Shallow Neural Networks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5d9e4a04afb9f3608ccc76c1ffa7573e-Paper.pdf",
    "abstract": "Overparameterization refers to the important phenomenon where the width of a neural network is chosen such that learning algorithms can provably attain zero loss in nonconvex training. The existing theory establishes such global convergence using various initialization strategies, training modifications, and width scalings. In particular, the state-of-the-art results require the width to scale quadratically with the number of training data under standard initialization strategies used in practice for best generalization performance. In contrast, the most recent results obtain linear scaling either with requiring initializations that lead to the \"lazy-training\",  or training only a single layer. In this work, we provide an analytical framework that allows us to adopt standard initialization strategies, possibly avoid lazy training, and train all layers simultaneously in basic shallow neural networks while attaining  a desirable subquadratic scaling on the network width. We achieve the desiderata via Polyak-Lojasiewicz condition, smoothness, and standard assumptions on data, and use tools from random matrix theory.",
    "authors": [
      "Song, ChaeHwan",
      "Ramezani-Kebrya, Ali",
      "Pethick, Thomas",
      "Eftekhari, Armin",
      "Cevher, Volkan"
    ]
  },
  {
    "id": "5da713a690c067105aeb2fae32403405",
    "title": "Continuous Doubly Constrained Batch Reinforcement Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5da713a690c067105aeb2fae32403405-Paper.pdf",
    "abstract": "Reliant on too many experiments to learn good actions, current Reinforcement Learning (RL) algorithms have limited applicability in real-world settings, which can be too expensive to allow exploration. We propose an algorithm for batch RL, where effective policies are learned using only a fixed offline dataset instead of online interactions with the environment. The limited data in batch RL produces inherent uncertainty in value estimates of states/actions that were insufficiently represented in the training data. This leads to particularly severe extrapolation when our candidate policies diverge from one that generated the data. We propose to mitigate this issue via two straightforward penalties: a policy-constraint to reduce this divergence and a value-constraint that discourages overly optimistic estimates. Over a comprehensive set of $32$ continuous-action batch RL benchmarks, our approach compares favorably to state-of-the-art methods, regardless of how the offline data were collected.",
    "authors": [
      "Fakoor, Rasool",
      "Mueller, Jonas W.",
      "Asadi, Kavosh",
      "Chaudhari, Pratik",
      "Smola, Alexander J."
    ]
  },
  {
    "id": "5db60c98209913790e4fcce4597ee37c",
    "title": "Bridging Explicit and Implicit Deep Generative Models via Neural Stein Estimators",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5db60c98209913790e4fcce4597ee37c-Paper.pdf",
    "abstract": "There are two types of deep generative models: explicit and implicit. The former defines an explicit density form that allows likelihood inference; while the latter targets a flexible transformation from random noise to generated samples.  While the two classes of generative models have shown great power in many applications, both of them, when used alone, suffer from respective limitations and drawbacks. To take full advantages of both models and enable mutual compensation, we propose a novel joint training framework that bridges an explicit (unnormalized) density estimator and an implicit sample generator via Stein discrepancy. We show that our method 1) induces novel mutual regularization via kernel Sobolev norm penalization and Moreau-Yosida regularization, and 2) stabilizes the training dynamics. Empirically, we demonstrate that proposed method can facilitate the density estimator to more accurately identify data modes and guide the generator to output higher-quality samples, comparing with training a single counterpart. The new approach also shows promising results when the training samples are contaminated or limited.",
    "authors": [
      "Wu, Qitian",
      "Gao, Rui",
      "Zha, Hongyuan"
    ]
  },
  {
    "id": "5dca4c6b9e244d24a30b4c45601d9720",
    "title": "Score-based Generative Modeling in Latent Space",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5dca4c6b9e244d24a30b4c45601d9720-Paper.pdf",
    "abstract": "Score-based generative models (SGMs) have recently demonstrated impressive results in terms of both sample quality and distribution coverage. However, they are usually applied directly in data space and often require thousands of network evaluations for sampling. Here, we propose the Latent Score-based Generative Model (LSGM), a novel approach that trains SGMs in a latent space, relying on the variational autoencoder framework. Moving from data to latent space allows us to train more expressive generative models, apply SGMs to non-continuous data, and learn smoother SGMs in a smaller space, resulting in fewer network evaluations and faster sampling. To enable training LSGMs end-to-end in a scalable and stable manner, we (i) introduce a new score-matching objective suitable to the LSGM setting, (ii) propose a novel parameterization of the score function that allows SGM to focus on the mismatch of the target distribution with respect to a simple Normal one, and (iii) analytically derive multiple techniques for variance reduction of the training objective. LSGM obtains a state-of-the-art FID score of 2.10 on CIFAR-10, outperforming all existing generative results on this dataset. On CelebA-HQ-256, LSGM is on a par with previous SGMs in sample quality while outperforming them in sampling time by two orders of magnitude. In modeling binary images, LSGM achieves state-of-the-art likelihood on the binarized OMNIGLOT dataset. ",
    "authors": [
      "Vahdat, Arash",
      "Kreis, Karsten",
      "Kautz, Jan"
    ]
  },
  {
    "id": "5dd9db5e033da9c6fb5ba83c7a7ebea9",
    "title": "Deep Conditional Gaussian Mixture Model for Constrained Clustering",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf",
    "abstract": "Constrained clustering has gained significant attention in the field of machine learning as it can leverage prior information on a growing amount of only partially labeled data. Following recent advances in deep generative models, we propose a novel framework for constrained clustering that is intuitive, interpretable, and can be trained efficiently in the framework of stochastic gradient variational inference. By explicitly integrating domain knowledge in the form of probabilistic relations, our proposed model (DC-GMM) uncovers the underlying distribution of data conditioned on prior clustering preferences, expressed as \\textit{pairwise constraints}. These constraints guide the clustering process towards a desirable partition of the data by indicating which samples should or should not belong to the same cluster. We provide extensive experiments to demonstrate that DC-GMM shows superior clustering performances and robustness compared to state-of-the-art deep constrained clustering methods on a wide range of data sets. We further demonstrate the usefulness of our approach on two challenging real-world applications.",
    "authors": [
      "Manduchi, Laura",
      "Chin-Cheong, Kieran",
      "Michel, Holger",
      "Wellmann, Sven",
      "Vogt, Julia"
    ]
  },
  {
    "id": "5e15fb59326e7a9c3d6558ca74621683",
    "title": "Bootstrap Your Object Detector via Mixed Training",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5e15fb59326e7a9c3d6558ca74621683-Paper.pdf",
    "abstract": "We introduce MixTraining, a new training paradigm for object detection that can improve the performance of existing detectors for free. MixTraining enhances data augmentation by utilizing augmentations of different strengths while excluding the strong augmentations of certain training samples that may be detrimental to training. In addition, it addresses localization noise and missing labels in human annotations by incorporating pseudo boxes that can compensate for these errors. Both of these MixTraining capabilities are made possible through bootstrapping on the detector, which can be used to predict the difficulty of training on a strong augmentation, as well as to generate reliable pseudo boxes thanks to the robustness of neural networks to labeling error. MixTraining is found to bring consistent improvements across various detectors on the COCO dataset. In particular, the performance of Faster R-CNN~\\cite{ren2015faster} with a ResNet-50~\\cite{he2016deep} backbone is improved from 41.7 mAP to 44.0 mAP, and the accuracy of Cascade-RCNN~\\cite{cai2018cascade} with a Swin-Small~\\cite{liu2021swin} backbone is raised from 50.9 mAP to 52.8 mAP.",
    "authors": [
      "Xu, Mengde",
      "Zhang, Zheng",
      "Wei, Fangyun",
      "Lin, Yutong",
      "Cao, Yue",
      "Lin, Stephen",
      "Hu, Han",
      "Bai, Xiang"
    ]
  },
  {
    "id": "5e34a2b4c23f4de585fb09a7f546f527",
    "title": "Tensor decompositions of higher-order correlations by nonlinear Hebbian plasticity",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5e34a2b4c23f4de585fb09a7f546f527-Paper.pdf",
    "abstract": "Biological synaptic plasticity exhibits nonlinearities that are not accounted for by classic Hebbian learning rules. Here, we introduce a simple family of generalized nonlinear Hebbian learning rules. We study the computations implemented by their dynamics in the simple setting of a neuron receiving feedforward inputs. These nonlinear Hebbian rules allow a neuron to learn tensor decompositions of its higher- order input correlations. The particular input correlation decomposed and the form of the decomposition depend on the location of nonlinearities in the plasticity rule. For simple, biologically motivated parameters, the neuron learns eigenvectors of higher-order input correlation tensors. We prove that tensor eigenvectors are attractors and determine their basins of attraction. We calculate the volume of those basins, showing that the dominant eigenvector has the largest basin of attraction. We then study arbitrary learning rules and find that any learning rule that admits a finite Taylor expansion into the neural input and output also has stable equilibria at generalized eigenvectors of higher-order input correlation tensors. Nonlinearities in synaptic plasticity thus allow a neuron to encode higher-order input correlations in a simple fashion.",
    "authors": [
      "Ocker, Gabriel",
      "Buice, Michael"
    ]
  },
  {
    "id": "5e6bd7a6970cd4325e587f02667f7f73",
    "title": "Online Adaptation to Label Distribution Shift",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5e6bd7a6970cd4325e587f02667f7f73-Paper.pdf",
    "abstract": "Machine learning models often encounter distribution shifts when deployed in the real world.  In this paper, we focus on adaptation to label distribution shift in the online setting, where the test-time label distribution is continually changing and the model must dynamically adapt to it without observing the true label. This setting is common in many real world scenarios such as medical diagnosis, where disease prevalences can vary substantially at different times of the year. Leveraging a novel analysis, we show that the lack of true label does not hinder estimation of the expected test loss, which enables the reduction of online label shift adaptation to conventional online learning. Informed by this observation, we propose adaptation algorithms inspired by classical online learning techniques such as Follow The Leader (FTL) and Online Gradient Descent (OGD) and derive their regret bounds. We empirically verify our findings under both simulated and real world label distribution shifts and show that OGD is particularly effective and robust to a variety of challenging label shift scenarios.",
    "authors": [
      "Wu, Ruihan",
      "Guo, Chuan",
      "Su, Yi",
      "Weinberger, Kilian Q."
    ]
  },
  {
    "id": "5e751896e527c862bf67251a474b3819",
    "title": "One Explanation is Not Enough: Structured Attention Graphs for Image Classification",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5e751896e527c862bf67251a474b3819-Paper.pdf",
    "abstract": "Attention maps are popular tools for explaining the decisions of convolutional neural networks (CNNs) for image classification. Typically, for each image of interest, a single attention map is produced, which assigns weights to pixels based on their importance to the classification. We argue that a single attention map provides an incomplete understanding since there are often many other maps that explain a classification equally well. In this paper, we propose to utilize a beam search algorithm to systematically search for multiple explanations for each image. Results show that there are indeed multiple relatively localized explanations for many images. However, naively showing multiple explanations to users can be overwhelming and does not reveal their common and distinct structures. We introduce structured attention graphs (SAGs), which compactly represent sets of attention maps for an image by visualizing how different combinations of image regions impact the confidence of a classifier. An approach to computing a compact and representative SAG for visualization is proposed via diverse sampling. We conduct a user study comparing the use of SAGs to traditional attention maps for answering comparative counterfactual questions about image classifications. Our results show that the users are significantly more accurate when presented with SAGs compared to standard attention map baselines. ",
    "authors": [
      "Shitole, Vivswan",
      "Li, Fuxin",
      "Kahng, Minsuk",
      "Tadepalli, Prasad",
      "Fern, Alan"
    ]
  },
  {
    "id": "5ea1649a31336092c05438df996a3e59",
    "title": "Integrating Expert ODEs into Neural ODEs: Pharmacology and Disease Progression",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5ea1649a31336092c05438df996a3e59-Paper.pdf",
    "abstract": "Modeling a system's temporal behaviour in reaction to external stimuli is a fundamental problem in many areas. Pure Machine Learning (ML) approaches often fail in the small sample regime and cannot provide actionable insights beyond predictions. A promising modification has been to incorporate expert domain knowledge into ML models. The application we consider is predicting the patient health status and disease progression over time, where a wealth of domain knowledge is available from pharmacology. Pharmacological models describe the dynamics of carefully-chosen medically meaningful variables in terms of systems of Ordinary Differential Equations (ODEs). However, these models only describe a limited collection of variables, and these variables are often not observable in clinical environments. To close this gap, we propose the latent hybridisation model (LHM) that integrates a system of expert-designed ODEs with machine-learned Neural ODEs to fully describe the dynamics of the system and to link the expert and latent variables to observable quantities. We evaluated LHM on synthetic data as well as real-world intensive care data of COVID-19 patients. LHM consistently outperforms previous works, especially when few training samples are available such as at the beginning of the pandemic.",
    "authors": [
      "Qian, Zhaozhi",
      "Zame, William",
      "Fleuren, Lucas",
      "Elbers, Paul",
      "van der Schaar, Mihaela"
    ]
  },
  {
    "id": "5edc4f7dce28c711afc6265b4f99bf57",
    "title": "Shifted Chunk Transformer for Spatio-Temporal Representational Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5edc4f7dce28c711afc6265b4f99bf57-Paper.pdf",
    "abstract": "Spatio-temporal representational learning has been widely adopted in various fields such as action recognition, video object segmentation, and action anticipation.Previous spatio-temporal representational learning approaches primarily employ ConvNets or sequential models, e.g., LSTM, to learn the intra-frame and inter-frame features.  Recently, Transformer models have successfully dominated the study of natural language processing (NLP), image classification, etc. However, the pure-Transformer based spatio-temporal learning can be prohibitively costly on memory and computation to extract fine-grained features from a tiny patch. To tackle the training difficulty and enhance the spatio-temporal learning, we construct a shifted chunk Transformer with pure self-attention blocks. Leveraging the recent efficient Transformer design in NLP, this shifted chunk Transformer can learn hierarchical spatio-temporal features from a local tiny patch to a global videoclip. Our shifted self-attention can also effectively model complicated inter-frame variances. Furthermore, we build a clip encoder based on Transformer to model long-term temporal dependencies. We conduct thorough ablation studies to validate each component and hyper-parameters in our shifted chunk Transformer, and it outperforms previous state-of-the-art approaches on Kinetics-400, Kinetics-600,UCF101, and HMDB51.",
    "authors": [
      "Zha, Xuefan",
      "Zhu, Wentao",
      "Xun, Lv",
      "Yang, Sen",
      "Liu, Ji"
    ]
  },
  {
    "id": "5ef78f63ba22e7dfb2fa44613311b932",
    "title": "Faster proximal algorithms for matrix optimization using Jacobi-based eigenvalue methods",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5ef78f63ba22e7dfb2fa44613311b932-Paper.pdf",
    "abstract": "We consider proximal splitting algorithms for convex optimization problems over matrices. A significant computational bottleneck in many of these algorithms is the need to compute a full eigenvalue or singular value decomposition at each iteration for the evaluation of a proximal operator.In this paper we propose to use an old and surprisingly simple method due to Jacobi to compute these eigenvalue and singular value decompositions, and we demonstrate that it can lead to substantial gains in terms of computation time compared to standard approaches. We rely on three essential properties of this method: (a) its ability to exploit an approximate decomposition as an initial point, which in the case of iterative optimization algorithms can be obtained from the previous iterate; (b) its parallel nature which makes it a great fit for hardware accelerators such as GPUs, now common in machine learning, and (c) its simple termination criterion which allows us to trade-off accuracy with computation time. We demonstrate the efficacy of this approach on a variety of algorithms and problems, and show that, on a GPU, we can obtain 5 to 10x speed-ups in the evaluation of proximal operators compared to standard CPU or GPU linear algebra routines. Our findings are supported by new theoretical results providing guarantees on the approximation quality of proximal operators obtained using approximate eigenvalue or singular value decompositions.",
    "authors": [
      "Fawzi, Hamza",
      "Goulbourne, Harry"
    ]
  },
  {
    "id": "5f1d3986fae10ed2994d14ecd89892d7",
    "title": "Decrypting Cryptic Crosswords: Semantically Complex Wordplay Puzzles as a Target for NLP",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5f1d3986fae10ed2994d14ecd89892d7-Paper.pdf",
    "abstract": "Cryptic crosswords, the dominant crossword variety in the UK, are a promising target for advancing NLP systems that seek to process semantically complex, highly compositional language. Cryptic clues read like fluent natural language but are adversarially composed of two parts: a definition and a wordplay cipher requiring character-level manipulations. Expert humans use creative intelligence to solve cryptics, flexibly combining linguistic, world, and domain knowledge. In this paper, we make two main contributions. First, we present a dataset of cryptic clues as a challenging new benchmark for NLP systems that seek to process compositional language in more creative, human-like ways. After showing that three non-neural approaches and T5, a state-of-the-art neural language model, do not achieve good performance, we make our second main contribution: a novel curriculum approach, in which the model is first fine-tuned on related tasks such as unscrambling words. We also introduce a challenging data split, examine the meta-linguistic capabilities of subword-tokenized models, and investigate model systematicity by perturbing the wordplay part of clues, showing that T5 exhibits behavior partially consistent with human solving strategies. Although our curricular approach considerably improves on the T5 baseline, our best-performing model still fails to generalize to the extent that humans can. Thus, cryptic crosswords remain an unsolved challenge for NLP systems and a potential source of future innovation.",
    "authors": [
      "Rozner, Josh",
      "Potts, Christopher",
      "Mahowald, Kyle"
    ]
  },
  {
    "id": "5f25fbe144e4a81a1b0080b6c1032778",
    "title": "An Improved Analysis of Gradient Tracking for Decentralized Machine Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5f25fbe144e4a81a1b0080b6c1032778-Paper.pdf",
    "abstract": "We consider decentralized machine learning over a network where the training data is distributed across $n$ agents, each of which can compute stochastic model updates on their local data. The agent's common goal is to find a model that minimizes the average of all local loss functions. While gradient tracking (GT) algorithms can overcome a key challenge, namely accounting for differences between workers' local data distributions, the known convergence rates for GT algorithms are not optimal with respect to their dependence on the mixing parameter $p$ (related to the spectral gap of the connectivity matrix).We provide a tighter analysis of the GT method in the stochastic strongly convex, convex and non-convex settings. We improve the dependency on $p$ from $\\mathcal{O}(p^{-2})$ to $\\mathcal{O}(p^{-1}c^{-1})$ in the noiseless case and from $\\mathcal{O}(p^{-3/2})$ to $\\mathcal{O}(p^{-1/2}c^{-1})$ in the general stochastic case, where $c \\geq p$ is related to the negative eigenvalues of the connectivity matrix (and is a constant in most practical applications). This improvement was possible due to a new proof technique which could be of independent interest.",
    "authors": [
      "Koloskova, Anastasiia",
      "Lin, Tao",
      "Stich, Sebastian U."
    ]
  },
  {
    "id": "5f7f02b7e4ade23430f345f954c938c1",
    "title": "Entropic Desired Dynamics for Intrinsic Control",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5f7f02b7e4ade23430f345f954c938c1-Paper.pdf",
    "abstract": "An agent might be said, informally, to have mastery of its environment when it has maximised the effective number of states it can reliably reach. In practice, this often means maximizing the number of latent codes that can be discriminated from future states under some short time horizon (e.g. \\cite{eysenbach2018diversity}). By situating these latent codes in a globally consistent coordinate system, we show that agents can reliably reach more states in the long term while still optimizing a local objective. A simple instantiation of this idea, \\textbf{E}ntropic \\textbf{D}esired \\textbf{D}ynamics for \\textbf{I}ntrinsic \\textbf{C}on\\textbf{T}rol (EDDICT), assumes fixed additive latent dynamics, which results in tractable learning and an interpretable latent space. Compared to prior methods, EDDICT's globally consistent codes allow it to be far more exploratory, as demonstrated by improved state coverage and increased unsupervised performance on hard exploration games such as Montezuma's Revenge.",
    "authors": [
      "Hansen, Steven",
      "Desjardins, Guillaume",
      "Baumli, Kate",
      "Warde-Farley, David",
      "Heess, Nicolas",
      "Osindero, Simon",
      "Mnih, Volodymyr"
    ]
  },
  {
    "id": "5f93f983524def3dca464469d2cf9f3e",
    "title": "Exploring Cross-Video and Cross-Modality Signals for Weakly-Supervised Audio-Visual Video Parsing",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5f93f983524def3dca464469d2cf9f3e-Paper.pdf",
    "abstract": "The audio-visual video parsing task aims to temporally parse a video into audio or visual event categories. However, it is labor intensive to temporally annotate audio and visual events and thus hampers the learning of a parsing model. To this end, we propose to explore additional cross-video and cross-modality supervisory signals to facilitate weakly-supervised audio-visual video parsing. The proposed method exploits both the common and diverse event semantics across videos to identify audio or visual events. In addition, our method explores event co-occurrence across audio, visual, and audio-visual streams. We leverage the explored cross-modality co-occurrence to localize segments of target events while excluding irrelevant ones. The discovered supervisory signals across different videos and modalities can greatly facilitate the training with only video-level annotations. Quantitative and qualitative results demonstrate that the proposed method performs favorably against existing methods on weakly-supervised audio-visual video parsing.",
    "authors": [
      "Lin, Yan-Bo",
      "Tseng, Hung-Yu",
      "Lee, Hsin-Ying",
      "Lin, Yen-Yu",
      "Yang, Ming-Hsuan"
    ]
  },
  {
    "id": "5fbb4eb0e7c2cedf731ec7c18e344141",
    "title": "Littlestone Classes are Privately Online Learnable",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5fbb4eb0e7c2cedf731ec7c18e344141-Paper.pdf",
    "abstract": "    We consider the problem of online classification under a privacy constraint. In this setting a learner observes sequentially a stream of labelled examples $(x_t, y_t)$, for $1 \\leq t \\leq T$, and returns at each iteration $t$ a hypothesis $h_t$ which is used to predict the label of each new example $x_t$. The learner's performance is measured by her regret against a known hypothesis class $\\mathcal{H}$. We require that the algorithm satisfies the following privacy constraint: the sequence $h_1, \\ldots, h_T$ of hypotheses output by the algorithm needs to be an $(\\epsilon, \\delta)$-differentially private function of the whole input sequence $(x_1, y_1), \\ldots, (x_T, y_T)$.We provide the first non-trivial regret bound for the realizable setting. Specifically, we show that if the class $\\mathcal{H}$ has constant Littlestone dimension then, given an oblivious sequence of labelled examples, there is a private learner that makes in expectation at most $O(\\log T)$ mistakes -- comparable to the optimal mistake bound in the non-private case, up to a logarithmic factor. Moreover, for general values of the Littlestone dimension $d$, the same mistake bound holds but with a doubly-exponential in $d$ factor.     A recent line of work has demonstrated a strong connection between classes that are online learnable and those that are differentially-private learnable. Our results strengthen this connection and show that an online learning algorithm can in fact be directly privatized (in the realizable setting).We also discuss an adaptive setting and provide a sublinear regret bound of $O(\\sqrt{T})$.",
    "authors": [
      "Golowich, Noah",
      "Livni, Roi"
    ]
  },
  {
    "id": "5fcc629edc0cfa360016263112fe8058",
    "title": "Dual Parameterization of Sparse Variational Gaussian Processes",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5fcc629edc0cfa360016263112fe8058-Paper.pdf",
    "abstract": "Sparse variational Gaussian process (SVGP) methods are a common choice for non-conjugate Gaussian process inference because of their computational benefits. In this paper, we improve their computational efficiency by using a dual parameterization where each data example is assigned dual parameters, similarly to site parameters used in expectation propagation. Our dual parameterization speeds-up inference using natural gradient descent, and provides a tighter evidence lower bound for hyperparameter learning. The approach has the same memory cost as the current SVGP methods, but it is faster and more accurate.",
    "authors": [
      "ADAM, Vincent",
      "Chang, Paul",
      "Khan, Mohammad Emtiyaz E.",
      "Solin, Arno"
    ]
  },
  {
    "id": "5fd0b37cd7dbbb00f97ba6ce92bf5add",
    "title": "Learning to dehaze with polarization",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5fd0b37cd7dbbb00f97ba6ce92bf5add-Paper.pdf",
    "abstract": "Haze, a common kind of bad weather caused by atmospheric scattering, decreases the visibility of scenes and degenerates the performance of computer vision algorithms. Single-image dehazing methods have shown their effectiveness in a large variety of scenes, however, they are based on handcrafted priors or learned features, which do not generalize well to real-world images. Polarization information can be used to relieve its ill-posedness, however, real-world images are still challenging since existing polarization-based methods usually assume that the transmitted light is not significantly polarized, and they require specific clues to estimate necessary physical parameters. In this paper, we propose a generalized physical formation model of hazy images and a robust polarization-based dehazing pipeline without the above assumption or requirement, along with a neural network tailored to the pipeline. Experimental results show that our approach achieves state-of-the-art performance on both synthetic data and real-world hazy images.",
    "authors": [
      "Zhou, Chu",
      "Teng, Minggui",
      "Han, Yufei",
      "Xu, Chao",
      "Shi, Boxin"
    ]
  },
  {
    "id": "5fd2c06f558321eff612bbbe455f6fbd",
    "title": "Conservative Data Sharing for Multi-Task Offline Reinforcement Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5fd2c06f558321eff612bbbe455f6fbd-Paper.pdf",
    "abstract": "Offline reinforcement learning (RL) algorithms have shown promising results in domains where abundant pre-collected data is available. However, prior methods focus on solving individual problems from scratch with an offline dataset without considering how an offline RL agent can acquire multiple skills. We argue that a natural use case of offline RL is in settings where we can pool large amounts of data collected in various scenarios for solving different tasks, and utilize all of this data to learn behaviors for all the tasks more effectively rather than training each one in isolation. However, sharing data across all tasks in multi-task offline RL performs surprisingly poorly in practice. Thorough empirical analysis, we find that sharing data can actually exacerbate the distributional shift between the learned policy and the dataset, which in turn can lead to divergence of the learned policy and poor performance. To address this challenge, we develop a simple technique for data- sharing in multi-task offline RL that routes data based on the improvement over the task-specific data. We call this approach conservative data sharing (CDS), and it can be applied with multiple single-task offline RL methods. On a range of challenging multi-task locomotion, navigation, and vision-based robotic manipulation problems, CDS achieves the best or comparable performance compared to prior offline multi- task RL methods and previous data sharing approaches.",
    "authors": [
      "Yu, Tianhe",
      "Kumar, Aviral",
      "Chebotar, Yevgen",
      "Hausman, Karol",
      "Levine, Sergey",
      "Finn, Chelsea"
    ]
  },
  {
    "id": "5fde40544cff0001484ecae2466ce96e",
    "title": "Universal Rate-Distortion-Perception Representations for Lossy Compression",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5fde40544cff0001484ecae2466ce96e-Paper.pdf",
    "abstract": "In the context of lossy compression, Blau \\& Michaeli (2019) adopt a mathematical notion of perceptual quality and define the information rate-distortion-perception function, generalizing the classical rate-distortion tradeoff. We consider the notion of universal representations in which one may fix an encoder and vary the decoder to achieve any point within a collection of distortion and perception constraints. We prove that the corresponding information-theoretic universal rate-distortion-perception function is operationally achievable in an approximate sense. Under MSE distortion, we show that the entire distortion-perception tradeoff of a Gaussian source can be achieved by a single encoder of the same rate asymptotically. We then characterize the achievable distortion-perception region for a fixed representation in the case of arbitrary distributions, and identify conditions under which the aforementioned results continue to hold approximately. This motivates the study of practical constructions that are approximately universal across the RDP tradeoff, thereby alleviating the need to design a new encoder for each objective. We provide experimental results on MNIST and SVHN suggesting that on image compression tasks, the operational tradeoffs achieved by machine learning models with a fixed encoder suffer only a small penalty when compared to their variable encoder counterparts.",
    "authors": [
      "Zhang, George",
      "Qian, Jingjing",
      "Chen, Jun",
      "Khisti, Ashish"
    ]
  },
  {
    "id": "5fe8fdc79ce292c39c5f209d734b7206",
    "title": "What\u2019s a good imputation to predict with missing values?",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5fe8fdc79ce292c39c5f209d734b7206-Paper.pdf",
    "abstract": "How to learn a good predictor on data with missing values? Most efforts focus on first imputing as well as possible and second learning on the completed data to predict the outcome. Yet, this widespread practice has no theoretical grounding. Here we show that for almost all imputation functions, an impute-then-regress procedure with a powerful learner is Bayes optimal. This result holds for all missing-values mechanisms, in contrast with the classic statistical results that require missing-at-random settings to use imputation in probabilistic modeling. Moreover, it implies that perfect conditional imputation is not needed for good prediction asymptotically. In fact, we show that on perfectly imputed data the best regression function will generally be discontinuous, which makes it hard to learn. Crafting instead the imputation so as to leave the regression function unchanged simply shifts the problem to learning discontinuous imputations. Rather, we suggest that it is easier to learn imputation and regression jointly. We propose such a procedure, adapting NeuMiss, a neural network capturing the conditional links across observed and unobserved variables whatever the missing-value pattern. Our experiments confirm that joint imputation and regression through NeuMiss is better than various two step procedures in a finite-sample regime.  ",
    "authors": [
      "Le Morvan, Marine",
      "Josse, Julie",
      "Scornet, Erwan",
      "Varoquaux, Gael"
    ]
  },
  {
    "id": "5ffaa9f5182c2a36843f438bb1fdbdea",
    "title": "Replacing Rewards with Examples: Example-Based Policy Search via Recursive Classification",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/5ffaa9f5182c2a36843f438bb1fdbdea-Paper.pdf",
    "abstract": "Reinforcement learning (RL) algorithms assume that users specify tasks by manually writing down a reward function. However, this process can be laborious and demands considerable technical expertise. Can we devise RL algorithms that instead enable users to specify tasks simply by providing examples of successful outcomes? In this paper, we derive a control algorithm that maximizes the future probability of these successful outcome examples. Prior work has approached similar problems with a two-stage process, first learning a reward function and then optimizing this reward function using another reinforcement learning algorithm. In contrast, our method directly learns a value function from transitions and successful outcomes, without learning this intermediate reward function. Our method therefore requires fewer hyperparameters to tune and lines of code to debug. We show that our method satisfies a new data-driven Bellman equation, where examples take the place of the typical reward function term. Experiments show that our approach outperforms prior methods that learn explicit reward functions.",
    "authors": [
      "Eysenbach, Ben",
      "Levine, Sergey",
      "Salakhutdinov, Russ R."
    ]
  },
  {
    "id": "60106888f8977b71e1f15db7bc9a88d1",
    "title": "Hierarchical Skills for Efficient Exploration",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/60106888f8977b71e1f15db7bc9a88d1-Paper.pdf",
    "abstract": "In reinforcement learning, pre-trained low-level skills have the potential to greatly facilitate exploration. However, prior knowledge of the downstream task is required to strike the right balance between generality (fine-grained control) and specificity (faster learning) in skill design. In previous work on continuous control, the sensitivity of methods to this trade-off has not been addressed explicitly, as locomotion provides a suitable prior for navigation tasks, which have been of foremost interest. In this work, we analyze this trade-off for low-level policy pre-training with a new benchmark suite of  diverse, sparse-reward tasks for bipedal robots. We alleviate the need for prior knowledge by proposing a hierarchical skill learning framework that acquires skills of varying complexity in an unsupervised manner. For utilization on downstream tasks, we present a three-layered hierarchical learning algorithm to automatically trade off between general and specific skills as required by the respective task. In our experiments, we show that our approach performs this trade-off effectively and achieves better results than current state-of-the-art methods for end-to-end hierarchical reinforcement learning and unsupervised skill discovery.",
    "authors": [
      "Gehring, Jonas",
      "Synnaeve, Gabriel",
      "Krause, Andreas",
      "Usunier, Nicolas"
    ]
  },
  {
    "id": "60243f9b1ac2dba11ff8131c8f4431e0",
    "title": "Evidential Softmax for Sparse Multimodal Distributions in Deep Generative Models",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/60243f9b1ac2dba11ff8131c8f4431e0-Paper.pdf",
    "abstract": "Many applications of generative models rely on the marginalization of their high-dimensional output probability distributions. Normalization functions that yield sparse probability distributions can make exact marginalization more computationally tractable. However, sparse normalization functions usually require alternative loss functions for training since the log-likelihood is undefined for sparse probability distributions. Furthermore, many sparse normalization functions often collapse the multimodality of distributions. In this work, we present ev-softmax, a sparse normalization function that preserves the multimodality of probability distributions. We derive its properties, including its gradient in closed-form, and introduce a continuous family of approximations to ev-softmax that have full support and can be trained with probabilistic loss functions such as negative log-likelihood and Kullback-Leibler divergence. We evaluate our method on a variety of generative models, including variational autoencoders and auto-regressive architectures. Our method outperforms existing dense and sparse normalization techniques in distributional accuracy. We demonstrate that ev-softmax successfully reduces the dimensionality of probability distributions while maintaining multimodality.",
    "authors": [
      "Chen, Phil",
      "Itkina, Mikhal",
      "Senanayake, Ransalu",
      "Kochenderfer, Mykel J"
    ]
  },
  {
    "id": "602443a3d6907117d8b4a308844e963e",
    "title": "Submodular + Concave",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/602443a3d6907117d8b4a308844e963e-Paper.pdf",
    "abstract": "It has been well established that first order optimization methods can converge to the maximal objective value of concave functions and provide constant factor approximation guarantees for (non-convex/non-concave) continuous submodular functions. In this work, we initiate the study of the maximization of functions of the form $F(x) = G(x) +C(x)$ over a solvable convex body $P$, where $G$ is a smooth DR-submodular function and $C$ is a smooth concave function. This class of functions is a strict extension of both concave and continuous DR-submodular functions for which no theoretical guarantee is known. We provide a suite of Frank-Wolfe style algorithms, which, depending on the nature of the objective function (i.e., if $G$ and $C$ are monotone or not, and non-negative or not) and on the nature of the set $P$ (i.e., whether it is downward closed or not), provide $1-1/e$, $1/e$, or $1/2$ approximation guarantees. We then use our algorithms to get a framework to smoothly interpolate between choosing a diverse set of elements from a given ground set (corresponding to the mode of a determinantal point process) and choosing a clustered set of elements (corresponding to the maxima of a suitable concave function). Additionally, we apply our algorithms to various functions in the above class (DR-submodular + concave) in both constrained and unconstrained settings, and show that our algorithms consistently outperform natural baselines.",
    "authors": [
      "Mitra, Siddharth",
      "Feldman, Moran",
      "Karbasi, Amin"
    ]
  },
  {
    "id": "606c90a06173d69682feb83037a68fec",
    "title": "DeepGEM: Generalized Expectation-Maximization for Blind Inversion",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/606c90a06173d69682feb83037a68fec-Paper.pdf",
    "abstract": "Typically, inversion algorithms assume that a forward model, which relates a source to its resulting measurements, is known and fixed. Using collected indirect measurements and the forward model, the goal becomes to recover the source. When the forward model is unknown, or imperfect, artifacts due to model mismatch occur in the recovery of the source. In this paper, we study the problem of blind inversion: solving an inverse problem with unknown or imperfect knowledge of the forward model parameters. We propose DeepGEM, a variational Expectation-Maximization (EM) framework that can be used to solve for the unknown parameters of the forward model in an unsupervised manner. DeepGEM makes use of a normalizing flow generative network to efficiently capture complex posterior distributions, which leads to more accurate evaluation of the source's posterior distribution used in EM. We showcase the effectiveness of our DeepGEM approach by achieving strong performance on the challenging problem of blind seismic tomography, where we significantly outperform the standard method used in seismology.  We also demonstrate the generality of DeepGEM by applying it to a simple case of blind deconvolution.",
    "authors": [
      "Gao, Angela",
      "Castellanos, Jorge",
      "Yue, Yisong",
      "Ross, Zachary",
      "Bouman, Katherine"
    ]
  },
  {
    "id": "60792d855cd8a912a97711f91a1f155c",
    "title": "Learning to Generate Visual Questions with Noisy Supervision",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/60792d855cd8a912a97711f91a1f155c-Paper.pdf",
    "abstract": "The task of visual question generation (VQG) aims to generate human-like neural questions from an image and potentially other side information (e.g., answer type or the answer itself). Existing works often suffer from the severe one image to many questions mapping problem, which generates uninformative and non-referential questions. Recent work has demonstrated that by leveraging double visual and answer hints, a model can faithfully generate much better quality questions. However, visual hints are not available naturally. Despite they proposed a simple rule-based similarity matching method to obtain candidate visual hints, they could be very noisy practically and thus restrict the quality of generated questions. In this paper, we present a novel learning approach for double-hints based VQG, which can be cast as a weakly supervised learning problem with noises. The key rationale is that the salient visual regions of interest can be viewed as a constraint to improve the generation procedure for producing high-quality questions. As a result, given the predicted salient visual regions of interest, we can focus on estimating the probability of being ground-truth questions, which in turn implicitly measures the quality of predicted visual hints. Experimental results on two benchmark datasets show that our proposed method outperforms the state-of-the-art approaches by a large margin on a variety of metrics, including both automatic machine metrics and human evaluation. ",
    "authors": [
      "Kai, Shen",
      "Wu, Lingfei",
      "Tang, Siliang",
      "Zhuang, Yueting",
      "he, zhen",
      "Ding, Zhuoye",
      "Xiao, Yun",
      "Long, Bo"
    ]
  },
  {
    "id": "6084e82a08cb979cf75ae28aed37ecd4",
    "title": "Pure Exploration in Kernel and Neural Bandits",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6084e82a08cb979cf75ae28aed37ecd4-Paper.pdf",
    "abstract": "We study pure exploration in bandits, where the dimension of the feature representation can be much larger than the number of arms. To overcome the curse of dimensionality, we propose to adaptively embed the feature representation of each arm into a lower-dimensional space and carefully deal with the induced model misspecifications. Our approach is conceptually very different from existing works that can either only handle low-dimensional linear bandits or passively deal with model misspecifications. We showcase the application of our approach to two pure exploration settings that were previously under-studied: (1) the reward function belongs to a possibly infinite-dimensional Reproducing Kernel Hilbert Space, and (2) the reward function is nonlinear and can be approximated by neural networks. Our main results provide sample complexity guarantees that only depend on the effective dimension of the feature spaces in the kernel or neural representations. Extensive experiments conducted on both synthetic and real-world datasets demonstrate the efficacy of our methods.",
    "authors": [
      "Zhu, Yinglun",
      "Zhou, Dongruo",
      "Jiang, Ruoxi",
      "Gu, Quanquan",
      "Willett, Rebecca",
      "Nowak, Robert"
    ]
  },
  {
    "id": "6097d8f3714205740f30debe1166744e",
    "title": "Numerical Composition of Differential Privacy",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6097d8f3714205740f30debe1166744e-Paper.pdf",
    "abstract": "We give a fast algorithm to compose privacy guarantees of differentially private (DP) algorithms to arbitrary accuracy. Our method is based on the notion of privacy loss random variables to quantify the privacy loss of DP algorithms. The running time and memory needed for our algorithm to approximate the privacy curve of a DP algorithm composed with itself $k$ times is $\\tilde{O}(\\sqrt{k})$. This improves over the best prior method by Koskela et al. (2020) which requires $\\tilde{\\Omega}(k^{1.5})$ running time. We demonstrate the utility of our algorithm by accurately computing the privacy loss of DP-SGD algorithm of Abadi et al. (2016) and showing that our algorithm speeds up the privacy computations by a few orders of magnitude compared to prior work, while maintaining similar accuracy.",
    "authors": [
      "Gopi, Sivakanth",
      "Lee, Yin Tat",
      "Wutschitz, Lukas"
    ]
  },
  {
    "id": "6098ed616e715171f0dabad60a8e5197",
    "title": "Coresets for Classification \u2013 Simplified and Strengthened",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6098ed616e715171f0dabad60a8e5197-Paper.pdf",
    "abstract": "We give relative error coresets for training linear classifiers with a broad class of loss functions, including the logistic loss and hinge loss. Our construction achieves $(1\\pm \\epsilon)$ relative error with $\\tilde O(d \\cdot \\mu_y(X)^2/\\epsilon^2)$ points, where $\\mu_y(X)$ is a natural complexity measure of the data matrix $X \\in \\mathbb{R}^{n \\times d}$ and label vector $y \\in \\{-1,1\\}^n$, introduced by Munteanu et al. 2018. Our result is based on subsampling data points with probabilities proportional to their  $\\ell_1$ $Lewis$ $weights$. It significantly improves on existing theoretical bounds and performs  well in practice, outperforming uniform subsampling along with other importance sampling methods. Our sampling distribution does not depend on the labels, so can be used for active learning. It also does not depend on the specific loss function, so a single coreset can be used  in multiple training scenarios.",
    "authors": [
      "Mai, Tung",
      "Musco, Cameron",
      "Rao, Anup"
    ]
  },
  {
    "id": "609c5e5089a9aa967232aba2a4d03114",
    "title": "Sequential Algorithms for Testing Closeness of Distributions",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/609c5e5089a9aa967232aba2a4d03114-Paper.pdf",
    "abstract": "  What advantage do sequential procedures provide over batch algorithms for testing properties of unknown distributions? Focusing on the problem of testing whether two distributions $\\mathcal{D}_1$ and $\\mathcal{D}_2$ on $\\{1,\\dots, n\\}$ are equal or $\\epsilon$-far, we give several answers to this question. We show that for a small alphabet size $n$, there is a sequential algorithm that outperforms any batch algorithm by a factor of at least $4$ in terms sample complexity. For a general alphabet size $n$, we give a sequential algorithm that uses no more samples than its batch counterpart, and possibly fewer if the actual distance between $\\mathcal{D}_1$ and $\\mathcal{D}_2$ is larger than $\\epsilon$. As a corollary, letting $\\epsilon$ go to $0$, we obtain a sequential algorithm for testing closeness (with no a priori bound on the distance between $\\mathcal{D}_1$ and $\\mathcal{D}_2$) with a sample complexity $\\tilde{\\mathcal{O}}(\\frac{n^{2/3}}{TV(\\mathcal{D}_1, \\mathcal{D}_2)^{4/3}})$: this improves over the $\\tilde{\\mathcal{O}}(\\frac{n/\\log n}{TV(\\mathcal{D}_1, \\mathcal{D}_2)^{2} })$ tester of [Daskalakis and Kawase 2017]  and is optimal up to multiplicative constants. We also establish limitations of sequential algorithms for the problem of testing closeness: they can improve the worst case number of samples by at most a constant factor. ",
    "authors": [
      "Oufkir, Aadil",
      "Fawzi, Omar",
      "Flammarion, Nicolas",
      "Garivier, Aur\u00e9lien"
    ]
  },
  {
    "id": "60b2149f6bafd1cc9d505496f09160ba",
    "title": "Overlapping Spaces for Compact Graph Representations",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/60b2149f6bafd1cc9d505496f09160ba-Paper.pdf",
    "abstract": "Various non-trivial spaces are becoming popular for embedding structured data such as graphs, texts, or images. Following spherical and hyperbolic spaces, more general product spaces have been proposed. However, searching for the best configuration of a product space is a resource-intensive procedure, which reduces the practical applicability of the idea. We generalize the concept of product space and introduce an overlapping space that does not have the configuration search problem. The main idea is to allow subsets of coordinates to be shared between spaces of different types (Euclidean, hyperbolic, spherical). As a result, we often need fewer coordinates to store the objects. Additionally, we propose an optimization algorithm that automatically learns the optimal configuration. Our experiments confirm that overlapping spaces outperform the competitors in graph embedding tasks with different evaluation metrics. We also perform an empirical analysis in a realistic information retrieval setup, where we compare all spaces by incorporating them into DSSM. In this case, the proposed overlapping space consistently achieves nearly optimal results without any configuration tuning. This allows for reducing training time, which can be essential in large-scale applications.",
    "authors": [
      "Shevkunov, Kirill",
      "Prokhorenkova, Liudmila"
    ]
  },
  {
    "id": "60c97bef031ec312b512c08565c1868e",
    "title": "Hyperparameter Tuning is All You Need for LISTA",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/60c97bef031ec312b512c08565c1868e-Paper.pdf",
    "abstract": "Learned Iterative Shrinkage-Thresholding Algorithm (LISTA) introduces the concept of unrolling an iterative algorithm and training it like a neural network. It has had great success on sparse recovery. In this paper, we show that adding momentum to intermediate variables in the LISTA network achieves a better convergence rate and, in particular, the network with instance-optimal parameters is superlinearly convergent. Moreover, our new theoretical results lead to a practical approach of automatically and adaptively calculating the parameters of a LISTA network layer based on its previous layers. Perhaps most surprisingly, such an adaptive-parameter procedure reduces the training of LISTA to tuning only three hyperparameters from data: a new record set in the context of the recent advances on trimming down LISTA complexity. We call this new ultra-light weight network HyperLISTA. Compared to state-of-the-art LISTA models, HyperLISTA achieves almost the same performance on seen data distributions and performs better when tested on unseen distributions (speci\ufb01cally, those with different sparsity levels and nonzero magnitudes). Code is available: https://github.com/VITA-Group/HyperLISTA.",
    "authors": [
      "Chen, Xiaohan",
      "Liu, Jialin",
      "Wang, Zhangyang",
      "Yin, Wotao"
    ]
  },
  {
    "id": "60cb558c40e4f18479664069d9642d5a",
    "title": "Foundations of Symbolic Languages for Model Interpretability",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/60cb558c40e4f18479664069d9642d5a-Paper.pdf",
    "abstract": "Several queries and scores have recently been proposed to explain individual predictions over ML models. Examples include queries based on \u201canchors\u201d, which are parts of an instance that are sufficient to justify its classification, and \u201cfeature-perturbation\u201d scores such as SHAP. Given the need for flexible, reliable, and easy-to-apply interpretability methods for ML models, we foresee the need for developing declarative languages to naturally specify different explainability queries. We do this in a principled way by rooting such a language in a logic called FOIL, which allows for expressing many simple but important explainability queries, and might serve as a core for more expressive interpretability languages. We study the computational complexity of FOIL queries over two classes of ML models often deemed to be easily interpretable: decision trees and more general decision diagrams. Since the number of possible inputs for an ML model is exponential in its dimension, tractability of the FOIL evaluation problem is delicate but can be achieved by either restricting the structure of the models, or the fragment of FOIL being evaluated.  We also present a prototype implementation of FOIL wrapped in a high-level declarative language and perform experiments showing that such a language can be used in practice.",
    "authors": [
      "Arenas, Marcelo",
      "B\u00e1ez, Daniel",
      "Barcel\u00f3, Pablo",
      "P\u00e9rez, Jorge",
      "Subercaseaux, Bernardo"
    ]
  },
  {
    "id": "60ce36723c17bbac504f2ef4c8a46995",
    "title": "Bridging Offline Reinforcement Learning and Imitation Learning: A Tale of Pessimism",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/60ce36723c17bbac504f2ef4c8a46995-Paper.pdf",
    "abstract": "Offline (or batch) reinforcement learning (RL) algorithms seek to learn an optimal policy from a fixed dataset without active data collection. Based on the composition of the offline dataset, two main methods are used: imitation learning which is suitable for expert datasets, and vanilla offline RL which often requires uniform coverage datasets. From a practical standpoint, datasets often deviate from these two extremes and the exact data composition is usually unknown. To bridge this gap, we present a new offline RL framework that smoothly interpolates between the two extremes of data composition, hence unifying imitation learning and vanilla offline RL. The new framework is centered around a weak version of the concentrability coefficient that measures the deviation of the behavior policy from the expert policy alone. Under this new framework, we ask: can one develop an algorithm that achieves a minimax optimal rate adaptive to unknown data composition? To address this question, we consider a lower confidence bound (LCB) algorithm developed based on pessimism in the face of uncertainty in offline RL. We study finite-sample properties of LCB as well as information-theoretic limits in multi-armed bandits, contextual bandits, and Markov decision processes (MDPs). Our analysis reveals surprising facts about optimality rates. In particular, in both contextual bandits and RL, LCB achieves a faster rate of $1/N$ for nearly-expert datasets compared to the usual rate of $1/\\sqrt{N}$ in offline RL, where $N$ is the batch dataset sample size. In contextual bandits with at least two contexts, we prove that LCB is adaptively optimal for the entire data composition range, achieving a smooth transition from imitation learning to offline RL. We further show that LCB is almost adaptively optimal in MDPs.",
    "authors": [
      "Rashidinejad, Paria",
      "Zhu, Banghua",
      "Ma, Cong",
      "Jiao, Jiantao",
      "Russell, Stuart"
    ]
  },
  {
    "id": "615299acbbac3e21302bbc435091ad9f",
    "title": "Impression learning: Online representation learning with synaptic plasticity",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/615299acbbac3e21302bbc435091ad9f-Paper.pdf",
    "abstract": "Understanding how the brain constructs statistical models of the sensory world remains a longstanding challenge for computational neuroscience. Here, we derive an unsupervised local synaptic plasticity rule that trains neural circuits to infer latent structure from sensory stimuli via a novel loss function for approximate online Bayesian inference. The learning algorithm is driven by a local error signal computed between two factors that jointly contribute to neural activity: stimulus drive and internal predictions --- the network's 'impression' of the stimulus. Physiologically, we associate these two components with the basal and apical dendrites of pyramidal neurons, respectively. We show that learning can be implemented online, is capable of capturing temporal dependencies in continuous input streams, and generalizes to hierarchical architectures. Furthermore, we demonstrate both analytically and empirically that the algorithm is more data-efficient than a three-factor plasticity alternative, enabling it to learn statistics of high-dimensional, naturalistic inputs. Overall, the model provides a bridge from mechanistic accounts of synaptic plasticity to algorithmic descriptions of unsupervised probabilistic learning and inference.",
    "authors": [
      "Bredenberg, Colin",
      "Lyo, Benjamin",
      "Simoncelli, Eero",
      "Savin, Cristina"
    ]
  },
  {
    "id": "618faa1728eb2ef6e3733645273ab145",
    "title": "How Well do Feature Visualizations Support Causal Understanding of CNN Activations?",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/618faa1728eb2ef6e3733645273ab145-Paper.pdf",
    "abstract": "A precise understanding of why units in an artificial network respond to certain stimuli would constitute a big step towards explainable artificial intelligence. One widely used approach towards this goal is to visualize unit responses via activation maximization. These feature visualizations are purported to provide humans with precise information about the image features that cause a unit to be activated - an advantage over other alternatives like strongly activating dataset samples. If humans indeed gain causal insight from visualizations, this should enable them to predict the effect of an intervention, such as how occluding a certain patch of the image (say, a dog's head) changes a unit's activation. Here, we test this hypothesis by asking humans to decide which of two square occlusions causes a larger change to a unit's activation.Both a large-scale crowdsourced experiment and measurements with experts show that on average the extremely activating feature visualizations by Olah et al. (2017) indeed help humans on this task ($68 \\pm 4$% accuracy; baseline performance without any visualizations is $60 \\pm 3$%). However, they do not provide any substantial advantage over other visualizations (such as e.g. dataset samples), which yield similar performance ($66\\pm3$% to $67 \\pm3$% accuracy). Taken together, we propose an objective psychophysical task to quantify the benefit of unit-level interpretability methods for humans, and find no evidence that a widely-used feature visualization method provides humans with better \"causal understanding\" of unit activations than simple alternative visualizations.",
    "authors": [
      "Zimmermann, Roland S.",
      "Borowski, Judy",
      "Geirhos, Robert",
      "Bethge, Matthias",
      "Wallis, Thomas",
      "Brendel, Wieland"
    ]
  },
  {
    "id": "619427579e7b067421f6aa89d4a8990c",
    "title": "Fixes That Fail: Self-Defeating Improvements in Machine-Learning Systems",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/619427579e7b067421f6aa89d4a8990c-Paper.pdf",
    "abstract": "Machine-learning systems such as self-driving cars or virtual assistants are composed of a large number of machine-learning models that recognize image content, transcribe speech, analyze natural language, infer preferences, rank options, etc. Models in these systems are often developed and trained independently, which raises an obvious concern: Can improving a machine-learning model make the overall system worse? We answer this question affirmatively by showing that improving a model can deteriorate the performance of downstream models, even after those downstream models are retrained. Such self-defeating improvements are the result of entanglement between the models in the system. We perform an error decomposition of systems with multiple machine-learning models, which sheds light on the types of errors that can lead to self-defeating improvements. We also present the results of experiments which show that self-defeating improvements emerge in a realistic stereo-based detection system for cars and pedestrians.",
    "authors": [
      "Wu, Ruihan",
      "Guo, Chuan",
      "Hannun, Awni",
      "van der Maaten, Laurens"
    ]
  },
  {
    "id": "6195f47dcff14b8f242aa333cdb2703e",
    "title": "Coarse-to-fine Animal Pose and Shape Estimation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6195f47dcff14b8f242aa333cdb2703e-Paper.pdf",
    "abstract": "Most existing animal pose and shape estimation approaches reconstruct animal meshes with a parametric SMAL model. This is because the low-dimensional pose and shape parameters of the SMAL model makes it easier for deep networks to learn the high-dimensional animal meshes. However, the SMAL model is learned from scans of toy animals with limited pose and shape variations, and thus may not be able to represent highly varying real animals well. This may result in poor fittings of the estimated meshes to the 2D evidences, e.g. 2D keypoints or silhouettes.  To mitigate this problem, we propose a coarse-to-fine approach to reconstruct 3D animal mesh from a single image. The coarse estimation stage first estimates the pose, shape and translation parameters of the SMAL model. The estimated meshes are then used as a starting point by a graph convolutional network (GCN) to predict a per-vertex deformation in the refinement stage. This combination of SMAL-based and vertex-based representations benefits from both parametric and non-parametric representations. We design our mesh refinement GCN (MRGCN) as an encoder-decoder structure with hierarchical feature representations to overcome the limited receptive field of traditional GCNs. Moreover, we observe that the global image feature used by existing animal mesh reconstruction works is unable to capture detailed shape information for mesh refinement. We thus introduce a local feature extractor to retrieve a vertex-level feature and use it together with the global feature as the input of the MRGCN. We test our approach on the StanfordExtra dataset and achieve state-of-the-art results. Furthermore, we test the generalization capacity of our approach on the Animal Pose and BADJA datasets. Our code is available at the project website.",
    "authors": [
      "Li, Chen",
      "Lee, Gim Hee"
    ]
  },
  {
    "id": "61b1fb3f59e28c67f3925f3c79be81a1",
    "title": "Meta-Learning Sparse Implicit Neural Representations",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/61b1fb3f59e28c67f3925f3c79be81a1-Paper.pdf",
    "abstract": "Implicit neural representations are a promising new avenue of representing general signals by learning a continuous function that, parameterized as a neural network, maps the domain of a signal to its codomain; the mapping from spatial coordinates of an image to its pixel values, for example. Being capable of conveying fine details in a high dimensional signal, unboundedly of its domain, implicit neural representations ensure many advantages over conventional discrete representations. However, the current approach is difficult to scale for a large number of signals or a data set, since learning a neural representation---which is parameter heavy by itself---for each signal individually requires a lot of memory and computations. To address this issue, we propose to leverage a meta-learning approach in combination with network compression under a sparsity constraint, such that it renders a well-initialized sparse parameterization that evolves quickly to represent a set of unseen signals in the subsequent training. We empirically demonstrate that meta-learned sparse neural representations achieve a much smaller loss than dense meta-learned models with the same number of parameters, when trained to fit each signal using the same number of optimization steps.",
    "authors": [
      "Lee, Jaeho",
      "Tack, Jihoon",
      "Lee, Namhoon",
      "Shin, Jinwoo"
    ]
  },
  {
    "id": "61b4a64be663682e8cb037d9719ad8cd",
    "title": "Rethinking Space-Time Networks with Improved Memory Coverage for Efficient Video Object Segmentation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/61b4a64be663682e8cb037d9719ad8cd-Paper.pdf",
    "abstract": "This paper presents a simple yet effective approach to modeling space-time correspondences in the context of video object segmentation. Unlike most existing approaches, we establish correspondences directly between frames without re-encoding the mask features for every object, leading to a highly efficient and robust framework. With the correspondences, every node in the current query frame is inferred by aggregating features from the past in an associative fashion. We cast the aggregation process as a voting problem and find that the existing inner-product affinity leads to poor use of memory with a small (fixed) subset of memory nodes dominating the votes, regardless of the query. In light of this phenomenon, we propose using the negative squared Euclidean distance instead to compute the affinities. We validated that every memory node now has a chance to contribute, and experimentally showed that such diversified voting is beneficial to both memory efficiency and inference accuracy. The synergy of correspondence networks and diversified voting works exceedingly well, achieves new state-of-the-art results on both DAVIS and YouTubeVOS datasets while running significantly faster at 20+ FPS for multiple objects without bells and whistles.",
    "authors": [
      "Cheng, Ho Kei",
      "Tai, Yu-Wing",
      "Tang, Chi-Keung"
    ]
  },
  {
    "id": "61f2585b0ebcf1f532c4d1ec9a7d51aa",
    "title": "Sparse Spiking Gradient Descent",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/61f2585b0ebcf1f532c4d1ec9a7d51aa-Paper.pdf",
    "abstract": "There is an increasing interest in emulating Spiking Neural Networks (SNNs) on neuromorphic computing devices due to their low energy consumption. Recent advances have allowed training SNNs to a point where they start to compete with traditional Artificial Neural Networks (ANNs) in terms of accuracy, while at the same time being energy efficient when run on neuromorphic hardware. However, the process of training SNNs is still based on dense tensor operations originally developed for ANNs which do not leverage the spatiotemporally sparse nature of SNNs. We present here the first sparse SNN backpropagation algorithm which achieves the same or better accuracy as current state of the art methods while being significantly faster and more memory efficient. We show the effectiveness of our method on real datasets of varying complexity (Fashion-MNIST, Neuromophic-MNIST and Spiking Heidelberg Digits) achieving a speedup in the backward pass of up to $150$x, and $85\\%$ more memory efficient, without losing accuracy. ",
    "authors": [
      "Perez-Nieves, Nicolas",
      "Goodman, Dan"
    ]
  },
  {
    "id": "61f3a6dbc9120ea78ef75544826c814e",
    "title": "Rethinking Calibration of Deep Neural Networks: Do Not Be Afraid of Overconfidence",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/61f3a6dbc9120ea78ef75544826c814e-Paper.pdf",
    "abstract": "Capturing accurate uncertainty quantification of the prediction from deep neural networks is important in many real-world decision-making applications. A reliable predictor is expected to be accurate when it is confident about its predictions and indicate high uncertainty when it is likely to be inaccurate. However, modern neural networks have been found to be poorly calibrated, primarily in the direction of overconfidence. In recent years, there is a surge of research on model calibration by leveraging implicit or explicit regularization techniques during training, which obtain well calibration by avoiding overconfident outputs. In our study, we empirically found that despite the predictions obtained from these regularized models are better calibrated, they suffer from not being as calibratable, namely, it is harder to further calibrate their predictions with post-hoc calibration methods like temperature scaling and histogram binning. We conduct a series of empirical studies showing that overconfidence may not hurt final calibration performance if post-hoc calibration is allowed, rather, the penalty of confident outputs will compress the room of potential improvements in post-hoc calibration phase. Our experimental findings point out a new direction to improve calibration of DNNs by considering main training and post-hoc calibration as a unified framework. ",
    "authors": [
      "Wang, Deng-Bao",
      "Feng, Lei",
      "Zhang, Min-Ling"
    ]
  },
  {
    "id": "62889e73828c756c961c5a6d6c01a463",
    "title": "Towards Efficient and Effective Adversarial Training",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/62889e73828c756c961c5a6d6c01a463-Paper.pdf",
    "abstract": "The vulnerability of Deep Neural Networks to adversarial attacks has spurred immense interest towards improving their robustness. However, present state-of-the-art adversarial defenses involve the use of 10-step adversaries during training, which renders them computationally infeasible for application to large-scale datasets. While the recent single-step defenses show promising direction, their robustness is not on par with multi-step training methods. In this work, we bridge this performance gap by introducing a novel Nuclear-Norm regularizer on network predictions to enforce function smoothing in the vicinity of data samples.  While prior works consider each data sample independently, the proposed regularizer uses the joint statistics of adversarial samples across a training minibatch to enhance optimization during both attack generation and training, obtaining state-of-the-art results amongst efficient defenses. We achieve further gains by incorporating exponential averaging of network weights over training iterations. We finally introduce a Hybrid training approach that combines the effectiveness of a two-step variant of the proposed defense with the efficiency of a single-step defense. We demonstrate superior results when compared to multi-step defenses such as TRADES and PGD-AT as well, at a significantly lower computational cost.",
    "authors": [
      "Sriramanan, Gaurang",
      "Addepalli, Sravanti",
      "Baburaj, Arya",
      "R, Venkatesh Babu"
    ]
  },
  {
    "id": "628f16b29939d1b060af49f66ae0f7f8",
    "title": "Intriguing Properties of Contrastive Losses",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/628f16b29939d1b060af49f66ae0f7f8-Paper.pdf",
    "abstract": "We study three intriguing properties of contrastive learning. First, we generalize the standard contrastive loss to a broader family of losses, and we find that various instantiations of the generalized loss perform similarly under the presence of a multi-layer non-linear projection head. Second, we study if instance-based contrastive learning (with a global image representation) can learn well on images with multiple objects present. We find that meaningful hierarchical local features can be learned despite the fact that these objectives operate on global instance-level features. Finally, we study the phenomenon of feature suppression among competing features shared across augmented views, such as \"color distribution\" vs \"object class\". We construct datasets with explicit and controllable competing features, and show that, for contrastive learning, a few bits of easy-to-learn shared features can suppress, and even fully prevent, the learning of other sets of competing features. In scenarios where there are multiple objects in an image, the dominant object would suppress the learning of smaller objects. Existing contrastive learning methods critically rely on data augmentation to favor certain sets of features over others, and could suffer from learning saturation for scenarios where existing augmentations cannot fully address the feature suppression. This poses open challenges to existing contrastive learning techniques.",
    "authors": [
      "Chen, Ting",
      "Luo, Calvin",
      "Li, Lala"
    ]
  },
  {
    "id": "62e0973455fd26eb03e91d5741a4a3bb",
    "title": "Detecting Moments and Highlights in Videos via Natural Language Queries",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/62e0973455fd26eb03e91d5741a4a3bb-Paper.pdf",
    "abstract": "Detecting customized moments and highlights from videos given natural language (NL) user queries is an important but under-studied topic. One of the challenges in pursuing this direction is the lack of annotated data. To address this issue, we present the Query-based Video Highlights (QVHighlights) dataset. It consists of over 10,000 YouTube videos, covering a wide range of topics, from everyday activities and travel in lifestyle vlog videos to social and political activities in news videos. Each video in the dataset is annotated with: (1) a human-written free-form NL query, (2) relevant moments in the video w.r.t. the query, and (3) five-point scale saliency scores for all query-relevant clips. This comprehensive annotation enables us to develop and evaluate systems that detect relevant moments as well as salient highlights for diverse, flexible user queries. We also present a strong baseline for this task, Moment-DETR, a transformer encoder-decoder model that views moment retrieval as a direct set prediction problem, taking extracted video and query representations as inputs and predicting moment coordinates and saliency scores end-to-end. While our model does not utilize any human prior, we show that it performs competitively when compared to well-engineered architectures. With weakly supervised pretraining using ASR captions, Moment-DETR substantially outperforms previous methods. Lastly, we present several ablations and visualizations of Moment-DETR. Data and code is publicly available at https://github.com/jayleicn/moment_detr.",
    "authors": [
      "Lei, Jie",
      "Berg, Tamara L",
      "Bansal, Mohit"
    ]
  },
  {
    "id": "62e7f2e090fe150ef8deb4466fdc81b3",
    "title": "Stochastic optimization under time drift: iterate averaging, step-decay schedules, and high probability guarantees",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/62e7f2e090fe150ef8deb4466fdc81b3-Paper.pdf",
    "abstract": "We consider the problem of minimizing a convex function that is evolving in time according to unknown and possibly stochastic dynamics. Such problems abound in the machine learning and signal processing literature, under the names of concept drift and stochastic tracking. We provide novel non-asymptotic convergence guarantees for stochastic algorithms with iterate averaging, focusing on bounds valid both in expectation and with high probability. Notably, we show that the tracking efficiency of the proximal stochastic gradient method depends only logarithmically on the initialization quality when equipped with a step-decay schedule.",
    "authors": [
      "Cutler, Joshua",
      "Drusvyatskiy, Dmitriy",
      "Harchaoui, Zaid"
    ]
  },
  {
    "id": "6332a8f62e3a9d5831724f2ffe55cae0",
    "title": "Learning Stable Deep Dynamics Models for Partially Observed or Delayed Dynamical Systems",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6332a8f62e3a9d5831724f2ffe55cae0-Paper.pdf",
    "abstract": "Learning how complex dynamical systems evolve over time is a key challenge in system identification. For safety critical systems, it is often crucial that the learned model is guaranteed to converge to some equilibrium point. To this end, neural ODEs regularized with neural Lyapunov functions are a promising approach when states are fully observed. For practical applications however, {\\em partial observations} are the norm. As we will demonstrate, initialization of unobserved augmented states can become a key problem for neural ODEs. To alleviate this issue, we propose to augment the system's state with its history. Inspired by state augmentation in discrete-time systems, we thus obtain {\\em neural delay differential equations}. Based on classical time delay stability analysis, we then show how to ensure stability of the learned models, and theoretically analyze our approach. Our experiments demonstrate its applicability to stable system identification of partially observed systems and learning a stabilizing feedback policy in delayed feedback control.",
    "authors": [
      "Schlaginhaufen, Andreas",
      "Wenk, Philippe",
      "Krause, Andreas",
      "Dorfler, Florian"
    ]
  },
  {
    "id": "639d79cc857a6c76c2723b7e014fccb0",
    "title": "An Uncertainty Principle is a Price of Privacy-Preserving Microdata",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/639d79cc857a6c76c2723b7e014fccb0-Paper.pdf",
    "abstract": "Privacy-protected microdata are often the desired output of a differentially private algorithm since  microdata is familiar and convenient for downstream users. However, there is a statistical price for this kind of convenience. We show that an uncertainty principle governs the trade-off between accuracy for a population of interest (``sum query'') vs. accuracy for its component sub-populations (``point queries''). Compared to differentially private query answering systems that are not required to produce microdata, accuracy can degrade by a logarithmic factor. For example, in the case of pure differential privacy, without the microdata requirement, one can provide noisy answers to the sum query and all point queries while guaranteeing that each answer has squared error $O(1/\\epsilon^2)$. With the microdata requirement, one must choose between allowing an additional $\\log^2(d)$ factor ($d$ is the number of point queries) for some point queries or allowing an extra $O(d^2)$ factor for the sum query. We present lower bounds for pure, approximate, and concentrated differential privacy. We propose mitigation strategies and create a collection of benchmark datasets that can be used for public study of this problem.",
    "authors": [
      "Abowd, John",
      "Ashmead, Robert",
      "Cumings-Menon, Ryan",
      "Garfinkel, Simson",
      "Kifer, Daniel",
      "Leclerc, Philip",
      "Sexton, William",
      "Simpson, Ashley",
      "Task, Christine",
      "Zhuravlev, Pavel"
    ]
  },
  {
    "id": "63c3ddcc7b23daa1e42dc41f9a44a873",
    "title": "Fairness in Ranking under Uncertainty",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/63c3ddcc7b23daa1e42dc41f9a44a873-Paper.pdf",
    "abstract": "Fairness has emerged as an important consideration in algorithmic decision making. Unfairness occurs when an agent with higher merit obtains a worse outcome than an agent with lower merit. Our central point is that a primary cause of unfairness is uncertainty. A principal or algorithm making decisions never has access to the agents' true merit, and instead uses proxy features that only imperfectly predict merit (e.g., GPA, star ratings, recommendation letters). None of these ever fully capture an agent's merit; yet existing approaches have mostly been defining fairness notions directly based on observed features and outcomes.Our primary point is that it is more principled to acknowledge and model the uncertainty explicitly. The role of observed features is to give rise to a posterior distribution of the agents' merits. We use this viewpoint to define a notion of approximate fairness in ranking. We call an algorithm $\\phi$-fair (for $\\phi \\in [0,1]$) if it has the following property for all agents $x$ and all $k$: if agent $x$ is among the top $k$ agents with respect to merit with probability at least $\\rho$ (according to the posterior merit distribution), then the algorithm places the agent among the top $k$ agents in its ranking with probability at least $\\phi \\rho$.We show how to compute rankings that optimally trade off approximate fairness against utility to the principal. In addition to the theoretical characterization, we present an empirical analysis of the potential impact of the approach in simulation studies. For real-world validation, we applied the approach in the context of a paper recommendation system that we built and fielded at the KDD 2020 conference.",
    "authors": [
      "Singh, Ashudeep",
      "Kempe, David",
      "Joachims, Thorsten"
    ]
  },
  {
    "id": "63c4b1baf3b4460fa9936b1a20919bec",
    "title": "Generalized Proximal Policy Optimization with Sample Reuse",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/63c4b1baf3b4460fa9936b1a20919bec-Paper.pdf",
    "abstract": "In real-world decision making tasks, it is critical for data-driven reinforcement learning methods to be both stable and sample efficient. On-policy methods typically generate reliable policy improvement throughout training, while off-policy methods make more efficient use of data through sample reuse. In this work, we combine the theoretically supported stability benefits of on-policy algorithms with the sample efficiency of off-policy algorithms. We develop policy improvement guarantees that are suitable for the off-policy setting, and connect these bounds to the clipping mechanism used in Proximal Policy Optimization. This motivates an off-policy version of the popular algorithm that we call Generalized Proximal Policy Optimization with Sample Reuse. We demonstrate both theoretically and empirically that our algorithm delivers improved performance by effectively balancing the competing goals of stability and sample efficiency.",
    "authors": [
      "Queeney, James",
      "Paschalidis, Yannis",
      "Cassandras, Christos G"
    ]
  },
  {
    "id": "63dc7ed1010d3c3b8269faf0ba7491d4",
    "title": "Mosaicking to Distill: Knowledge Distillation from Out-of-Domain Data",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/63dc7ed1010d3c3b8269faf0ba7491d4-Paper.pdf",
    "abstract": "Knowledge distillation~(KD) aims to craft a compact student model that imitates the behavior of a pre-trained teacher in a target domain. Prior KD approaches, despite their gratifying results, have largely relied on the premise that \\emph{in-domain} data is available to carry out the knowledge transfer. Such an assumption, unfortunately, in many cases violates the practical setting, since the original training data or even the data domain is often unreachable due to privacy or copyright reasons. In this paper, we attempt to tackle an ambitious task, termed as \\emph{out-of-domain} knowledge distillation~(OOD-KD), which allows us to conduct KD using only OOD data that can be readily obtained at a very low cost. Admittedly,  OOD-KD is by nature a highly challenging task due to the agnostic domain gap. To this end, we introduce a handy yet surprisingly efficacious approach, dubbed as~\\textit{MosaicKD}. The key insight behind MosaicKD lies in that, samples from various domains share common local patterns, even though their global semantic may vary significantly; these shared local patterns, in turn, can be re-assembled analogous to mosaic tiling, to approximate the in-domain data and to further alleviating the domain discrepancy. In MosaicKD, this is achieved through a four-player min-max game, in which a generator, a discriminator, a student network,  are collectively trained in an adversarial manner, partially under the guidance of a pre-trained teacher. We validate MosaicKD over {classification and semantic segmentation tasks} across various benchmarks, and demonstrate that it yields results much superior to the state-of-the-art counterparts on OOD data. Our code is available at \\url{https://github.com/zju-vipa/MosaicKD}.",
    "authors": [
      "Fang, Gongfan",
      "Bao, Yifan",
      "Song, Jie",
      "Wang, Xinchao",
      "Xie, Donglin",
      "Shen, Chengchao",
      "Song, Mingli"
    ]
  },
  {
    "id": "64254db8396e404d9223914a0bd355d2",
    "title": "Batch Active Learning at Scale",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/64254db8396e404d9223914a0bd355d2-Paper.pdf",
    "abstract": "The ability to train complex and highly effective models often requires an abundance of training data, which can easily become a bottleneck in cost, time, and computational resources. Batch active learning, which adaptively issues batched queries to a labeling oracle, is a common approach for addressing this problem. The practical benefits of batch sampling come with the downside of less adaptivity and the risk of sampling redundant examples within a batch -- a risk that grows with the batch size. In this work, we analyze an efficient active learning algorithm, which focuses on the large batch setting. In particular, we show that our sampling method, which combines notions of uncertainty and diversity, easily scales to batch sizes (100K-1M) several orders of magnitude larger than used in previous studies and provides significant improvements in model training efficiency compared to recent baselines. Finally, we provide an initial theoretical analysis, proving label complexity guarantees for a related sampling method, which we show is approximately equivalent to our sampling method in specific settings.",
    "authors": [
      "Citovsky, Gui",
      "DeSalvo, Giulia",
      "Gentile, Claudio",
      "Karydas, Lazaros",
      "Rajagopalan, Anand",
      "Rostamizadeh, Afshin",
      "Kumar, Sanjiv"
    ]
  },
  {
    "id": "642e92efb79421734881b53e1e1b18b6",
    "title": "Joint Semantic Mining for Weakly Supervised RGB-D Salient Object Detection",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/642e92efb79421734881b53e1e1b18b6-Paper.pdf",
    "abstract": "Training saliency detection models with weak supervisions, e.g., image-level tags or captions, is appealing as it removes the costly demand of per-pixel annotations. Despite the rapid progress of RGB-D saliency detection in fully-supervised setting, it however remains an unexplored territory when only weak supervision signals are available. This paper is set to tackle the problem of weakly-supervised RGB-D salient object detection. The key insight in this effort is the idea of maintaining per-pixel pseudo-labels with iterative refinements by reconciling the multimodal input signals in our joint semantic mining (JSM). Considering the large variations in the raw depth map and the lack of explicit pixel-level supervisions, we propose spatial semantic modeling (SSM) to capture saliency-specific depth cues from the raw depth and produce depth-refined pseudo-labels. Moreover, tags and captions are incorporated via a fill-in-the-blank training in our textual semantic modeling (TSM) to estimate the confidences of competing pseudo-labels. At test time, our model involves only a light-weight sub-network of the training pipeline, i.e., it requires only an RGB image as input, thus allowing efficient inference. Extensive evaluations demonstrate the effectiveness of our approach under the weakly-supervised setting. Importantly, our method could also be adapted to work in both fully-supervised and unsupervised paradigms. In each of these scenarios, superior performance has been attained by our approach with comparing to the state-of-the-art dedicated methods. As a by-product, a CapS dataset is constructed by augmenting existing benchmark training set with additional image tags and captions. ",
    "authors": [
      "Li, Jingjing",
      "Ji, Wei",
      "Bi, Qi",
      "Yan, Cheng",
      "Zhang, Miao",
      "Piao, Yongri",
      "Lu, Huchuan",
      "cheng, Li"
    ]
  },
  {
    "id": "64517d8435994992e682b3e4aa0a0661",
    "title": "Not All Images are Worth 16x16 Words: Dynamic Transformers for Efficient Image Recognition",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/64517d8435994992e682b3e4aa0a0661-Paper.pdf",
    "abstract": "Vision Transformers (ViT) have achieved remarkable success in large-scale image recognition. They split every 2D image into a fixed number of patches, each of which is treated as a token. Generally, representing an image with more tokens would lead to higher prediction accuracy, while it also results in drastically increased computational cost. To achieve a decent trade-off between accuracy and speed, the number of tokens is empirically set to 16x16 or 14x14. In this paper, we argue that every image has its own characteristics, and ideally the token number should be conditioned on each individual input. In fact, we have observed that there exist a considerable number of \u201ceasy\u201d images which can be accurately predicted with a mere number of 4x4 tokens, while only a small fraction of \u201chard\u201d ones need a finer representation. Inspired by this phenomenon, we propose a Dynamic Transformer to automatically configure a proper number of tokens for each input image. This is achieved by cascading multiple Transformers with increasing numbers of tokens, which are sequentially activated in an adaptive fashion at test time, i.e., the inference is terminated once a sufficiently confident prediction is produced. We further design efficient feature reuse and relationship reuse mechanisms across different components of the Dynamic Transformer to reduce redundant computations. Extensive empirical results on ImageNet, CIFAR-10, and CIFAR-100 demonstrate that our method significantly outperforms the competitive baselines in terms of both theoretical computational efficiency and practical inference speed. Code and pre-trained models (based on PyTorch and MindSpore) are available at https://github.com/blackfeather-wang/Dynamic-Vision-Transformer and https://github.com/blackfeather-wang/Dynamic-Vision-Transformer-MindSpore.",
    "authors": [
      "Wang, Yulin",
      "Huang, Rui",
      "Song, Shiji",
      "Huang, Zeyi",
      "Huang, Gao"
    ]
  },
  {
    "id": "6467c327eaf8940b4dd07a08c63c5e85",
    "title": "Contrastive Learning for Neural Topic Model",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6467c327eaf8940b4dd07a08c63c5e85-Paper.pdf",
    "abstract": "Recent empirical studies show that adversarial topic models (ATM) can successfully capture semantic patterns of the document by differentiating a document with another dissimilar sample. However, utilizing that discriminative-generative architecture has two important drawbacks: (1) the architecture does not relate similar documents, which has the same document-word distribution of salient words; (2) it restricts the ability to integrate external information, such as sentiments of the document, which has been shown to benefit the training of neural topic model. To address those issues, we revisit the adversarial topic architecture in the view point of mathematical analysis, propose a novel approach to re-formulate discriminative goal as an optimization problem, and design a novel sampling method which facilitates the integration of external variables. The reformulation encourages the model to incorporate the relations among similar samples and enforces the constraint on the similarity among dissimilar ones; while the sampling method, which is based on the internal input and reconstructed output, helps inform the model of salient words contributing to the main topic. Experimental results show that our framework outperforms other state-of-the-art neural topic models in three common benchmark datasets that belong to various domains, vocabulary sizes, and document lengths in terms of topic coherence.",
    "authors": [
      "Nguyen, Thong",
      "Luu, Anh Tuan"
    ]
  },
  {
    "id": "646c9941d7fb1bc793a7929328ae3f2f",
    "title": "Learning in two-player zero-sum partially observable Markov games with perfect recall",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/646c9941d7fb1bc793a7929328ae3f2f-Paper.pdf",
    "abstract": "We study the problem of learning a Nash equilibrium (NE) in an extensive game with imperfect information (EGII) through self-play. Precisely, we focus on two-player, zero-sum, episodic, tabular EGII under the \\textit{perfect-recall} assumption where the only feedback is realizations of the game (bandit feedback). In particular the \\textit{dynamics of the EGII is not known}---we can only access it by sampling or interacting with a game simulator. For this learning setting, we provide the Implicit Exploration Online Mirror Descent (IXOMD) algorithm. It is a model-free algorithm with a high-probability bound on convergence rate to the NE of order $1/\\sqrt{T}$ where~$T$ is the number of played games. Moreover IXOMD is computationally efficient as it needs to perform the updates only along the sampled trajectory.",
    "authors": [
      "Kozuno, Tadashi",
      "M\u00e9nard, Pierre",
      "Munos, Remi",
      "Valko, Michal"
    ]
  },
  {
    "id": "647c722bf90a49140184672e0d3723e3",
    "title": "A Geometric Structure of Acceleration and Its Role in Making Gradients Small Fast",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/647c722bf90a49140184672e0d3723e3-Paper.pdf",
    "abstract": "Since Nesterov's seminal 1983 work, many accelerated first-order optimization methods have been proposed, but their analyses lacks a common unifying structure. In this work, we identify a geometric structure satisfied by a wide range of first-order accelerated methods. Using this geometric insight, we present several novel generalizations of accelerated methods. Most interesting among them is a method that reduces the squared gradient norm with $\\mathcal{O}(1/K^4)$ rate in the prox-grad setup, faster than the $\\mathcal{O}(1/K^3)$ rates of Nesterov's FGM or Kim and Fessler's FPGM-m.",
    "authors": [
      "Lee, Jongmin",
      "Park, Chanwoo",
      "Ryu, Ernest"
    ]
  },
  {
    "id": "64986d86a17424eeac96b08a6d519059",
    "title": "ATISS: Autoregressive Transformers for Indoor Scene Synthesis",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/64986d86a17424eeac96b08a6d519059-Paper.pdf",
    "abstract": "The ability to synthesize realistic and diverse indoor furniture layouts automatically or based on partial input, unlocks many applications, from better interactive 3D tools to data synthesis for training and simulation. In this paper, we present ATISS, a novel autoregressive transformer architecture for creating diverse and plausible synthetic indoor environments, given only the room type and its floor plan. In contrast to prior work, which poses scene synthesis as sequence generation, our model generates rooms as unordered sets of objects. We argue that this formulation is more natural, as it makes ATISS generally useful beyond fully automatic room layout synthesis. For example, the same trained model can be used in interactive applications for general scene completion, partial room re-arrangement with any objects specified by the user, as well as object suggestions for any partial room. To enable this, our model leverages the permutation equivariance of the transformer when conditioning on the partial scene, and is trained to be permutation-invariant across object orderings. Our model is trained end-to-end as an autoregressive generative model using only labeled 3D bounding boxes as supervision. Evaluations on four room types in the 3D-FRONT dataset demonstrate that our model consistently generates plausible room layouts that are more realistic than existing methods.In addition, it has fewer parameters, is simpler to implement and train and runs up to 8 times faster than existing methods.",
    "authors": [
      "Paschalidou, Despoina",
      "Kar, Amlan",
      "Shugrina, Maria",
      "Kreis, Karsten",
      "Geiger, Andreas",
      "Fidler, Sanja"
    ]
  },
  {
    "id": "649adc59afdef2a8b9e943f94a04b02f",
    "title": "Generalized Depthwise-Separable Convolutions for Adversarially Robust and Efficient Neural Networks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/649adc59afdef2a8b9e943f94a04b02f-Paper.pdf",
    "abstract": "Despite their tremendous successes, convolutional neural networks (CNNs) incur high computational/storage costs and are vulnerable to adversarial perturbations. Recent works on robust model compression address these challenges by combining model compression techniques with adversarial training. But these methods are unable to improve throughput (frames-per-second) on real-life hardware while simultaneously preserving robustness to adversarial perturbations. To overcome this problem, we propose the method of Generalized Depthwise-Separable (GDWS) convolution - an efficient, universal, post-training approximation of a standard 2D convolution. GDWS dramatically improves the throughput of a standard pre-trained network on real-life hardware while preserving its robustness. Lastly, GDWS is scalable to large problem sizes since it operates on pre-trained models and doesn't require any additional training. We establish the optimality of GDWS as a 2D convolution approximator and present exact algorithms for constructing optimal GDWS convolutions under complexity and error constraints. We demonstrate the effectiveness of GDWS via extensive experiments on CIFAR-10, SVHN, and ImageNet datasets. Our code can be found at https://github.com/hsndbk4/GDWS.",
    "authors": [
      "Dbouk, Hassan",
      "Shanbhag, Naresh"
    ]
  },
  {
    "id": "649d45bf179296e31731adfd4df25588",
    "title": "A Provably Efficient Model-Free Posterior Sampling Method for Episodic Reinforcement Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/649d45bf179296e31731adfd4df25588-Paper.pdf",
    "abstract": "Thompson Sampling is one of the most effective methods for contextual bandits and has been generalized to posterior sampling for certain MDP settings. However, existing posterior sampling methods for reinforcement learning are limited by being model-based or lack worst-case theoretical guarantees beyond linear MDPs. This paper proposes a new model-free formulation of posterior sampling that applies to more general episodic reinforcement learning problems with theoretical guarantees. We introduce novel proof techniques to show that under suitable conditions, the worst-case regret of our posterior sampling method matches the best known results of optimization based methods. In the linear MDP setting with dimension, the regret of our algorithm scales linearly with the dimension as compared to a quadratic dependence of the existing posterior sampling-based exploration algorithms.",
    "authors": [
      "Dann, Christoph",
      "Mohri, Mehryar",
      "Zhang, Tong",
      "Zimmert, Julian"
    ]
  },
  {
    "id": "64be20f6dd1dd46adf110cf871e3ed35",
    "title": "Fast Federated Learning in the Presence of Arbitrary Device Unavailability",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/64be20f6dd1dd46adf110cf871e3ed35-Paper.pdf",
    "abstract": "Federated learning (FL) coordinates with numerous heterogeneous devices to collaboratively train a shared model while preserving user privacy. Despite its multiple advantages, FL faces new challenges. One challenge arises when devices drop out of the training process. In this case, the convergence of popular FL algorithms such as FedAvg is severely influenced by the straggling devices. To tackle this challenge, we study federated learning algorithms in the presence of arbitrary device unavailability and propose an algorithm named Memory-augmented Impatient Federated Averaging (MIFA). Our algorithm efficiently avoids excessive latency induced by inactive devices, and corrects the gradient bias using the memorized latest updates from them. We prove that MIFA achieves minimax optimal convergence rates on non-i.i.d. data for both strongly convex and non-convex smooth functions. We also provide an explicit characterization of the improvement over baseline algorithms through a case study, and validate the results by numerical experiments on real-world datasets.",
    "authors": [
      "Gu, Xinran",
      "Huang, Kaixuan",
      "Zhang, Jingzhao",
      "Huang, Longbo"
    ]
  },
  {
    "id": "64dafb11e52edd3cd840bf24e56ddce6",
    "title": "On The Structure of Parametric Tournaments with Application to Ranking from Pairwise Comparisons",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/64dafb11e52edd3cd840bf24e56ddce6-Paper.pdf",
    "abstract": " We consider the classical problem of finding the minimum feedback arc set on tournaments (MFAST). The problem is NP-hard in general and we study it for important classes of tournaments that arise naturally in the problem of learning to rank from pairwise comparisons. Specifically, we consider tournaments classes that arise out of parametric preference matrices that can lead to cyclic preference relations. We investigate their structural properties via forbidden sub tournament configurations.  Towards this, we introduce \\emph{Tournament Dimension} - a combinatorial parameter that characterizes the size of a forbidden configuration for rank $r$ tournament classes i.e., classes that arise out pairwise preference matrices which lead to rank $r$ skew-symmetric matrices under a suitable link function. Our main result is a polynomial-time algorithm - \\texttt{Rank2Rank} - that solves the MFAST problem for the rank $2$ tournament class. This is achieved via a  geometric characterization that relies on our explicit construction of a forbidden configuration for this class.   Building on our understanding of the rank-$2$ tournament class, we propose a very general and flexible parametric pairwise preference model called the local-global model which subsumes the popular Bradley-Terry-Luce/Thurstone classes to capture locally cyclic as well as globally acyclic preference relations. We develop a polynomial-time algorithm - \\texttt{BlockRank2Rank}- to solve the MFAST problem on the associated Block-Rank $2$ tournament class.  As an application, we study the problem of learning to rank from pairwise comparisons under the proposed local-global preference model. Exploiting our structural characterization, we propose  \\texttt{PairwiseBlockRank} - a pairwise ranking algorithm for this class. We show sample complexity bounds of \\texttt{PairwiseBlockRank}  to learn a good ranking under the proposed model.  Finally, we conduct experiments on synthetic and real-world datasets to show the efficacy of the proposed algorithm.",
    "authors": [
      "Veerathu, Vishnu",
      "Rajkumar, Arun"
    ]
  },
  {
    "id": "64f1f27bf1b4ec22924fd0acb550c235",
    "title": "SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/64f1f27bf1b4ec22924fd0acb550c235-Paper.pdf",
    "abstract": "We present SegFormer, a simple, efficient yet powerful semantic segmentation framework which unifies Transformers with lightweight multilayer perceptron (MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a novel hierarchically structured Transformer encoder which outputs multiscale features. It does not need positional encoding, thereby avoiding the interpolation of positional codes which leads to decreased performance when the testing resolution differs from training. 2) SegFormer avoids complex decoders. The proposed MLP decoder aggregates information from different layers, and thus combining both local attention and global attention to render powerful representations. We show that this simple and lightweight design is the key to efficient segmentation on Transformers.  We scale our approach up to obtain a series of models from SegFormer-B0 to Segformer-B5, which reaches much better performance and efficiency than previous counterparts.For example, SegFormer-B4 achieves 50.3% mIoU on ADE20K with 64M parameters, being 5x smaller and 2.2% better than the previous best method. Our best model, SegFormer-B5, achieves 84.0% mIoU on Cityscapes validation set and shows excellent zero-shot robustness on Cityscapes-C.",
    "authors": [
      "Xie, Enze",
      "Wang, Wenhai",
      "Yu, Zhiding",
      "Anandkumar, Anima",
      "Alvarez, Jose M.",
      "Luo, Ping"
    ]
  },
  {
    "id": "64ff7983a47d331b13a81156e2f4d29d",
    "title": "Fairness via Representation Neutralization",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/64ff7983a47d331b13a81156e2f4d29d-Paper.pdf",
    "abstract": "Existing bias mitigation methods for DNN models primarily work on learning debiased encoders. This process not only requires a lot of instance-level annotations for sensitive attributes, it also does not guarantee that all fairness sensitive information has been removed from the encoder. To address these limitations, we explore the following research question: Can we reduce the discrimination of DNN models by only debiasing the classification head, even with biased representations as inputs? To this end, we propose a new mitigation technique, namely, Representation Neutralization for Fairness (RNF) that achieves fairness by debiasing only the task-specific classification head of DNN models. To this end, we leverage samples with the same ground-truth label but different sensitive attributes, and use their neutralized representations to train the classification head of the DNN model. The key idea of RNF is to discourage the classification head from capturing spurious correlation between fairness sensitive information in encoder representations with specific class labels. To address low-resource settings with no access to sensitive attribute annotations, we leverage a bias-amplified model to generate proxy annotations for sensitive attributes. Experimental results over several benchmark datasets demonstrate our RNF framework to effectively reduce discrimination of DNN models with minimal degradation in task-specific performance.",
    "authors": [
      "Du, Mengnan",
      "Mukherjee, Subhabrata",
      "Wang, Guanchu",
      "Tang, Ruixiang",
      "Awadallah, Ahmed",
      "Hu, Xia"
    ]
  },
  {
    "id": "6516c28727509c3db6280ae16254e916",
    "title": "Residual Relaxation for Multi-view Representation Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6516c28727509c3db6280ae16254e916-Paper.pdf",
    "abstract": "Multi-view methods learn representations by aligning multiple views of the same image and their performance largely depends on the choice of data augmentation. In this paper, we notice that some other useful augmentations, such as image rotation, are harmful for multi-view methods because they cause a semantic shift that is too large to be aligned well. This observation motivates us to relax the exact alignment objective to better cultivate stronger augmentations. Taking image rotation as a case study, we develop a generic approach, Pretext-aware Residual Relaxation (Prelax), that relaxes the exact alignment by allowing an adaptive residual vector between different views and encoding the semantic shift through pretext-aware learning. Extensive experiments on different backbones show that our method can not only improve multi-view methods with existing augmentations, but also benefit from stronger image augmentations like rotation.",
    "authors": [
      "Wang, Yifei",
      "Geng, Zhengyang",
      "Jiang, Feng",
      "Li, Chuming",
      "Wang, Yisen",
      "Yang, Jiansheng",
      "Lin, Zhouchen"
    ]
  },
  {
    "id": "652cf38361a209088302ba2b8b7f51e0",
    "title": "Do Vision Transformers See Like Convolutional Neural Networks?",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/652cf38361a209088302ba2b8b7f51e0-Paper.pdf",
    "abstract": "Convolutional neural networks (CNNs) have so far been the de-facto model for visual data. Recent work has shown that (Vision) Transformer models (ViT) can achieve comparable or even superior performance on image classification tasks. This raises a central question: how are Vision Transformers solving these tasks? Are they acting like convolutional networks, or learning entirely different visual representations? Analyzing the internal representation structure of ViTs and CNNs on image classification benchmarks, we find striking differences between the two architectures, such as ViT having more uniform representations across all layers. We explore how these differences arise, finding crucial roles played by self-attention, which enables early aggregation of global information, and ViT residual connections, which strongly propagate features from lower to higher layers. We study the ramifications for spatial localization, demonstrating ViTs successfully preserve input spatial information, with noticeable effects from different classification methods. Finally, we study the effect of (pretraining) dataset scale on intermediate features and transfer learning, and conclude with a discussion on connections to new architectures such as the MLP-Mixer. ",
    "authors": [
      "Raghu, Maithra",
      "Unterthiner, Thomas",
      "Kornblith, Simon",
      "Zhang, Chiyuan",
      "Dosovitskiy, Alexey"
    ]
  },
  {
    "id": "6531b32f8d02fece98ff36a64a7c8260",
    "title": "Optimization-Based Algebraic Multigrid Coarsening Using Reinforcement Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6531b32f8d02fece98ff36a64a7c8260-Paper.pdf",
    "abstract": "Large sparse linear systems of equations are ubiquitous in science and engineering, such as those arising from discretizations of partial differential equations. Algebraic multigrid (AMG) methods are one of the most common methods of solving such linear systems, with an extensive body of underlying mathematical theory. A system of linear equations defines a graph on the set of unknowns and each level of a multigrid solver requires the selection of an appropriate coarse graph along with restriction and interpolation operators that map to and from the coarse representation. The efficiency of the multigrid solver depends critically on this selection and many selection methods have been developed over the years. Recently, it has been demonstrated that it is possible to directly learn the AMG interpolation and restriction operators, given a coarse graph selection. In this paper, we consider the complementary problem of learning to coarsen graphs for a multigrid solver, a necessary step in developing fully learnable AMG methods. We propose a method using a reinforcement learning (RL) agent based on graph neural networks (GNNs), which can learn to perform graph coarsening on small planar training graphs and then be applied to unstructured large planar graphs, assuming bounded node degree. We demonstrate that this method can produce better coarse graphs than existing algorithms, even as the graph size increases and other properties of the graph are varied. We also propose an efficient inference procedure for performing graph coarsening that results in linear time complexity in graph size.",
    "authors": [
      "Taghibakhshi, Ali",
      "MacLachlan, Scott",
      "Olson, Luke",
      "West, Matthew"
    ]
  },
  {
    "id": "654516d1b4df6917094de807156adc14",
    "title": "Delayed Propagation Transformer: A Universal Computation Engine towards Practical Control in Cyber-Physical Systems",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/654516d1b4df6917094de807156adc14-Paper.pdf",
    "abstract": "Multi-agent control is a central theme in the Cyber-Physical Systems (CPS). However, current control methods either receive non-Markovian states due to insufficient sensing and decentralized design, or suffer from poor convergence. This paper presents the Delayed Propagation Transformer (DePT), a new transformer-based model that specializes in the global modeling of CPS while taking into account the immutable constraints from the physical world. DePT induces a cone-shaped spatial-temporal attention prior, which injects the information propagation and aggregation principles and enables a global view. With physical constraint inductive bias baked into its design, our DePT is ready to plug and play for a broad class of multi-agent systems. The experimental results on one of the most challenging CPS -- network-scale traffic signal control system in the open world -- show that our model outperformed the state-of-the-art expert methods on synthetic and real-world datasets. Our codes are released at: https://github.com/VITA-Group/DePT.",
    "authors": [
      "Zheng, Wenqing",
      "Guo, Qiangqiang",
      "Yang, Hao",
      "Wang, Peihao",
      "Wang, Zhangyang"
    ]
  },
  {
    "id": "65658fde58ab3c2b6e5132a39fae7cb9",
    "title": "Explaining Latent Representations with a Corpus of Examples",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/65658fde58ab3c2b6e5132a39fae7cb9-Paper.pdf",
    "abstract": "Modern machine learning models are complicated. Most of them rely on convoluted latent representations of their input to issue a prediction. To achieve greater transparency than a black-box that connects inputs to predictions, it is necessary to gain a deeper understanding of these latent representations. To that aim, we propose SimplEx: a user-centred method that provides example-based explanations with reference to a freely selected set of examples, called the corpus. SimplEx uses the corpus to improve the user\u2019s understanding of the latent space with post-hoc explanations answering two questions: (1) Which corpus examples explain the prediction issued for a given test example? (2) What features of these corpus examples are relevant for the model to relate them to the test example? SimplEx provides an answer by reconstructing the test latent representation as a mixture of corpus latent representations. Further, we propose a novel approach, the integrated Jacobian, that allows SimplEx to make explicit the contribution of each corpus feature in the mixture. Through experiments on tasks ranging from mortality prediction to image classification, we demonstrate that these decompositions are robust and accurate. With illustrative use cases in medicine, we show that SimplEx empowers the user by highlighting relevant patterns in the corpus that explain model representations. Moreover, we demonstrate how the freedom in choosing the corpus allows the user to have personalized explanations in terms of examples that are meaningful for them.",
    "authors": [
      "Crabbe, Jonathan",
      "Qian, Zhaozhi",
      "Imrie, Fergus",
      "van der Schaar, Mihaela"
    ]
  },
  {
    "id": "656f0dbf9392657eed7feefc486781fb",
    "title": "Explaining heterogeneity in medial entorhinal cortex with task-driven neural networks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/656f0dbf9392657eed7feefc486781fb-Paper.pdf",
    "abstract": "Medial entorhinal cortex (MEC) supports a wide range of navigational and memory related behaviors.Well-known experimental results have revealed specialized cell types in MEC --- e.g. grid, border, and head-direction cells --- whose highly stereotypical response profiles are suggestive of the role they might play in supporting MEC functionality. However, the majority of MEC neurons do not exhibit stereotypical firing patterns.How should the response profiles of these more \"heterogeneous\" cells be described, and how do they contribute to behavior?In this work, we took a computational approach to addressing these questions.We first performed a statistical analysis that shows that heterogeneous MEC cells are just as reliable in their response patterns as the more stereotypical cell types, suggesting that they have a coherent functional role.Next, we evaluated a spectrum of candidate models in terms of their ability to describe the response profiles of both stereotypical and heterogeneous MEC cells.We found that recently developed task-optimized neural network models are substantially better than traditional grid cell-centric models at matching most MEC neuronal response profiles --- including those of grid cells themselves --- despite not being explicitly trained for this purpose.Specific choices of network architecture (such as gated nonlinearities and an explicit intermediate place cell representation) have an important effect on the ability of the model to generalize to novel scenarios, with the best of these models closely approaching the noise ceiling of the data itself.We then performed in silico experiments on this model to address questions involving the relative functional relevance of various cell types, finding that heterogeneous cells are likely to be just as involved in downstream functional outcomes (such as path integration) as grid and border cells.Finally, inspired by recent data showing that, going beyond their spatial response selectivity, MEC cells are also responsive to non-spatial rewards, we introduce a new MEC model that performs reward-modulated path integration.We find that this unified model matches neural recordings across all variable-reward conditions.Taken together, our results point toward a conceptually principled goal-driven modeling approach for moving future experimental and computational efforts beyond overly-simplistic single-cell stereotypes.",
    "authors": [
      "Nayebi, Aran",
      "Attinger, Alexander",
      "Campbell, Malcolm",
      "Hardcastle, Kiah",
      "Low, Isabel",
      "Mallory, Caitlin S",
      "Mel, Gabriel",
      "Sorscher, Ben",
      "Williams, Alex H",
      "Ganguli, Surya",
      "Giocomo, Lisa",
      "Yamins, Dan"
    ]
  },
  {
    "id": "6591d327f6f731e589b0e869adadf940",
    "title": "Beyond Smoothness: Incorporating Low-Rank Analysis into Nonparametric Density Estimation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6591d327f6f731e589b0e869adadf940-Paper.pdf",
    "abstract": "The construction and theoretical analysis of the most popular universally consistent nonparametric density estimators hinge on one functional property: smoothness. In this paper we investigate the theoretical implications of incorporating a multi-view latent variable model, a type of low-rank model, into nonparametric density estimation. To do this we perform extensive analysis on histogram-style estimators that integrate a multi-view model. Our analysis culminates in showing that there exists a universally consistent histogram-style estimator that converges to any multi-view model with a finite number of Lipschitz continuous components at a rate of $\\widetilde{O}(1/\\sqrt[3]{n})$ in $L^1$ error. In contrast, the standard histogram estimator can converge at a rate slower than $1/\\sqrt[d]{n}$ on the same class of densities. We also introduce a new nonparametric latent variable model based on the Tucker decomposition. A rudimentary implementation of our estimators experimentally demonstrates a considerable performance improvement over the standard histogram estimator. We also provide a thorough analysis of the sample complexity of our Tucker decomposition-based model and a variety of other results. Thus, our paper provides solid theoretical foundations for extending low-rank techniques to the nonparametric setting.",
    "authors": [
      "Vandermeulen, Robert A.",
      "Ledent, Antoine"
    ]
  },
  {
    "id": "65a99bb7a3115fdede20da98b08a370f",
    "title": "Multi-View Representation Learning via Total Correlation Objective",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/65a99bb7a3115fdede20da98b08a370f-Paper.pdf",
    "abstract": "Multi-View Representation Learning (MVRL) aims to discover a shared representation of observations from different views with the complex underlying correlation. In this paper, we propose a variational approach which casts MVRL as maximizing the amount of total correlation reduced by the representation, aiming to learn a shared latent representation that is informative yet succinct to capture the correlation among multiple views. To this end, we introduce a tractable surrogate objective function under the proposed framework, which allows our method to fuse and calibrate the observations in the representation space. From the information-theoretic perspective, we show that our framework subsumes existing multi-view generative models. Lastly, we show that our approach straightforwardly extends to the Partial MVRL (PMVRL) setting, where the observations are missing without any regular pattern. We demonstrate the effectiveness of our approach in the multi-view translation and classification tasks, outperforming strong baseline methods.",
    "authors": [
      "Hwang, HyeongJoo",
      "Kim, Geon-Hyeong",
      "Hong, Seunghoon",
      "Kim, Kee-Eung"
    ]
  },
  {
    "id": "65b9eea6e1cc6bb9f0cd2a47751a186f",
    "title": "FACMAC: Factored Multi-Agent Centralised Policy Gradients",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/65b9eea6e1cc6bb9f0cd2a47751a186f-Paper.pdf",
    "abstract": "We propose FACtored Multi-Agent Centralised policy gradients (FACMAC), a new method for cooperative multi-agent reinforcement learning in both discrete and continuous action spaces. Like MADDPG, a popular multi-agent actor-critic method, our approach uses deep deterministic policy gradients to learn policies. However, FACMAC learns a centralised but factored critic, which combines per-agent utilities into the joint action-value function via a non-linear monotonic function, as in QMIX, a popular multi-agent $Q$-learning algorithm. However, unlike QMIX, there are no inherent constraints on factoring the critic. We thus also employ a nonmonotonic factorisation and empirically demonstrate that its increased representational capacity allows it to solve some tasks that cannot be solved with monolithic, or monotonically factored critics. In addition, FACMAC uses a centralised policy gradient estimator that optimises over the entire joint action space, rather than optimising over each agent's action space separately as in MADDPG. This allows for more coordinated policy changes and fully reaps the benefits of a centralised critic. We evaluate FACMAC on variants of the multi-agent particle environments, a novel multi-agent MuJoCo benchmark, and a challenging set of StarCraft II micromanagement tasks. Empirical results demonstrate FACMAC's superior performance over MADDPG and other baselines on all three domains.",
    "authors": [
      "Peng, Bei",
      "Rashid, Tabish",
      "Schroeder de Witt, Christian",
      "Kamienny, Pierre-Alexandre",
      "Torr, Philip",
      "Boehmer, Wendelin",
      "Whiteson, Shimon"
    ]
  },
  {
    "id": "65c89f5a9501a04c073b354f03791b1f",
    "title": "EDGE: Explaining Deep Reinforcement Learning Policies",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/65c89f5a9501a04c073b354f03791b1f-Paper.pdf",
    "abstract": "With the rapid development of deep reinforcement learning (DRL) techniques, there is an increasing need to understand and interpret DRL policies. While recent research has developed explanation methods to interpret how an agent determines its moves, they cannot capture the importance of actions/states to a game's final result. In this work, we propose a novel self-explainable model that augments a Gaussian process with a customized kernel function and an interpretable predictor. Together with the proposed model, we also develop a parameter learning procedure that leverages inducing points and variational inference to improve learning efficiency. Using our proposed model, we can predict an agent's final rewards from its game episodes and extract time step importance within episodes as strategy-level explanations for that agent. Through experiments on Atari and MuJoCo games, we verify the explanation fidelity of our method and demonstrate how to employ interpretation to understand agent behavior, discover policy vulnerabilities, remediate policy errors, and even defend against adversarial attacks.",
    "authors": [
      "Guo, Wenbo",
      "Wu, Xian",
      "Khan, Usmann",
      "Xing, Xinyu"
    ]
  },
  {
    "id": "65cc2c8205a05d7379fa3a6386f710e1",
    "title": "Learning to Assimilate in Chaotic Dynamical Systems",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/65cc2c8205a05d7379fa3a6386f710e1-Paper.pdf",
    "abstract": "The accuracy of simulation-based forecasting in chaotic systems is heavily dependent on high-quality estimates of the system state at the beginning of the forecast. Data assimilation methods are used to infer these initial conditions by systematically combining noisy, incomplete observations and numerical models of system dynamics to produce highly effective estimation schemes. We introduce a self-supervised framework, which we call \\textit{amortized assimilation}, for learning to assimilate in dynamical systems. Amortized assimilation combines deep learning-based denoising with differentiable simulation, using independent neural networks to assimilate specific observation types while connecting the gradient flow between these sub-tasks with differentiable simulation and shared recurrent memory. This hybrid architecture admits a self-supervised training objective which is minimized by an unbiased estimator of the true system state even in the presence of only noisy training data. Numerical experiments across several chaotic benchmark systems highlight the improved effectiveness of our approach compared to widely-used data assimilation methods.",
    "authors": [
      "McCabe, Michael",
      "Brown, Jed"
    ]
  },
  {
    "id": "65d2ea03425887a717c435081cfc5dbb",
    "title": "Object-aware Contrastive Learning for Debiased Scene Representation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/65d2ea03425887a717c435081cfc5dbb-Paper.pdf",
    "abstract": "Contrastive self-supervised learning has shown impressive results in learning visual representations from unlabeled images by enforcing invariance against different data augmentations. However, the learned representations are often contextually biased to the spurious scene correlations of different objects or object and background, which may harm their generalization on the downstream tasks. To tackle the issue, we develop a novel object-aware contrastive learning framework that first (a) localizes objects in a self-supervised manner and then (b) debias scene correlations via appropriate data augmentations considering the inferred object locations. For (a), we propose the contrastive class activation map (ContraCAM), which finds the most discriminative regions (e.g., objects) in the image compared to the other images using the contrastively trained models. We further improve the ContraCAM to detect multiple objects and entire shapes via an iterative refinement procedure. For (b), we introduce two data augmentations based on ContraCAM, object-aware random crop and background mixup, which reduce contextual and background biases during contrastive self-supervised learning, respectively. Our experiments demonstrate the effectiveness of our representation learning framework, particularly when trained under multi-object images or evaluated under the background (and distribution) shifted images. Code is available at https://github.com/alinlab/object-aware-contrastive.",
    "authors": [
      "Mo, Sangwoo",
      "Kang, Hyunwoo",
      "Sohn, Kihyuk",
      "Li, Chun-Liang",
      "Shin, Jinwoo"
    ]
  },
  {
    "id": "65d90fc6d307590b14e9e1800d4e8eab",
    "title": "Evaluating Efficient Performance Estimators of Neural Architectures",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/65d90fc6d307590b14e9e1800d4e8eab-Paper.pdf",
    "abstract": "Conducting efficient performance estimations of neural architectures is a major challenge in neural architecture search (NAS). To reduce the architecture training costs in NAS, one-shot estimators (OSEs) amortize the architecture training costs by sharing the parameters of one supernet between all architectures. Recently, zero-shot estimators (ZSEs) that involve no training are proposed to further reduce the architecture evaluation cost. Despite the high efficiency of these estimators, the quality of such estimations has not been thoroughly studied. In this paper, we conduct an extensive and organized assessment of OSEs and ZSEs on five NAS benchmarks: NAS-Bench-101/201/301, and NDS ResNet/ResNeXt-A. Specifically, we employ a set of NAS-oriented criteria to study the behavior of OSEs and ZSEs, and reveal their biases and variances. After analyzing how and why the OSE estimations are unsatisfying, we explore how to mitigate the correlation gap of OSEs from three perspectives. Through our analysis, we give out suggestions for future application and development of efficient architecture performance estimators. Furthermore, the analysis framework proposed in our work could be utilized in future research to give a more comprehensive understanding of newly designed architecture performance estimators. The code is available at https://github.com/walkerning/aw_nas.",
    "authors": [
      "Ning, Xuefei",
      "Tang, Changcheng",
      "Li, Wenshuo",
      "Zhou, Zixuan",
      "Liang, Shuang",
      "Yang, Huazhong",
      "Wang, Yu"
    ]
  },
  {
    "id": "65fc9fb4897a89789352e211ca2d398f",
    "title": "A-NeRF: Articulated Neural Radiance Fields for Learning Human Shape, Appearance, and Pose",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/65fc9fb4897a89789352e211ca2d398f-Paper.pdf",
    "abstract": "While deep learning reshaped the classical motion capture pipeline with feed-forward networks, generative models are required to recover fine alignment via iterative refinement. Unfortunately, the existing models are usually hand-crafted or learned in controlled conditions, only applicable to limited domains. We propose a method to learn a generative neural body model from unlabelled monocular videos by extending Neural Radiance Fields (NeRFs). We equip them with a skeleton to apply to time-varying and articulated motion. A key insight is that implicit models require the inverse of the forward kinematics used in explicit surface models. Our reparameterization defines spatial latent variables relative to the pose of body parts and thereby overcomes ill-posed inverse operations with an overparameterization. This enables learning volumetric body shape and appearance from scratch while jointly refining the articulated pose; all without ground truth labels for appearance, pose, or 3D shape on the input videos. When used for novel-view-synthesis and motion capture, our neural model improves accuracy on diverse datasets.",
    "authors": [
      "Su, Shih-Yang",
      "Yu, Frank",
      "Zollhoefer, Michael",
      "Rhodin, Helge"
    ]
  },
  {
    "id": "6600e06fe9350b62c1e343504d4a7b86",
    "title": "Differential Privacy Over Riemannian Manifolds",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6600e06fe9350b62c1e343504d4a7b86-Paper.pdf",
    "abstract": "In this work we consider the problem of releasing a differentially private statistical summary that resides on a Riemannian manifold.  We present an extension of the Laplace or K-norm mechanism that utilizes intrinsic distances and volumes on the manifold.  We also consider in detail the specific case where the summary is the Fr\\'echet mean of data residing on a manifold.  We demonstrate that our mechanism is rate optimal and depends only on the dimension of the manifold, not on the dimension of any ambient space, while also showing how ignoring the manifold structure can decrease the utility of the sanitized summary.  We illustrate our framework in two examples of particular interest in statistics: the space of symmetric positive definite matrices, which is used for covariance matrices, and the sphere, which can be used as a space for modeling discrete distributions. ",
    "authors": [
      "Reimherr, Matthew",
      "Bharath, Karthik",
      "Soto, Carlos"
    ]
  },
  {
    "id": "66121d1f782d29b62a286909165517bc",
    "title": "How can classical multidimensional scaling go wrong?",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/66121d1f782d29b62a286909165517bc-Paper.pdf",
    "abstract": "Given a matrix $D$ describing the pairwise dissimilarities of a data set, a common task is to embed the data points into Euclidean space. The classical multidimensional scaling (cMDS) algorithm is a widespread method to do this. However, theoretical analysis of the robustness of the algorithm and an in-depth analysis of its performance on non-Euclidean metrics is lacking. In this paper, we derive a formula, based on the eigenvalues of a matrix obtained from $D$, for the Frobenius norm of the difference between $D$ and the metric $D_{\\text{cmds}}$ returned by cMDS. This error analysis leads us to the conclusion that when the derived matrix has a significant number of negative eigenvalues, then $\\|D-D_{\\text{cmds}}\\|_F$, after initially decreasing, willeventually increase as we increase the dimension. Hence, counterintuitively, the quality of the embedding degrades as we increase the dimension. We empirically verify that the Frobenius norm increases as we increase the dimension for a variety of non-Euclidean metrics. We also show on several benchmark datasets that this degradation in the embedding results in the classification accuracy of both simple (e.g., 1-nearest neighbor) and complex (e.g., multi-layer neural nets) classifiers decreasing as we increase the embedding dimension.Finally, our analysis leads us to a new efficiently computable algorithm that returns a matrix $D_l$ that is at least as close to the original distances as $D_t$ (the Euclidean metric closest in $\\ell_2$ distance). While $D_l$ is not metric, when given as input to cMDS instead of $D$, it empirically results in solutions whose distance to $D$ does not increase when we increase the dimension and the classification accuracy degrades less than the cMDS solution.  ",
    "authors": [
      "Sonthalia, Rishi",
      "Van Buskirk, Greg",
      "Raichel, Benjamin",
      "Gilbert, Anna"
    ]
  },
  {
    "id": "662a2e96162905620397b19c9d249781",
    "title": "Modeling Heterogeneous Hierarchies with Relation-specific Hyperbolic Cones",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/662a2e96162905620397b19c9d249781-Paper.pdf",
    "abstract": "Hierarchical relations are prevalent and indispensable for organizing human knowledge captured by a knowledge graph (KG). The key property of hierarchical relations is that they induce a partial ordering over the entities, which needs to be modeled in order to allow for hierarchical reasoning. However, current KG embeddings can model only a single global hierarchy (single global partial ordering) and fail to model multiple heterogeneous hierarchies that exist in a single KG. Here we present ConE (Cone Embedding), a KG embedding model that is able to simultaneously model multiple hierarchical as well as non-hierarchical relations in a knowledge graph. ConE embeds entities into hyperbolic cones and models relations as transformations between the cones. In particular, ConE uses cone containment constraints in different subspaces of the hyperbolic embedding space to capture multiple heterogeneous hierarchies. Experiments on standard knowledge graph benchmarks show that ConE obtains state-of-the-art performance on hierarchical reasoning tasks as well as knowledge graph completion task on hierarchical graphs. In particular, our approach yields new state-of-the-art Hits@1 of 45.3% on WN18RR and 16.1% on DDB14 (0.231 MRR). As for hierarchical reasoning task, our approach outperforms previous best results by an average of 20% across the three datasets.",
    "authors": [
      "Bai, Yushi",
      "Ying, Zhitao",
      "Ren, Hongyu",
      "Leskovec, Jure"
    ]
  },
  {
    "id": "66be31e4c40d676991f2405aaecc6934",
    "title": "Non-asymptotic Error Bounds for Bidirectional GANs",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/66be31e4c40d676991f2405aaecc6934-Paper.pdf",
    "abstract": "We derive nearly sharp bounds for the bidirectional GAN (BiGAN) estimation error under the Dudley distance between the latent joint distribution and the data joint distribution with appropriately specified  architecture of the neural networks used in the model. To the best of our knowledge, this is the first theoretical guarantee for the bidirectional GAN learning approach. An appealing feature of our results is that they do not assume the reference and the data distributions to have the same dimensions or these distributions to have bounded support. These assumptions are commonly assumed in the existing convergence analysis of the unidirectional GANs but may not be satisfied in practice. Our results are also applicable to the Wasserstein bidirectional GAN if the target distribution is assumed to have a bounded support. To prove these results, we construct neural network functions that push forward an empirical distribution to another arbitrary empirical distribution on a possibly different-dimensional space. We also develop a novel decomposition of the integral probability metric for the error analysis of bidirectional GANs. These basic theoretical results are of independent interest and can be applied to other related learning problems.",
    "authors": [
      "Liu, Shiao",
      "Yang, Yunfei",
      "Huang, Jian",
      "Jiao, Yuling",
      "Wang, Yang"
    ]
  },
  {
    "id": "670e8a43b246801ca1eaca97b3e19189",
    "title": "Confidence-Aware Imitation Learning from Demonstrations with Varying Optimality",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/670e8a43b246801ca1eaca97b3e19189-Paper.pdf",
    "abstract": "Most existing imitation learning approaches assume the demonstrations are drawn from experts who are optimal, but relaxing this assumption enables us to use a wider range of data. Standard imitation learning may learn a suboptimal policy from demonstrations with varying optimality. Prior works use confidence scores or rankings to capture beneficial information from demonstrations with varying optimality, but they suffer from many limitations, e.g., manually annotated confidence scores or high average optimality of demonstrations. In this paper, we propose a general framework to learn from demonstrations with varying optimality that jointly learns the confidence score and a well-performing policy. Our approach, Confidence-Aware Imitation Learning (CAIL) learns a well-performing policy from confidence-reweighted demonstrations, while using an outer loss to track the performance of our model and to learn the confidence. We provide theoretical guarantees on the convergence of CAIL and evaluate its performance in both simulated and real robot experiments.Our results show that CAIL significantly outperforms other imitation learning methods from demonstrations with varying optimality. We further show that even without access to any optimal demonstrations, CAIL can still learn a successful policy, and outperforms prior work.",
    "authors": [
      "Zhang, Songyuan",
      "CAO, ZHANGJIE",
      "Sadigh, Dorsa",
      "Sui, Yanan"
    ]
  },
  {
    "id": "670f0c94cc5271fe6017eeffa642b7d3",
    "title": "Answering Complex Causal Queries With the Maximum Causal Set Effect",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/670f0c94cc5271fe6017eeffa642b7d3-Paper.pdf",
    "abstract": "The standard tools of causal inference have been developed to answer simple causal queries which can be easily formalized as a small number of statistical estimands in the context of a particular structural causal model (SCM); however, scientific theories often make diffuse predictions about a large number of causal variables. This article proposes a framework for parameterizing such complex causal queries as the maximum difference in causal effects associated with two sets of causal variables that have a researcher specified probability of occurring. We term this estimand the Maximum Causal Set Effect (MCSE) and develop an estimator for it that is asymptotically consistent and conservative in finite samples under assumptions that are standard in the causal inference literature. This estimator is also asymptotically normal and amenable to the non-parametric bootstrap, facilitating classical statistical inference about this novel estimand. We compare this estimator to more common latent variable approaches and find that it can uncover larger causal effects in both real world and simulated data. ",
    "authors": [
      "Markovich, Zachary"
    ]
  },
  {
    "id": "671f0311e2754fcdd37f70a8550379bc",
    "title": "Identifiability in inverse reinforcement learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/671f0311e2754fcdd37f70a8550379bc-Paper.pdf",
    "abstract": "Inverse reinforcement learning attempts to reconstruct the reward function in a Markov decision problem, using observations of agent actions. As already observed in Russell [1998] the problem is ill-posed, and the reward function is not identifiable, even under the presence of perfect information about optimal behavior. We provide a resolution to this non-identifiability for problems with entropy regularization. For a given environment, we fully characterize the reward functions leading to a given policy and demonstrate that, given demonstrations of actions for the same reward under two distinct discount factors, or under sufficiently different environments, the unobserved reward can be recovered up to a constant. We also give general necessary and sufficient conditions for reconstruction of time-homogeneous rewards on finite horizons, and for action-independent rewards, generalizing recent results of Kim et al. [2021] and Fu et al. [2018].",
    "authors": [
      "Cao, Haoyang",
      "Cohen, Samuel",
      "Szpruch, Lukasz"
    ]
  },
  {
    "id": "6734fa703f6633ab896eecbdfad8953a",
    "title": "A Probabilistic State Space Model for Joint Inference from Differential Equations and Data",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6734fa703f6633ab896eecbdfad8953a-Paper.pdf",
    "abstract": "Mechanistic models with differential equations are a key component of scientific applications of machine learning. Inference in such models is usually computationally demanding because it involves repeatedly solving the differential equation. The main problem here is that the numerical solver is hard to combine with standard inference techniques. Recent work in probabilistic numerics has developed a new class of solvers for ordinary differential equations (ODEs) that phrase the solution process directly in terms of Bayesian filtering. We here show that this allows such methods to be combined very directly, with conceptual and numerical ease, with latent force models in the ODE itself. It then becomes possible to perform approximate Bayesian inference on the latent force as well as the ODE solution in a single, linear complexity pass of an extended Kalman filter / smoother \u2014 that is, at the cost of computing a single ODE solution. We demonstrate the expressiveness and performance of the algorithm by training, among others, a non-parametric SIRD model on data from the COVID-19 outbreak.",
    "authors": [
      "Schmidt, Jonathan",
      "Kr\u00e4mer, Nicholas",
      "Hennig, Philipp"
    ]
  },
  {
    "id": "6738fc33dd0b3906cd3626397cd247a7",
    "title": "On Plasticity, Invariance, and Mutually Frozen Weights in Sequential Task Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6738fc33dd0b3906cd3626397cd247a7-Paper.pdf",
    "abstract": "Plastic neural networks have the ability to adapt to new tasks. However, in a continual learning setting, the configuration of parameters learned in previous tasks can severely reduce the adaptability to future tasks. In particular, we show that, when using weight decay, weights in successive layers of a deep network may become \"mutually frozen\". This has a double effect: on the one hand, it makes the network updates more invariant to nuisance factors, providing a useful bias for future tasks. On the other hand, it can prevent the network from learning new tasks that require significantly different features. In this context, we find that the local input sensitivity of a deep model is correlated with its ability to adapt, thus leading to an intriguing trade-off between adaptability and invariance when training a deep model more than once. We then show that a simple intervention that \"resets\" the mutually frozen connections can improve transfer learning on a variety of visual classification tasks. The efficacy of \"resetting\" itself depends on the size of the target dataset and the difference of the pre-training and target domains, allowing us to achieve state-of-the-art results on some datasets.",
    "authors": [
      "Zilly, Julian",
      "Achille, Alessandro",
      "Censi, Andrea",
      "Frazzoli, Emilio"
    ]
  },
  {
    "id": "678004486c119599ed7d199f47da043a",
    "title": "Provably Efficient Black-Box Action Poisoning Attacks Against Reinforcement Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/678004486c119599ed7d199f47da043a-Paper.pdf",
    "abstract": "Due to the broad range of applications of reinforcement learning (RL), understanding the effects of adversarial attacks against RL model is essential for the safe applications of this model. Prior theoretical works on adversarial attacks against RL mainly focus on either reward poisoning attacks or environment poisoning attacks. In this paper, we introduce a new class of attacks named action poisoning attacks, where an adversary can change the action signal selected by the agent. Compared with existing attack models, the attacker\u2019s ability in the proposed action poisoning attack model is more restricted, which brings some design challenges. We study the action poisoning attack in both white-box and black-box settings. We introduce an adaptive attack scheme called LCB-H, which works for most RL agents in the black-box setting. We prove that LCB-H attack can force any efficient RL agent, whose dynamic regret scales sublinearly with the total number of steps taken, to choose actions according to a policy selected by the attacker very frequently, with only sublinear cost. In addition, we apply LCB-H attack against a very popular model-free RL algorithm: UCB-H. We show that, even in black-box setting, by spending only logarithm cost, the proposed LCB-H attack scheme can force the UCB-H agent to choose actions according to the policy selected by the attacker very frequently.",
    "authors": [
      "Liu, Guanlin",
      "LAI, Lifeng"
    ]
  },
  {
    "id": "6786f3c62fbf9021694f6e51cc07fe3c",
    "title": "Fast Approximation of the Sliced-Wasserstein Distance Using Concentration of Random Projections",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6786f3c62fbf9021694f6e51cc07fe3c-Paper.pdf",
    "abstract": "The Sliced-Wasserstein distance (SW) is being increasingly used in machine learning applications as an alternative to the Wasserstein distance and offers significant computational and statistical benefits. Since it is defined as an expectation over random projections, SW is commonly approximated by Monte Carlo. We adopt a new perspective to approximate SW by making use of the concentration of measure phenomenon: under mild assumptions, one-dimensional projections of a high-dimensional random vector are approximately Gaussian. Based on this observation, we develop a simple deterministic approximation for SW. Our method does not require sampling a number of random projections, and is therefore both accurate and easy to use compared to the usual Monte Carlo approximation. We derive nonasymptotical guarantees for our approach, and show that the approximation error goes to zero as the dimension increases, under a weak dependence condition on the data distribution. We validate our theoretical findings on synthetic datasets, and illustrate the proposed approximation on a generative modeling problem.",
    "authors": [
      "Nadjahi, Kimia",
      "Durmus, Alain",
      "Jacob, Pierre E",
      "Badeau, Roland",
      "Simsekli, Umut"
    ]
  },
  {
    "id": "67ba02d73c54f0b83c05507b7fb7267f",
    "title": "Causal Navigation by Continuous-time Neural Networks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/67ba02d73c54f0b83c05507b7fb7267f-Paper.pdf",
    "abstract": "Imitation learning enables high-fidelity, vision-based learning of policies within rich, photorealistic environments. However, such techniques often rely on traditional discrete-time neural models and face difficulties in generalizing to domain shifts by failing to account for the causal relationships between the agent and the environment. In this paper, we propose a theoretical and experimental framework for learning causal representations using continuous-time neural networks, specifically over their discrete-time counterparts. We evaluate our method in the context of visual-control learning of drones over a series of complex tasks, ranging from short- and long-term navigation, to chasing static and dynamic objects through photorealistic environments. Our results demonstrate that causal continuous-time deep models can perform robust navigation tasks, where advanced recurrent models fail. These models learn complex causal control representations directly from raw visual inputs and scale to solve a variety of tasks using imitation learning.",
    "authors": [
      "Vorbach, Charles",
      "Hasani, Ramin",
      "Amini, Alexander",
      "Lechner, Mathias",
      "Rus, Daniela"
    ]
  },
  {
    "id": "67d16d00201083a2b118dd5128dd6f59",
    "title": "Global Convergence of Online Optimization for Nonlinear Model Predictive Control",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/67d16d00201083a2b118dd5128dd6f59-Paper.pdf",
    "abstract": "We study a real-time iteration (RTI) scheme for solving online optimization problem appeared in nonlinear optimal control. The proposed RTI scheme modifies the existing RTI-based model predictive control (MPC) algorithm, by selecting the stepsize of each Newton step at each sampling time using a differentiable exact augmented Lagrangian. The scheme can adaptively select the penalty parameters of augmented Lagrangian on the fly, which are shown to be stabilized after certain time periods. We prove under generic assumptions that, by involving stepsize selection instead of always using a full Newton step (like what most of the existing RTIs do), the scheme converges globally: for any initial point, the KKT residuals of the subproblems converge to zero. A key step is to show that augmented Lagrangian keeps decreasing as horizon moves forward. We demonstrate the global convergence behavior of the proposed RTI scheme in a numerical experiment.",
    "authors": [
      "Na, Sen"
    ]
  },
  {
    "id": "67d96d458abdef21792e6d8e590244e7",
    "title": "Argmax Flows and Multinomial Diffusion: Learning Categorical Distributions",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/67d96d458abdef21792e6d8e590244e7-Paper.pdf",
    "abstract": "Generative flows and diffusion models have been predominantly trained on ordinal data, for example natural images. This paper introduces two extensions of flows and diffusion for categorical data such as language or image segmentation: Argmax Flows and Multinomial Diffusion. Argmax Flows are defined by a composition of a continuous distribution (such as a normalizing flow), and an argmax function. To optimize this model, we learn a probabilistic inverse for the argmax that lifts the categorical data to a continuous space. Multinomial Diffusion gradually adds categorical noise in a diffusion process, for which the generative denoising process is learned. We demonstrate that our method outperforms existing dequantization approaches on text modelling and modelling on image segmentation maps in log-likelihood. ",
    "authors": [
      "Hoogeboom, Emiel",
      "Nielsen, Didrik",
      "Jaini, Priyank",
      "Forr\u00e9, Patrick",
      "Welling, Max"
    ]
  },
  {
    "id": "67e235e7f2fa8800d8375409b566e6b6",
    "title": "Learning with User-Level Privacy",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/67e235e7f2fa8800d8375409b566e6b6-Paper.pdf",
    "abstract": "We propose and analyze algorithms to solve a range of learning tasks under user-level differential privacy constraints. Rather than guaranteeing only the privacy of individual samples, user-level DP protects a user's entire contribution ($m \\ge 1$ samples), providing more stringent but more realistic protection against information leaks.  We show that for high-dimensional meanestimation, empirical risk minimization with smooth losses, stochastic convex optimization, and learning hypothesis classes with finite metric entropy, the privacy cost decreases as $O(1/\\sqrt{m})$ as users provide more samples. In contrast, when increasing the number of users $n$, the privacy cost decreases at a faster $O(1/n)$ rate.  We complement these results with lower bounds showing the minimax optimality of our algorithms for mean estimation and stochastic convex optimization. Our algorithms rely on novel techniques for private mean estimation in arbitrary dimension with error scaling as the concentration radius $\\tau$ of the distribution rather than the entire range.",
    "authors": [
      "Levy, Daniel",
      "Sun, Ziteng",
      "Amin, Kareem",
      "Kale, Satyen",
      "Kulesza, Alex",
      "Mohri, Mehryar",
      "Suresh, Ananda Theertha"
    ]
  },
  {
    "id": "67ed94744426295f96268f4ac1881b46",
    "title": "Don\u2019t Generate Me: Training Differentially Private Generative Models with Sinkhorn Divergence",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/67ed94744426295f96268f4ac1881b46-Paper.pdf",
    "abstract": "Although machine learning models trained on massive data have led to breakthroughs in several areas, their deployment in privacy-sensitive domains remains limited due to restricted access to data. Generative models trained with privacy constraints on private data can sidestep this challenge, providing indirect access to private data instead. We propose DP-Sinkhorn, a novel optimal transport-based generative method for learning data distributions from private data with differential privacy. DP-Sinkhorn minimizes the Sinkhorn divergence, a computationally efficient approximation to the exact optimal transport distance, between the model and data in a differentially private manner and uses a novel technique for controlling the bias-variance trade-off of gradient estimates. Unlike existing approaches for training differentially private generative models, which are mostly based on generative adversarial networks, we do not rely on adversarial objectives, which are notoriously difficult to optimize, especially in the presence of noise imposed by privacy constraints. Hence, DP-Sinkhorn is easy to train and deploy. Experimentally, we improve upon the state-of-the-art on multiple image modeling benchmarks and show differentially private synthesis of informative RGB images. ",
    "authors": [
      "Cao, Tianshi",
      "Bie, Alex",
      "Vahdat, Arash",
      "Fidler, Sanja",
      "Kreis, Karsten"
    ]
  },
  {
    "id": "67f7fb873eaf29526a11a9b7ac33bfac",
    "title": "Keeping Your Eye on the Ball: Trajectory Attention in Video Transformers",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/67f7fb873eaf29526a11a9b7ac33bfac-Paper.pdf",
    "abstract": "In video transformers, the time dimension is often treated in the same way as the two spatial dimensions. However, in a scene where objects or the camera may move, a physical point imaged at one location in frame $t$ may be entirely unrelated to what is found at that location in frame $t+k$. These temporal correspondences should be modeled to facilitate learning about dynamic scenes. To this end, we propose a new drop-in block for video transformers - trajectory attention - that aggregates information along implicitly determined motion paths. We additionally propose a new method to address the quadratic dependence of computation and memory on the input size, which is particularly important for high resolution or long videos. While these ideas are useful in a range of settings, we apply them to the specific task of video action recognition with a transformer model and obtain state-of-the-art results on the Kinetics, Something-Something V2, and Epic-Kitchens datasets.",
    "authors": [
      "Patrick, Mandela",
      "Campbell, Dylan",
      "Asano, Yuki",
      "Misra, Ishan",
      "Metze, Florian",
      "Feichtenhofer, Christoph",
      "Vedaldi, Andrea",
      "Henriques, Jo\u00e3o F."
    ]
  },
  {
    "id": "680390c55bbd9ce416d1d69a9ab4760d",
    "title": "Variational Bayesian Optimistic Sampling",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/680390c55bbd9ce416d1d69a9ab4760d-Paper.pdf",
    "abstract": "We consider online sequential decision problems where an agent must balance  exploration and exploitation. We derive a set of Bayesian `optimistic' policies  which, in the stochastic multi-armed bandit case, includes the Thompson sampling  policy. We provide a new analysis showing that any algorithm producing policies in the optimistic set enjoys $\\tilde O(\\sqrt{AT})$ Bayesian regret for a problem with $A$ actions after $T$ rounds. We extend the regret analysis for optimistic policies to bilinear saddle-point problems which include zero-sum matrix games and constrained bandits as special cases. In this case we show that Thompson sampling can produce policies outside of the optimistic set and suffer linear regret in some instances. Finding a policy inside the optimistic set amounts to solving a convex optimization problem and we call the resulting algorithm `variational Bayesian optimistic sampling' (VBOS). The procedure works for any posteriors, \\ie, it does not require the posterior to have any special properties, such as log-concavity, unimodality, or smoothness. The variational view of the problem has many useful properties, including the ability to tune the exploration-exploitation tradeoff, add regularization, incorporate constraints, and linearly parameterize the policy.",
    "authors": [
      "O'Donoghue, Brendan",
      "Lattimore, Tor"
    ]
  },
  {
    "id": "68264bdb65b97eeae6788aa3348e553c",
    "title": "Cross-modal Domain Adaptation for Cost-Efficient Visual Reinforcement Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/68264bdb65b97eeae6788aa3348e553c-Paper.pdf",
    "abstract": "In visual-input sim-to-real scenarios, to overcome the reality gap between images rendered in simulators and those from the real world, domain adaptation, i.e., learning an aligned representation space between simulators and the real world, then training and deploying policies in the aligned representation, is a promising direction. Previous methods focus on same-modal domain adaptation. However, those methods require building and running simulators that render high-quality images, which can be difficult and costly. In this paper, we consider a more cost-efficient setting of visual-input sim-to-real where only low-dimensional states are simulated. We first point out that the objective of learning mapping functions in previous methods that align the representation spaces is ill-posed, prone to yield an incorrect mapping. When the mapping crosses modalities, previous methods are easier to fail. Our algorithm, Cross-mOdal Domain Adaptation with Sequential structure (CODAS), mitigates the ill-posedness by utilizing the sequential nature of the data sampling process in RL tasks. Experiments on MuJoCo and Hand Manipulation Suite tasks show that the agents deployed with our method achieve similar performance as it has in the source domain, while those deployed with previous methods designed for same-modal domain adaptation suffer a larger performance gap.",
    "authors": [
      "Chen, Xiong-Hui",
      "Jiang, Shengyi",
      "Xu, Feng",
      "Zhang, Zongzhang",
      "Yu, Yang"
    ]
  },
  {
    "id": "682e0e796084e163c5ca053dd8573b0c",
    "title": "D2C: Diffusion-Decoding Models for Few-Shot Conditional Generation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/682e0e796084e163c5ca053dd8573b0c-Paper.pdf",
    "abstract": "Conditional generative models of high-dimensional images have many applications, but supervision signals from conditions to images can be expensive to acquire. This paper describes Diffusion-Decoding models with Contrastive representations (D2C), a paradigm for training unconditional variational autoencoders (VAE) for few-shot conditional image generation. D2C uses a learned diffusion-based prior over the latent representations to improve generation and contrastive self-supervised learning to improve representation quality. D2C can adapt to novel generation tasks, conditioned on labels or manipulation constraints, by learning from as few as 100 labeled examples. On conditional generation from new labels, D2C achieves superior performance over state-of-the-art VAEs and diffusion models. On conditional image manipulation, D2C generations are two orders of magnitude faster to produce over StyleGAN2 ones and are preferred by 50% - 60% of the human evaluators in a double-blind study. We release our code at https://github.com/jiamings/d2c.",
    "authors": [
      "Sinha, Abhishek",
      "Song, Jiaming",
      "Meng, Chenlin",
      "Ermon, Stefano"
    ]
  },
  {
    "id": "68331ff0427b551b68e911eebe35233b",
    "title": "Continual Auxiliary Task Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/68331ff0427b551b68e911eebe35233b-Paper.pdf",
    "abstract": "Learning auxiliary tasks, such as multiple predictions about the world, can provide many benefits to reinforcement learning systems. A variety of off-policy learning algorithms have been developed to learn such predictions, but as yet there is little work on how to adapt the behavior to gather useful data for those off-policy predictions. In this work, we investigate a reinforcement learning system designed to learn a collection of auxiliary tasks, with a behavior policy learning to take actions to improve those auxiliary predictions. We highlight the inherent non-stationarity in this continual auxiliary task learning problem, for both prediction learners and the behavior learner. We develop an algorithm based on successor features that facilitates tracking under non-stationary rewards, and prove the separation into learning successor features and rewards provides convergence rate improvements. We conduct an in-depth study into the resulting multi-prediction learning system. ",
    "authors": [
      "McLeod, Matthew",
      "Lo, Chunlok",
      "Schlegel, Matthew",
      "Jacobsen, Andrew",
      "Kumaraswamy, Raksha",
      "White, Martha",
      "White, Adam"
    ]
  },
  {
    "id": "685217557383cd194b4f10ae4b39eebf",
    "title": "Constrained Two-step Look-Ahead Bayesian Optimization",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/685217557383cd194b4f10ae4b39eebf-Paper.pdf",
    "abstract": "Recent advances in computationally efficient non-myopic Bayesian optimization offer improved query efficiency over traditional myopic methods like expected improvement, with only a modest increase in computational cost. These advances have been largely limited to unconstrained BO methods with only a few exceptions which require heavy computation. For instance, one existing multi-step lookahead constrained BO method (Lam & Willcox, 2017) relies on computationally expensive unreliable brute force derivative-free optimization of a Monte Carlo rollout acquisition function. Methods that use the reparameterization trick for more efficient derivative-based optimization of non-myopic acquisition functions in the unconstrained setting, like sample average approximation and infinitesimal perturbation analysis, do not extend: constraints introduce discontinuities in the sampled acquisition function surface. Moreover, we argue here that being non-myopic is even more important in constrained problems because fear of violating constraints pushes myopic methods away from sampling the boundary between feasible and infeasible regions, slowing the discovery of optimal solutions with tight constraints. In this paper, we propose a computationally efficient two-step lookahead constrained Bayesian optimization acquisition function (2-OPT-C) supporting both sequential and batch settings. To enable fast acquisition function optimization, we develop a novel likelihood ratio-based unbiased estimator of the gradient of the two-step optimal acquisition function that does not use the reparameterization trick. In numerical experiments, 2-OPT-C typically improves query efficiency by 2x or more over previous methods, and in some cases by 10x or more.",
    "authors": [
      "Zhang, Yunxiang",
      "Zhang, Xiangyu",
      "Frazier, Peter"
    ]
  },
  {
    "id": "689041c2baed0f6d91050495d632d6e0",
    "title": "Learning with Labeling Induced Abstentions",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/689041c2baed0f6d91050495d632d6e0-Paper.pdf",
    "abstract": "Consider a setting where we wish to automate an expensive task with a machine learning algorithm using a limited labeling resource. In such settings, examples routed for labeling are often out of scope for the machine learning algorithm. For example, in a spam detection setting, human reviewers not only provide labeled data but are such high-quality detectors of spam that examples routed to them no longer require machine evaluation. As a consequence, the distribution of examples routed to the machine is intimately tied to the process generating labels. We introduce a formalization of this setting, and give an algorithm that simultaneously learns a model and decides when to request a label by leveraging ideas from both the abstention and active learning literatures. We prove an upper bound on the algorithm's label complexity and a matching lower bound for any algorithm in this setting. We conduct a thorough set of experiments including an ablation study to test different components of our algorithm. We demonstrate the effectiveness of an efficient version of our algorithm over margin sampling on a variety of datasets. ",
    "authors": [
      "Amin, Kareem",
      "DeSalvo, Giulia",
      "Rostamizadeh, Afshin"
    ]
  },
  {
    "id": "68bd22864919297c8c8a8c32378e89b4",
    "title": "SQALER: Scaling Question Answering by Decoupling Multi-Hop and Logical Reasoning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/68bd22864919297c8c8a8c32378e89b4-Paper.pdf",
    "abstract": "State-of-the-art approaches to reasoning and question answering over knowledge graphs (KGs) usually scale with the number of edges and can only be applied effectively on small instance-dependent subgraphs. In this paper, we address this issue by showing that multi-hop and more complex logical reasoning can be accomplished separately without losing expressive power. Motivated by this insight, we propose an approach to multi-hop reasoning that scales linearly with the number of relation types in the graph, which is usually significantly smaller than the number of edges or nodes. This produces a set of candidate solutions that can be provably refined to recover the solution to the original problem. Our experiments on knowledge-based question answering show that our approach solves the multi-hop MetaQA dataset, achieves a new state-of-the-art on the more challenging WebQuestionsSP, is orders of magnitude more scalable than competitive approaches, and can achieve compositional generalization out of the training distribution.",
    "authors": [
      "Atzeni, Mattia",
      "Bogojeska, Jasmina",
      "Loukas, Andreas"
    ]
  },
  {
    "id": "691dcb1d65f31967a874d18383b9da75",
    "title": "Out-of-Distribution Generalization in Kernel Regression",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/691dcb1d65f31967a874d18383b9da75-Paper.pdf",
    "abstract": "In real word applications, data generating process for training a machine learning model often differs from what the model encounters in the test stage. Understanding how and whether machine learning models generalize  under such distributional shifts have been a theoretical challenge. Here, we study generalization in kernel regression when the training and test distributions are different using methods from statistical physics. Using the replica method, we derive an analytical formula for the out-of-distribution  generalization error applicable to any kernel and real datasets. We identify an overlap matrix that quantifies the mismatch between distributions for a given kernel as a key determinant of generalization performance under distribution shift. Using our analytical expressions we elucidate various generalization phenomena including possible improvement in generalization when there is a mismatch. We develop procedures for optimizing training and test distributions for a given data budget to find best and worst case generalizations under the shift.  We present applications of our theory to real and synthetic datasets and for many kernels. We compare results of our theory applied to Neural Tangent Kernel with simulations of wide networks and show agreement. We analyze linear regression in further depth.",
    "authors": [
      "Canatar, Abdulkadir",
      "Bordelon, Blake",
      "Pehlevan, Cengiz"
    ]
  },
  {
    "id": "692baebec3bb4b53d7ebc3b9fabac31b",
    "title": "FL-WBC: Enhancing Robustness against Model Poisoning Attacks in Federated Learning from a Client Perspective",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/692baebec3bb4b53d7ebc3b9fabac31b-Paper.pdf",
    "abstract": "Federated learning (FL) is a popular distributed learning framework that trains a global model through iterative communications between a central server and edge devices. Recent works have demonstrated that FL is vulnerable to model poisoning attacks. Several server-based defense approaches (e.g. robust aggregation), have been proposed to mitigate such attacks. However, we empirically show that under extremely strong attacks, these defensive methods fail to guarantee the robustness of FL. More importantly, we observe that as long as the global model is polluted, the impact of attacks on the global model will remain in subsequent rounds even if there are no subsequent attacks. In this work, we propose a client-based defense, named White Blood Cell for Federated Learning (FL-WBC), which can mitigate model poisoning attacks that have already polluted the global model. The key idea of FL-WBC is to identify the parameter space where long-lasting attack effect on parameters resides and perturb that space during local training. Furthermore, we derive a certified robustness guarantee against model poisoning attacks and a convergence guarantee to FedAvg after applying our FL-WBC. We conduct experiments on FasionMNIST and CIFAR10 to evaluate the defense against state-of-the-art model poisoning attacks. The results demonstrate that our method can effectively mitigate model poisoning attack impact on the global model within 5 communication rounds with nearly no accuracy drop under both IID and Non-IID settings. Our defense is also complementary to existing server-based robust aggregation approaches and can further improve the robustness of FL under extremely strong attacks.",
    "authors": [
      "Sun, Jingwei",
      "Li, Ang",
      "DiValentin, Louis",
      "Hassanzadeh, Amin",
      "Chen, Yiran",
      "Li, Hai"
    ]
  },
  {
    "id": "69386f6bb1dfed68692a24c8686939b9",
    "title": "Chebyshev-Cantelli PAC-Bayes-Bennett Inequality for the Weighted Majority Vote",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf",
    "abstract": "We present a new second-order oracle bound for the expected risk of a weighted majority vote. The bound is based on a novel parametric form of the Chebyshev-Cantelli inequality (a.k.a. one-sided Chebyshev\u2019s), which is amenable to efficient minimization. The new form resolves the optimization challenge faced by prior oracle bounds based on the Chebyshev-Cantelli inequality, the C-bounds [Germain et al., 2015], and, at the same time, it improves on the oracle bound based on second order Markov\u2019s inequality introduced by Masegosa et al. [2020]. We also derive a new concentration of measure inequality, which we name PAC-Bayes-Bennett, since it combines PAC-Bayesian bounding with Bennett\u2019s inequality. We use it for empirical estimation of the oracle bound. The PAC-Bayes-Bennett inequality improves on the PAC-Bayes-Bernstein inequality of Seldin et al. [2012]. We provide an empirical evaluation demonstrating that the new bounds can improve on the work of Masegosa et al. [2020]. Both the parametric form of the Chebyshev-Cantelli inequality and the PAC-Bayes-Bennett inequality may be of independent interest for the study of concentration of measure in other domains.",
    "authors": [
      "Wu, Yi-Shan",
      "Masegosa, Andres",
      "Lorenzen, Stephan",
      "Igel, Christian",
      "Seldin, Yevgeny"
    ]
  },
  {
    "id": "6948bd44c91acd2b54ecdd1b132f10fb",
    "title": "A Multi-Implicit Neural Representation for Fonts",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6948bd44c91acd2b54ecdd1b132f10fb-Paper.pdf",
    "abstract": "Fonts are ubiquitous across documents and come in a variety of styles.  They are either represented in a native vector format or rasterized to produce fixed resolution images. In the first case, the non-standard representation prevents benefiting from latest network architectures for neural representations; while, in the latter case, the rasterized representation, when encoded via networks, results in loss of data fidelity, as font-specific discontinuities like edges and corners are difficult to represent using neural networks. Based on the observation that complex fonts can be represented by a superposition of a set of simpler occupancy functions, we introduce multi-implicits to represent fonts as a permutation-invariant set of learned implict functions, without losing features (e.g., edges and corners). However, while multi-implicits locally preserve font features, obtaining supervision in the form of ground truth multi-channel signals is a problem in itself. Instead, we propose how to train such a representation with only local  supervision, while the proposed neural architecture directly finds globally consistent multi-implicits for font families. We extensively evaluate the proposed representation for various tasks including reconstruction, interpolation, and synthesis to demonstrate clear advantages with existing alternatives. Additionally, the representation naturally enables glyph completion, wherein a single characteristic font is used to synthesize a whole font family in the target style. ",
    "authors": [
      "Reddy, Pradyumna",
      "Zhang, Zhifei",
      "Wang, Zhaowen",
      "Fisher, Matthew",
      "Jin, Hailin",
      "Mitra, Niloy"
    ]
  },
  {
    "id": "698d51a19d8a121ce581499d7b701668",
    "title": "OctField: Hierarchical Implicit Functions for 3D Modeling",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/698d51a19d8a121ce581499d7b701668-Paper.pdf",
    "abstract": "Recent advances in localized implicit functions have enabled neural implicit representation to be scalable to large scenes.However, the regular subdivision of 3D space employed by these approaches fails to take into account the sparsity of the surface occupancy and the varying granularities of geometric details. As a result, its memory footprint grows cubically with the input volume, leading to a prohibitive computational cost even at a moderately dense decomposition. In this work, we present a learnable hierarchical implicit representation for 3D surfaces, coded OctField, that allows high-precision encoding of intricate surfaces with low memory and computational budget. The key to our approach is an adaptive decomposition of 3D scenes that only distributes local implicit functions around the surface of interest. We achieve this goal by introducing a hierarchical octree structure to adaptively subdivide the 3D space according to the surface occupancy and the richness of part geometry. As octree is discrete and non-differentiable, we further propose a novel hierarchical network that models the subdivision of octree cells as a probabilistic process and recursively encodes and decodes both octree structure and surface geometry in a differentiable manner. We demonstrate the value of OctField for a range of shape modeling and reconstruction tasks, showing superiority over alternative approaches.",
    "authors": [
      "Tang, Jia-Heng",
      "Chen, Weikai",
      "Yang, jie",
      "Wang, Bo",
      "Liu, Songrun",
      "Yang, Bo",
      "Gao, Lin"
    ]
  },
  {
    "id": "69adc1e107f7f7d035d7baf04342e1ca",
    "title": "The Inductive Bias of Quantum Kernels",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/69adc1e107f7f7d035d7baf04342e1ca-Paper.pdf",
    "abstract": "It has been hypothesized that quantum computers may lend themselves well to applications in machine learning. In the present work, we analyze function classes defined via quantum kernels. Quantum computers offer the possibility to efficiently compute inner products of exponentially large density operators that are classically hard to compute. However, having an exponentially large feature space renders the problem of generalization hard. Furthermore, being able to evaluate inner products in high dimensional spaces efficiently by itself does not guarantee a quantum advantage, as already classically tractable kernels can correspond to high- or infinite-dimensional reproducing kernel Hilbert spaces (RKHS).   We analyze the spectral properties of quantum kernels and find that we can expect an advantage if their RKHS is low dimensional and contains functions that are hard to compute classically. If the target function is known to lie in this class, this implies a quantum advantage, as the quantum computer can encode this inductive bias, whereas there is no classically efficient way to constrain the function class in the same way. However, we show that finding suitable quantum kernels is not easy because the kernel evaluation might require exponentially many measurements.   In conclusion, our message is a somewhat sobering one: we conjecture that quantum machine learning models can offer speed-ups only if we manage to encode knowledge about the problem at hand into quantum circuits, while encoding the same bias into a classical model would be hard. These situations may plausibly occur when learning on data generated by a quantum process, however, they appear to be harder to come by for classical datasets.",
    "authors": [
      "K\u00fcbler, Jonas",
      "Buchholz, Simon",
      "Sch\u00f6lkopf, Bernhard"
    ]
  },
  {
    "id": "69dd2eff9b6a421d5ce262b093bdab23",
    "title": "An Exponential Improvement on the Memorization Capacity of Deep Threshold Networks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/69dd2eff9b6a421d5ce262b093bdab23-Paper.pdf",
    "abstract": "It is well known that modern deep neural networks are powerful enough to memorize datasets even when the labels have been randomized. Recently, Vershynin(2020) settled a long standing question by Baum(1988), proving that deep threshold networks can memorize $n$ points in $d$ dimensions using $\\widetilde{\\mathcal{O}}(e^{1/\\delta^2}+\\sqrt{n})$ neurons and $\\widetilde{\\mathcal{O}}(e^{1/\\delta^2}(d+\\sqrt{n})+n)$ weights, where $\\delta$ is the minimum distance between the points. In this work, we improve the dependence on $\\delta$ from exponential to almost linear, proving that $\\widetilde{\\mathcal{O}}(\\frac{1}{\\delta}+\\sqrt{n})$ neurons and $\\widetilde{\\mathcal{O}}(\\frac{d}{\\delta}+n)$ weights are sufficient. Our construction uses Gaussian random weights only in the first layer, while all the subsequent layers use binary or integer weights. We also prove new lower bounds by connecting memorization in neural networks to the purely geometric problem of separating $n$ points on a sphere using hyperplanes.",
    "authors": [
      "Rajput, Shashank",
      "Sreenivasan, Kartik",
      "Papailiopoulos, Dimitris",
      "Karbasi, Amin"
    ]
  },
  {
    "id": "69eba34671b3ef1ef38ee85caae6b2a1",
    "title": "Pretraining Representations for Data-Efficient Reinforcement Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/69eba34671b3ef1ef38ee85caae6b2a1-Paper.pdf",
    "abstract": "Data efficiency is a key challenge for deep reinforcement learning. We address this problem by using unlabeled data to pretrain an encoder which is then finetuned on a small amount of task-specific data. To encourage learning representations which capture diverse aspects of the underlying MDP, we employ a combination of latent dynamics modelling and unsupervised goal-conditioned RL. When limited to 100k steps of interaction on Atari games (equivalent to two hours of human experience), our approach significantly surpasses prior work combining offline representation pretraining with task-specific finetuning, and compares favourably with other pretraining methods that require orders of magnitude more data. Our approach shows particular promise when combined with larger models as well as more diverse, task-aligned observational data -- approaching human-level performance and data-efficiency on Atari in our best setting.",
    "authors": [
      "Schwarzer, Max",
      "Rajkumar, Nitarshan",
      "Noukhovitch, Michael",
      "Anand, Ankesh",
      "Charlin, Laurent",
      "Hjelm, R Devon",
      "Bachman, Philip",
      "Courville, Aaron C."
    ]
  },
  {
    "id": "69ec5030f78a9b735402d133317bf5f6",
    "title": "Universal Approximation Using Well-Conditioned Normalizing Flows",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/69ec5030f78a9b735402d133317bf5f6-Paper.pdf",
    "abstract": "Normalizing flows are a widely used class of latent-variable generative models with a tractable likelihood. Affine-coupling models [Dinh et al., 2014, 2016] are a particularly common type of normalizing flows, for which the Jacobian of the latent-to-observable-variable transformation is triangular, allowing the likelihood to be computed in linear time. Despite the widespread usage of affine couplings, the special structure of the architecture makes understanding their representational power challenging. The question of universal approximation was only recently resolved by three parallel papers [Huang et al., 2020, Zhang et al., 2020, Koehler et al., 2020] \u2013 who showed reasonably regular distributions can be approximated arbitrarily well using affine couplings \u2013 albeit with networks with a nearly-singular Jacobian. As ill-conditioned Jacobians are an obstacle for likelihood-based training, the fundamental question remains: which distributions can be approximated using well-conditioned affine coupling flows? In this paper, we show that any log-concave distribution can be approximated using well-conditioned affine-coupling flows.  In terms of proof techniques, we uncover and leverage deep connections between affine coupling architectures, underdamped Langevin dynamics (a stochastic differential equation often used to sample from Gibbs measures) and H\u00e9non maps (a structured dynamical system that appears in the study of symplectic diffeomorphisms). In terms of informing practice, we approximate a padded version of the input distribution with iid Gaussians \u2013 a strategy which Koehler et al. [2020] empirically observed to result in better-conditioned flows, but had hitherto no theoretical grounding. Our proof can thus be seen as providing theoretical evidence for the benefits of Gaussian padding when training normalizing flows.",
    "authors": [
      "Lee, Holden",
      "Pabbaraju, Chirag",
      "Sevekari, Anish Prasad",
      "Risteski, Andrej"
    ]
  },
  {
    "id": "69f62956429865909921fa916d61c1f8",
    "title": "On the Validity of Modeling SGD with Stochastic Differential Equations (SDEs)",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/69f62956429865909921fa916d61c1f8-Paper.pdf",
    "abstract": "It is generally recognized that finite learning rate (LR), in contrast to infinitesimal LR, is important for good generalization in real-life deep nets. Most attempted explanations propose approximating finite-LR SGD with It\u00f4 Stochastic Differential Equations (SDEs), but formal justification for this approximation (e.g., Li et al., 2019) only applies to SGD with tiny LR. Experimental verification of the approximation appears computationally infeasible. The current paper clarifies the picture with the following contributions: (a) An efficient simulation algorithm SVAG that provably converges to the conventionally used It\u00f4 SDE approximation. (b) A theoretically motivated testable necessary condition for the SDE approximation and its most famous implication, the linear scaling rule (Goyal et al., 2017), to hold.(c) Experiments using this simulation to demonstrate that the previously proposed SDE approximation can meaningfully capture the training and generalization properties of common deep nets. ",
    "authors": [
      "Li, Zhiyuan",
      "Malladi, Sadhika",
      "Arora, Sanjeev"
    ]
  },
  {
    "id": "69f8ea31de0c00502b2ae571fbab1f95",
    "title": "Proportional Participatory Budgeting with Additive Utilities",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/69f8ea31de0c00502b2ae571fbab1f95-Paper.pdf",
    "abstract": "We study voting rules for participatory budgeting, where a group of voters collectively decides which projects should be funded using a common budget. We allow the projects to have arbitrary costs, and the voters to have arbitrary additive valuations over the projects. We formulate two axioms that guarantee proportional representation to groups of voters with common interests. To the best of our knowledge, all known rules for participatory budgeting do not satisfy either of the two axioms; in addition we show that the most prominent proportional rule for committee elections, Proportional Approval Voting, cannot be adapted to arbitrary costs nor to additive valuations so that it would satisfy our axioms of proportionality. We construct a simple and attractive voting rule that satisfies one of our axioms (for arbitrary costs and arbitrary additive valuations), and that can be evaluated in polynomial time. We prove that our other stronger axiom is also satisfiable, though by a computationally more expensive and less natural voting rule.",
    "authors": [
      "Peters, Dominik",
      "Pierczy\u0144ski, Grzegorz",
      "Skowron, Piotr"
    ]
  },
  {
    "id": "6a12d7ebc27cae44623468302c47ad74",
    "title": "Disentangling the Roles of Curation, Data-Augmentation and the Prior in the Cold Posterior Effect",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6a12d7ebc27cae44623468302c47ad74-Paper.pdf",
    "abstract": "The \u201ccold posterior effect\u201d (CPE) in Bayesian deep learning describes the disturbing observation that the predictive performance of Bayesian neural networks can be significantly improved if the Bayes posterior is artificially sharpened using a temperature parameter T <1.  The CPE is problematic in theory and practice and since the effect was identified many researchers have proposed hypotheses to explain the phenomenon. However, despite this intensive research effort the effect remains poorly understood. In this work we provide novel and nuanced evidence relevant to existing explanations for the cold posterior effect, disentangling three hypotheses: 1. The dataset curation hypothesis of Aitchison (2020): we show empirically that the CPE does not arise in a real curated data set but can be produced in a controlled experiment with varying curation strength. 2. The data augmentation hypothesis of Izmailov et al. (2021) and Fortuin et al. (2021): we show empirically that data augmentation is sufficient but not necessary for the CPE to be present. 3. The bad prior hypothesis of Wenzel et al. (2020): we use a simple experiment evaluating the relative importance of the prior and the likelihood, strongly linking the CPE to the prior. Our results demonstrate how the CPE can arise in isolation from synthetic curation, data augmentation, and bad priors. Cold posteriors observed \u201cin the wild\u201d are therefore unlikely to arise from a single simple cause; as a result, we do not expect a simple \u201cfix\u201d for cold posteriors.",
    "authors": [
      "Noci, Lorenzo",
      "Roth, Kevin",
      "Bachmann, Gregor",
      "Nowozin, Sebastian",
      "Hofmann, Thomas"
    ]
  },
  {
    "id": "6a130f1dc6f0c829f874e92e5458dced",
    "title": "Sanity Checks for Lottery Tickets: Does Your Winning Ticket Really Win the Jackpot?",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6a130f1dc6f0c829f874e92e5458dced-Paper.pdf",
    "abstract": "There have been long-standing controversies and inconsistencies over the experiment setup and criteria for identifying the \"winning ticket\" in literature. To reconcile such, we revisit the definition of lottery ticket hypothesis, with comprehensive and more rigorous conditions. Under our new definition, we show concrete evidence to clarify whether the winning ticket exists across the major DNN architectures and/or applications. Through extensive experiments, we perform quantitative analysis on the correlations between winning tickets and various experimental factors, and empirically study the patterns of our observations. We find that the key training hyperparameters, such as learning rate and training epochs, as well as the architecture characteristics such as capacities and residual connections, are all highly correlated with whether and when the winning tickets can be identified. Based on our analysis, we summarize a guideline for parameter settings in regards of specific architecture characteristics, which we hope to catalyze the research progress on the topic of lottery ticket hypothesis. Our codes are publicly available at: https://github.com/boone891214/sanity-check-LTH.",
    "authors": [
      "Ma, Xiaolong",
      "Yuan, Geng",
      "Shen, Xuan",
      "Chen, Tianlong",
      "Chen, Xuxi",
      "Chen, Xiaohan",
      "Liu, Ning",
      "Qin, Minghai",
      "Liu, Sijia",
      "Wang, Zhangyang",
      "Wang, Yanzhi"
    ]
  },
  {
    "id": "6a1a681b16826ba2e48fedb229db3b65",
    "title": "Collaborative Causal Discovery with Atomic Interventions",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6a1a681b16826ba2e48fedb229db3b65-Paper.pdf",
    "abstract": "We introduce a new Collaborative Causal Discovery problem, through which we model a common scenario in which we have multiple independent entities each with their own causal graph, and the goal is to simultaneously learn all these causal graphs. We study this problem without the causal sufficiency assumption, using Maximal Ancestral Graphs (MAG) to model the causal graphs, and assuming that we have the ability to actively perform independent single vertex (or atomic) interventions on the entities. If the $M$ underlying (unknown) causal graphs of the entities satisfy a natural notion of clustering, we give algorithms that leverage this property and recovers all the causal graphs using roughly logarithmic in $M$ number of atomic interventions per entity. These are significantly fewer than $n$ atomic interventions per entity required to learn each causal graph separately, where $n$ is the number of observable nodes in the causal graph. We complement our results with a lower bound and discuss various extensions of our collaborative setting.",
    "authors": [
      "Addanki, Raghavendra",
      "Kasiviswanathan, Shiva"
    ]
  },
  {
    "id": "6a26c75d6a576c94654bfc4dda548c72",
    "title": "Towards optimally abstaining from prediction with OOD test examples",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6a26c75d6a576c94654bfc4dda548c72-Paper.pdf",
    "abstract": "A common challenge across all areas of machine learning is that training data is not distributed like test data, due to natural shifts or adversarial examples; such examples are referred to as out-of-distribution (OOD) test examples. We consider a model where one may abstain from predicting, at a fixed cost. In particular, our transductive abstention algorithm takes labeled training examples and unlabeled test examples as input, and provides predictions with optimal prediction loss guarantees. The loss bounds match standard generalization bounds when test examples are i.i.d. from the training distribution, but add an additional term that is the cost of abstaining times the statistical distance between the train and test distribution (or the fraction of adversarial examples). For linear regression, we give a polynomial-time algorithm based on Celis-Dennis-Tapia optimization algorithms. For binary classification, we show how to efficiently implement it using a proper agnostic learner (i.e., an Empirical Risk Minimizer) for the class of interest. Our work builds on recent work of Goldwasser, Kalais, and Montasser (2020) who gave error and abstention guarantees for transductive binary classification.",
    "authors": [
      "Kalai, Adam",
      "Kanade, Varun"
    ]
  },
  {
    "id": "6a30e32e56fce5cf381895dfe6ca7b6f",
    "title": "TokenLearner: Adaptive Space-Time Tokenization for Videos",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6a30e32e56fce5cf381895dfe6ca7b6f-Paper.pdf",
    "abstract": "In this paper, we introduce a novel visual representation learning which relies on a handful of adaptively learned tokens, and which is applicable to both image and video understanding tasks. Instead of relying on hand-designed splitting strategies to obtain visual tokens and processing a large number of densely sampled patches for attention, our approach learns to mine important tokens in visual data. This results in efficiently and effectively finding a few important visual tokens and enables modeling of pairwise attention between such tokens, over a longer temporal horizon for videos, or the spatial content in image frames. Our experiments demonstrate strong performance on several challenging benchmarks for video recognition tasks. Importantly, due to our tokens being adaptive, we accomplish competitive results at significantly reduced computational cost. We establish new state-of-the-arts on multiple video datasets, including Kinetics-400, Kinetics-600, Charades, and AViD.",
    "authors": [
      "Ryoo, Michael",
      "Piergiovanni, AJ",
      "Arnab, Anurag",
      "Dehghani, Mostafa",
      "Angelova, Anelia"
    ]
  },
  {
    "id": "6a571fe98a2ba453e84923b447d79cff",
    "title": "Learning in Multi-Stage Decentralized Matching Markets",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6a571fe98a2ba453e84923b447d79cff-Paper.pdf",
    "abstract": "Matching markets are often organized in a multi-stage and decentralized manner.  Moreover, participants in real-world matching markets often have uncertain preferences. This article develops a framework for learning optimal strategies in such settings, based on a nonparametric statistical approach and variational analysis.  We propose an efficient algorithm, built upon concepts of \"lower uncertainty bound\" and \"calibrated decentralized matching,\" for maximizing the participants' expected payoff. We show that there exists a welfare-versus-fairness trade-off that is characterized by the uncertainty level of acceptance.  Participants will strategically act in favor of a low uncertainty level to reduce competition and increase expected payoff. We prove that participants can be better off with multi-stage matching compared to single-stage matching. We demonstrate aspects of the theoretical predictions through simulations and an experiment using real data from college admissions.",
    "authors": [
      "Dai, Xiaowu",
      "Jordan, Michael"
    ]
  },
  {
    "id": "6a61d423d02a1c56250dc23ae7ff12f3",
    "title": "Non-asymptotic convergence bounds for Wasserstein approximation using point clouds",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6a61d423d02a1c56250dc23ae7ff12f3-Paper.pdf",
    "abstract": "Several issues in machine learning and inverse problems require to generate discrete data, as if sampled from a model probabilitydistribution. A common way to do so relies on the construction of a uniform probability distribution over a set of $N$ points whichminimizes the Wasserstein distance to the model distribution. This minimization problem, where the unknowns are the positions of the atoms, is non-convex. Yet, in most cases, a suitably adjusted version of Lloyd's algorithm in which Voronoi cells are replaced by Power cells, leads to configurations with small Wasserstein error. This is surprising because, again, of the non-convex nature of the problem, which moreover admits spurious critical points. We provide explicit upper bounds for the convergence speed of this Lloyd-type algorithm, starting from a cloud of points sufficiently far from each other. This already works after one step of the iteration procedure, and similar bounds can be deduced, for the corresponding gradient descent. These bounds naturally lead to a sort of Poliak-\u0141ojasiewicz inequality for the Wasserstein distance cost, with an error term depending on the distances between Dirac masses in the discrete distribution.",
    "authors": [
      "M\u00e9rigot, Quentin",
      "Santambrogio, Filippo",
      "SARRAZIN, Cl\u00e9ment"
    ]
  },
  {
    "id": "6a711a119a8a7a9f877b5f379bfe9ea2",
    "title": "Understanding Interlocking Dynamics of Cooperative Rationalization",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6a711a119a8a7a9f877b5f379bfe9ea2-Paper.pdf",
    "abstract": "Selective rationalization explains the prediction of complex neural networks by finding a small subset of the input that is sufficient to predict the neural model output. The selection mechanism is commonly integrated into the model itself by specifying a two-component cascaded system consisting of a rationale generator, which makes a binary selection of the input features (which is the rationale), and a predictor, which predicts the output based only on the selected features. The components are trained jointly to optimize prediction performance. In this paper, we reveal a major problem with such cooperative rationalization paradigm --- model interlocking. Inter-locking arises when the predictor overfits to the features selected by the generator thus reinforcing the generator's selection even if the selected rationales are sub-optimal. The fundamental cause of the interlocking problem is that the rationalization objective to be minimized is concave with respect to the generator\u2019s selection policy. We propose a new rationalization framework, called A2R, which introduces a third component into the architecture, a predictor driven by soft attention as opposed to selection. The generator now realizes both soft and hard attention over the features and these are fed into the two different predictors. While the generator still seeks to support the original predictor performance, it also minimizes a gap between the two predictors. As we will show theoretically, since the attention-based predictor exhibits a better convexity property, A2R can overcome the concavity barrier. Our experiments on two synthetic benchmarks and two real datasets demonstrate that A2R can significantly alleviate the interlock problem and find explanations that better align with human judgments.",
    "authors": [
      "Yu, Mo",
      "Zhang, Yang",
      "Chang, Shiyu",
      "Jaakkola, Tommi"
    ]
  },
  {
    "id": "6a971e08a01e6676d0f1a6e0dacbbd67",
    "title": "Adversarial Robustness without Adversarial Training: A Teacher-Guided Curriculum Learning Approach",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6a971e08a01e6676d0f1a6e0dacbbd67-Paper.pdf",
    "abstract": "Current SOTA adversarially robust models are mostly based on adversarial training (AT) and differ only by some regularizers either at inner maximization or outer minimization steps. Being repetitive in nature during the inner maximization step, they take a huge time to train. We propose a non-iterative method that enforces the following ideas during training. Attribution maps are more aligned to the actual object in the image for adversarially robust models compared to naturally trained models. Also, the allowed set of pixels to perturb an image (that changes model decision) should be restricted to the object pixels only, which reduces the attack strength by limiting the attack space. Our method achieves significant performance gains with a little extra effort (10-20%) over existing AT models and outperforms all other methods in terms of adversarial as well as natural accuracy. We have performed extensive experimentation with CIFAR-10, CIFAR-100, and TinyImageNet datasets and reported results against many popular strong adversarial attacks to prove the effectiveness of our method.",
    "authors": [
      "Sarkar, Anindya",
      "Sarkar, Anirban",
      "Gali, Sowrya",
      "N Balasubramanian, Vineeth"
    ]
  },
  {
    "id": "6abcc8f24321d1eb8c95855eab78ee95",
    "title": "Tactical Optimism and Pessimism for Deep Reinforcement Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6abcc8f24321d1eb8c95855eab78ee95-Paper.pdf",
    "abstract": "In recent years, deep off-policy actor-critic algorithms have become a dominant approach to reinforcement learning for continuous control. One of the primary drivers of this improved performance is the use of pessimistic value updates to address function approximation errors, which previously led to disappointing performance. However, a direct consequence of pessimism is reduced exploration, running counter to theoretical support for the efficacy of optimism in the face of uncertainty. So which approach is best? In this work, we show that the most effective degree of optimism can vary both across tasks and over the course of learning. Inspired by this insight, we introduce a novel deep actor-critic framework, Tactical Optimistic and Pessimistic (TOP) estimation, which switches between optimistic and pessimistic value learning online.  This is achieved by formulating the selection as a multi-arm bandit problem. We show in a series of continuous control tasks that TOP outperforms existing methods which rely on a fixed degree of optimism, setting a new state of the art in challenging pixel-based environments. Since our changes are simple to implement, we believe these insights can easily be incorporated into a multitude of off-policy algorithms. ",
    "authors": [
      "Moskovitz, Ted",
      "Parker-Holder, Jack",
      "Pacchiano, Aldo",
      "Arbel, Michael",
      "Jordan, Michael"
    ]
  },
  {
    "id": "6add07cf50424b14fdf649da87843d01",
    "title": "Towards Hyperparameter-free Policy Selection for Offline Reinforcement Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6add07cf50424b14fdf649da87843d01-Paper.pdf",
    "abstract": "How to select between policies and value functions produced by different training algorithms in offline reinforcement learning (RL)---which is crucial for hyperparameter tuning---is an important open question. Existing approaches based on off-policy evaluation (OPE) often require additional function approximation and hence hyperparameters, creating a chicken-and-egg situation.  In this paper, we design  hyperparameter-free algorithms for policy selection based on BVFT [XJ21], a recent theoretical advance in value-function selection, and demonstrate their effectiveness in discrete-action benchmarks such as Atari. To address performance degradation due to poor critics in continuous-action domains, we further combine BVFT with OPE to get the best of both worlds, and obtain a hyperparameter-tuning method for $Q$-function based OPE with theoretical guarantees as a side product. ",
    "authors": [
      "Zhang, Siyuan",
      "Jiang, Nan"
    ]
  },
  {
    "id": "6aed000af86a084f9cb0264161e29dd3",
    "title": "FjORD: Fair and Accurate Federated Learning under heterogeneous targets with Ordered Dropout",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6aed000af86a084f9cb0264161e29dd3-Paper.pdf",
    "abstract": "Federated Learning (FL) has been gaining significant traction across different ML tasks, ranging from vision to keyboard predictions. In large-scale deployments, client heterogeneity is a fact and constitutes a primary problem for fairness, training performance and accuracy. Although significant efforts have been made into tackling statistical data heterogeneity, the diversity in the processing capabilities and network bandwidth of clients, termed system heterogeneity, has remained largely unexplored. Current solutions either disregard a large portion of available devices or set a uniform limit on the model's capacity, restricted by the least capable participants.In this work, we introduce Ordered Dropout, a mechanism that achieves an ordered, nested representation of knowledge in Neural Networks and enables the extraction of lower footprint submodels without the need for retraining. We further show that for linear maps our Ordered Dropout is equivalent to SVD.  We employ this technique, along with a self-distillation methodology, in the realm of FL in a framework called FjORD. FjORD alleviates the problem of client system heterogeneity by tailoring the model width to the client's capabilities. Extensive evaluation on both CNNs and RNNs across diverse modalities shows that FjORD consistently leads to significant performance gains over state-of-the-art baselines while maintaining its nested structure.",
    "authors": [
      "Horv\u00e1th, Samuel",
      "Laskaridis, Stefanos",
      "Almeida, Mario",
      "Leontiadis, Ilias",
      "Venieris, Stylianos",
      "Lane, Nicholas"
    ]
  },
  {
    "id": "6b3c49bdba5be0d322334e30c459f8bd",
    "title": "Optimal Uniform OPE and Model-based Offline Reinforcement Learning in Time-Homogeneous, Reward-Free and Task-Agnostic Settings ",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6b3c49bdba5be0d322334e30c459f8bd-Paper.pdf",
    "abstract": "This work studies the statistical limits of uniform convergence for offline policy evaluation (OPE) problems with model-based methods (for episodic MDP) and provides a unified framework towards optimal learning for several well-motivated offline tasks. Uniform OPE $\\sup_\\Pi|Q^\\pi-\\hat{Q}^\\pi|<\\epsilon$ is a stronger measure than the point-wise OPE and ensures offline learning when $\\Pi$ contains all policies (the global class). In this paper, we establish an $\\Omega(H^2 S/d_m\\epsilon^2)$ lower bound (over model-based family) for the global uniform OPE and our main result establishes an upper bound of $\\tilde{O}(H^2/d_m\\epsilon^2)$ for the \\emph{local} uniform convergence that applies to all \\emph{near-empirically optimal} policies for the MDPs with \\emph{stationary} transition. Here $d_m$ is the minimal marginal state-action probability. Critically, the highlight in achieving the optimal rate $\\tilde{O}(H^2/d_m\\epsilon^2)$ is our design of \\emph{singleton absorbing MDP}, which is a new sharp analysis tool that works with the model-based approach. We generalize such a model-based framework to the new settings: offline task-agnostic and the offline reward-free with optimal complexity $\\tilde{O}(H^2\\log(K)/d_m\\epsilon^2)$ ($K$ is the number of tasks) and $\\tilde{O}(H^2S/d_m\\epsilon^2)$ respectively. These results provide a unified solution for simultaneously solving different offline RL problems.",
    "authors": [
      "Yin, Ming",
      "Wang, Yu-Xiang"
    ]
  },
  {
    "id": "6b5754d737784b51ec5075c0dc437bf0",
    "title": "MixSeq: Connecting Macroscopic Time Series Forecasting with Microscopic Time Series Data",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6b5754d737784b51ec5075c0dc437bf0-Paper.pdf",
    "abstract": "Time series forecasting is widely used in business intelligence, e.g., forecast stock market price, sales, and help the analysis of data trend. Most time series of interest are macroscopic time series that are aggregated from microscopic data. However, instead of directly modeling the macroscopic time series, rare literature studied the forecasting of macroscopic time series by leveraging data on the microscopic level. In this paper, we assume that the microscopic time series follow some unknown mixture probabilistic distributions. We theoretically show that as we identify the ground truth latent mixture components, the estimation of time series from each component could be improved because of lower variance, thus benefitting the estimation of macroscopic time series as well. Inspired by the power of Seq2seq and its variants on the modeling of time series data, we propose Mixture of Seq2seq (MixSeq), an end2end mixture model to cluster microscopic time series, where all the components come from a family of Seq2seq models parameterized by different parameters. Extensive experiments on both synthetic and real-world data show the superiority of our approach.",
    "authors": [
      "Zhu, Zhibo",
      "Liu, Ziqi",
      "Jin, Ge",
      "Zhang, Zhiqiang",
      "Chen, Lei",
      "Zhou, Jun",
      "Zhou, Jianyong"
    ]
  },
  {
    "id": "6ba3af5d7b2790e73f0de32e5c8c1798",
    "title": "Pareto Domain Adaptation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6ba3af5d7b2790e73f0de32e5c8c1798-Paper.pdf",
    "abstract": "Domain adaptation (DA) attempts to transfer the knowledge from a labeled source domain to an unlabeled target domain that follows different distribution from the source. To achieve this, DA methods include a source classification objective to extract the source knowledge and a domain alignment objective to diminish the domain shift, ensuring knowledge transfer. Typically, former DA methods adopt some weight hyper-parameters to linearly combine the training objectives to form an overall objective. However, the gradient directions of these objectives may conflict with each other due to domain shift. Under such circumstances, the linear optimization scheme might decrease the overall objective value at the expense of damaging one of the training objectives, leading to restricted solutions. In this paper, we rethink the optimization scheme for DA from a gradient-based perspective. We propose a Pareto Domain Adaptation (ParetoDA) approach to control the overall optimization direction, aiming to cooperatively optimize all training objectives. Specifically, to reach a desirable solution on the target domain, we design a surrogate loss mimicking target classification. To improve target-prediction accuracy to support the mimicking, we propose a target-prediction refining mechanism which exploits domain labels via Bayes\u2019 theorem. On the other hand, since prior knowledge of weighting schemes for objectives is often unavailable to guide optimization to approach the optimal solution on the target domain, we propose a dynamic preference mechanism to dynamically guide our cooperative optimization by the gradient of the surrogate loss on a held-out unlabeled target dataset. Our theoretical analyses show that the held-out data can guide but will not be over-fitted by the optimization. Extensive experiments on image classification and semantic segmentation benchmarks demonstrate the effectiveness of ParetoDA",
    "authors": [
      "lv, fangrui",
      "Liang, Jian",
      "Gong, Kaixiong",
      "Li, Shuang",
      "Liu, Chi Harold",
      "Li, Han",
      "Liu, Di",
      "Wang, Guoren"
    ]
  },
  {
    "id": "6bf733bb7f81e866306e9b5f012419cb",
    "title": "Divergence Frontiers for Generative Models: Sample Complexity, Quantization Effects, and Frontier Integrals",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6bf733bb7f81e866306e9b5f012419cb-Paper.pdf",
    "abstract": "The spectacular success of deep generative models calls for quantitative tools to measure their statistical performance. Divergence frontiers have recently been proposed as an evaluation framework for generative models, due to their ability to measure the quality-diversity trade-off inherent to deep generative modeling. We establish non-asymptotic bounds on the sample complexity of divergence frontiers. We also introduce frontier integrals which provide summary statistics of divergence frontiers. We show how smoothed estimators such as Good-Turing or Krichevsky-Trofimov can overcome the missing mass problem and lead to faster rates of convergence. We illustrate the theoretical results with numerical examples from natural language processing and computer vision.",
    "authors": [
      "Liu, Lang",
      "Pillutla, Krishna",
      "Welleck, Sean",
      "Oh, Sewoong",
      "Choi, Yejin",
      "Harchaoui, Zaid"
    ]
  },
  {
    "id": "6c19e0a6da12dc02239312f151072ddd",
    "title": "Consistency Regularization for Variational Auto-Encoders",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6c19e0a6da12dc02239312f151072ddd-Paper.pdf",
    "abstract": "Variational Auto-Encoders (VAEs) are a powerful approach to unsupervised learning. They enable scalable approximate posterior inference in latent-variable models using variational inference. A VAE posits a variational family parameterized by a deep neural network---called an encoder---that takes data as input. This encoder is shared across all the observations, which amortizes the cost of inference. However the encoder of a VAE has the undesirable property that it maps a given observation and a semantics-preserving transformation of it to different latent representations. This \"inconsistency\" of the encoder lowers the quality of the learned representations, especially for downstream tasks, and also negatively affects generalization. In this paper, we propose a regularization method to enforce consistency in VAEs. The idea is to minimize the Kullback-Leibler (KL) divergence between the variational distribution when conditioning on the observation and the variational distribution when conditioning on a random semantics-preserving transformation of this observation. This regularization is applicable to any VAE. In our experiments we apply it to four different VAE variants on several benchmark datasets and found it always improves the quality of the learned representations but also leads to better generalization. In particular, when applied to the Nouveau VAE (NVAE), our regularization method yields state-of-the-art performance on MNIST, CIFAR-10, and CELEBA. We also applied our method to 3D data and found it learns representations of superior quality as measured by accuracy on a downstream classification task. Finally, we show our method can even outperform the triplet loss, an advanced and popular contrastive learning-based method for representation learning.",
    "authors": [
      "Sinha, Samarth",
      "Dieng, Adji Bousso"
    ]
  },
  {
    "id": "6c2e49911b68d315555d5b3eb0dd45bf",
    "title": "Score-based Generative Neural Networks for Large-Scale Optimal Transport",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6c2e49911b68d315555d5b3eb0dd45bf-Paper.pdf",
    "abstract": "We consider the fundamental problem of sampling the optimal transport coupling between given source and target distributions. In certain cases, the optimal transport plan takes the form of a one-to-one mapping from the source support to the target support, but learning or even approximating such a map is computationally challenging for large and high-dimensional datasets due to the high cost of linear programming routines and an intrinsic curse of dimensionality. We study instead the Sinkhorn problem, a regularized form of optimal transport whose solutions are couplings between the source and the target distribution. We introduce a novel framework for learning the Sinkhorn coupling between two distributions in the form of a score-based generative model. Conditioned on source data, our procedure iterates Langevin Dynamics to sample target data according to the regularized optimal coupling. Key to this approach is a neural network parametrization of the Sinkhorn problem, and we prove convergence of gradient descent with respect to network parameters in this formulation. We demonstrate its empirical success on a variety of large scale optimal transport tasks. ",
    "authors": [
      "Daniels, Max",
      "Maunu, Tyler",
      "Hand, Paul"
    ]
  },
  {
    "id": "6c349155b122aa8ad5c877007e05f24f",
    "title": "Interactive Label Cleaning with Example-based Explanations",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6c349155b122aa8ad5c877007e05f24f-Paper.pdf",
    "abstract": "We tackle sequential learning under label noise in applications where a human supervisor can be queried to relabel suspicious examples. Existing approaches are flawed, in that they only relabel incoming examples that look \"suspicious\" to the model. As a consequence, those mislabeled examples that elude (or don't undergo) this cleaning step end up tainting the training data and the model with no further chance of being cleaned. We propose CINCER, a novel approach that cleans both new and past data by identifying \\emph{pairs of mutually incompatible examples}. Whenever it detects a suspicious example, CINCER identifies a counter-example in the training set that - according to the model - is maximally incompatible with the suspicious example, and asks the annotator to relabel either or both examples, resolving this possible inconsistency. The counter-examples are chosen to be maximally incompatible, so to serve as \\emph{explanations} of the model's suspicion, and highly influential, so to convey as much information as possible if relabeled. CINCER achieves this by leveraging an efficient and robust approximation of influence functions based on the Fisher information matrix (FIM). Our extensive empirical evaluation shows that clarifying the reasons behind the model's suspicions by cleaning the counter-examples helps in acquiring substantially better data and models, especially when paired with our FIM approximation.",
    "authors": [
      "Teso, Stefano",
      "Bontempelli, Andrea",
      "Giunchiglia, Fausto",
      "Passerini, Andrea"
    ]
  },
  {
    "id": "6c351da15b5e8a743a21ee96a86e25df",
    "title": "Gradient Descent on Two-layer Nets: Margin Maximization and Simplicity Bias",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6c351da15b5e8a743a21ee96a86e25df-Paper.pdf",
    "abstract": "The generalization mystery of overparametrized deep nets has motivated efforts to understand how gradient descent (GD) converges to low-loss solutions that generalize well. Real-life neural networks are initialized from small random values and trained with cross-entropy loss for classification (unlike the \"lazy\" or \"NTK\" regime of training where analysis was more successful), and a recent sequence of results (Lyu and Li, 2020; Chizat and Bach, 2020; Ji and Telgarsky, 2020) provide theoretical evidence that GD may converge to the \"max-margin\" solution with zero loss, which presumably generalizes well. However, the global optimality of margin is proved only in some settings where neural nets are infinitely or exponentially wide. The current paper is able to establish this global optimality for two-layer Leaky ReLU nets trained with gradient flow on linearly separable and symmetric data, regardless of the width. The analysis also gives some theoretical justification for recent empirical findings (Kalimeris et al., 2019) on the so-called simplicity bias of GD towards linear or other \"simple\" classes of solutions, especially early in training. On the pessimistic side, the paper suggests that such results are fragile. A simple data manipulation can make gradient flow converge to a linear classifier with suboptimal margin.",
    "authors": [
      "Lyu, Kaifeng",
      "Li, Zhiyuan",
      "Wang, Runzhe",
      "Arora, Sanjeev"
    ]
  },
  {
    "id": "6c524f9d5d7027454a783c841250ba71",
    "title": "Glance-and-Gaze Vision Transformer",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6c524f9d5d7027454a783c841250ba71-Paper.pdf",
    "abstract": "Recently, there emerges a series of vision Transformers, which show superior performance with a more compact model size than conventional convolutional neural networks, thanks to the strong ability of Transformers to model long-range dependencies. However, the advantages of vision Transformers also come with a price: Self-attention, the core part of Transformer, has a quadratic complexity to the input sequence length. This leads to a dramatic increase of computation and memory cost with the increase of sequence length, thus introducing difficulties when applying Transformers to the vision tasks that require dense predictions based on high-resolution feature maps.In this paper, we propose a new vision Transformer, named Glance-and-Gaze Transformer (GG-Transformer), to address the aforementioned issues. It is motivated by the Glance and Gaze behavior of human beings when recognizing objects in natural scenes, with the ability to efficiently model both long-range dependencies and local context. In GG-Transformer, the Glance and Gaze behavior is realized by two parallel branches: The Glance branch is achieved by performing self-attention on the adaptively-dilated partitions of the input, which leads to a linear complexity while still enjoying a global receptive field; The Gaze branch is implemented by a simple depth-wise convolutional layer, which compensates local image context to the features obtained by the Glance mechanism. We empirically demonstrate our method achieves consistently superior performance over previous state-of-the-art Transformers on various vision tasks and benchmarks.",
    "authors": [
      "Yu, Qihang",
      "Xia, Yingda",
      "Bai, Yutong",
      "Lu, Yongyi",
      "Yuille, Alan L.",
      "Shen, Wei"
    ]
  },
  {
    "id": "6c81c83c4bd0b58850495f603ab45a93",
    "title": "Stochastic $L^\\natural$-convex Function Minimization",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6c81c83c4bd0b58850495f603ab45a93-Paper.pdf",
    "abstract": "We study an extension of the stochastic submodular minimization problem, namely, the stochastic $L^\\natural$-convex minimization problem. We develop the first polynomial-time algorithms that return a near-optimal solution with high probability. We design a novel truncation operation to further reduce the computational complexity of the proposed algorithms. When applied to a stochastic submodular function, the computational complexity of the proposed algorithms is lower than that of the existing stochastic submodular minimization algorithms. In addition, we provide a strongly polynomial approximate algorithm. The algorithm execution also does not require any prior knowledge about the objective function except the $L^\\natural$-convexity. A lower bound on the computational complexity that is required to achieve a high probability error bound is also derived. Numerical experiments are implemented to demonstrate the efficiency of our theoretical findings.",
    "authors": [
      "Zhang, Haixiang",
      "Zheng, Zeyu",
      "Lavaei, Javad"
    ]
  },
  {
    "id": "6cb5da3513bd26085ee3fad631ebb37a",
    "title": "Self-Supervised GANs with Label Augmentation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6cb5da3513bd26085ee3fad631ebb37a-Paper.pdf",
    "abstract": "Recently, transformation-based self-supervised learning has been applied to generative adversarial networks (GANs) to mitigate catastrophic forgetting in the discriminator by introducing a stationary learning environment. However, the separate self-supervised tasks in existing self-supervised GANs cause a goal inconsistent with generative modeling due to the fact that their self-supervised classifiers are agnostic to the generator distribution. To address this problem, we propose a novel self-supervised GAN that unifies the GAN task with the self-supervised task by augmenting the GAN labels (real or fake) via self-supervision of data transformation. Specifically, the original discriminator and self-supervised classifier are unified into a label-augmented discriminator that predicts the augmented labels to be aware of both the generator distribution and the data distribution under every transformation, and then provide the discrepancy between them to optimize the generator. Theoretically, we prove that the optimal generator could converge to replicate the real data distribution. Empirically, we show that the proposed method significantly outperforms previous self-supervised and data augmentation GANs on both generative modeling and representation learning across benchmark datasets.",
    "authors": [
      "Hou, Liang",
      "Shen, Huawei",
      "Cao, Qi",
      "Cheng, Xueqi"
    ]
  },
  {
    "id": "6cd9313ed34ef58bad3fdd504355e72c",
    "title": "Shape As Points: A Differentiable Poisson Solver",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6cd9313ed34ef58bad3fdd504355e72c-Paper.pdf",
    "abstract": "In recent years, neural implicit representations gained popularity in 3D reconstruction due to their expressiveness and flexibility. However, the implicit nature of neural implicit representations results in slow inference times and requires careful initialization. In this paper, we revisit the classic yet ubiquitous point cloud representation and introduce a differentiable point-to-mesh layer using a differentiable formulation of Poisson Surface Reconstruction (PSR) which allows for a GPU-accelerated fast solution of the indicator function given an oriented point cloud. The differentiable PSR layer allows us to efficiently and differentiably bridge the explicit 3D point representation with the 3D mesh via the implicit indicator field, enabling end-to-end optimization of surface reconstruction metrics such as Chamfer distance. This duality between points and meshes hence allows us to represent shapes as oriented point clouds, which are explicit, lightweight and expressive. Compared to neural implicit representations, our Shape-As-Points (SAP) model is more interpretable, lightweight, and accelerates inference time by one order of magnitude. Compared to other explicit representations such as points, patches, and meshes, SAP produces topology-agnostic, watertight manifold surfaces. We demonstrate the effectiveness of SAP on the task of surface reconstruction from unoriented point clouds and learning-based reconstruction.",
    "authors": [
      "Peng, Songyou",
      "Jiang, Chiyu",
      "Liao, Yiyi",
      "Niemeyer, Michael",
      "Pollefeys, Marc",
      "Geiger, Andreas"
    ]
  },
  {
    "id": "6cdd60ea0045eb7a6ec44c54d29ed402",
    "title": "Outcome-Driven Reinforcement Learning via Variational Inference",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6cdd60ea0045eb7a6ec44c54d29ed402-Paper.pdf",
    "abstract": "While reinforcement learning algorithms provide automated acquisition of optimal policies, practical application of such methods requires a number of design decisions, such as manually designing reward functions that not only define the task, but also provide sufficient shaping to accomplish it. In this paper, we view reinforcement learning as inferring policies that achieve desired outcomes, rather than as a problem of maximizing rewards. To solve this inference problem, we establish a novel variational inference formulation that allows us to derive a well-shaped reward function which can be learned directly from environment interactions. From the corresponding variational objective, we also derive a new probabilistic Bellman backup operator and use it to develop an off-policy algorithm to solve goal-directed tasks. We empirically demonstrate that this method eliminates the need to hand-craft reward functions for a suite of diverse manipulation and locomotion tasks and leads to effective goal-directed behaviors.",
    "authors": [
      "Rudner, Tim G. J.",
      "Pong, Vitchyr",
      "McAllister, Rowan",
      "Gal, Yarin",
      "Levine, Sergey"
    ]
  },
  {
    "id": "6ce8d8f3b038f737cefcdafcf3752452",
    "title": "Drawing Robust Scratch Tickets: Subnetworks with Inborn Robustness Are Found within Randomly Initialized Networks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6ce8d8f3b038f737cefcdafcf3752452-Paper.pdf",
    "abstract": "Deep Neural Networks (DNNs) are known to be vulnerable to adversarial attacks, i.e., an imperceptible perturbation to the input can mislead DNNs trained on clean images into making erroneous predictions. To tackle this, adversarial training is currently the most effective defense method, by augmenting the training set with adversarial samples generated on the fly. \\textbf{Interestingly, we discover for the first time that there exist subnetworks with inborn robustness, matching or surpassing the robust accuracy of the adversarially trained networks with comparable model sizes, within randomly initialized networks without any model training}, indicating that adversarial training on model weights is not indispensable towards adversarial robustness. We name such subnetworks Robust Scratch Tickets (RSTs), which are also by nature efficient. Distinct from the popular lottery ticket hypothesis, neither the original dense networks nor the identified RSTs need to be trained. To validate and understand this fascinating finding, we further conduct extensive experiments to study the existence and properties of RSTs under different models, datasets, sparsity patterns, and attacks, drawing insights regarding the relationship between DNNs\u2019 robustness and their initialization/overparameterization. Furthermore, we identify the poor adversarial transferability between RSTs of different sparsity ratios drawn from the same randomly initialized dense network, and propose a Random RST Switch (R2S) technique, which randomly switches between different RSTs, as a novel defense method built on top of RSTs. We believe our findings about RSTs have opened up a new perspective to study model robustness and extend the lottery ticket hypothesis.",
    "authors": [
      "Fu, Yonggan",
      "Yu, Qixuan",
      "Zhang, Yang",
      "Wu, Shang",
      "Ouyang, Xu",
      "Cox, David",
      "Lin, Yingyan"
    ]
  },
  {
    "id": "6cfe0e6127fa25df2a0ef2ae1067d915",
    "title": "Rectifying the Shortcut Learning of Background for Few-Shot Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6cfe0e6127fa25df2a0ef2ae1067d915-Paper.pdf",
    "abstract": "The category gap between training and evaluation has been characterised as one of the main obstacles to the success of Few-Shot Learning (FSL). In this paper, we for the first time empirically identify image background, common in realistic images, as a shortcut knowledge helpful for in-class classification but ungeneralizable beyond training categories in FSL. A novel framework, COSOC, is designed to tackle this problem by extracting foreground objects in images at both training and evaluation without any extra supervision. Extensive experiments carried on inductive FSL tasks demonstrate the effectiveness of our approaches. ",
    "authors": [
      "Luo, Xu",
      "Wei, Longhui",
      "Wen, Liangjian",
      "Yang, Jinrong",
      "Xie, Lingxi",
      "Xu, Zenglin",
      "Tian, Qi"
    ]
  },
  {
    "id": "6d0c932802f6953f70eb20931645fa40",
    "title": "SEAL: Self-supervised Embodied Active Learning using Exploration and 3D Consistency",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6d0c932802f6953f70eb20931645fa40-Paper.pdf",
    "abstract": "In this paper, we explore how we can build upon the data and models of Internet images and use them to adapt to robot vision without requiring any extra labels. We present a framework called Self-supervised Embodied Active Learning (SEAL). It utilizes perception models trained on internet images to learn an active exploration policy. The observations gathered by this exploration policy are labelled using 3D consistency and used to improve the perception model. We build and utilize 3D semantic maps to learn both action and perception in a completely self-supervised manner. The semantic map is used to compute an intrinsic motivation reward for training the exploration policy and for labelling the agent observations using spatio-temporal 3D consistency and label propagation. We demonstrate that the SEAL framework can be used to close the action-perception loop: it improves object detection and instance segmentation performance of a pretrained perception model by just moving around in training environments and the improved perception model can be used to improve Object Goal Navigation.",
    "authors": [
      "Chaplot, Devendra Singh",
      "Dalal, Murtaza",
      "Gupta, Saurabh",
      "Malik, Jitendra",
      "Salakhutdinov, Russ R."
    ]
  },
  {
    "id": "6d65b5ac1f4ec80b9a7309311f4f9b13",
    "title": "Sifting through the noise: Universal first-order methods for stochastic variational inequalities",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6d65b5ac1f4ec80b9a7309311f4f9b13-Paper.pdf",
    "abstract": "We examine a flexible algorithmic framework for solving monotone variational inequalities in the presence of randomness and uncertainty. The proposed template encompasses a wide range of popular first-order methods, including dual averaging, dual extrapolation and optimistic gradient algorithms \u2013 both adaptive and non-adaptive. Our first result is that the algorithm achieves the optimal rates of convergence for cocoercive problems when the profile of the randomness is known to the optimizer: $\\mathcal{O}(1/\\sqrt{T})$ for absolute noise profiles, and $\\mathcal{O}(1/T)$ for relative ones. Subsequently, we drop all prior knowledge requirements (the absolute/relative variance of the randomness affecting the problem, the operator's cocoercivity constant, etc.), and we analyze an adaptive instance of the method that gracefully interpolates between the above rates \u2013 i.e. it achieves $\\mathcal{O}(1/\\sqrt{T})$ and $\\mathcal{O}(1/T)$ in the absolute and relative cases, respectively. To our knowledge, this is the first universality result of its kind in the literature and, somewhat surprisingly, it shows that an extra-gradient proxy step is not required to achieve optimal rates.",
    "authors": [
      "Antonakopoulos, Kimon",
      "Pethick, Thomas",
      "Kavis, Ali",
      "Mertikopoulos, Panayotis",
      "Cevher, Volkan"
    ]
  },
  {
    "id": "6d7d394c9d0c886e9247542e06ebb705",
    "title": "Accommodating Picky Customers: Regret Bound and Exploration Complexity for Multi-Objective Reinforcement Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6d7d394c9d0c886e9247542e06ebb705-Paper.pdf",
    "abstract": "In this paper we consider multi-objective reinforcement learning where the objectives are balanced using preferences. In practice, the preferences are often given in an adversarial manner, e.g., customers can be picky in many applications. We formalize this problem as an episodic learning problem on a Markov decision process, where transitions are unknown and a reward function is the inner product of a preference vector with pre-specified multi-objective reward functions. We consider two settings. In the online setting, the agent receives a (adversarial) preference every episode and proposes policies to interact with the environment. We provide a model-based algorithm that achieves a nearly minimax optimal regret bound $\\widetilde{\\mathcal{O}}\\bigl(\\sqrt{\\min\\{d,S\\}\\cdot H^2 SAK}\\bigr)$, where $d$ is the number of objectives, $S$ is the number of states, $A$ is the number of actions, $H$ is the length of the horizon, and $K$ is the number of episodes. Furthermore, we consider preference-free exploration, i.e., the agent first interacts with the environment without specifying any preference and then is able to accommodate arbitrary preference vector up to $\\epsilon$ error. Our proposed algorithm is provably efficient with a nearly optimal trajectory complexity $\\widetilde{\\mathcal{O}}\\bigl({\\min\\{d,S\\}\\cdot H^3 SA}/{\\epsilon^2}\\bigr)$. This result partly resolves an open problem raised by \\citet{jin2020reward}.",
    "authors": [
      "Wu, Jingfeng",
      "Braverman, Vladimir",
      "Yang, Lin"
    ]
  },
  {
    "id": "6d96718a701f5bfba283bbdc71dfa5c4",
    "title": "Exact Privacy Guarantees for Markov Chain Implementations of the Exponential Mechanism with Artificial Atoms",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6d96718a701f5bfba283bbdc71dfa5c4-Paper.pdf",
    "abstract": "Implementations of the exponential mechanism in differential privacy often require sampling from intractable distributions. When approximate procedures like Markov chain Monte Carlo (MCMC) are used, the end result incurs costs to both privacy and accuracy. Existing work has examined these effects asymptotically, but implementable finite sample results are needed in practice so that users can specify privacy budgets in advance and implement samplers with exact privacy guarantees. In this paper, we use tools from ergodic theory and perfect simulation to design exact finite runtime sampling algorithms for the exponential mechanism by introducing an intermediate modified target distribution using artificial atoms. We propose an additional modification of this sampling algorithm that maintains its $\\epsilon$-DP guarantee and has improved runtime at the cost of some utility. We then compare these methods in scenarios where we can explicitly calculate a $\\delta$ cost (as in $(\\epsilon, \\delta)$-DP) incurred when using standard MCMC techniques. Much as there is a well known trade-off between privacy and utility, we demonstrate that there is also a trade-off between privacy guarantees and runtime. ",
    "authors": [
      "Seeman, Jeremy",
      "Reimherr, Matthew",
      "Slavkovi\u0107, Aleksandra"
    ]
  },
  {
    "id": "6d9cb7de5e8ac30bd5e8734bc96a35c1",
    "title": "The Emergence of Objectness: Learning Zero-shot Segmentation from Videos",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6d9cb7de5e8ac30bd5e8734bc96a35c1-Paper.pdf",
    "abstract": "Humans can easily detect and segment moving objects simply by observing how they move, even without knowledge of object semantics. Inspired by this, we develop a zero-shot unsupervised approach for learning object segmentations. The model comprises two visual pathways: an appearance pathway that segments individual RGB images into coherent object regions, and a motion pathway that predicts the flow vector for each region between consecutive video frames. The two pathways jointly reconstruct a new representation called segment flow. This decoupled representation of appearance and motion is trained in a self-supervised manner to reconstruct one frame from another.When pretrained on an unlabeled video corpus, the model can be useful for a variety of applications, including 1) primary object segmentation from a single image in a zero-shot fashion; 2) moving object segmentation from a video with unsupervised test-time adaptation; 3) image semantic segmentation by supervised fine-tuning on a labeled image dataset. We demonstrate encouraging experimental results on all of these tasks using  pretrained models.",
    "authors": [
      "Liu, Runtao",
      "Wu, Zhirong",
      "Yu, Stella",
      "Lin, Stephen"
    ]
  },
  {
    "id": "6da9003b743b65f4c0ccd295cc484e57",
    "title": "Direct Multi-view Multi-person 3D  Pose Estimation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6da9003b743b65f4c0ccd295cc484e57-Paper.pdf",
    "abstract": "We present Multi-view Pose transformer (MvP) for estimating multi-person 3D poses from multi-view images. Instead of estimating 3D joint locations from costly volumetric representation or reconstructing the per-person 3D pose from multiple detected 2D poses as in previous methods, MvP directly regresses the multi-person 3D  poses  in  a  clean  and  efficient  way,  without  relying  on  intermediate  tasks. Specifically, MvP represents skeleton joints as learnable query embeddings and let them progressively attend to and reason over the multi-view information from the input images to directly regress the actual 3D joint locations. To improve the accuracy of such a simple pipeline, MvP presents a hierarchical scheme to concisely represent query embeddings of multi-person skeleton joints and introduces an input-dependent query adaptation approach. Further, MvP designs a novel geometrically guided attention mechanism, called projective attention, to more precisely fuse the cross-view information for each joint. MvP also introduces a RayConv operation to integrate the view-dependent camera geometry into the feature representations for augmenting the projective attention.  We show experimentally that our MvP model outperforms the state-of-the-art methods on several benchmarks while being much more efficient. Notably, it achieves 92.3% AP25 on the challenging Panoptic dataset, improving upon the previous best approach [35] by 9.8%. MvP is general and also extendable to recovering human mesh represented by the SMPL model, thus useful for modeling multi-person body shapes. Code and models are available at https://github.com/sail-sg/mvp.",
    "authors": [
      "wang, tao",
      "Zhang, Jianfeng",
      "Cai, Yujun",
      "Yan, Shuicheng",
      "Feng, Jiashi"
    ]
  },
  {
    "id": "6dbbe6abe5f14af882ff977fc3f35501",
    "title": "MST: Masked Self-Supervised Transformer for Visual Representation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6dbbe6abe5f14af882ff977fc3f35501-Paper.pdf",
    "abstract": "Transformer has been widely used for self-supervised pre-training in Natural Language Processing (NLP) and achieved great success. However, it has not been fully explored in visual self-supervised learning. Meanwhile, previous methods only consider the high-level feature and learning representation from a global perspective, which may fail to transfer to the downstream dense prediction tasks focusing on local features. In this paper, we present a novel Masked Self-supervised Transformer approach named MST, which can explicitly capture the local context of an image while preserving the global semantic information. Specifically, inspired by the Masked Language Modeling (MLM) in NLP, we propose a masked token strategy based on the multi-head self-attention map, which dynamically masks some tokens of local patches without damaging the crucial structure for self-supervised learning. More importantly, the masked tokens together with the remaining tokens are further recovered by a global image decoder, which preserves the spatial information of the image and is more friendly to the downstream dense prediction tasks. The experiments on multiple datasets demonstrate the effectiveness and generality of the proposed method. For instance, MST achieves Top-1 accuracy of 76.9% with DeiT-S only using 300-epoch pre-training by linear evaluation, which outperforms supervised methods with the same epoch by 0.4% and its comparable variant DINO by 1.0%. For dense prediction tasks, MST also achieves 42.7% mAP on MS COCO object detection and 74.04% mIoU on Cityscapes segmentation only with 100-epoch pre-training.",
    "authors": [
      "Li, Zhaowen",
      "Chen, Zhiyang",
      "Yang, Fan",
      "Li, Wei",
      "Zhu, Yousong",
      "Zhao, Chaoyang",
      "Deng, Rui",
      "Wu, Liwei",
      "Zhao, Rui",
      "Tang, Ming",
      "Wang, Jinqiao"
    ]
  },
  {
    "id": "6de0f2761a44ff1e2ca60131058d8297",
    "title": "Exploiting Opponents Under Utility Constraints in Sequential Games",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6de0f2761a44ff1e2ca60131058d8297-Paper.pdf",
    "abstract": "Recently, game-playing agents based on AI techniques have demonstrated super-human performance in several sequential games, such as chess, Go, and poker. Surprisingly, the multi-agent learning techniques that allowed to reach these achievements do not take into account the actual behavior of the human player, potentially leading to an impressive gap in performances. In this paper, we address the problem of designing artificial agents that learn how to effectively exploit unknown human opponents while playing repeatedly against them in an online fashion. We study the case in which the agent's strategy during each repetition of the game is subject to constraints ensuring that the human's expected utility is within some lower and upper thresholds. Our framework encompasses several real-world problems, such as human engagement in repeated game playing and human education by means of serious games. As a first result, we formalize a set of linear inequalities encoding the conditions that the agent's strategy must satisfy at each iteration in order to do not violate the given bounds for the human's expected utility. Then, we use such formulation in an upper confidence bound algorithm, and we prove that the resulting procedure suffers from sublinear regret and guarantees that the constraints are satisfied with high probability at each iteration. Finally, we empirically evaluate the convergence of our algorithm on standard testbeds of sequential games.",
    "authors": [
      "Bernasconi-de-Luca, Martino",
      "Cacciamani, Federico",
      "Fioravanti, Simone",
      "Gatti, Nicola",
      "Marchesi, Alberto",
      "Trov\u00f2, Francesco "
    ]
  },
  {
    "id": "6e01383fd96a17ae51cc3e15447e7533",
    "title": "A Compositional Atlas of Tractable Circuit Operations for Probabilistic Inference",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6e01383fd96a17ae51cc3e15447e7533-Paper.pdf",
    "abstract": "Circuit representations are becoming the lingua franca to express and reason about tractable generative and discriminative models. In this paper, we show how complex inference scenarios for these models that commonly arise in machine learning---from  computing the expectations of decision tree ensembles to information-theoretic divergences of sum-product networks---can be represented in terms of tractable modular operations over circuits. Specifically, we characterize the tractability of simple transformations---sums, products, quotients, powers, logarithms, and exponentials---in terms of sufficient structural constraints of the circuits they operate on, and present novel hardness results for the cases in which these properties are not satisfied. Building on these operations, we derive a unified framework for reasoning about tractable models that generalizes several results in the literature and opens up novel tractable inference scenarios.",
    "authors": [
      "Vergari, Antonio",
      "Choi, YooJung",
      "Liu, Anji",
      "Teso, Stefano",
      "Van den Broeck, Guy"
    ]
  },
  {
    "id": "6e0cf80a83327822a972bcde3c1d9740",
    "title": "Demystifying and Generalizing BinaryConnect",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6e0cf80a83327822a972bcde3c1d9740-Paper.pdf",
    "abstract": "BinaryConnect (BC) and its many variations have become the de facto standard for neural network quantization. However, our understanding of the inner workings of BC is still quite limited. We attempt to close this gap in four different aspects: (a) we show that existing quantization algorithms, including post-training quantization, are surprisingly similar to each other; (b) we argue for proximal maps as a natural family of quantizers that is both easy to design and analyze; (c) we refine the observation that BC is a special case of dual averaging, which itself is a special case of the generalized conditional gradient algorithm; (d) consequently, we propose ProxConnect (PC) as a generalization of BC and we prove its convergence properties by exploiting the established connections. We conduct experiments on CIFAR-10 and ImageNet, and verify that PC achieves competitive performance.",
    "authors": [
      "Dockhorn, Tim",
      "Yu, Yaoliang",
      "Sari, Eyy\u00fcb",
      "Zolnouri, Mahdi",
      "Partovi Nia, Vahid"
    ]
  },
  {
    "id": "6e16656a6ee1de7232164767ccfa7920",
    "title": "CARMS: Categorical-Antithetic-REINFORCE Multi-Sample Gradient Estimator",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6e16656a6ee1de7232164767ccfa7920-Paper.pdf",
    "abstract": "Accurately backpropagating the gradient through categorical variables is a challenging task that arises in various domains, such as training discrete latent variable models. To this end, we propose CARMS, an unbiased estimator for categorical random variables based on multiple mutually negatively correlated (jointly antithetic) samples. CARMS combines REINFORCE with copula based sampling to avoid duplicate samples and reduce its variance, while keeping the estimator unbiased using importance sampling. It generalizes both the ARMS antithetic estimator for binary variables, which is CARMS for two categories, as well as LOORF/VarGrad, the leave-one-out REINFORCE estimator, which is CARMS with independent samples.  We evaluate CARMS on several benchmark datasets on a generative modeling task, as well as a structured output prediction task, and find it to outperform competing methods including a strong self-control baseline. The code is publicly available.",
    "authors": [
      "Dimitriev, Alek",
      "Zhou, Mingyuan"
    ]
  },
  {
    "id": "6e2713a6efee97bacb63e52c54f0ada0",
    "title": "Learning to Learn Dense Gaussian Processes for Few-Shot Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6e2713a6efee97bacb63e52c54f0ada0-Paper.pdf",
    "abstract": "Gaussian processes with deep neural networks demonstrate to be a strong learner for few-shot learning since they combine the strength of deep learning and kernels while being able to well capture uncertainty. However, it remains an open problem to leverage the shared knowledge provided by related tasks. In this paper, we propose to learn Gaussian processes with dense inducing variables by meta-learning for few-shot learning. In contrast to sparse Gaussian processes, we define a set of dense inducing variables to be of a much larger size than the support set in each task, which collects prior knowledge from experienced tasks. The dense inducing variables specify a shared Gaussian process prior over prediction functions of all tasks, which are learned in a variational inference framework and offer a strong inductive bias for learning new tasks. To achieve task-specific prediction functions, we propose to adapt the inducing variables to each task by efficient gradient descent. We conduct extensive experiments on common benchmark datasets for a variety of few-shot learning tasks. Our dense Gaussian processes present significant improvements over vanilla Gaussian processes and comparable or even better performance with state-of-the-art methods. ",
    "authors": [
      "Wang, Ze",
      "Miao, Zichen",
      "Zhen, Xiantong",
      "Qiu, Qiang"
    ]
  },
  {
    "id": "6e28943943dbed3c7f82fc05f269947a",
    "title": "Stochastic Solutions for Linear Inverse Problems using the Prior Implicit in a Denoiser",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6e28943943dbed3c7f82fc05f269947a-Paper.pdf",
    "abstract": "Deep neural networks have provided state-of-the-art solutions for problems such as image denoising, which implicitly rely on a prior probability model of natural images. Two recent lines of work \u2013 Denoising Score Matching and Plug-and-Play \u2013 propose methodologies for drawing samples from this implicit prior and using it to solve inverse problems, respectively. Here, we develop a parsimonious and robust generalization of these ideas. We rely on a classic statistical result that shows the least-squares solution for removing additive Gaussian noise can be written directly in terms of the gradient of the log of the noisy signal density. We use this to derive a stochastic coarse-to-fine gradient ascent procedure for drawing high-probability samples from the implicit prior embedded within a CNN trained to perform blind denoising. A generalization of this algorithm to constrained sampling provides a method for using the implicit prior to solve any deterministic linear inverse problem, with no additional training, thus extending the power of supervised learning for denoising to a much broader set of problems. The algorithm relies on minimal assumptions and exhibits robust convergence over a wide range of parameter choices. To demonstrate the generality of our method, we use it to obtain state-of-the-art levels of unsupervised performance for deblurring, super-resolution, and compressive sensing.",
    "authors": [
      "Kadkhodaie, Zahra",
      "Simoncelli, Eero"
    ]
  },
  {
    "id": "6e3197aae95c2ff8fcab35cb730f6a86",
    "title": "Towards Stable and Robust AdderNets",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6e3197aae95c2ff8fcab35cb730f6a86-Paper.pdf",
    "abstract": "Adder neural network (AdderNet) replaces the original convolutions with massive multiplications by cheap additions while achieving comparable performance thus yields a series of energy-efficient neural networks. Compared with convolutional neural networks (CNNs), the training of AdderNets is much more sophisticated including several techniques for adjusting gradient and batch normalization. In addition, variances of both weights and activations in resulting adder networks are very enormous which limits its performance and the potential for applying to other tasks. To enhance the stability and robustness of AdderNets, we first thoroughly analyze the variance estimation of weight parameters and output features of an arbitrary adder layer. Then, we develop a weight normalization scheme for adaptively optimizing the weight distribution of AdderNets during the training procedure, which can reduce the perturbation on running mean and variance in batch normalization layers. Meanwhile, the proposed weight normalization can also be utilized to enhance the adversarial robustness of resulting networks. Experiments conducted on several benchmarks demonstrate the superiority of the proposed approach for generating AdderNets with higher performance.",
    "authors": [
      "Dong, Minjing",
      "Wang, Yunhe",
      "Chen, Xinghao",
      "Xu, Chang"
    ]
  },
  {
    "id": "6e67691b60ed3e4a55935261314dd534",
    "title": "Representing Long-Range Context for Graph Neural Networks with Global Attention",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6e67691b60ed3e4a55935261314dd534-Paper.pdf",
    "abstract": "Graph neural networks are powerful architectures for structured datasets. However, current methods struggle to represent long-range dependencies. Scaling the depth or width of GNNs is insufficient to broaden receptive fields as larger GNNs encounter optimization instabilities such as vanishing gradients and representation oversmoothing, while pooling-based approaches have yet to become as universally useful as in computer vision. In this work, we propose the use of Transformer-based self-attention to learn long-range pairwise relationships, with a novel \u201creadout\u201d mechanism to obtain a global graph embedding. Inspired by recent computer vision results that find position-invariant attention performant in learning long-range relationships, our method, which we call GraphTrans, applies a permutation-invariant Transformer module after a standard GNN module. This simple architecture leads to state-of-the-art results on several graph classification tasks, outperforming methods that explicitly encode graph structure. Our results suggest that purely-learning-based approaches without graph structure may be suitable for learning high-level, long-range relationships on graphs. Code for GraphTrans is available at https://github.com/ucbrise/graphtrans.",
    "authors": [
      "Wu, Zhanghao",
      "Jain, Paras",
      "Wright, Matthew",
      "Mirhoseini, Azalia",
      "Gonzalez, Joseph E.",
      "Stoica, Ion"
    ]
  },
  {
    "id": "6e79ed05baec2754e25b4eac73a332d2",
    "title": "Beyond Bandit Feedback in Online Multiclass Classification",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6e79ed05baec2754e25b4eac73a332d2-Paper.pdf",
    "abstract": "We study the problem of online multiclass classification in a setting where the learner's feedback is determined by an arbitrary directed graph. While including bandit feedback as a special case, feedback graphs allow a much richer set of applications, including filtering and label efficient classification.We introduce \\textproc{Gappletron}, the first online multiclass algorithm that works with arbitrary feedback graphs. For this new algorithm,we prove surrogate regret bounds that hold, both in expectation and with high probability, for a large class of surrogate losses. Our bounds are of order $B\\sqrt{\\rho KT}$, where $B$ is the diameter of the prediction space, $K$ is the number of classes, $T$ is the time horizon, and $\\rho$ is the domination number (a graph-theoretic parameter affecting the amount of exploration). In the full information case, we show that \\textproc{Gappletron} achieves a constant surrogate regret of order $B^2K$. We also prove a general lower bound of order $\\max\\big\\{B^2K,\\sqrt{T}\\big\\}$ showing that our upper bounds are not significantly improvable. Experiments on synthetic data show that for various feedback graphs our algorithm is competitive against known baselines.",
    "authors": [
      "van der Hoeven, Dirk",
      "Fusco, Federico",
      "Cesa-Bianchi, Nicol\u00f2"
    ]
  },
  {
    "id": "6e7d2da6d3953058db75714ac400b584",
    "title": "Learning Student-Friendly Teacher Networks for Knowledge Distillation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6e7d2da6d3953058db75714ac400b584-Paper.pdf",
    "abstract": "We propose a novel knowledge distillation approach to facilitate the transfer of dark knowledge from a teacher to a student. Contrary to most of the existing methods that rely on effective training of student models given pretrained teachers, we aim to learn the teacher models that are friendly to students and, consequently, more appropriate for knowledge transfer. In other words, at the time of optimizing a teacher model, the proposed algorithm learns the student branches jointly to obtain student-friendly representations. Since the main goal of our approach lies in training teacher models and the subsequent knowledge distillation procedure is straightforward, most of the existing knowledge distillation methods can adopt this technique to improve the performance of diverse student models in terms of accuracy and convergence speed. The proposed algorithm demonstrates outstanding accuracy in several well-known knowledge distillation techniques with various combinations of teacher and student models even in the case that their architectures are heterogeneous and there is no prior knowledge about student models at the time of training teacher networks",
    "authors": [
      "Park, Dae Young",
      "Cha, Moon-Hyun",
      "jeong, changwook",
      "Kim, Daesin",
      "Han, Bohyung"
    ]
  },
  {
    "id": "6e7d5d259be7bf56ed79029c4e621f44",
    "title": "Implicit Transformer Network for Screen Content Image Continuous Super-Resolution",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6e7d5d259be7bf56ed79029c4e621f44-Paper.pdf",
    "abstract": "Nowadays, there is an explosive growth of screen contents due to the wide application of screen sharing, remote cooperation, and online education. To match the limited terminal bandwidth,  high-resolution (HR) screen contents may be downsampled and compressed.  At the receiver side, the super-resolution (SR)of low-resolution (LR) screen content images (SCIs) is highly demanded by the HR display or by the users to zoom in for detail observation.  However, image SR methods mostly designed for natural images do not generalize well for SCIs due to the very different image characteristics as well as the requirement of SCI browsing at arbitrary scales. To this end, we propose a novel Implicit Transformer Super-Resolution Network (ITSRN) for SCISR. For high-quality continuous SR at arbitrary ratios, pixel values at query coordinates are inferred from image features at key coordinates by the proposed implicit transformer and an implicit position encoding scheme is proposed to aggregate similar neighboring pixel values to the query one. We construct benchmark SCI1K and SCI1K-compression datasets withLR and HR SCI pairs.  Extensive experiments show that the proposed ITSRN significantly outperforms several competitive continuous and discrete SR methods for both compressed and uncompressed SCIs.",
    "authors": [
      "Yang, Jingyu",
      "Shen, Sheng",
      "Yue, Huanjing",
      "Li, Kun"
    ]
  },
  {
    "id": "6e8404c3b93a9527c8db241a1846599a",
    "title": "Channel Permutations for N:M Sparsity",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6e8404c3b93a9527c8db241a1846599a-Paper.pdf",
    "abstract": "We introduce channel permutations as a method to maximize the accuracy of N:M sparse networks. N:M sparsity requires N out of M consecutive elements to be zero and has been shown to maintain accuracy for many models and tasks with a simple prune and fine-tune workflow. By permuting weight matrices along their channel dimension and adjusting the surrounding layers appropriately, we demonstrate accuracy recovery for even small, parameter-efficient networks, without affecting inference run-time. We also present both a quality metric to simplify judging permutations as well as efficient methods to search for high-quality permutations, including two optimizations to escape local minima. Finally, we share an ablation study to show the importance of each part of our search algorithm, experimental results showing correlation between our quality metric and final network accuracy, improved sparse network accuracy using our techniques with insignificant overhead to training time, and the transformation of unstructured to structured sparse workloads. Code to use these techniques when generating a 2:4 sparse network is available at https://github.com/NVIDIA/apex/tree/master/apex/contrib/sparsity.",
    "authors": [
      "Pool, Jeff",
      "Yu, Chong"
    ]
  },
  {
    "id": "6f0442558302a6ededff195daf67f79b",
    "title": "Curriculum Learning for Vision-and-Language Navigation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6f0442558302a6ededff195daf67f79b-Paper.pdf",
    "abstract": "Vision-and-Language Navigation (VLN) is a task where an agent navigates in an embodied indoor environment under human instructions. Previous works ignore the distribution of sample difficulty and we argue that this potentially degrade their agent performance. To tackle this issue, we propose a novel curriculum- based training paradigm for VLN tasks that can balance human prior knowledge and agent learning progress about training samples. We develop the principle of curriculum design and re-arrange the benchmark Room-to-Room (R2R) dataset to make it suitable for curriculum training. Experiments show that our method is model-agnostic and can significantly improve the performance, the generalizability, and the training efficiency of current state-of-the-art navigation agents without increasing model complexity.",
    "authors": [
      "Zhang, Jiwen",
      "wei, zhongyu",
      "Fan, Jianqing",
      "Peng, Jiajie"
    ]
  },
  {
    "id": "6f221fcb5c504fe96789df252123770b",
    "title": "Better Algorithms for Individually Fair $k$-Clustering",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6f221fcb5c504fe96789df252123770b-Paper.pdf",
    "abstract": "We study data clustering problems with $\\ell_p$-norm objectives (e.g. \\textsc{$k$-Median} and \\textsc{$k$-Means}) in the context of individual fairness. The dataset consists of $n$ points, and we want to find $k$ centers such that (a) the objective is minimized, while (b) respecting the individual fairness constraint that every point $v$ has a center within a distance at most $r(v)$, where $r(v)$ is $v$'s distance to its $(n/k)$th nearest point. Jung, Kannan, and Lutz [FORC 2020] introduced this concept and designed a clustering algorithm with provable (approximate) fairness and objective guarantees for the $\\ell_\\infty$ or \\textsc{$k$-Center} objective.  Mahabadi and Vakilian [ICML 2020] revisited this problem to give a local-search algorithm for all $\\ell_p$-norms. Empirically, their algorithms outperform Jung et. al.'s by a large margin in terms of cost (for \\textsc{$k$-Median} and \\textsc{$k$-Means}), but they incur a reasonable loss in fairness. In this paper, our main contribution is to use Linear Programming (LP) techniques to obtain better algorithms for this problem, both in theory and in practice. We prove that by modifying known LP rounding techniques, one gets a worst-case guarantee on the objective which is much better than in MV20, and empirically, this objective is extremely close to the optimal.  Furthermore, our theoretical fairness guarantees are comparable with MV20 in theory, and empirically, we obtain noticeably fairer solutions.Although solving the LP {\\em exactly} might be prohibitive, we demonstrate that in practice, a simple sparsification technique drastically improves the run-time of our algorithm.",
    "authors": [
      "Negahbani, Maryam",
      "Chakrabarty, Deeparnab"
    ]
  },
  {
    "id": "6f2688a5fce7d48c8d19762b88c32c3b",
    "title": "Video Instance Segmentation using Inter-Frame Communication Transformers",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6f2688a5fce7d48c8d19762b88c32c3b-Paper.pdf",
    "abstract": "We propose a novel end-to-end solution for video instance segmentation (VIS) based on transformers. Recently, the per-clip pipeline shows superior performance over per-frame methods leveraging richer information from multiple frames. However, previous per-clip models require heavy computation and memory usage to achieve frame-to-frame communications, limiting practicality.In this work, we propose Inter-frame Communication Transformers (IFC), which significantly reduces the overhead for information-passing between frames by efficiently encoding the context within the input clip.Specifically, we propose to utilize concise memory tokens as a means of conveying information as well as summarizing each frame scene.The features of each frame are enriched and correlated with other frames through exchange of information between the precisely encoded memory tokens.We validate our method on the latest benchmark sets and achieved state-of-the-art performance (AP 42.6 on YouTube-VIS 2019 val set using the offline inference) while having a considerably fast runtime (89.4 FPS). Our method can also be applied to near-online inference for processing a video in real-time with only a small delay.The code is available at https://github.com/sukjunhwang/IFC",
    "authors": [
      "Hwang, Sukjun",
      "Heo, Miran",
      "Oh, Seoung Wug",
      "Kim, Seon Joo"
    ]
  },
  {
    "id": "6f3ef77ac0e3619e98159e9b6febf557",
    "title": "Progressive Coordinate Transforms for Monocular 3D Object Detection",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6f3ef77ac0e3619e98159e9b6febf557-Paper.pdf",
    "abstract": "Recognizing and localizing objects in the 3D space is a crucial ability for an AI agent to perceive its surrounding environment. While significant progress has been achieved with expensive LiDAR point clouds, it poses a great challenge for 3D object detection given only a monocular image. While there exist different alternatives for tackling this problem, it is found that they are either equipped with heavy networks to fuse RGB and depth information or empirically ineffective to process millions of pseudo-LiDAR points. With in-depth examination, we realize that these limitations are rooted in inaccurate object localization. In this paper, we propose a novel and lightweight approach, dubbed {\\em Progressive Coordinate Transforms} (PCT) to facilitate learning coordinate representations. Specifically, a localization boosting mechanism with confidence-aware loss is introduced to progressively refine the localization prediction. In addition, semantic image representation is also exploited to compensate for the usage of patch proposals. Despite being lightweight and simple, our strategy allows us to establish a new state-of-the-art among the monocular 3D detectors on the competitive KITTI benchmark. At the same time, our proposed PCT shows great generalization to most coordinate-based 3D detection frameworks.",
    "authors": [
      "Wang, Li",
      "Zhang, Li",
      "Zhu, Yi",
      "Zhang, Zhi",
      "He, Tong",
      "Li, Mu",
      "Xue, Xiangyang"
    ]
  },
  {
    "id": "6f46dd176364ccec308c2760189a4605",
    "title": "Structured Reordering for Modeling Latent Alignments in Sequence Transduction",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6f46dd176364ccec308c2760189a4605-Paper.pdf",
    "abstract": "Despite success in many domains, neural models struggle in settings where train and test examples are drawn from different distributions. In particular, in contrast to humans, conventional sequence-to-sequence (seq2seq) models fail to generalize systematically, i.e., interpret sentences representing novel combinations of concepts (e.g., text segments) seen in training. Traditional grammar formalisms excel in such settings by implicitly encoding alignments between input and output segments, but are hard to scale and maintain.  Instead of engineering a grammar, we directly model segment-to-segment alignments as discrete structured latent variables within a neural seq2seq model. To efficiently explore the large space of alignments, we introduce a reorder-first align-later framework whose central component is a neural reordering module producing separable permutations. We present an efficient dynamic programming algorithm performing exact marginal inference of separable permutations, and, thus, enabling end-to-end differentiable training of our model.  The resulting seq2seq model exhibits better systematic generalization than standard models on synthetic problems and NLP tasks (i.e., semantic parsing and machine translation).",
    "authors": [
      "wang, bailin",
      "Lapata, Mirella",
      "Titov, Ivan"
    ]
  },
  {
    "id": "6f5216f8d89b086c18298e043bfe48ed",
    "title": "A universal probabilistic spike count model reveals ongoing modulation of neural variability",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6f5216f8d89b086c18298e043bfe48ed-Paper.pdf",
    "abstract": "Neural responses are variable: even under identical experimental conditions, single neuron and population responses typically differ from trial to trial and across time. Recent work has demonstrated that this variability has predictable structure, can be modulated by sensory input and behaviour, and bears critical signatures of the underlying network dynamics and computations. However, current methods for characterising neural variability are primarily geared towards sensory coding in the laboratory: they require trials with repeatable experimental stimuli and behavioural covariates. In addition, they make strong assumptions about the parametric form of variability, rely on assumption-free but data-inefficient histogram-based approaches, or are altogether ill-suited for capturing variability modulation by covariates. Here we present a universal probabilistic spike count model that eliminates these shortcomings. Our method builds on sparse Gaussian processes and can model arbitrary spike count distributions (SCDs) with flexible dependence on observed as well as latent covariates, using scalable variational inference to jointly infer the covariate-to-SCD mappings and latent trajectories in a data efficient way. Without requiring repeatable trials, it can flexibly capture covariate-dependent joint SCDs, and provide interpretable latent causes underlying the statistical dependencies between neurons. We apply the model to recordings from a canonical non-sensory neural population: head direction cells in the mouse. We find that variability in these cells defies a simple parametric relationship with mean spike count as assumed in standard models, its modulation by external covariates can be comparably strong to that of the mean firing rate, and slow low-dimensional latent factors explain away neural correlations. Our approach paves the way to understanding the mechanisms and computations underlying neural variability under naturalistic conditions, beyond the realm of sensory coding with repeatable stimuli.",
    "authors": [
      "Liu, David",
      "Lengyel, Mate"
    ]
  },
  {
    "id": "6f5e4e86a87220e5d361ad82f1ebc335",
    "title": "Bellman Eluder Dimension: New Rich Classes of RL Problems, and Sample-Efficient Algorithms",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6f5e4e86a87220e5d361ad82f1ebc335-Paper.pdf",
    "abstract": "Finding the minimal structural assumptions that empower sample-efficient learning is one of the most important research directions in Reinforcement Learning (RL). This paper advances our understanding of this fundamental question by introducing a new complexity measure\u2014Bellman Eluder (BE) dimension. We show that the family of RL problems of low BE dimension is remarkably rich, which subsumes a vast majority of existing tractable RL problems including but not limited to tabular MDPs, linear MDPs, reactive POMDPs, low Bellman rank problems as well as low Eluder dimension problems. This paper further designs a new optimization-based algorithm\u2014 GOLF, and reanalyzes a hypothesis elimination-based algorithm\u2014OLIVE (proposed in Jiang et al. (2017)). We prove that both algorithms learn the near-optimal policies of low BE dimension problems in a number of samples that is polynomial in all relevant parameters, but independent of the size of state-action space. Our regret and sample complexity results match or improve the best existing results for several well-known subclasses of low BE dimension problems.",
    "authors": [
      "Jin, Chi",
      "Liu, Qinghua",
      "Miryoosefi, Sobhan"
    ]
  },
  {
    "id": "6faa8040da20ef399b63a72d0e4ab575",
    "title": "Detecting Anomalous Event Sequences with Temporal Point Processes",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6faa8040da20ef399b63a72d0e4ab575-Paper.pdf",
    "abstract": "Automatically detecting anomalies in event data can provide substantial value in domains such as healthcare, DevOps, and information security. In this paper, we frame the problem of detecting anomalous continuous-time event sequences as out-of-distribution (OOD) detection for temporal point processes (TPPs). First, we show how this problem can be approached using goodness-of-fit (GoF) tests. We then demonstrate the limitations of popular GoF statistics for TPPs and propose a new test that addresses these shortcomings. The proposed method can be combined with various TPP models, such as neural TPPs, and is easy to implement. In our experiments, we show that the proposed statistic excels at both traditional GoF testing, as well as at detecting anomalies in simulated and real-world data. ",
    "authors": [
      "Shchur, Oleksandr",
      "Turkmen, Ali Caner",
      "Januschowski, Tim",
      "Gasthaus, Jan",
      "G\u00fcnnemann, Stephan"
    ]
  },
  {
    "id": "6fbd841e2e4b2938351a4f9b68f12e6b",
    "title": "HNPE: Leveraging Global Parameters for Neural Posterior Estimation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6fbd841e2e4b2938351a4f9b68f12e6b-Paper.pdf",
    "abstract": "Inferring the parameters of a stochastic model based on experimental observations is central to the scientific method. A particularly challenging setting is when the model is strongly indeterminate, i.e. when distinct sets of parameters yield identical observations. This arises in many practical situations, such as when inferring the distance and power of a radio source (is the source close and weak or far and strong?) or when estimating the amplifier gain and underlying brain activity of an electrophysiological experiment. In this work, we present hierarchical neural posterior estimation (HNPE), a novel method for cracking such indeterminacy by exploiting additional information conveyed by an auxiliary set of observations sharing global parameters. Our method extends recent developments in simulation-based inference (SBI) based on normalizing flows to Bayesian hierarchical models. We validate quantitatively our proposal on a motivating example amenable to analytical solutions and then apply it to invert a well known non-linear model from computational neuroscience, using both simulated and real EEG data.",
    "authors": [
      "Rodrigues, Pedro",
      "Moreau, Thomas",
      "Louppe, Gilles",
      "Gramfort, Alexandre"
    ]
  },
  {
    "id": "6fd6b030c6afec018415662d0db43f9d",
    "title": "Alignment Attention by Matching Key and Query Distributions",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6fd6b030c6afec018415662d0db43f9d-Paper.pdf",
    "abstract": "The neural attention mechanism has been incorporated into deep neural networks to achieve state-of-the-art performance in various domains. Most such models use multi-head self-attention which is appealing for the ability to attend to information from different perspectives. This paper introduces alignment attention that explicitly encourages self-attention to match the distributions of the key and query within each head. The resulting alignment attention networks can be optimized as an unsupervised regularization in the existing attention framework. It is simple to convert any models with self-attention, including pre-trained ones, to the proposed alignment attention. On a variety of language understanding tasks, we show the effectiveness of our method in accuracy, uncertainty estimation, generalization across domains, and robustness to adversarial attacks. We further demonstrate the general applicability of our approach on graph attention and visual question answering, showing the great potential of incorporating our alignment method into various attention-related tasks.",
    "authors": [
      "Zhang, Shujian",
      "Fan, Xinjie",
      "Zheng, Huangjie",
      "Tanwisuth, Korawat",
      "Zhou, Mingyuan"
    ]
  },
  {
    "id": "6fe6a8a6e6cb710584efc4af0c34ce50",
    "title": "Settling the Variance of Multi-Agent Policy Gradients",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6fe6a8a6e6cb710584efc4af0c34ce50-Paper.pdf",
    "abstract": "Policy gradient (PG) methods are popular reinforcement learning (RL) methods where a baseline is often applied to reduce the variance of gradient estimates. In multi-agent RL (MARL), although the PG theorem can be naturally extended, the effectiveness of multi-agent PG (MAPG)  methods degrades as the variance of gradient estimates increases rapidly with the number of agents.  In this paper, we offer a rigorous analysis of MAPG methods by, firstly, quantifying the contributions of the number of agents and agents' explorations to the variance of MAPG estimators. Based on this analysis, we derive the optimal baseline (OB) that achieves the minimal variance. In comparison to the OB, we measure the excess variance of existing MARL algorithms such as vanilla MAPG and COMA. Considering using deep neural networks,  we also propose a surrogate version of OB, which can be seamlessly plugged into any existing PG methods in MARL.   On benchmarks of Multi-Agent MuJoCo and StarCraft challenges, our OB technique effectively stabilises training and improves the performance of multi-agent PPO  and COMA algorithms by a significant margin.  Code is released at  \\url{https://github.com/morning9393/Optimal-Baseline-for-Multi-agent-Policy-Gradients}. ",
    "authors": [
      "Kuba, Jakub Grudzien",
      "Wen, Muning",
      "Meng, Linghui",
      "gu, shangding",
      "Zhang, Haifeng",
      "Mguni, David",
      "Wang, Jun",
      "Yang, Yaodong"
    ]
  },
  {
    "id": "6ffad86b9a8dd4a3e98df1b0830d1c8c",
    "title": "For high-dimensional hierarchical models, consider exchangeability of effects across covariates instead of across datasets",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/6ffad86b9a8dd4a3e98df1b0830d1c8c-Paper.pdf",
    "abstract": "Hierarchical Bayesian methods enable information sharing across regression problems on multiple groups of data. While standard practice is to model regression parameters (effects) as (1) exchangeable across the groups and (2) correlated to differing degrees across covariates, we show that this approach exhibits poor statistical performance when the number of covariates exceeds the number of groups. For instance, in statistical genetics, we might regress dozens of traits (defining groups) for thousands of individuals (responses) on up to millions of genetic variants (covariates). When an analyst has more covariates than groups, we argue that it is often preferable to instead model effects as (1) exchangeable across covariates and (2) correlated to differing degrees across groups. To this end, we propose a hierarchical model expressing our alternative perspective. We devise an empirical Bayes estimator for learning the degree of correlation between groups. We develop theory that demonstrates that our method outperforms the classic approach when the number of covariates dominates the number of groups, and corroborate this result empirically on several high-dimensional multiple regression and classification problems.",
    "authors": [
      "Trippe, Brian",
      "Finucane, Hilary",
      "Broderick, Tamara"
    ]
  },
  {
    "id": "700fdb2ba62d4554dc268c65add4b16e",
    "title": "Efficient Algorithms for Learning Depth-2 Neural Networks with General ReLU Activations",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/700fdb2ba62d4554dc268c65add4b16e-Paper.pdf",
    "abstract": "We present polynomial time and sample efficient algorithms for learning an unknown depth-2 feedforward neural network with general ReLU activations, under mild non-degeneracy assumptions. In particular, we consider learning an unknown network of the form $f(x) = {a}^{\\mathsf{T}}\\sigma({W}^\\mathsf{T}x+b)$, where $x$ is drawn from the Gaussian distribution, and $\\sigma(t) = \\max(t,0)$ is the ReLU activation. Prior works for learning networks with ReLU activations assume that the bias ($b$) is zero. In order to deal with the presence of the bias terms, our proposed algorithm consists of robustly decomposing multiple higher order tensors arising from the Hermite expansion of the function $f(x)$. Using these ideas we also establish identifiability of the network parameters under very mild assumptions.",
    "authors": [
      "Awasthi, Pranjal",
      "Tang, Alex",
      "Vijayaraghavan, Aravindan"
    ]
  },
  {
    "id": "701d804549a4a23d3cae801dac6c2c75",
    "title": "Controllable and Compositional Generation with Latent-Space Energy-Based Models",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/701d804549a4a23d3cae801dac6c2c75-Paper.pdf",
    "abstract": "Controllable generation is one of the key requirements for successful adoption of deep generative models in real-world applications, but it still remains as a great challenge. In particular, the compositional ability to generate novel concept combinations is out of reach for most current models. In this work, we use energy-based models (EBMs) to handle compositional generation over a set of attributes. To make them scalable to high-resolution image generation, we introduce an EBM in the latent space of a pre-trained generative model such as StyleGAN. We propose a novel EBM formulation representing the joint distribution of data and attributes together, and we show how sampling from it is formulated as solving an ordinary differential equation (ODE). Given a pre-trained generator, all we need for controllable generation is to train an attribute classifier. Sampling with ODEs is done efficiently in the latent space and is robust to hyperparameters. Thus, our method is simple, fast to train, and efficient to sample. Experimental results show that our method outperforms the state-of-the-art in both conditional sampling and sequential editing. In compositional generation, our method excels at zero-shot generation of unseen attribute combinations. Also, by composing energy functions with logical operators, this work is the first to achieve such compositionality in generating photo-realistic images of resolution 1024x1024.",
    "authors": [
      "Nie, Weili",
      "Vahdat, Arash",
      "Anandkumar, Anima"
    ]
  },
  {
    "id": "706608cfdbcc1886bb7eea5513f90133",
    "title": "Reverse-Complement Equivariant Networks for DNA Sequences",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/706608cfdbcc1886bb7eea5513f90133-Paper.pdf",
    "abstract": "As DNA sequencing technologies keep improving in scale and cost, there is a growing need to develop machine learning models to analyze DNA sequences, e.g., to decipher regulatory signals from DNA fragments bound by a particular protein of interest.  As a double helix made of two complementary strands, a DNA fragment can be sequenced as two equivalent, so-called reverse complement (RC) sequences of nucleotides. To take into account this inherent symmetry of the data in machine learning models can facilitate learning. In this sense, several authors have recently proposed particular RC-equivariant convolutional neural networks (CNNs). However, it remains unknown whether other RC-equivariant architecture exist, which could potentially increase the set of basic models adapted to DNA sequences for practitioners. Here, we close this gap by characterizing the set of all linear RC-equivariant layers, and show in particular that new architectures exist beyond the ones already explored. We further discuss RC-equivariant pointwise nonlinearities adapted to different architectures, as well as RC-equivariant embeddings of $k$-mers as an alternative to one-hot encoding of nucleotides. We show experimentally that the new architectures can outperform existing ones.",
    "authors": [
      "Mallet, Vincent",
      "Vert, Jean-Philippe"
    ]
  },
  {
    "id": "70a32110fff0f26d301e58ebbca9cb9f",
    "title": "Provably Efficient Reinforcement Learning with Linear Function Approximation under Adaptivity Constraints",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/70a32110fff0f26d301e58ebbca9cb9f-Paper.pdf",
    "abstract": "We study reinforcement learning (RL) with linear function approximation under the adaptivity constraint. We consider two popular limited adaptivity models: the batch learning model and the rare policy switch model, and propose two efficient online RL algorithms for episodic linear Markov decision processes, where the transition probability and the reward function can be represented as a linear function of some known feature mapping. In specific, for the batch learning model, our proposed LSVI-UCB-Batch algorithm achieves an $\\tilde O(\\sqrt{d^3H^3T} + dHT/B)$ regret, where $d$ is the dimension of the feature mapping, $H$ is the episode length, $T$ is the number of interactions and $B$ is the number of batches. Our result suggests that it suffices to use only $\\sqrt{T/dH}$ batches to obtain $\\tilde O(\\sqrt{d^3H^3T})$ regret. For the rare policy switch model, our proposed LSVI-UCB-RareSwitch algorithm enjoys an $\\tilde O(\\sqrt{d^3H^3T[1+T/(dH)]^{dH/B}})$ regret, which implies that $dH\\log T$ policy switches suffice to obtain the $\\tilde O(\\sqrt{d^3H^3T})$ regret. Our algorithms achieve the same regret as the LSVI-UCB algorithm \\citep{jin2020provably}, yet with a substantially smaller amount of adaptivity. We also establish a lower bound for the batch learning model, which suggests that the dependency on $B$ in our regret bound is tight.",
    "authors": [
      "Wang, Tianhao",
      "Zhou, Dongruo",
      "Gu, Quanquan"
    ]
  },
  {
    "id": "70afbf2259b4449d8ae1429e054df1b1",
    "title": "Nonsmooth Implicit Differentiation for Machine-Learning and Optimization",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/70afbf2259b4449d8ae1429e054df1b1-Paper.pdf",
    "abstract": "In view of training increasingly complex learning architectures, we establish a nonsmooth implicit function theorem with an operational calculus. Our result applies to most practical problems (i.e., definable problems) provided that a nonsmooth form of the classical invertibility condition is fulfilled. This approach allows for formal subdifferentiation: for instance, replacing derivatives by Clarke Jacobians in the usual differentiation formulas is fully justified for a wide class of nonsmooth problems. Moreover this calculus is entirely compatible with algorithmic differentiation (e.g., backpropagation). We provide several applications such as training deep equilibrium networks, training neural nets with conic optimization layers, or hyperparameter-tuning for nonsmooth Lasso-type models. To show the sharpness of our assumptions, we present numerical experiments showcasing the extremely pathological gradient dynamics one can encounter when applying implicit algorithmic differentiation without any hypothesis.",
    "authors": [
      "Bolte, J\u00e9r\u00f4me",
      "Le, Tam",
      "Pauwels, Edouard",
      "Silveti-Falls, Tony"
    ]
  },
  {
    "id": "70d31b87bd021441e5e6bf23eb84a306",
    "title": "Heuristic-Guided Reinforcement Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/70d31b87bd021441e5e6bf23eb84a306-Paper.pdf",
    "abstract": "We provide a framework to accelerate reinforcement learning (RL) algorithms by heuristics that are constructed by domain knowledge or offline data.  Tabula rasa RL algorithms require environment interactions or computation that scales with the horizon of the sequential decision-making task.  Using our framework, we show how heuristic-guided RL induces a much shorter horizon sub-problem that provably solves the original task. Our framework can be viewed as a horizon-based regularization for controlling bias and variance in RL under a finite interaction budget.  In theory, we characterize the properties of a good heuristic and the resulting impact on RL acceleration. In particular, we introduce the novel concept of an improvable heuristic that can allow any RL agent to conservatively extrapolate beyond its prior knowledge.  In practice, we instantiate our framework to accelerate several state-of-the-art algorithms in simulated robotic control tasks and procedurally generated games. Our framework complements the rich literature on warm-starting RL using expert demonstrations or exploratory data-sets, and creates a unified channel to inject prior knowledge into RL.",
    "authors": [
      "Cheng, Ching-An",
      "Kolobov, Andrey",
      "Swaminathan, Adith"
    ]
  },
  {
    "id": "70d355680e628fe1c552221f690d8da4",
    "title": "Statistical Undecidability in Linear, Non-Gaussian Causal Models in the Presence of Latent Confounders",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/70d355680e628fe1c552221f690d8da4-Paper.pdf",
    "abstract": "If causal relationships are linear and acyclic and noise terms are independent and Gaussian, causal orientation is not identified from observational data --- even if faithfulness is satisfied (Spirtes et al., 2002). Shimizu et al. (2006) showed that acyclic, linear, {\\bf non}-Gaussian (LiNGAM) causal models {\\em are} identified from observational data, so long as no latent confounders are present. That holds even when faithfulness fails. Genin and Mayo-Wilson (2020) refine that result:  not only are causal relationships identified, but causal orientation is {\\em statistically decidable}. That means that for every $\\epsilon>0,$ there is a method that converges in probability to the correct orientation and, at every sample size, outputs an incorrect orientation with probability less than $\\epsilon.$ These results naturally raise questions about what happens in the presence of latent confounders. Hoyer et al. (2008) and Salehkaleybar et al. (2020) show that, although the causal model is not uniquely identified, causal orientation among observed variables is identified in the presence of latent confounders, so long as faithfulness is satisfied. This paper refines these results: although it is possible to converge to the right orientation in the limit, causal orientation is no longer statistically decidable---it is not possible to converge to the correct orientation with finite-sample bounds on the probability of orientation errors, even if faithfulness is satisfied. However, that limiting result suggests several adjustments to the LiNGAM model that may recover decidability.",
    "authors": [
      "Genin, Konstantin"
    ]
  },
  {
    "id": "70d5212dd052b2ef06e5e562f6f9ab9c",
    "title": "A novel notion of barycenter for probability distributions based on optimal weak mass transport",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/70d5212dd052b2ef06e5e562f6f9ab9c-Paper.pdf",
    "abstract": "We introduce weak barycenters of a family of probability distributions, based on the recently developed notion of optimal weak transport of mass by Gozlan et al. (2017) and Backhoff-Veraguas et al. (2020). We provide a theoretical analysis of this object and discuss its interpretation in the light of convex ordering between probability measures. In particular, we show that, rather than averaging the input distributions in a geometric way (as the Wasserstein barycenter based on classic optimal transport does) weak barycenters extract common geometric information shared by all the input distributions, encoded as a latent random variable that underlies all of them. We also provide an iterative algorithm to compute a weak barycenter for a finite family of input distributions, and a stochastic algorithm that computes them for arbitrary populations of laws.  The latter approach is particularly well suited for the streaming setting, i.e., when distributions are observed sequentially. The notion of weak barycenter and our approaches to compute it are illustrated on synthetic examples, validated on 2D real-world data and compared to standard Wasserstein barycenters.",
    "authors": [
      "Cazelles, Elsa",
      "Tobar, Felipe",
      "Fontbona, Joaquin"
    ]
  },
  {
    "id": "70efdf2ec9b086079795c442636b55fb",
    "title": "Temporal-attentive Covariance Pooling Networks for Video Recognition",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/70efdf2ec9b086079795c442636b55fb-Paper.pdf",
    "abstract": "For video recognition task, a global representation summarizing the whole contents of the video snippets plays an important role for the final performance. However, existing video architectures usually generate it by using a simple, global average pooling (GAP) method, which has limited ability to capture complex dynamics of videos. For image recognition task, there exist evidences showing that covariance pooling has stronger representation ability than GAP. Unfortunately, such plain covariance pooling used in image recognition is an orderless representative, which cannot model spatio-temporal structure inherent in videos. Therefore, this paper proposes a Temporal-attentive Covariance Pooling (TCP), inserted at the end of deep architectures, to produce powerful video representations. Specifically, our TCP first develops a temporal attention module to adaptively calibrate spatio-temporal features for the succeeding covariance pooling, approximatively producing attentive covariance representations. Then, a temporal covariance pooling performs temporal pooling of the attentive covariance representations to characterize both intra-frame correlations and inter-frame cross-correlations of the calibrated features. As such, the proposed TCP can capture complex temporal dynamics. Finally, a fast matrix power normalization is introduced to exploit geometry of covariance representations. Note that our TCP is model-agnostic and can be flexibly integrated into any video architectures, resulting in TCPNet for effective video recognition. The extensive experiments on six benchmarks (e.g., Kinetics, Something-Something V1 and Charades) using various video architectures show our TCPNet is clearly superior to its counterparts, while having strong generalization ability. The source code is publicly available.",
    "authors": [
      "Gao, Zilin",
      "Wang, Qilong",
      "Zhang, Bingbing",
      "Hu, Qinghua",
      "Li, Peihua"
    ]
  },
  {
    "id": "70fc5f043205720a49d973d280eb83e7",
    "title": "Revisiting Smoothed Online Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/70fc5f043205720a49d973d280eb83e7-Paper.pdf",
    "abstract": "In this paper, we revisit the problem of smoothed online learning, in which the online learner suffers both a hitting cost and a switching cost, and target two performance metrics: competitive ratio and dynamic regret with switching cost. To bound the competitive ratio, we assume the hitting cost is known to the learner in each round, and investigate the simple idea of balancing the two costs by an optimization problem. Surprisingly, we find that minimizing the hitting cost alone is $\\max(1, \\frac{2}{\\alpha})$-competitive for $\\alpha$-polyhedral functions and $1 + \\frac{4}{\\lambda}$-competitive for $\\lambda$-quadratic growth functions, both of which improve state-of-the-art results significantly. Moreover, when the hitting cost is both convex and $\\lambda$-quadratic growth, we reduce the competitive ratio to $1 + \\frac{2}{\\sqrt{\\lambda}}$  by minimizing the weighted sum of the hitting cost and the switching cost. To bound the dynamic regret with switching cost, we follow the standard setting of online convex optimization, in which the hitting cost is convex but hidden from the learner before making predictions. We modify Ader, an existing algorithm designed for dynamic regret, slightly to take into account the switching cost when measuring the performance. The proposed algorithm, named as Smoothed Ader, attains an optimal $O(\\sqrt{T(1+P_T)})$ bound for dynamic regret with switching cost, where $P_T$ is the path-length of the comparator sequence. Furthermore, if the hitting cost is accessible in the beginning of each round, we obtain a similar guarantee without the bounded gradient condition, and establish an $\\Omega(\\sqrt{T(1+P_T)})$ lower bound to confirm the optimality.",
    "authors": [
      "Zhang, Lijun",
      "Jiang, Wei",
      "Lu, Shiyin",
      "Yang, Tianbao"
    ]
  },
  {
    "id": "712a67567ec10c52c2b966224cf94d1e",
    "title": "Marginalised Gaussian Processes with Nested Sampling",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/712a67567ec10c52c2b966224cf94d1e-Paper.pdf",
    "abstract": "Gaussian Process models are a rich distribution over functions with inductive biases controlled by a kernel function. Learning occurs through optimisation of the kernel hyperparameters using the marginal likelihood as the objective. This work proposes nested sampling as a means of marginalising kernel hyperparameters,  because it is a technique that is well-suited to exploring complex, multi-modal distributions. We benchmark against Hamiltonian Monte Carlo on time-series and two-dimensional regression tasks, finding that a principled approach to quantifying hyperparameter uncertainty substantially improves the quality of prediction intervals.",
    "authors": [
      "Simpson, Fergus",
      "Lalchand, Vidhi",
      "Rasmussen, Carl Edward"
    ]
  },
  {
    "id": "713fd63d76c8a57b16fc433fb4ae718a",
    "title": "Provable Benefits of Actor-Critic Methods for Offline Reinforcement Learning",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/713fd63d76c8a57b16fc433fb4ae718a-Paper.pdf",
    "abstract": "Actor-critic methods are widely used in offline reinforcement learningpractice, but are not so well-understood theoretically. We propose a newoffline actor-critic algorithm that naturally incorporates the pessimism principle, leading to several key advantages compared to the state of the art. The algorithm can operate when the Bellman evaluation operator is closed with respect to the action value function of the actor's policies; this is a more general setting than the low-rank MDP model. Despite the added generality, the procedure is computationally tractable as it involves the solution of a sequence of second-order programs.We prove an upper bound on the suboptimality gap of the policy returned by the procedure that depends on the data coverage of any arbitrary, possibly data dependent comparator policy.The achievable guarantee is complemented with a minimax lower bound that is matching up to logarithmic factors.",
    "authors": [
      "Zanette, Andrea",
      "Wainwright, Martin J",
      "Brunskill, Emma"
    ]
  },
  {
    "id": "7180cffd6a8e829dacfc2a31b3f72ece",
    "title": "Bayesian Bellman Operators",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/7180cffd6a8e829dacfc2a31b3f72ece-Paper.pdf",
    "abstract": "We introduce a novel perspective on Bayesian reinforcement learning (RL); whereas existing approaches infer a posterior over the transition distribution or Q-function, we characterise the uncertainty in the Bellman operator. Our Bayesian Bellman operator (BBO) framework is motivated by the insight that when bootstrapping is introduced, model-free approaches actually infer a posterior over Bellman operators, not value functions. In this paper, we use BBO to provide a rigorous theoretical analysis of model-free Bayesian RL to better understand its relationship to established frequentist RL methodologies. We prove that Bayesian solutions are consistent with frequentist RL solutions, even when approximate inference is used, and derive conditions for which  convergence properties hold. Empirically, we demonstrate that algorithms derived from the BBO framework have sophisticated deep exploration properties that enable them to solve continuous control tasks at which state-of-the-art regularised actor-critic algorithms fail catastrophically. ",
    "authors": [
      "Fellows, Mattie",
      "Hartikainen, Kristian",
      "Whiteson, Shimon"
    ]
  },
  {
    "id": "71a8b2ffe0b594a5c1b3c28090384fd7",
    "title": "Uncertainty Calibration for Ensemble-Based Debiasing Methods",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/71a8b2ffe0b594a5c1b3c28090384fd7-Paper.pdf",
    "abstract": "Ensemble-based debiasing methods have been shown effective in mitigating the reliance of classifiers on specific dataset bias, by exploiting the output of a bias-only model to adjust the learning target. In this paper, we focus on the bias-only model in these ensemble-based methods, which plays an important role but has not gained much attention in the existing literature. Theoretically, we prove that the debiasing performance can be damaged by inaccurate uncertainty estimations of the bias-only model. Empirically, we show that existing bias-only models fall short in producing accurate uncertainty estimations. Motivated by these findings, we propose to conduct calibration on the bias-only model, thus achieving a three-stage ensemble-based debiasing framework, including bias modeling, model calibrating, and debiasing. Experimental results on NLI and fact verification tasks show that our proposed three-stage debiasing framework consistently outperforms the traditional two-stage one in out-of-distribution accuracy.",
    "authors": [
      "Xiong, Ruibin",
      "Chen, Yimeng",
      "Pang, Liang",
      "Cheng, Xueqi",
      "Ma, Zhi-Ming",
      "Lan, Yanyan"
    ]
  },
  {
    "id": "71cc107d2e0408e60a3d3c44f47507bd",
    "title": "Provably Faster Algorithms for Bilevel Optimization",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/71cc107d2e0408e60a3d3c44f47507bd-Paper.pdf",
    "abstract": "Bilevel optimization has been widely applied in many important machine learning applications such as hyperparameter optimization and meta-learning. Recently, several momentum-based algorithms have been proposed to solve bilevel optimization problems faster. However, those momentum-based algorithms do not achieve provably better computational complexity than $\\mathcal{\\widetilde O}(\\epsilon^{-2})$ of the SGD-based algorithm. In this paper, we propose two new algorithms for bilevel optimization, where the first algorithm adopts momentum-based recursive iterations, and the second algorithm adopts recursive gradient estimations in nested loops to decrease the variance. We show that both algorithms achieve the complexity of $\\mathcal{\\widetilde O}(\\epsilon^{-1.5})$, which outperforms all existing algorithms by the order of magnitude. Our experiments validate our theoretical results and demonstrate the superior empirical performance of our algorithms in hyperparameter applications.",
    "authors": [
      "Yang, Junjie",
      "Ji, Kaiyi",
      "Liang, Yingbin"
    ]
  },
  {
    "id": "71ddb91e8fa0541e426a54e538075a5a",
    "title": "Neo-GNNs: Neighborhood Overlap-aware Graph Neural Networks for Link Prediction",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/71ddb91e8fa0541e426a54e538075a5a-Paper.pdf",
    "abstract": "Graph Neural Networks (GNNs) have been widely applied to various fields for learning over graph-structured data. They have shown significant improvements over traditional heuristic methods in various tasks such as node classification and graph classification. However, since GNNs heavily rely on smoothed node features rather than graph structure, they often show poor performance than simple heuristic methods in link prediction where the structural information, e.g., overlapped neighborhoods, degrees, and shortest paths, is crucial. To address this limitation, we propose Neighborhood Overlap-aware Graph Neural Networks (Neo-GNNs) that learn useful structural features from an adjacency matrix and estimate overlapped neighborhoods for link prediction. Our Neo-GNNs generalize neighborhood overlap-based heuristic methods and handle overlapped multi-hop neighborhoods. Our extensive experiments on Open Graph Benchmark datasets (OGB) demonstrate that Neo-GNNs consistently achieve state-of-the-art performance in link prediction.",
    "authors": [
      "Yun, Seongjun",
      "Kim, Seoyoon",
      "Lee, Junhyun",
      "Kang, Jaewoo",
      "Kim, Hyunwoo J."
    ]
  },
  {
    "id": "71e09b16e21f7b6919bbfc43f6a5b2f0",
    "title": "Self-Supervised Multi-Object Tracking with Cross-input Consistency",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/71e09b16e21f7b6919bbfc43f6a5b2f0-Paper.pdf",
    "abstract": "In this paper, we propose a self-supervised learning procedure for training a robust multi-object tracking (MOT) model given only unlabeled video. While several self-supervisory learning signals have been proposed in prior work on single-object tracking, such as color propagation and cycle-consistency, these signals are not effective for training RNN models, which are needed to achieve accurate MOT: they yield degenerate models that, for instance, always match new detections to tracks with the closest initial detections. We propose a novel self-supervisory signal that we call cross-input consistency: we construct two distinct inputs for the same sequence of video, by hiding different information about the sequence in each input. We then compute tracks in that sequence by applying an RNN model independently on each input, and train the model to produce consistent tracks across the two inputs. We evaluate our unsupervised method on MOT17 and KITTI --- remarkably, we find that, despite training only on unlabeled video, our unsupervised approach outperforms four supervised methods published in the last 1--2 years, including Tracktor++, FAMNet, GSM, and mmMOT.",
    "authors": [
      "Bastani, Favyen",
      "He, Songtao",
      "Madden, Samuel"
    ]
  },
  {
    "id": "71f6278d140af599e06ad9bf1ba03cb0",
    "title": "Tree in Tree: from Decision Trees to Decision Graphs",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/71f6278d140af599e06ad9bf1ba03cb0-Paper.pdf",
    "abstract": "Decision trees have been widely used as classifiers in many machine learning applications thanks to their lightweight and interpretable decision process. This paper introduces Tree in Tree decision graph (TnT), a framework that extends the conventional decision tree to a more generic and powerful directed acyclic graph. TnT constructs decision graphs by recursively growing decision trees inside the internal or leaf nodes instead of greedy training. The time complexity of TnT is linear to the number of nodes in the graph, therefore it can construct decision graphs on large datasets. Compared to decision trees, we show that TnT achieves better classification performance with reduced model size, both as a stand-alone classifier and as a base-estimator in bagging/AdaBoost ensembles. Our proposed model is a novel, more efficient and accurate alternative to the widely-used decision trees.",
    "authors": [
      "Zhu, Bingzhao",
      "Shoaran, Mahsa"
    ]
  },
  {
    "id": "722caafb4825ef5d8670710fa29087cf",
    "title": "Test-time Collective Prediction",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/722caafb4825ef5d8670710fa29087cf-Paper.pdf",
    "abstract": "An increasingly common setting in machine learning involves multiple parties, each with their own data, who want to jointly make predictions on future test points. Agents wish to benefit from the collective expertise of the full set of agents to make better predictions than they would individually, but may not be willing to release labeled data or model parameters. In this work, we explore a decentralized mechanism to make collective predictions at test time, that is inspired by the literature in social science on human consensus-making. Building on a query model to facilitate information exchange among agents, our approach leverages each agent\u2019s pre-trained model without relying on external validation, model retraining, or data pooling. A theoretical analysis shows that our approach recovers inverse mean-squared-error (MSE) weighting in the large-sample limit which is known to be the optimal way to combine independent, unbiased estimators. Empirically, we demonstrate that our scheme effectively combines models with differing quality across the input space: the proposed consensus prediction achieves significant gains over classical model averaging, and even outperforms weighted averaging schemes that have access to additional validation data. Finally, we propose a decentralized Jackknife procedure as a tool to evaluate the sensitivity of the collective predictions with respect to a single agent's opinion.",
    "authors": [
      "Mendler-D\u00fcnner, Celestine",
      "Guo, Wenshuo",
      "Bates, Stephen",
      "Jordan, Michael"
    ]
  },
  {
    "id": "7230b2b03e2da37352abf1a659545b44",
    "title": "A Continuous Mapping For Augmentation Design",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/7230b2b03e2da37352abf1a659545b44-Paper.pdf",
    "abstract": "Automated data augmentation (ADA) techniques have played an important role in boosting the performance of deep models. Such techniques mostly aim to optimize a parameterized distribution over a discrete augmentation space. Thus, are restricted by the discretization of the search space which normally is handcrafted. To overcome the limitations, we take the first step to constructing a continuous mapping from $\\mathbb{R}^d$ to image transformations (an augmentation space). Using this mapping, we take a novel approach where 1) we pose the ADA as a continuous optimization problem over the parameters of the augmentation distribution; and 2) use Stochastic Gradient Langevin Dynamics to learn and sample augmentations. This allows us to potentially explore the space of infinitely many possible augmentations, which otherwise was not possible due to the discretization of the space. This view of ADA is radically different from the standard discretization based view of ADA, and it opens avenues for utilizing the vast efficient gradient-based algorithms available for continuous optimization problems. Results over multiple benchmarks demonstrate the efficiency improvement of this work compared with previous methods.",
    "authors": [
      "Tian, Keyu",
      "Lin, Chen",
      "Lim, Ser Nam",
      "Ouyang, Wanli",
      "Dokania, Puneet",
      "Torr, Philip"
    ]
  },
  {
    "id": "7241bd19bb709da0f46807bde88aed25",
    "title": "Neural Routing by Memory",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/7241bd19bb709da0f46807bde88aed25-Paper.pdf",
    "abstract": "Recent Convolutional Neural Networks (CNNs) have achieved significant success by stacking multiple convolutional blocks, named procedures in this paper, to extract semantic features. However, they use the same procedure sequence for all inputs, regardless of the intermediate features.This paper proffers a simple yet effective idea of constructing parallel procedures and assigning similar intermediate features to the same specialized procedures in a divide-and-conquer fashion. It relieves each procedure's learning difficulty and thus leads to superior performance. Specifically, we propose a routing-by-memory mechanism for existing CNN architectures. In each stage of the network, we introduce parallel Procedural Units (PUs). A PU consists of a memory head and a procedure. The memory head maintains a summary of a type of features. For an intermediate feature, we search its closest memory and forward it to the corresponding procedure in both training and testing. In this way, different procedures are tailored to different features and therefore tackle them better.Networks with the proposed mechanism can be trained efficiently using a four-step training strategy. Experimental results show that our method improves VGGNet, ResNet, and EfficientNet's accuracies on Tiny ImageNet, ImageNet, and CIFAR-100 benchmarks with a negligible extra computational cost.",
    "authors": [
      "Zhang, Kaipeng",
      "Li, Zhenqiang",
      "Li, Zhifeng",
      "Liu, Wei",
      "Sato, Yoichi"
    ]
  },
  {
    "id": "725215ed82ab6306919b485b81ff9615",
    "title": "GeoMol: Torsional Geometric Generation of Molecular 3D Conformer Ensembles",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/725215ed82ab6306919b485b81ff9615-Paper.pdf",
    "abstract": "Prediction of a molecule\u2019s 3D conformer ensemble from the molecular graph holds a key role in areas of cheminformatics and drug discovery. Existing generative models have several drawbacks including lack of modeling important molecular geometry elements (e.g., torsion angles), separate optimization stages prone to error accumulation, and the need for structure fine-tuning based on approximate classical force-fields or computationally expensive methods. We propose GEOMOL --- an end-to-end, non-autoregressive, and SE(3)-invariant machine learning approach to generate distributions of low-energy molecular 3D conformers. Leveraging the power of message passing neural networks (MPNNs) to capture local and global graph information, we predict local atomic 3D structures and torsion angles, avoid- ing unnecessary over-parameterization of the geometric degrees of freedom (e.g., one angle per non-terminal bond). Such local predictions suffice both for both the training loss computation and for the full deterministic conformer assembly (at test time). We devise a non-adversarial optimal transport based loss function to promote diverse conformer generation. GEOMOL predominantly outperforms popular open-source, commercial, or state-of-the-art machine learning (ML) models, while achieving significant speed-ups. We expect such differentiable 3D structure generators to significantly impact molecular modeling and related applications.",
    "authors": [
      "Ganea, Octavian",
      "Pattanaik, Lagnajit",
      "Coley, Connor",
      "Barzilay, Regina",
      "Jensen, Klavs",
      "Green, William",
      "Jaakkola, Tommi"
    ]
  },
  {
    "id": "7274a60c83145b1082be9caa91926ecf",
    "title": "CANITA: Faster Rates for Distributed Convex Optimization with Communication Compression",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/7274a60c83145b1082be9caa91926ecf-Paper.pdf",
    "abstract": "Due to the high communication cost in distributed and federated learning, methods relying on compressed communication are becoming increasingly popular. Besides, the best theoretically and practically performing gradient-type methods invariably rely on some form of acceleration/momentum to reduce the number of communications (faster convergence), e.g., Nesterov's accelerated gradient descent [31, 32] and Adam [14]. In order to combine the benefits of communication compression and convergence acceleration, we propose a \\emph{compressed and accelerated} gradient method based on ANITA [20] for distributed optimization, which we call CANITA. Our CANITA achieves the \\emph{first accelerated rate} $O\\bigg(\\sqrt{\\Big(1+\\sqrt{\\frac{\\omega^3}{n}}\\Big)\\frac{L}{\\epsilon}} + \\omega\\big(\\frac{1}{\\epsilon}\\big)^{\\frac{1}{3}}\\bigg)$, which improves upon the state-of-the-art non-accelerated rate  $O\\left((1+\\frac{\\omega}{n})\\frac{L}{\\epsilon} + \\frac{\\omega^2+\\omega}{\\omega+n}\\frac{1}{\\epsilon}\\right)$ of DIANA [12] for distributed general convex problems, where $\\epsilon$ is the target error,  $L$ is the smooth parameter of the objective, $n$ is the number of machines/devices, and $\\omega$ is the compression parameter (larger $\\omega$ means more compression can be applied, and no compression implies $\\omega=0$). Our results show that as long as the number of devices $n$ is large (often true in distributed/federated learning), or the compression $\\omega$ is not very high,  CANITA achieves the faster convergence rate $O\\Big(\\sqrt{\\frac{L}{\\epsilon}}\\Big)$, i.e., the number of communication rounds is $O\\Big(\\sqrt{\\frac{L}{\\epsilon}}\\Big)$ (vs. $O\\big(\\frac{L}{\\epsilon}\\big)$ achieved by previous works). As a result, CANITA enjoys the advantages of both compression (compressed communication in each round) and acceleration (much fewer communication rounds). ",
    "authors": [
      "Li, Zhize",
      "Richtarik, Peter"
    ]
  },
  {
    "id": "729c68884bd359ade15d5f163166738a",
    "title": "Drop-DTW: Aligning Common Signal Between Sequences While Dropping Outliers",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/729c68884bd359ade15d5f163166738a-Paper.pdf",
    "abstract": "In this work, we consider the problem of sequence-to-sequence alignment for signals containing outliers.  Assuming the absence of outliers, the standard Dynamic Time Warping (DTW) algorithm efficiently computes the optimal alignment between two (generally) variable-length sequences.  While DTW is robust to temporal shifts and dilations of the signal, it fails to align sequences in a meaningful way in the presence of outliers that can be arbitrarily interspersed in the sequences.  To address this problem, we introduce Drop-DTW, a novel algorithm that aligns the common signal between the sequences while automatically dropping the outlier elements from the matching.  The entire procedure is implemented as a single dynamic program that is efficient and fully differentiable.  In our experiments, we show that Drop-DTW is a robust similarity measure for sequence retrieval and demonstrate its effectiveness as a training loss on diverse applications. With Drop-DTW, we address temporal step localization on instructional videos, representation learning from noisy videos, and cross-modal representation learning for audio-visual retrieval and localization. In all applications, we take a weakly- or unsupervised approach and demonstrate state-of-the-art results under these settings.",
    "authors": [
      "Dvornik, Mikita",
      "Hadji, Isma",
      "Derpanis, Konstantinos G.",
      "Garg, Animesh",
      "Jepson, Allan"
    ]
  },
  {
    "id": "72f67e70f6b7cdc4cc893edaddf0c4c6",
    "title": "Safe Reinforcement Learning with Natural Language Constraints",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/72f67e70f6b7cdc4cc893edaddf0c4c6-Paper.pdf",
    "abstract": "While safe reinforcement learning (RL) holds great promise for many practical applications like robotics or autonomous cars, current approaches require specifying constraints in mathematical form. Such specifications demand domain expertise, limiting the adoption of safe RL. In this paper, we propose learning to interpret natural language constraints for safe RL. To this end, we first introduce HAZARDWORLD, a new multi-task benchmark that requires an agent to optimize reward while not violating constraints specified in free-form text. We then develop an agent with a modular architecture that can interpret and adhere to such textual constraints while learning new tasks. Our model consists of (1) a constraint interpreter that encodes textual constraints into spatial and temporal representations of forbidden states, and (2) a policy network that uses these representations to produce a policy achieving minimal constraint violations during training. Across different domains in HAZARDWORLD, we show that our method achieves higher rewards (up to11x) and fewer constraint violations (by 1.8x) compared to existing approaches. However, in terms of absolute performance, HAZARDWORLD still poses significant challenges for agents to learn efficiently, motivating the need for future work.",
    "authors": [
      "Yang, Tsung-Yen",
      "Hu, Michael Y",
      "Chow, Yinlam",
      "Ramadge, Peter J",
      "Narasimhan, Karthik"
    ]
  },
  {
    "id": "72fe6f9fdab5f4d465ac6da028e4544c",
    "title": "Compositional Modeling of Nonlinear Dynamical Systems with ODE-based Random Features",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/72fe6f9fdab5f4d465ac6da028e4544c-Paper.pdf",
    "abstract": "Effectively modeling phenomena present in highly nonlinear dynamical systems whilst also accurately quantifying uncertainty is a challenging task, which often requires problem-specific techniques. We present a novel, domain-agnostic approach to tackling this problem, using compositions of physics-informed random features, derived from ordinary differential equations. The architecture of our model leverages recent advances in approximate inference for deep Gaussian processes, such as layer-wise weight-space approximations which allow us to incorporate random Fourier features, and stochastic variational inference for approximate Bayesian inference. We provide evidence that our model is capable of capturing highly nonlinear behaviour in real-world multivariate time series data. In addition, we find that our approach achieves comparable performance to a number of other probabilistic models on benchmark regression tasks.",
    "authors": [
      "McDonald, Thomas",
      "\u00c1lvarez, Mauricio"
    ]
  },
  {
    "id": "731b03008e834f92a03085ef47061c4a",
    "title": "Implicit Semantic Response Alignment for Partial Domain Adaptation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/731b03008e834f92a03085ef47061c4a-Paper.pdf",
    "abstract": "Partial Domain Adaptation (PDA) addresses the unsupervised domain adaptation problem where the target label space is a subset of the source label space. Most state-of-art PDA methods tackle the inconsistent label space by assigning weights to classes or individual samples, in an attempt to discard the source data that belongs to the irrelevant classes. However, we believe samples from those extra categories would still contain valuable information to promote positive transfer. In this paper, we propose the Implicit Semantic Response Alignment to explore the intrinsic relationships among different categories by applying a weighted schema on the feature level. Specifically, we design a class2vec module to extract the implicit semantic topics from the visual features. With an attention layer, we calculate the semantic response according to each implicit semantic topic. Then semantic responses of source and target data are aligned to retain the relevant information contained in multiple categories by weighting the features, instead of samples. Experiments on several cross-domain benchmark datasets demonstrate the effectiveness of our method over the state-of-the-art PDA methods. Moreover, we elaborate in-depth analyses to further explore implicit semantic alignment. ",
    "authors": [
      "Xiao, Wenxiao",
      "Ding, Zhengming",
      "Liu, Hongfu"
    ]
  },
  {
    "id": "731c83db8d2ff01bdc000083fd3c3740",
    "title": "ToAlign: Task-Oriented Alignment for Unsupervised Domain Adaptation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/731c83db8d2ff01bdc000083fd3c3740-Paper.pdf",
    "abstract": "Unsupervised domain adaptive classifcation intends to improve the classifcation performance on unlabeled target domain. To alleviate the adverse effect of domain shift, many approaches align the source and target domains in the feature space. However, a feature is usually taken as a whole for alignment without explicitly making domain alignment proactively serve the classifcation task, leading to sub-optimal solution. In this paper, we propose an effective Task-oriented Alignment (ToAlign) for unsupervised domain adaptation (UDA). We study what features should be aligned across domains and propose to make the domain alignment proactively serve classifcation by performing feature decomposition and alignment under the guidance of the prior knowledge induced from the classifcation task itself. Particularly, we explicitly decompose a feature in the source domain into a task-related/discriminative feature that should be aligned, and a task-irrelevant feature that should be avoided/ignored, based on the classifcation meta-knowledge. Extensive experimental results on various benchmarks (e.g., Offce-Home, Visda-2017, and DomainNet) under different domain adaptation settings demonstrate the effectiveness of ToAlign which helps achieve the state-of-the-art performance. The code is publicly available at https://github.com/microsoft/UDA.",
    "authors": [
      "Wei, Guoqiang",
      "Lan, Cuiling",
      "Zeng, Wenjun",
      "Zhang, Zhizheng",
      "Chen, Zhibo"
    ]
  },
  {
    "id": "735143e9ff8c47def504f1ba0442df98",
    "title": "Prior-independent Dynamic Auctions for a Value-maximizing Buyer",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/735143e9ff8c47def504f1ba0442df98-Paper.pdf",
    "abstract": "We study prior-independent dynamic auction design with production costs for a value-maximizing buyer, a paradigm that is becoming prevalent recently following the development of automatic bidding algorithms in advertising platforms. In contrast to a utility-maximizing buyer, who maximizes the difference between her total value and total payment, a value-maximizing buyer aims to maximize her total value subject to a return on investment (ROI) constraint. Our main result is a dynamic mechanism with regret $\\tilde{O}(T^{2/3})$, where $T$ is the time horizon, against the first-best benchmark, i.e., the maximum amount of revenue the seller can extract assuming all values of the buyer are publicly known.",
    "authors": [
      "Deng, Yuan",
      "Zhang, Hanrui"
    ]
  },
  {
    "id": "73b277c11266681122132d024f53a75b",
    "title": "Safe Reinforcement Learning by Imagining the Near Future",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/73b277c11266681122132d024f53a75b-Paper.pdf",
    "abstract": "Safe reinforcement learning is a promising path toward applying reinforcement learning algorithms to real-world problems, where suboptimal behaviors may lead to actual negative consequences. In this work, we focus on the setting where unsafe states can be avoided by planning ahead a short time into the future. In this setting, a model-based agent with a sufficiently accurate model can avoid unsafe states.We devise a model-based algorithm that heavily penalizes unsafe trajectories, and derive guarantees that our algorithm can avoid unsafe states under certain assumptions. Experiments demonstrate that our algorithm can achieve competitive rewards with fewer safety violations in several continuous control tasks.",
    "authors": [
      "Thomas, Garrett",
      "Luo, Yuping",
      "Ma, Tengyu"
    ]
  },
  {
    "id": "73c730319cf839f143bf40954448ce39",
    "title": "Contrastive Active Inference",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/73c730319cf839f143bf40954448ce39-Paper.pdf",
    "abstract": "Active inference is a unifying theory for perception and action resting upon the idea that the brain maintains an internal model of the world by minimizing free energy. From a behavioral perspective, active inference agents can be seen as self-evidencing beings that act to fulfill their optimistic predictions, namely preferred outcomes or goals. In contrast, reinforcement learning requires human-designed rewards to accomplish any desired outcome. Although active inference could provide a more natural self-supervised objective for control, its applicability has been limited because of the shortcomings in scaling the approach to complex environments. In this work, we propose a contrastive objective for active inference that strongly reduces the computational burden in learning the agent's generative model and planning future actions. Our method performs notably better than likelihood-based active inference in image-based tasks, while also being computationally cheaper and easier to train. We compare to reinforcement learning agents that have access to human-designed reward functions, showing that our approach closely matches their performance. Finally, we also show that contrastive methods perform significantly better in the case of distractors in the environment and that our method is able to generalize goals to variations in the background.",
    "authors": [
      "Mazzaglia, Pietro",
      "Verbelen, Tim",
      "Dhoedt, Bart"
    ]
  },
  {
    "id": "73fed7fd472e502d8908794430511f4d",
    "title": "Overparameterization Improves Robustness to Covariate Shift in High Dimensions",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/73fed7fd472e502d8908794430511f4d-Paper.pdf",
    "abstract": "A significant obstacle in the development of robust machine learning models is \\emph{covariate shift}, a form of distribution shift that occurs when the input distributions of the training and test sets differ while the conditional label distributions remain the same. Despite the prevalence of covariate shift in real-world applications, a theoretical understanding in the context of modern machine learning has remained lacking. In this work, we examine the exact high-dimensional asymptotics of random feature regression under covariate shift and present a precise characterization of the limiting test error, bias, and variance in this setting. Our results motivate a natural partial order over covariate shifts that provides a sufficient condition for determining when the shift will harm (or even help) test performance. We find that overparameterized models exhibit enhanced robustness to covariate shift, providing one of the first theoretical explanations for this ubiquitous empirical phenomenon. Additionally, our analysis reveals an exact linear relationship between the in-distribution and out-of-distribution generalization performance, offering an explanation for this surprising recent observation.",
    "authors": [
      "Tripuraneni, Nilesh",
      "Adlam, Ben",
      "Pennington, Jeffrey"
    ]
  },
  {
    "id": "742141ceda6b8f6786609d31c8ef129f",
    "title": "Logarithmic Regret in Feature-based Dynamic Pricing",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/742141ceda6b8f6786609d31c8ef129f-Paper.pdf",
    "abstract": "Feature-based dynamic pricing is an increasingly popular model of setting prices for highly differentiated products with applications in digital marketing, online sales, real estate and so on. The problem was formally studied as an online learning problem [Javanmard & Nazerzadeh, 2019] where a seller needs to propose prices on the fly for a sequence of $T$ products based on their features $x$ while having a small regret relative to the best ---\"omniscient\"--- pricing strategy she could have come up with in hindsight. We revisit this problem and provide two algorithms (EMLP and ONSP) for stochastic and adversarial feature settings, respectively, and prove the optimal $O(d\\log{T})$ regret bounds for both. In comparison, the best existing results are $O\\left(\\min\\left\\{\\frac{1}{\\lambda_{\\min}^2}\\log{T}, \\sqrt{T}\\right\\}\\right)$ and $O(T^{2/3})$ respectively, with $\\lambda_{\\min}$ being the smallest eigenvalue of $\\mathbb{E}[xx^T]$ that could be arbitrarily close to $0$.  We also prove an $\\Omega(\\sqrt{T})$ information-theoretic lower bound for a slightly more general setting, which demonstrates that \"knowing-the-demand-curve\" leads to an exponential improvement in feature-based dynamic pricing. ",
    "authors": [
      "Xu, Jianyu",
      "Wang, Yu-Xiang"
    ]
  },
  {
    "id": "74378afe5e8b20910cf1f939e57f0480",
    "title": "Dimension-free empirical entropy estimation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/74378afe5e8b20910cf1f939e57f0480-Paper.pdf",
    "abstract": "We seek an entropy estimator for discrete distributions with fully empirical accuracy bounds. As stated, this goal is infeasible without some prior assumptions on the distribution. We discover that a certain information moment assumption renders the problem feasible. We argue that the moment assumption is natural and, in some sense, {\\em minimalistic} --- weaker than finite support or tail decay conditions. Under the moment assumption, we provide the first finite-sample entropy estimates for infinite alphabets, nearly recovering the known minimax rates. Moreover, we demonstrate that our empirical bounds are significantly sharper than the state-of-the-art bounds, for various natural distributions and non-trivial sample regimes. Along the way, we give a dimension-free analogue of the Cover-Thomas result on entropy continuity (with respect to total variation distance) for finite alphabets, which may be of independent interest.",
    "authors": [
      "Cohen, Doron",
      "Kontorovich, Aryeh",
      "Koolyk, Aaron",
      "Wolfer, Geoffrey"
    ]
  },
  {
    "id": "746b02b6680562f44ad7526675bac026",
    "title": "Towards Biologically Plausible Convolutional Networks",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/746b02b6680562f44ad7526675bac026-Paper.pdf",
    "abstract": "Convolutional networks are ubiquitous in deep learning. They are particularly useful for images, as they reduce the number of parameters, reduce training time, and increase accuracy. However, as a model of the brain they are seriously problematic, since they require weight sharing - something real neurons simply cannot do. Consequently, while neurons in the brain can be locally connected (one of the features of convolutional networks), they cannot be convolutional. Locally connected but non-convolutional networks, however, significantly underperform convolutional ones. This is troublesome for studies that use convolutional networks to explain activity in the visual system. Here we study plausible alternatives to weight sharing that aim at the same regularization principle, which is to make each neuron within a pool react similarly to identical inputs. The most natural way to do that is by showing the network multiple translations of the same image, akin to saccades in animal vision. However, this approach requires many translations, and doesn't remove the performance gap. We propose instead to add lateral connectivity to a locally connected network, and allow learning via Hebbian plasticity. This requires the network to pause occasionally for a sleep-like phase of \"weight sharing\". This method enables locally connected networks to achieve nearly convolutional performance on ImageNet and improves their fit to the ventral stream data, thus supporting convolutional networks as a model of the visual stream.",
    "authors": [
      "Pogodin, Roman",
      "Mehta, Yash",
      "Lillicrap, Timothy",
      "Latham, Peter E"
    ]
  },
  {
    "id": "747d3443e319a22747fbb873e8b2f9f2",
    "title": "DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/747d3443e319a22747fbb873e8b2f9f2-Paper.pdf",
    "abstract": "Attention is sparse in vision transformers. We observe the final prediction in vision transformers is only based on a subset of most informative tokens, which is sufficient for accurate image recognition. Based on this observation, we propose a dynamic token sparsification framework to prune redundant tokens progressively and dynamically based on the input. Specifically, we devise a lightweight prediction module to estimate the importance score of each token given the current features. The module is added to different layers to prune redundant tokens hierarchically. To optimize the prediction module in an end-to-end manner, we propose an attention masking strategy to differentiably prune a token by blocking its interactions with other tokens. Benefiting from the nature of self-attention, the unstructured sparse tokens are still hardware friendly, which makes our framework easy to achieve actual speed-up. By hierarchically pruning 66% of the input tokens, our method greatly reduces 31% $\\sim$ 37%  FLOPs and improves the throughput by over 40% while the drop of accuracy is within 0.5% for various vision transformers. Equipped with the dynamic token sparsification framework,  DynamicViT models can achieve very competitive complexity/accuracy trade-offs compared to state-of-the-art CNNs and vision transformers on ImageNet. Code is available at https://github.com/raoyongming/DynamicViT",
    "authors": [
      "Rao, Yongming",
      "Zhao, Wenliang",
      "Liu, Benlin",
      "Lu, Jiwen",
      "Zhou, Jie",
      "Hsieh, Cho-Jui"
    ]
  },
  {
    "id": "7486cef2522ee03547cfb970a404a874",
    "title": "Learning Transferable Adversarial Perturbations",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/7486cef2522ee03547cfb970a404a874-Paper.pdf",
    "abstract": "While effective, deep neural networks (DNNs) are vulnerable to adversarial attacks. In particular, recent work has shown that such attacks could be generated by another deep network, leading to significant speedups over optimization-based perturbations. However, the ability of such generative methods to generalize to different test-time situations has not been systematically studied. In this paper, we, therefore, investigate the transferability of generated perturbations when the conditions at inference time differ from the training ones in terms of the target architecture, target data, and target task. Specifically, we identify the mid-level features extracted by the intermediate layers of DNNs as common ground across different architectures, datasets, and tasks. This lets us introduce a loss function based on such mid-level features to learn an effective, transferable perturbation generator. Our experiments demonstrate that our approach outperforms the state-of-the-art universal and transferable attack strategies.",
    "authors": [
      "Nakka, Krishna kanth",
      "Salzmann, Mathieu"
    ]
  },
  {
    "id": "748d6b6ed8e13f857ceaa6cfbdca14b8",
    "title": "PortaSpeech: Portable and High-Quality Generative Text-to-Speech",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/748d6b6ed8e13f857ceaa6cfbdca14b8-Paper.pdf",
    "abstract": "Non-autoregressive text-to-speech (NAR-TTS) models such as FastSpeech 2 and Glow-TTS can synthesize high-quality speech from the given text in parallel. After analyzing two kinds of generative NAR-TTS models (VAE and normalizing flow), we find that: VAE is good at capturing the long-range semantics features (e.g., prosody) even with small model size but suffers from blurry and unnatural results; and normalizing flow is good at reconstructing the frequency bin-wise details but performs poorly when the number of model parameters is limited. Inspired by these observations, to generate diverse speech with natural details and rich prosody using a lightweight architecture, we propose PortaSpeech, a portable and high-quality generative text-to-speech model. Specifically, 1) to model both the prosody and mel-spectrogram details accurately, we adopt a lightweight VAE with an enhanced prior followed by a flow-based post-net with strong conditional inputs as the main architecture. 2) To further compress the model size and memory footprint, we introduce the grouped parameter sharing mechanism to the affine coupling layers in the post-net. 3) To improve the expressiveness of synthesized speech and reduce the dependency on accurate fine-grained alignment between text and speech, we propose a linguistic encoder with mixture alignment combining hard word-level alignment and soft phoneme-level alignment, which explicitly extracts word-level semantic information.  Experimental results show that PortaSpeech outperforms other TTS models in both voice quality and prosody modeling in terms of subjective and objective evaluation metrics, and shows only a slight performance degradation when reducing the model parameters to 6.7M (about 4x model size and 3x runtime memory compression ratio compared with FastSpeech 2). Our extensive ablation studies demonstrate that each design in PortaSpeech is effective.",
    "authors": [
      "Ren, Yi",
      "Liu, Jinglin",
      "Zhao, Zhou"
    ]
  },
  {
    "id": "74e1ed8b55ea44fd7dbb685c412568a4",
    "title": "Exponential Graph is Provably Efficient for Decentralized Deep Training",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/74e1ed8b55ea44fd7dbb685c412568a4-Paper.pdf",
    "abstract": "Decentralized SGD is an emerging training method for deep learning known for its much less (thus faster) communication per iteration, which relaxes the averaging step in parallel SGD to inexact averaging. The less exact the averaging is, however, the more the total iterations the training needs to take. Therefore, the key to making decentralized SGD efficient is to realize nearly-exact averaging using little communication. This requires a skillful choice of communication topology, which is an under-studied topic in decentralized optimization.In this paper, we study so-called exponential graphs where every node is connected to $O(\\log(n))$ neighbors and $n$ is the total number of nodes. This work proves such graphs can lead to both fast communication and effective averaging simultaneously. We also discover that a sequence of $\\log(n)$ one-peer exponential graphs, in which each node communicates to one single neighbor per iteration, can together achieve exact averaging. This favorable property enables one-peer exponential graph to average as effective as its static counterpart but communicates more efficiently. We apply these exponential graphs in decentralized (momentum) SGD to obtain the state-of-the-art balance between per-iteration communication and iteration complexity among all commonly-used topologies. Experimental results on a variety of tasks and models demonstrate that decentralized (momentum) SGD over exponential graphs promises both fast and high-quality training. Our code is implemented through BlueFog and available at https://github.com/Bluefog-Lib/NeurIPS2021-Exponential-Graph.",
    "authors": [
      "Ying, Bicheng",
      "Yuan, Kun",
      "Chen, Yiming",
      "Hu, Hanbin",
      "PAN, PAN",
      "Yin, Wotao"
    ]
  },
  {
    "id": "7503cfacd12053d309b6bed5c89de212",
    "title": "CLIP-It! Language-Guided Video Summarization",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/7503cfacd12053d309b6bed5c89de212-Paper.pdf",
    "abstract": "A generic video summary is an abridged version of a video that conveys the whole story and features the most important scenes. Yet the importance of scenes in a video is often subjective, and users should have the option of customizing the summary by using natural language to specify what is important to them. Further, existing models for fully automatic generic summarization have not exploited available language models, which can serve as an effective prior for saliency. This work introduces CLIP-It, a single framework for addressing both generic and query-focused video summarization, typically approached separately in the literature. We propose a language-guided multimodal transformer that learns to score frames in a video based on their importance relative to one another and their correlation with a user-defined query (for query-focused summarization) or an automatically generated dense video caption (for generic video summarization). Our model can be extended to the unsupervised setting by training without ground-truth supervision. We outperform baselines and prior work by a significant margin on both standard video summarization datasets (TVSum and SumMe) and a query-focused video summarization dataset (QFVS). Particularly, we achieve large improvements in the transfer setting, attesting to our method's strong generalization capabilities.",
    "authors": [
      "Narasimhan, Medhini",
      "Rohrbach, Anna",
      "Darrell, Trevor"
    ]
  },
  {
    "id": "7504adad8bb96320eb3afdd4df6e1f60",
    "title": "Learning Treatment Effects in Panels with General Intervention Patterns",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/7504adad8bb96320eb3afdd4df6e1f60-Paper.pdf",
    "abstract": "The problem of causal inference with panel data is a central econometric question. The following is a fundamental version of this problem: Let $M^*$ be a low rank matrix and $E$ be a zero-mean noise matrix. For a `treatment' matrix $Z$ with entries in $\\{0,1\\}$ we observe the matrix $O$ with entries $O_{ij} := M^*_{ij} + E_{ij} + \\mathcal{T}_{ij} Z_{ij}$ where $\\mathcal{T}_{ij} $ are unknown, heterogenous treatment effects. The problem requires we estimate the average treatment effect $\\tau^* := \\sum_{ij} \\mathcal{T}_{ij} Z_{ij} / \\sum_{ij} Z_{ij}$. The synthetic control paradigm provides an approach to estimating $\\tau^*$ when $Z$ places support on a single row. This paper extends that framework to allow rate-optimal recovery of $\\tau^*$ for general $Z$, thus broadly expanding its applicability. Our guarantees are the first of their type in this general setting. Computational experiments on synthetic and real-world data show a substantial advantage over competing estimators. ",
    "authors": [
      "Farias, Vivek",
      "Li, Andrew",
      "Peng, Tianyi"
    ]
  },
  {
    "id": "7535bbb91c8fde347ad861f293126633",
    "title": "Lossy Compression for Lossless Prediction",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/7535bbb91c8fde347ad861f293126633-Paper.pdf",
    "abstract": "Most data is automatically collected and only ever \"seen\" by algorithms. Yet, data compressors preserve perceptual fidelity rather than just the information needed by algorithms performing downstream tasks. In this paper, we characterize the bit-rate required to ensure high performance on all predictive tasks that are invariant under a set of transformations, such as data augmentations. Based on our theory, we design unsupervised objectives for training neural compressors. Using these objectives, we train a generic image compressor that achieves substantial rate savings (more than 1000x on ImageNet) compared to JPEG on 8 datasets, without decreasing downstream classification performance.",
    "authors": [
      "Dubois, Yann",
      "Bloem-Reddy, Benjamin",
      "Ullrich, Karen",
      "Maddison, Chris J."
    ]
  },
  {
    "id": "75429d136f65d2d6168b9b6c5f6ec951",
    "title": "From Optimality to Robustness: Adaptive Re-Sampling Strategies in Stochastic Bandits",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/75429d136f65d2d6168b9b6c5f6ec951-Paper.pdf",
    "abstract": "The stochastic multi-arm bandit problem has been extensively studied under standard assumptions on the arm's distribution (e.g bounded with known support, exponential family, etc). These assumptions are suitable for many real-world problems but sometimes they require knowledge (on tails for instance) that may not be precisely accessible to the practitioner, raising the question of the robustness of bandit algorithms to model misspecification. In this paper we study a generic \\emph{Dirichlet Sampling} (DS) algorithm, based on pairwise comparisons of empirical indices computed with \\textit{re-sampling} of the arms' observations and a data-dependent \\textit{exploration bonus}. We show that different variants of this strategy achieve provably optimal regret guarantees when the distributions are bounded and logarithmic regret for semi-bounded distributions with a mild quantile condition. We also show that a  simple tuning achieve robustness with respect to a large class of unbounded distributions, at the cost of slightly worse than logarithmic asymptotic regret. We finally provide numerical experiments showing the merits of DS in a decision-making problem on synthetic agriculture data.",
    "authors": [
      "Baudry, Dorian",
      "Saux, Patrick",
      "Maillard, Odalric-Ambrym"
    ]
  },
  {
    "id": "757b505cfd34c64c85ca5b5690ee5293",
    "title": "CCVS: Context-aware Controllable Video Synthesis",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/757b505cfd34c64c85ca5b5690ee5293-Paper.pdf",
    "abstract": "This presentation introduces a self-supervised learning approach to the synthesis of new videos clips from old ones, with several new key elements for improved spatial resolution and realism: It conditions the synthesis process on contextual information for temporal continuity and ancillary information for fine control. The prediction model is doubly autoregressive, in the latent space of an autoencoder for forecasting, and in image space for updating contextual information, which is also used to enforce spatio-temporal consistency through a learnable optical flow module. Adversarial training of the autoencoder in the appearance and temporal domains is used to further improve the realism of its output. A quantizer inserted between the encoder and the transformer in charge of forecasting future frames in latent space (and its inverse inserted between the transformer and the decoder) adds even more flexibility by affording simple mechanisms for handling multimodal ancillary information for controlling the synthesis process (e.g., a few sample frames, an audio track, a trajectory in image space) and taking into account the intrinsically uncertain nature of the future by allowing multiple predictions. Experiments with an implementation of the proposed approach give very good qualitative and quantitative results on multiple tasks and standard benchmarks.",
    "authors": [
      "Le Moing, Guillaume",
      "Ponce, Jean",
      "Schmid, Cordelia"
    ]
  },
  {
    "id": "758a06618c69880a6cee5314ee42d52f",
    "title": "An Online Riemannian PCA for Stochastic Canonical Correlation Analysis",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/758a06618c69880a6cee5314ee42d52f-Paper.pdf",
    "abstract": "We present an efficient stochastic algorithm (RSG+) for canonical correlation analysis (CCA) using a reparametrization of the projection matrices. We show how this reparametrization (into structured matrices), simple in hindsight, directly presents an opportunity to repurpose/adjust mature techniques for numerical optimization on Riemannian manifolds. Our developments nicely complement existing methods for this problem which either require $O(d^3)$ time complexity per iteration with $O(\\frac{1}{\\sqrt{t}})$ convergence rate (where $d$ is the dimensionality) or only extract the top $1$ component with $O(\\frac{1}{t})$ convergence rate. In contrast, our algorithm offers a strict improvement for this classical problem: it achieves $O(d^2k)$ runtime complexity per iteration for extracting the top $k$ canonical components with $O(\\frac{1}{t})$ convergence rate. While the paper primarily focuses on the formulation and technical analysis of its properties, our experiments show that the empirical behavior on common datasets is quite promising, We also explore a potential application in training fair models where the label of protected attribute is missing or otherwise unavailable. ",
    "authors": [
      "Meng, Zihang",
      "Chakraborty, Rudrasis",
      "Singh, Vikas"
    ]
  },
  {
    "id": "75c58d36157505a600e0695ed0b3a22d",
    "title": "Predify: Augmenting deep neural networks with brain-inspired predictive coding dynamics",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/75c58d36157505a600e0695ed0b3a22d-Paper.pdf",
    "abstract": "Deep neural networks excel at image classification, but their performance is far less robust to input perturbations than human perception. In this work we explore whether this shortcoming may be partly addressed by incorporating brain-inspired recurrent dynamics in deep convolutional networks. We take inspiration from a popular framework in neuroscience: \"predictive coding\". At each layer of the hierarchical model, generative feedback \"predicts\" (i.e., reconstructs) the pattern of activity in the previous layer. The reconstruction errors are used to iteratively update the network\u2019s representations across timesteps, and to optimize the network's feedback weights over the natural image dataset--a form of unsupervised training. We show that implementing this strategy into two popular networks, VGG16 and EfficientNetB0, improves their robustness against various corruptions and adversarial attacks. We hypothesize that other feedforward networks could similarly benefit from the proposed framework. To promote research in this direction, we provide an open-sourced PyTorch-based package called \\textit{Predify}, which can be used to implement and investigate the impacts of the predictive coding dynamics in any convolutional neural network. ",
    "authors": [
      "Choksi, Bhavin",
      "Mozafari, Milad",
      "Biggs O'May, Callum",
      "ADOR, B.",
      "Alamia, Andrea",
      "VanRullen, Rufin"
    ]
  },
  {
    "id": "75da5036f659fe64b53f3d9b39412967",
    "title": "Deep Extrapolation for Attribute-Enhanced Generation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/75da5036f659fe64b53f3d9b39412967-Paper.pdf",
    "abstract": "Attribute extrapolation in sample generation is challenging for deep neural networks operating beyond the training distribution. We formulate a new task for extrapolation in sequence generation, focusing on natural language and proteins, and propose GENhance, a generative framework that enhances attributes through a learned latent space. Trained on movie reviews and a computed protein stability dataset, GENhance can generate strongly-positive text reviews and highly stable protein sequences without being exposed to similar data during training. We release our benchmark tasks and models to contribute to the study of generative modeling extrapolation and data-driven design in biology and chemistry.",
    "authors": [
      "Chan, Alvin",
      "Madani, Ali",
      "Krause, Ben",
      "Naik, Nikhil"
    ]
  },
  {
    "id": "75ebb02f92fc30a8040bbd625af999f1",
    "title": "Generalized DataWeighting via Class-Level Gradient Manipulation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/75ebb02f92fc30a8040bbd625af999f1-Paper.pdf",
    "abstract": "Label noise and class imbalance are two major issues coexisting in real-world datasets. To alleviate the two issues, state-of-the-art methods reweight each instance by leveraging a small amount of clean and unbiased data. Yet, these methods overlook class-level information within each instance, which can be further utilized to improve performance. To this end, in this paper, we propose Generalized Data Weighting (GDW) to simultaneously mitigate label noise and class imbalance by manipulating gradients at the class level. To be specific, GDW unrolls the loss gradient to class-level gradients by the chain rule and reweights the flow of each gradient separately. In this way, GDW achieves remarkable performance improvement on both issues. Aside from the performance gain, GDW efficiently obtains class-level weights without introducing any extra computational cost compared with instance weighting methods. Specifically, GDW performs a gradient descent step on class-level weights, which only relies on intermediate gradients. Extensive experiments in various settings verify the effectiveness of GDW. For example, GDW outperforms state-of-the-art methods by $2.56\\%$ under the $60\\%$ uniform noise setting in CIFAR10. Our code is available at https://github.com/GGchen1997/GDW-NIPS2021.",
    "authors": [
      "Chen, Can",
      "Zheng, Shuhao",
      "Chen, Xi",
      "Dong, Erqun",
      "Liu, Xue (Steve)",
      "Liu, Hao",
      "Dou, Dejing"
    ]
  },
  {
    "id": "75fc093c0ee742f6dddaa13fff98f104",
    "title": "Slow Learning and Fast Inference: Efficient Graph Similarity Computation via Knowledge Distillation",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/75fc093c0ee742f6dddaa13fff98f104-Paper.pdf",
    "abstract": "Graph Similarity Computation (GSC) is essential to wide-ranging graph applications such as retrieval, plagiarism/anomaly detection, etc. The exact computation of graph similarity, e.g., Graph Edit Distance (GED), is an NP-hard problem that cannot be exactly solved within an adequate time given large graphs. Thanks to the strong representation power of graph neural network (GNN), a variety of GNN-based inexact methods emerged. To capture the subtle difference across graphs, the key success is designing the dense interaction with features fusion at the early stage, which, however, is a trade-off between speed and accuracy. For slow learning of graph similarity, this paper proposes a novel early-fusion approach by designing a co-attention-based feature fusion network on multilevel GNN features. To further improve the speed without much accuracy drop, we introduce an efficient GSC solution by distilling the knowledge from the slow early-fusion model to the student one for fast inference. Such a student model also enables the offline collection of individual graph embeddings, speeding up the inference time in orders. To address the instability through knowledge transfer, we decompose the dynamic joint embedding into the static pseudo individual ones for precise teacher-student alignment. The experimental analysis on the real-world datasets demonstrates the superiority of our approach over the state-of-the-art methods on both accuracy and efficiency. Particularly, we speed up the prior art by more than 10x on the benchmark AIDS data.",
    "authors": [
      "Qin, Can",
      "Zhao, Handong",
      "Wang, Lichen",
      "Wang, Huan",
      "Zhang, Yulun",
      "Fu, Yun"
    ]
  },
  {
    "id": "7608de7a475c0c878f60960d72a92654",
    "title": "Meta Learning Backpropagation And Improving It",
    "year": "2021-12-06",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/7608de7a475c0c878f60960d72a92654-Paper.pdf",
    "abstract": "Many concepts have been proposed for meta learning with neural networks (NNs), e.g., NNs that learn to reprogram fast weights, Hebbian plasticity, learned learning rules, and meta recurrent NNs. Our Variable Shared Meta Learning (VSML) unifies the above and demonstrates that simple weight-sharing and sparsity in an NN is sufficient to express powerful learning algorithms (LAs) in a reusable fashion. A simple implementation of VSML where the weights of a neural network are replaced by tiny LSTMs allows for implementing the backpropagation LA solely by running in forward-mode. It can even meta learn new LAs that differ from online backpropagation and generalize to datasets outside of the meta training distribution without explicit gradient calculation. Introspection reveals that our meta learned LAs learn through fast association in a way that is qualitatively different from gradient descent.",
    "authors": [
      "Kirsch, Louis",
      "Schmidhuber, J\u00fcrgen"
    ]
  }
]